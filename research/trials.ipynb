{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e92a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/akashsharma/PycharmProjects/chatbot\\/Chabot_data/research\n"
     ]
    }
   ],
   "source": [
    "#Now since i am in inside research directory and my data and everything is inside chatbot directory so i need to \n",
    "#go one step back cd ..\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c112e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trick for going one step back in jupyter notebook\n",
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bd5e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/akashsharma/PycharmProjects/chatbot\\/Chabot_data\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "#now i will be inside chatbot_Data which is what i want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40b6d71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader, PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter,RecursiveCharacterTextSplitter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87837739",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting text from pdf file\n",
    "def extract_text_from_pdf(data):\n",
    "    loader = DirectoryLoader(\n",
    "        data, \n",
    "        glob=\"*.pdf\", \n",
    "        loader_cls=PyPDFLoader\n",
    "    )\n",
    "\n",
    "    documents = loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "070d3888",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 3 0 (offset 0)\n",
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 15 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 32 0 (offset 0)\n",
      "Ignoring wrong pointing object 35 0 (offset 0)\n",
      "Ignoring wrong pointing object 42 0 (offset 0)\n",
      "Ignoring wrong pointing object 51 0 (offset 0)\n",
      "Ignoring wrong pointing object 54 0 (offset 0)\n",
      "Ignoring wrong pointing object 57 0 (offset 0)\n",
      "Ignoring wrong pointing object 60 0 (offset 0)\n",
      "Ignoring wrong pointing object 69 0 (offset 0)\n",
      "Ignoring wrong pointing object 80 0 (offset 0)\n",
      "Ignoring wrong pointing object 92 0 (offset 0)\n",
      "Ignoring wrong pointing object 97 0 (offset 0)\n",
      "Ignoring wrong pointing object 102 0 (offset 0)\n",
      "Ignoring wrong pointing object 119 0 (offset 0)\n",
      "Ignoring wrong pointing object 128 0 (offset 0)\n",
      "Ignoring wrong pointing object 135 0 (offset 0)\n",
      "Ignoring wrong pointing object 138 0 (offset 0)\n",
      "Ignoring wrong pointing object 142 0 (offset 0)\n",
      "Ignoring wrong pointing object 149 0 (offset 0)\n",
      "Ignoring wrong pointing object 154 0 (offset 0)\n",
      "Ignoring wrong pointing object 170 0 (offset 0)\n",
      "Ignoring wrong pointing object 189 0 (offset 0)\n",
      "Ignoring wrong pointing object 200 0 (offset 0)\n",
      "Ignoring wrong pointing object 215 0 (offset 0)\n",
      "Ignoring wrong pointing object 231 0 (offset 0)\n",
      "Ignoring wrong pointing object 234 0 (offset 0)\n",
      "Ignoring wrong pointing object 239 0 (offset 0)\n",
      "Ignoring wrong pointing object 242 0 (offset 0)\n",
      "Ignoring wrong pointing object 249 0 (offset 0)\n",
      "Ignoring wrong pointing object 253 0 (offset 0)\n",
      "Ignoring wrong pointing object 258 0 (offset 0)\n",
      "Ignoring wrong pointing object 274 0 (offset 0)\n",
      "Ignoring wrong pointing object 277 0 (offset 0)\n",
      "Ignoring wrong pointing object 281 0 (offset 0)\n",
      "Ignoring wrong pointing object 286 0 (offset 0)\n",
      "Ignoring wrong pointing object 291 0 (offset 0)\n",
      "Ignoring wrong pointing object 294 0 (offset 0)\n",
      "Ignoring wrong pointing object 297 0 (offset 0)\n",
      "Ignoring wrong pointing object 302 0 (offset 0)\n",
      "Ignoring wrong pointing object 305 0 (offset 0)\n",
      "Ignoring wrong pointing object 309 0 (offset 0)\n",
      "Ignoring wrong pointing object 332 0 (offset 0)\n",
      "Ignoring wrong pointing object 337 0 (offset 0)\n",
      "Ignoring wrong pointing object 342 0 (offset 0)\n",
      "Ignoring wrong pointing object 345 0 (offset 0)\n",
      "Ignoring wrong pointing object 354 0 (offset 0)\n",
      "Ignoring wrong pointing object 361 0 (offset 0)\n",
      "Ignoring wrong pointing object 370 0 (offset 0)\n",
      "Ignoring wrong pointing object 375 0 (offset 0)\n",
      "Ignoring wrong pointing object 378 0 (offset 0)\n",
      "Ignoring wrong pointing object 387 0 (offset 0)\n",
      "Ignoring wrong pointing object 396 0 (offset 0)\n",
      "Ignoring wrong pointing object 401 0 (offset 0)\n",
      "Ignoring wrong pointing object 416 0 (offset 0)\n",
      "Ignoring wrong pointing object 423 0 (offset 0)\n",
      "Ignoring wrong pointing object 436 0 (offset 0)\n",
      "Ignoring wrong pointing object 441 0 (offset 0)\n",
      "Ignoring wrong pointing object 448 0 (offset 0)\n",
      "Ignoring wrong pointing object 457 0 (offset 0)\n",
      "Ignoring wrong pointing object 464 0 (offset 0)\n",
      "Ignoring wrong pointing object 475 0 (offset 0)\n",
      "Ignoring wrong pointing object 488 0 (offset 0)\n",
      "Ignoring wrong pointing object 529 0 (offset 0)\n",
      "Ignoring wrong pointing object 540 0 (offset 0)\n",
      "Ignoring wrong pointing object 549 0 (offset 0)\n",
      "Ignoring wrong pointing object 557 0 (offset 0)\n",
      "Ignoring wrong pointing object 564 0 (offset 0)\n",
      "Ignoring wrong pointing object 571 0 (offset 0)\n",
      "Ignoring wrong pointing object 575 0 (offset 0)\n",
      "Ignoring wrong pointing object 582 0 (offset 0)\n",
      "Ignoring wrong pointing object 587 0 (offset 0)\n",
      "Ignoring wrong pointing object 591 0 (offset 0)\n",
      "Ignoring wrong pointing object 594 0 (offset 0)\n",
      "Ignoring wrong pointing object 603 0 (offset 0)\n",
      "Ignoring wrong pointing object 607 0 (offset 0)\n",
      "Ignoring wrong pointing object 611 0 (offset 0)\n",
      "Ignoring wrong pointing object 614 0 (offset 0)\n",
      "Ignoring wrong pointing object 618 0 (offset 0)\n",
      "Ignoring wrong pointing object 623 0 (offset 0)\n",
      "Ignoring wrong pointing object 627 0 (offset 0)\n",
      "Ignoring wrong pointing object 631 0 (offset 0)\n",
      "Ignoring wrong pointing object 638 0 (offset 0)\n",
      "Ignoring wrong pointing object 647 0 (offset 0)\n",
      "Ignoring wrong pointing object 651 0 (offset 0)\n",
      "Ignoring wrong pointing object 655 0 (offset 0)\n",
      "Ignoring wrong pointing object 659 0 (offset 0)\n",
      "Ignoring wrong pointing object 669 0 (offset 0)\n",
      "Ignoring wrong pointing object 684 0 (offset 0)\n",
      "Ignoring wrong pointing object 689 0 (offset 0)\n",
      "Ignoring wrong pointing object 692 0 (offset 0)\n",
      "Ignoring wrong pointing object 701 0 (offset 0)\n",
      "Ignoring wrong pointing object 706 0 (offset 0)\n",
      "Ignoring wrong pointing object 725 0 (offset 0)\n",
      "Ignoring wrong pointing object 728 0 (offset 0)\n",
      "Ignoring wrong pointing object 738 0 (offset 0)\n",
      "Ignoring wrong pointing object 746 0 (offset 0)\n",
      "Ignoring wrong pointing object 749 0 (offset 0)\n",
      "Ignoring wrong pointing object 752 0 (offset 0)\n",
      "Ignoring wrong pointing object 760 0 (offset 0)\n",
      "Ignoring wrong pointing object 763 0 (offset 0)\n",
      "Ignoring wrong pointing object 780 0 (offset 0)\n",
      "Ignoring wrong pointing object 799 0 (offset 0)\n",
      "Ignoring wrong pointing object 803 0 (offset 0)\n",
      "Ignoring wrong pointing object 818 0 (offset 0)\n",
      "Ignoring wrong pointing object 827 0 (offset 0)\n",
      "Ignoring wrong pointing object 832 0 (offset 0)\n",
      "Ignoring wrong pointing object 837 0 (offset 0)\n",
      "Ignoring wrong pointing object 845 0 (offset 0)\n",
      "Ignoring wrong pointing object 848 0 (offset 0)\n",
      "Ignoring wrong pointing object 853 0 (offset 0)\n",
      "Ignoring wrong pointing object 878 0 (offset 0)\n",
      "Ignoring wrong pointing object 887 0 (offset 0)\n",
      "Ignoring wrong pointing object 897 0 (offset 0)\n",
      "Ignoring wrong pointing object 900 0 (offset 0)\n",
      "Ignoring wrong pointing object 907 0 (offset 0)\n",
      "Ignoring wrong pointing object 914 0 (offset 0)\n",
      "Ignoring wrong pointing object 919 0 (offset 0)\n",
      "Ignoring wrong pointing object 927 0 (offset 0)\n",
      "Ignoring wrong pointing object 935 0 (offset 0)\n",
      "Ignoring wrong pointing object 941 0 (offset 0)\n",
      "Ignoring wrong pointing object 946 0 (offset 0)\n",
      "Ignoring wrong pointing object 952 0 (offset 0)\n",
      "Ignoring wrong pointing object 956 0 (offset 0)\n",
      "Ignoring wrong pointing object 960 0 (offset 0)\n",
      "Ignoring wrong pointing object 968 0 (offset 0)\n",
      "Ignoring wrong pointing object 985 0 (offset 0)\n",
      "Ignoring wrong pointing object 992 0 (offset 0)\n",
      "Ignoring wrong pointing object 997 0 (offset 0)\n",
      "Ignoring wrong pointing object 1000 0 (offset 0)\n",
      "Ignoring wrong pointing object 1007 0 (offset 0)\n",
      "Ignoring wrong pointing object 1010 0 (offset 0)\n",
      "Ignoring wrong pointing object 1018 0 (offset 0)\n",
      "Ignoring wrong pointing object 1027 0 (offset 0)\n",
      "Ignoring wrong pointing object 1032 0 (offset 0)\n",
      "Ignoring wrong pointing object 1039 0 (offset 0)\n",
      "Ignoring wrong pointing object 1044 0 (offset 0)\n",
      "Ignoring wrong pointing object 1068 0 (offset 0)\n",
      "Ignoring wrong pointing object 1081 0 (offset 0)\n",
      "Ignoring wrong pointing object 1108 0 (offset 0)\n",
      "Ignoring wrong pointing object 1111 0 (offset 0)\n",
      "Ignoring wrong pointing object 1116 0 (offset 0)\n",
      "Ignoring wrong pointing object 1119 0 (offset 0)\n",
      "Ignoring wrong pointing object 1120 0 (offset 0)\n",
      "Ignoring wrong pointing object 1123 0 (offset 0)\n",
      "Ignoring wrong pointing object 1124 0 (offset 0)\n",
      "Ignoring wrong pointing object 1127 0 (offset 0)\n",
      "Ignoring wrong pointing object 1128 0 (offset 0)\n",
      "Ignoring wrong pointing object 1131 0 (offset 0)\n",
      "Ignoring wrong pointing object 1132 0 (offset 0)\n",
      "Ignoring wrong pointing object 1135 0 (offset 0)\n",
      "Ignoring wrong pointing object 1136 0 (offset 0)\n",
      "Ignoring wrong pointing object 1139 0 (offset 0)\n",
      "Ignoring wrong pointing object 1140 0 (offset 0)\n",
      "Ignoring wrong pointing object 1143 0 (offset 0)\n",
      "Ignoring wrong pointing object 1144 0 (offset 0)\n",
      "Ignoring wrong pointing object 1147 0 (offset 0)\n",
      "Ignoring wrong pointing object 1148 0 (offset 0)\n",
      "Ignoring wrong pointing object 1151 0 (offset 0)\n",
      "Ignoring wrong pointing object 1152 0 (offset 0)\n",
      "Ignoring wrong pointing object 1155 0 (offset 0)\n",
      "Ignoring wrong pointing object 1156 0 (offset 0)\n",
      "Ignoring wrong pointing object 1159 0 (offset 0)\n",
      "Ignoring wrong pointing object 1160 0 (offset 0)\n",
      "Ignoring wrong pointing object 1163 0 (offset 0)\n",
      "Ignoring wrong pointing object 1164 0 (offset 0)\n",
      "Ignoring wrong pointing object 1167 0 (offset 0)\n",
      "Ignoring wrong pointing object 1168 0 (offset 0)\n",
      "Ignoring wrong pointing object 1171 0 (offset 0)\n",
      "Ignoring wrong pointing object 1172 0 (offset 0)\n",
      "Ignoring wrong pointing object 1175 0 (offset 0)\n",
      "Ignoring wrong pointing object 1176 0 (offset 0)\n",
      "Ignoring wrong pointing object 1179 0 (offset 0)\n",
      "Ignoring wrong pointing object 1180 0 (offset 0)\n",
      "Ignoring wrong pointing object 1183 0 (offset 0)\n",
      "Ignoring wrong pointing object 1184 0 (offset 0)\n",
      "Ignoring wrong pointing object 1187 0 (offset 0)\n",
      "Ignoring wrong pointing object 1188 0 (offset 0)\n",
      "Ignoring wrong pointing object 1191 0 (offset 0)\n",
      "Ignoring wrong pointing object 1197 0 (offset 0)\n",
      "Ignoring wrong pointing object 1226 0 (offset 0)\n",
      "Ignoring wrong pointing object 1227 0 (offset 0)\n",
      "Ignoring wrong pointing object 1228 0 (offset 0)\n",
      "Ignoring wrong pointing object 1229 0 (offset 0)\n",
      "Ignoring wrong pointing object 1230 0 (offset 0)\n",
      "Ignoring wrong pointing object 1231 0 (offset 0)\n",
      "Ignoring wrong pointing object 1232 0 (offset 0)\n",
      "Ignoring wrong pointing object 1233 0 (offset 0)\n",
      "Ignoring wrong pointing object 1234 0 (offset 0)\n",
      "Ignoring wrong pointing object 1235 0 (offset 0)\n",
      "Ignoring wrong pointing object 1236 0 (offset 0)\n",
      "Ignoring wrong pointing object 1237 0 (offset 0)\n",
      "Ignoring wrong pointing object 1238 0 (offset 0)\n",
      "Ignoring wrong pointing object 1239 0 (offset 0)\n",
      "Ignoring wrong pointing object 1240 0 (offset 0)\n",
      "Ignoring wrong pointing object 1241 0 (offset 0)\n",
      "Ignoring wrong pointing object 1242 0 (offset 0)\n",
      "Ignoring wrong pointing object 1243 0 (offset 0)\n",
      "Ignoring wrong pointing object 1244 0 (offset 0)\n",
      "Ignoring wrong pointing object 1245 0 (offset 0)\n",
      "Ignoring wrong pointing object 1246 0 (offset 0)\n",
      "Ignoring wrong pointing object 1247 0 (offset 0)\n",
      "Ignoring wrong pointing object 1248 0 (offset 0)\n",
      "Ignoring wrong pointing object 1249 0 (offset 0)\n",
      "Ignoring wrong pointing object 1250 0 (offset 0)\n",
      "Ignoring wrong pointing object 1251 0 (offset 0)\n",
      "Ignoring wrong pointing object 1252 0 (offset 0)\n",
      "Ignoring wrong pointing object 1253 0 (offset 0)\n",
      "Ignoring wrong pointing object 1254 0 (offset 0)\n",
      "Ignoring wrong pointing object 1255 0 (offset 0)\n",
      "Ignoring wrong pointing object 1256 0 (offset 0)\n",
      "Ignoring wrong pointing object 1257 0 (offset 0)\n",
      "Ignoring wrong pointing object 1258 0 (offset 0)\n",
      "Ignoring wrong pointing object 1259 0 (offset 0)\n",
      "Ignoring wrong pointing object 1260 0 (offset 0)\n",
      "Ignoring wrong pointing object 1261 0 (offset 0)\n",
      "Ignoring wrong pointing object 1262 0 (offset 0)\n",
      "Ignoring wrong pointing object 1263 0 (offset 0)\n",
      "Ignoring wrong pointing object 1264 0 (offset 0)\n",
      "Ignoring wrong pointing object 1265 0 (offset 0)\n",
      "Ignoring wrong pointing object 1266 0 (offset 0)\n",
      "Ignoring wrong pointing object 1267 0 (offset 0)\n",
      "Ignoring wrong pointing object 1268 0 (offset 0)\n",
      "Ignoring wrong pointing object 1269 0 (offset 0)\n",
      "Ignoring wrong pointing object 1270 0 (offset 0)\n",
      "Ignoring wrong pointing object 1271 0 (offset 0)\n",
      "Ignoring wrong pointing object 1272 0 (offset 0)\n",
      "Ignoring wrong pointing object 1273 0 (offset 0)\n",
      "Ignoring wrong pointing object 1274 0 (offset 0)\n",
      "Ignoring wrong pointing object 1275 0 (offset 0)\n",
      "Ignoring wrong pointing object 1276 0 (offset 0)\n",
      "Ignoring wrong pointing object 1277 0 (offset 0)\n",
      "Ignoring wrong pointing object 1278 0 (offset 0)\n",
      "Ignoring wrong pointing object 1280 0 (offset 0)\n",
      "Ignoring wrong pointing object 1281 0 (offset 0)\n",
      "Ignoring wrong pointing object 1282 0 (offset 0)\n",
      "Ignoring wrong pointing object 10007 0 (offset 0)\n",
      "Ignoring wrong pointing object 10011 0 (offset 0)\n",
      "Ignoring wrong pointing object 10020 0 (offset 0)\n",
      "Ignoring wrong pointing object 10021 0 (offset 0)\n",
      "Ignoring wrong pointing object 10022 0 (offset 0)\n",
      "Ignoring wrong pointing object 10023 0 (offset 0)\n",
      "Ignoring wrong pointing object 10024 0 (offset 0)\n",
      "Ignoring wrong pointing object 10025 0 (offset 0)\n",
      "Ignoring wrong pointing object 10026 0 (offset 0)\n",
      "Ignoring wrong pointing object 10027 0 (offset 0)\n",
      "Ignoring wrong pointing object 10028 0 (offset 0)\n",
      "Ignoring wrong pointing object 10762 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "extract_text=extract_text_from_pdf(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2ce378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "470"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is lenght of pages of pdf file extracted\n",
    "len(extract_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64136998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 0, 'page_label': 'Cover'}, page_content='www.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 1, 'page_label': 'BackCover'}, page_content='www.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 2, 'page_label': '2'}, page_content='Python for Data Analysis\\nWes McKinney\\nBeijing • Cambridge • Farnham • Köln • Sebastopol • Tokyo\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 3, 'page_label': '3'}, page_content='Python for Data Analysis\\nby Wes McKinney\\nCopyright © 2013 Wes McKinney. All rights reserved.\\nPrinted in the United States of America.\\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions\\nare also available for most titles ( http://my.safaribooksonline.com). For more information, contact our\\ncorporate/institutional sales department: 800-998-9938 or corporate@oreilly.com.\\nEditors: Julie Steele and Meghan Blanchette\\nProduction Editor: Melanie Yarbrough\\nCopyeditor: Teresa Exley\\nProofreader: BIM Publishing Services\\nIndexer: BIM Publishing Services\\nCover Designer: Karen Montgomery\\nInterior Designer: David Futato\\nIllustrator: Rebecca Demarest\\nOctober 2012: First Edition. \\nRevision History for the First Edition:\\n2012-10-05 First release\\nSee http://oreilly.com/catalog/errata.csp?isbn=9781449319793 for release details.\\nNutshell Handbook, the Nutshell Handbook logo, and the O’Reilly logo are registered trademarks of\\nO’Reilly Media, Inc. Python for Data Analysis, the cover image of a golden-tailed tree shrew, and related\\ntrade dress are trademarks of O’Reilly Media, Inc.\\nMany of the designations used by manufacturers and sellers to distinguish their products are claimed as\\ntrademarks. Where those designations appear in this book, and O’Reilly Media, Inc., was aware of a\\ntrademark claim, the designations have been printed in caps or initial caps.\\nWhile every precaution has been taken in the preparation of this book, the publisher and author assume\\nno responsibility for errors or omissions, or for damages resulting from the use of the information con-\\ntained herein.\\nISBN: 978-1-449-31979-3\\n[LSI]\\n1349356084\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 4, 'page_label': 'iii'}, page_content='Table of Contents\\nPreface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xi\\n1. Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1\\nWhat Is This Book About? 1\\nWhy Python for Data Analysis? 2\\nPython as Glue 2\\nSolving the “Two-Language” Problem 2\\nWhy Not Python? 3\\nEssential Python Libraries 3\\nNumPy 4\\npandas 4\\nmatplotlib 5\\nIPython 5\\nSciPy 6\\nInstallation and Setup 6\\nWindows 7\\nApple OS X 9\\nGNU/Linux 10\\nPython 2 and Python 3 11\\nIntegrated Development Environments (IDEs) 11\\nCommunity and Conferences 12\\nNavigating This Book 12\\nCode Examples 13\\nData for Examples 13\\nImport Conventions 13\\nJargon 13\\nAcknowledgements 14\\n2. Introductory Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  17\\n1.usa.gov data from bit.ly 17\\nCounting Time Zones in Pure Python 19\\niii\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 5, 'page_label': 'iv'}, page_content='Counting Time Zones with pandas 21\\nMovieLens 1M Data Set 26\\nMeasuring rating disagreement 30\\nUS Baby Names 1880-2010 32\\nAnalyzing Naming Trends 36\\nConclusions and The Path Ahead 43\\n3. IPython: An Interactive Computing and Development Environment . . . . . . . . . . . .  45\\nIPython Basics 46\\nTab Completion 47\\nIntrospection 48\\nThe %run Command 49\\nExecuting Code from the Clipboard 50\\nKeyboard Shortcuts 52\\nExceptions and Tracebacks 53\\nMagic Commands 54\\nQt-based Rich GUI Console 55\\nMatplotlib Integration and Pylab Mode 56\\nUsing the Command History 58\\nSearching and Reusing the Command History 58\\nInput and Output Variables 58\\nLogging the Input and Output 59\\nInteracting with the Operating System 60\\nShell Commands and Aliases 60\\nDirectory Bookmark System 62\\nSoftware Development Tools 62\\nInteractive Debugger 62\\nTiming Code: %time and %timeit 67\\nBasic Profiling: %prun and %run -p 68\\nProfiling a Function Line-by-Line 70\\nIPython HTML Notebook 72\\nTips for Productive Code Development Using IPython 72\\nReloading Module Dependencies 74\\nCode Design Tips 74\\nAdvanced IPython Features 76\\nMaking Your Own Classes IPython-friendly 76\\nProfiles and Configuration 77\\nCredits 78\\n4. NumPy Basics: Arrays and Vectorized Computation . . . . . . . . . . . . . . . . . . . . . . . . . .  79\\nThe NumPy ndarray: A Multidimensional Array Object 80\\nCreating ndarrays 81\\nData Types for ndarrays 83\\niv | Table of Contents\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 6, 'page_label': 'v'}, page_content='Operations between Arrays and Scalars 85\\nBasic Indexing and Slicing 86\\nBoolean Indexing 89\\nFancy Indexing 92\\nTransposing Arrays and Swapping Axes 93\\nUniversal Functions: Fast Element-wise Array Functions 95\\nData Processing Using Arrays 97\\nExpressing Conditional Logic as Array Operations 98\\nMathematical and Statistical Methods 100\\nMethods for Boolean Arrays 101\\nSorting 101\\nUnique and Other Set Logic 102\\nFile Input and Output with Arrays 103\\nStoring Arrays on Disk in Binary Format 103\\nSaving and Loading Text Files 104\\nLinear Algebra 105\\nRandom Number Generation 106\\nExample: Random Walks 108\\nSimulating Many Random Walks at Once 109\\n5. Getting Started with pandas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  111\\nIntroduction to pandas Data Structures 112\\nSeries 112\\nDataFrame 115\\nIndex Objects 120\\nEssential Functionality 122\\nReindexing 122\\nDropping entries from an axis 125\\nIndexing, selection, and filtering 125\\nArithmetic and data alignment 128\\nFunction application and mapping 132\\nSorting and ranking 133\\nAxis indexes with duplicate values 136\\nSummarizing and Computing Descriptive Statistics 137\\nCorrelation and Covariance 139\\nUnique Values, Value Counts, and Membership 141\\nHandling Missing Data 142\\nFiltering Out Missing Data 143\\nFilling in Missing Data 145\\nHierarchical Indexing 147\\nReordering and Sorting Levels 149\\nSummary Statistics by Level 150\\nUsing a DataFrame’s Columns 150\\nTable of Contents | v\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 7, 'page_label': 'vi'}, page_content='Other pandas Topics 151\\nInteger Indexing 151\\nPanel Data 152\\n6. Data Loading, Storage, and File Formats . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  155\\nReading and Writing Data in Text Format 155\\nReading Text Files in Pieces 160\\nWriting Data Out to Text Format 162\\nManually Working with Delimited Formats 163\\nJSON Data 165\\nXML and HTML: Web Scraping 166\\nBinary Data Formats 171\\nUsing HDF5 Format 171\\nReading Microsoft Excel Files 172\\nInteracting with HTML and Web APIs 173\\nInteracting with Databases 174\\nStoring and Loading Data in MongoDB 176\\n7. Data Wrangling: Clean, Transform, Merge, Reshape . . . . . . . . . . . . . . . . . . . . . . . .  177\\nCombining and Merging Data Sets 177\\nDatabase-style DataFrame Merges 178\\nMerging on Index 182\\nConcatenating Along an Axis 185\\nCombining Data with Overlap 188\\nReshaping and Pivoting 189\\nReshaping with Hierarchical Indexing 190\\nPivoting “long” to “wide” Format 192\\nData Transformation 194\\nRemoving Duplicates 194\\nTransforming Data Using a Function or Mapping 195\\nReplacing Values 196\\nRenaming Axis Indexes 197\\nDiscretization and Binning 199\\nDetecting and Filtering Outliers 201\\nPermutation and Random Sampling 202\\nComputing Indicator/Dummy Variables 203\\nString Manipulation 205\\nString Object Methods 206\\nRegular expressions 207\\nVectorized string functions in pandas 210\\nExample: USDA Food Database 212\\nvi | Table of Contents\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 8, 'page_label': 'vii'}, page_content='8. Plotting and Visualization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  219\\nA Brief matplotlib API Primer 219\\nFigures and Subplots 220\\nColors, Markers, and Line Styles 224\\nTicks, Labels, and Legends 225\\nAnnotations and Drawing on a Subplot 228\\nSaving Plots to File 231\\nmatplotlib Configuration 231\\nPlotting Functions in pandas 232\\nLine Plots 232\\nBar Plots 235\\nHistograms and Density Plots 238\\nScatter Plots 239\\nPlotting Maps: Visualizing Haiti Earthquake Crisis Data 241\\nPython Visualization Tool Ecosystem 247\\nChaco 248\\nmayavi 248\\nOther Packages 248\\nThe Future of Visualization Tools? 249\\n9. Data Aggregation and Group Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  251\\nGroupBy Mechanics 252\\nIterating Over Groups 255\\nSelecting a Column or Subset of Columns 256\\nGrouping with Dicts and Series 257\\nGrouping with Functions 258\\nGrouping by Index Levels 259\\nData Aggregation 259\\nColumn-wise and Multiple Function Application 262\\nReturning Aggregated Data in “unindexed” Form 264\\nGroup-wise Operations and Transformations 264\\nApply: General split-apply-combine 266\\nQuantile and Bucket Analysis 268\\nExample: Filling Missing Values with Group-specific Values 270\\nExample: Random Sampling and Permutation 271\\nExample: Group Weighted Average and Correlation 273\\nExample: Group-wise Linear Regression 274\\nPivot Tables and Cross-Tabulation 275\\nCross-Tabulations: Crosstab 277\\nExample: 2012 Federal Election Commission Database 278\\nDonation Statistics by Occupation and Employer 280\\nBucketing Donation Amounts 283\\nDonation Statistics by State 285\\nTable of Contents | vii\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 9, 'page_label': 'viii'}, page_content='10. Time Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  289\\nDate and Time Data Types and Tools 290\\nConverting between string and datetime 291\\nTime Series Basics 293\\nIndexing, Selection, Subsetting 294\\nTime Series with Duplicate Indices 296\\nDate Ranges, Frequencies, and Shifting 297\\nGenerating Date Ranges 298\\nFrequencies and Date Offsets 299\\nShifting (Leading and Lagging) Data 301\\nTime Zone Handling 303\\nLocalization and Conversion 304\\nOperations with Time Zone−aware Timestamp Objects 305\\nOperations between Different Time Zones 306\\nPeriods and Period Arithmetic 307\\nPeriod Frequency Conversion 308\\nQuarterly Period Frequencies 309\\nConverting Timestamps to Periods (and Back) 311\\nCreating a PeriodIndex from Arrays 312\\nResampling and Frequency Conversion 312\\nDownsampling 314\\nUpsampling and Interpolation 316\\nResampling with Periods 318\\nTime Series Plotting 319\\nMoving Window Functions 320\\nExponentially-weighted functions 324\\nBinary Moving Window Functions 324\\nUser-Defined Moving Window Functions 326\\nPerformance and Memory Usage Notes 327\\n11. Financial and Economic Data Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  329\\nData Munging Topics 329\\nTime Series and Cross-Section Alignment 330\\nOperations with Time Series of Different Frequencies 332\\nTime of Day and “as of” Data Selection 334\\nSplicing Together Data Sources 336\\nReturn Indexes and Cumulative Returns 338\\nGroup Transforms and Analysis 340\\nGroup Factor Exposures 342\\nDecile and Quartile Analysis 343\\nMore Example Applications 345\\nSignal Frontier Analysis 345\\nFuture Contract Rolling 347\\nviii | Table of Contents\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 10, 'page_label': 'ix'}, page_content='Rolling Correlation and Linear Regression 350\\n12. Advanced NumPy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  353\\nndarray Object Internals 353\\nNumPy dtype Hierarchy 354\\nAdvanced Array Manipulation 355\\nReshaping Arrays 355\\nC versus Fortran Order 356\\nConcatenating and Splitting Arrays 357\\nRepeating Elements: Tile and Repeat 360\\nFancy Indexing Equivalents: Take and Put 361\\nBroadcasting 362\\nBroadcasting Over Other Axes 364\\nSetting Array Values by Broadcasting 367\\nAdvanced ufunc Usage 367\\nufunc Instance Methods 368\\nCustom ufuncs 370\\nStructured and Record Arrays 370\\nNested dtypes and Multidimensional Fields 371\\nWhy Use Structured Arrays? 372\\nStructured Array Manipulations: numpy.lib.recfunctions 372\\nMore About Sorting 373\\nIndirect Sorts: argsort and lexsort 374\\nAlternate Sort Algorithms 375\\nnumpy.searchsorted: Finding elements in a Sorted Array 376\\nNumPy Matrix Class 377\\nAdvanced Array Input and Output 379\\nMemory-mapped Files 379\\nHDF5 and Other Array Storage Options 380\\nPerformance Tips 380\\nThe Importance of Contiguous Memory 381\\nOther Speed Options: Cython, f2py, C 382\\nAppendix: Python Language Essentials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  385\\nIndex . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  433\\nTable of Contents | ix\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 11, 'page_label': 'x'}, page_content='www.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 12, 'page_label': 'xi'}, page_content='Preface\\nThe scientific Python ecosystem of open source libraries has grown substantially over\\nthe last 10 years. By late 2011, I had long felt that the lack of centralized learning\\nresources for data analysis and statistical applications was a stumbling block for new\\nPython programmers engaged in such work. Key projects for data analysis (especially\\nNumPy, IPython, matplotlib, and pandas) had also matured enough that a book written\\nabout them would likely not go out-of-date very quickly. Thus, I mustered the nerve\\nto embark on this writing project. This is the book that I wish existed when I started\\nusing Python for data analysis in 2007. I hope you find it useful and are able to apply\\nthese tools productively in your work.\\nConventions Used in This Book\\nThe following typographical conventions are used in this book:\\nItalic\\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\\nConstant width\\nUsed for program listings, as well as within paragraphs to refer to program elements\\nsuch as variable or function names, databases, data types, environment variables,\\nstatements, and keywords.\\nConstant width bold\\nShows commands or other text that should be typed literally by the user.\\nConstant width italic\\nShows text that should be replaced with user-supplied values or by values deter-\\nmined by context.\\nThis icon signifies a tip, suggestion, or general note.\\nxi\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 13, 'page_label': 'xii'}, page_content='This icon indicates a warning or caution.\\nUsing Code Examples\\nThis book is here to help you get your job done. In general, you may use the code in\\nthis book in your programs and documentation. You do not need to contact us for\\npermission unless you’re reproducing a significant portion of the code. For example,\\nwriting a program that uses several chunks of code from this book does not require\\npermission. Selling or distributing a CD-ROM of examples from O’Reilly books does\\nrequire permission. Answering a question by citing this book and quoting example\\ncode does not require permission. Incorporating a significant amount of example code\\nfrom this book into your product’s documentation does require permission.\\nWe appreciate, but do not require, attribution. An attribution usually includes the title,\\nauthor, publisher, and ISBN. For example: “Python for Data Analysis by William Wes-\\nley McKinney (O’Reilly). Copyright 2012 William McKinney, 978-1-449-31979-3.”\\nIf you feel your use of code examples falls outside fair use or the permission given above,\\nfeel free to contact us at permissions@oreilly.com.\\nSafari® Books Online\\nSafari Books Online (www.safaribooksonline.com) is an on-demand digital\\nlibrary that delivers expert content in both book and video form from the\\nworld’s leading authors in technology and business.\\nTechnology professionals, software developers, web designers, and business and cre-\\native professionals use Safari Books Online as their primary resource for research,\\nproblem solving, learning, and certification training.\\nSafari Books Online offers a range of product mixes and pricing programs for organi-\\nzations, government agencies, and individuals. Subscribers have access to thousands\\nof books, training videos, and prepublication manuscripts in one fully searchable da-\\ntabase from publishers like O’Reilly Media, Prentice Hall Professional, Addison-Wesley\\nProfessional, Microsoft Press, Sams, Que, Peachpit Press, Focal Press, Cisco Press, John\\nWiley & Sons, Syngress, Morgan Kaufmann, IBM Redbooks, Packt, Adobe Press, FT\\nPress, Apress, Manning, New Riders, McGraw-Hill, Jones & Bartlett, Course Tech-\\nnology, and dozens more. For more information about Safari Books Online, please visit\\nus online.\\nxii | Preface\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 14, 'page_label': 'xiii'}, page_content='How to Contact Us\\nPlease address comments and questions concerning this book to the publisher:\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\n707-829-0515 (international or local)\\n707-829-0104 (fax)\\nWe have a web page for this book, where we list errata, examples, and any additional\\ninformation. You can access this page at http://oreil.ly/python_for_data_analysis.\\nTo comment or ask technical questions about this book, send email to\\nbookquestions@oreilly.com.\\nFor more information about our books, courses, conferences, and news, see our website\\nat http://www.oreilly.com.\\nFind us on Facebook: http://facebook.com/oreilly\\nFollow us on Twitter: http://twitter.com/oreillymedia\\nWatch us on YouTube: http://www.youtube.com/oreillymedia\\nPreface | xiii\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 15, 'page_label': 'xiv'}, page_content='www.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 16, 'page_label': '1'}, page_content='CHAPTER 1\\nPreliminaries\\nWhat Is This Book About?\\nThis book is concerned with the nuts and bolts of manipulating, processing, cleaning,\\nand crunching data in Python. It is also a practical, modern introduction to scientific\\ncomputing in Python, tailored for data-intensive applications. This is a book about the\\nparts of the Python language and libraries you’ll need to effectively solve a broad set of\\ndata analysis problems. This book is not an exposition on analytical methods using\\nPython as the implementation language.\\nWhen I say “data”, what am I referring to exactly? The primary focus is on structured\\ndata, a deliberately vague term that encompasses many different common forms of\\ndata, such as\\n• Multidimensional arrays (matrices)\\n• Tabular or spreadsheet-like data in which each column may be a different type\\n(string, numeric, date, or otherwise). This includes most kinds of data commonly\\nstored in relational databases or tab- or comma-delimited text files\\n• Multiple tables of data interrelated by key columns (what would be primary or\\nforeign keys for a SQL user)\\n• Evenly or unevenly spaced time series\\nThis is by no means a complete list. Even though it may not always be obvious, a large\\npercentage of data sets can be transformed into a structured form that is more suitable\\nfor analysis and modeling. If not, it may be possible to extract features from a data set\\ninto a structured form. As an example, a collection of news articles could be processed\\ninto a word frequency table which could then be used to perform sentiment analysis.\\nMost users of spreadsheet programs like Microsoft Excel, perhaps the most widely used\\ndata analysis tool in the world, will not be strangers to these kinds of data.\\n1\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 17, 'page_label': '2'}, page_content='Why Python for Data Analysis?\\nFor many people (myself among them), the Python language is easy to fall in love with.\\nSince its first appearance in 1991, Python has become one of the most popular dynamic,\\nprogramming languages, along with Perl, Ruby, and others. Python and Ruby have\\nbecome especially popular in recent years for building websites using their numerous\\nweb frameworks, like Rails (Ruby) and Django (Python). Such languages are often\\ncalled scripting languages as they can be used to write quick-and-dirty small programs,\\nor scripts. I don’t like the term “scripting language” as it carries a connotation that they\\ncannot be used for building mission-critical software. Among interpreted languages\\nPython is distinguished by its large and active scientific computing community. Adop-\\ntion of Python for scientific computing in both industry applications and academic\\nresearch has increased significantly since the early 2000s.\\nFor data analysis and interactive, exploratory computing and data visualization, Python\\nwill inevitably draw comparisons with the many other domain-specific open source\\nand commercial programming languages and tools in wide use, such as R, MATLAB,\\nSAS, Stata, and others. In recent years, Python’s improved library support (primarily\\npandas) has made it a strong alternative for data manipulation tasks. Combined with\\nPython’s strength in general purpose programming, it is an excellent choice as a single\\nlanguage for building data-centric applications.\\nPython as Glue\\nPart of Python’s success as a scientific computing platform is the ease of integrating C,\\nC++, and FORTRAN code. Most modern computing environments share a similar set\\nof legacy FORTRAN and C libraries for doing linear algebra, optimization, integration,\\nfast fourier transforms, and other such algorithms. The same story has held true for\\nmany companies and national labs that have used Python to glue together 30 years’\\nworth of legacy software.\\nMost programs consist of small portions of code where most of the time is spent, with\\nlarge amounts of “glue code” that doesn’t run often. In many cases, the execution time\\nof the glue code is insignificant; effort is most fruitfully invested in optimizing the\\ncomputational bottlenecks, sometimes by moving the code to a lower-level language\\nlike C.\\nIn the last few years, the Cython project ( http://cython.org) has become one of the\\npreferred ways of both creating fast compiled extensions for Python and also interfacing\\nwith C and C++ code.\\nSolving the “Two-Language” Problem\\nIn many organizations, it is common to research, prototype, and test new ideas using\\na more domain-specific computing language like MATLAB or R then later port those\\n2 | Chapter 1: \\u2002Preliminaries\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 18, 'page_label': '3'}, page_content='ideas to be part of a larger production system written in, say, Java, C#, or C++. What\\npeople are increasingly finding is that Python is a suitable language not only for doing\\nresearch and prototyping but also building the production systems, too. I believe that\\nmore and more companies will go down this path as there are often significant organ-\\nizational benefits to having both scientists and technologists using the same set of pro-\\ngrammatic tools.\\nWhy Not Python?\\nWhile Python is an excellent environment for building computationally-intensive sci-\\nentific applications and building most kinds of general purpose systems, there are a\\nnumber of uses for which Python may be less suitable.\\nAs Python is an interpreted programming language, in general most Python code will\\nrun substantially slower than code written in a compiled language like Java or C++. As\\nprogrammer time is typically more valuable than CPU time, many are happy to make\\nthis tradeoff. However, in an application with very low latency requirements (for ex-\\nample, a high frequency trading system), the time spent programming in a lower-level,\\nlower-productivity language like C++ to achieve the maximum possible performance\\nmight be time well spent.\\nPython is not an ideal language for highly concurrent, multithreaded applications, par-\\nticularly applications with many CPU-bound threads. The reason for this is that it has\\nwhat is known as the global interpreter lock  (GIL), a mechanism which prevents the\\ninterpreter from executing more than one Python bytecode instruction at a time. The\\ntechnical reasons for why the GIL exists are beyond the scope of this book, but as of\\nthis writing it does not seem likely that the GIL will disappear anytime soon. While it\\nis true that in many big data processing applications, a cluster of computers may be\\nrequired to process a data set in a reasonable amount of time, there are still situations\\nwhere a single-process, multithreaded system is desirable.\\nThis is not to say that Python cannot execute truly multithreaded, parallel code; that\\ncode just cannot be executed in a single Python process. As an example, the Cython\\nproject features easy integration with OpenMP, a C framework for parallel computing,\\nin order to to parallelize loops and thus significantly speed up numerical algorithms.\\nEssential Python Libraries\\nFor those who are less familiar with the scientific Python ecosystem and the libraries\\nused throughout the book, I present the following overview of each library.\\nEssential Python Libraries | 3\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 19, 'page_label': '4'}, page_content='NumPy\\nNumPy, short for Numerical Python, is the foundational package for scientific com-\\nputing in Python. The majority of this book will be based on NumPy and libraries built\\non top of NumPy. It provides, among other things:\\n• A fast and efficient multidimensional array object ndarray\\n• Functions for performing element-wise computations with arrays or mathematical\\noperations between arrays\\n• Tools for reading and writing array-based data sets to disk\\n• Linear algebra operations, Fourier transform, and random number generation\\n• Tools for integrating connecting C, C++, and Fortran code to Python\\nBeyond the fast array-processing capabilities that NumPy adds to Python, one of its\\nprimary purposes with regards to data analysis is as the primary container for data to\\nbe passed between algorithms. For numerical data, NumPy arrays are a much more\\nefficient way of storing and manipulating data than the other built-in Python data\\nstructures. Also, libraries written in a lower-level language, such as C or Fortran, can\\noperate on the data stored in a NumPy array without copying any data.\\npandas\\npandas provides rich data structures and functions designed to make working with\\nstructured data fast, easy, and expressive. It is, as you will see, one of the critical in-\\ngredients enabling Python to be a powerful and productive data analysis environment.\\nThe primary object in pandas that will be used in this book is the DataFrame, a two-\\ndimensional tabular, column-oriented data structure with both row and column labels:\\n>>> frame\\n    total_bill  tip   sex     smoker  day  time    size\\n1   16.99       1.01  Female  No      Sun  Dinner  2\\n2   10.34       1.66  Male    No      Sun  Dinner  3\\n3   21.01       3.5   Male    No      Sun  Dinner  3\\n4   23.68       3.31  Male    No      Sun  Dinner  2\\n5   24.59       3.61  Female  No      Sun  Dinner  4\\n6   25.29       4.71  Male    No      Sun  Dinner  4\\n7   8.77        2     Male    No      Sun  Dinner  2\\n8   26.88       3.12  Male    No      Sun  Dinner  4\\n9   15.04       1.96  Male    No      Sun  Dinner  2\\n10  14.78       3.23  Male    No      Sun  Dinner  2\\npandas combines the high performance array-computing features of NumPy with the\\nflexible data manipulation capabilities of spreadsheets and relational databases (such\\nas SQL). It provides sophisticated indexing functionality to make it easy to reshape,\\nslice and dice, perform aggregations, and select subsets of data. pandas is the primary\\ntool that we will use in this book.\\n4 | Chapter 1: \\u2002Preliminaries\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 20, 'page_label': '5'}, page_content='For financial users, pandas features rich, high-performance time series functionality\\nand tools well-suited for working with financial data. In fact, I initially designed pandas\\nas an ideal tool for financial data analysis applications.\\nFor users of the R language for statistical computing, the DataFrame name will be\\nfamiliar, as the object was named after the similar R data.frame object. They are not\\nthe same, however; the functionality provided by data.frame in R is essentially a strict\\nsubset of that provided by the pandas DataFrame. While this is a book about Python, I\\nwill occasionally draw comparisons with R as it is one of the most widely-used open\\nsource data analysis environments and will be familiar to many readers.\\nThe pandas name itself is derived from panel data, an econometrics term for multidi-\\nmensional structured data sets, and Python data analysis itself.\\nmatplotlib\\nmatplotlib is the most popular Python library for producing plots and other 2D data\\nvisualizations. It was originally created by John D. Hunter (JDH) and is now maintained\\nby a large team of developers. It is well-suited for creating plots suitable for publication.\\nIt integrates well with IPython (see below), thus providing a comfortable interactive\\nenvironment for plotting and exploring data. The plots are also interactive; you can\\nzoom in on a section of the plot and pan around the plot using the toolbar in the plot\\nwindow.\\nIPython\\nIPython is the component in the standard scientific Python toolset that ties everything\\ntogether. It provides a robust and productive environment for interactive and explor-\\natory computing. It is an enhanced Python shell designed to accelerate the writing,\\ntesting, and debugging of Python code. It is particularly useful for interactively working\\nwith data and visualizing data with matplotlib. IPython is usually involved with the\\nmajority of my Python work, including running, debugging, and testing code.\\nAside from the standard terminal-based IPython shell, the project also provides\\n• A Mathematica-like HTML notebook for connecting to IPython through a web\\nbrowser (more on this later).\\n• A Qt framework-based GUI console with inline plotting, multiline editing, and\\nsyntax highlighting\\n• An infrastructure for interactive parallel and distributed computing\\nI will devote a chapter to IPython and how to get the most out of its features. I strongly\\nrecommend using it while working through this book.\\nEssential Python Libraries | 5\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 21, 'page_label': '6'}, page_content='SciPy\\nSciPy is a collection of packages addressing a number of different standard problem\\ndomains in scientific computing. Here is a sampling of the packages included:\\n• scipy.integrate: numerical integration routines and differential equation solvers\\n• scipy.linalg: linear algebra routines and matrix decompositions extending be-\\nyond those provided in numpy.linalg.\\n• scipy.optimize: function optimizers (minimizers) and root finding algorithms\\n• scipy.signal: signal processing tools\\n• scipy.sparse: sparse matrices and sparse linear system solvers\\n• scipy.special: wrapper around SPECFUN, a Fortran library implementing many\\ncommon mathematical functions, such as the gamma function\\n• scipy.stats: standard continuous and discrete probability distributions (density\\nfunctions, samplers, continuous distribution functions), various statistical tests,\\nand more descriptive statistics\\n• scipy.weave: tool for using inline C++ code to accelerate array computations\\nTogether NumPy and SciPy form a reasonably complete computational replacement\\nfor much of MATLAB along with some of its add-on toolboxes.\\nInstallation and Setup\\nSince everyone uses Python for different applications, there is no single solution for\\nsetting up Python and required add-on packages. Many readers will not have a complete\\nscientific Python environment suitable for following along with this book, so here I will\\ngive detailed instructions to get set up on each operating system. I recommend using\\none of the following base Python distributions:\\n• Enthought Python Distribution: a scientific-oriented Python distribution from En-\\nthought (http://www.enthought.com). This includes EPDFree, a free base scientific\\ndistribution (with NumPy, SciPy, matplotlib, Chaco, and IPython) and EPD Full,\\na comprehensive suite of more than 100 scientific packages across many domains.\\nEPD Full is free for academic use but has an annual subscription for non-academic\\nusers.\\n• Python(x,y) ( http://pythonxy.googlecode.com): A free scientific-oriented Python\\ndistribution for Windows.\\nI will be using EPDFree for the installation guides, though you are welcome to take\\nanother approach depending on your needs. At the time of this writing, EPD includes\\nPython 2.7, though this might change at some point in the future. After installing, you\\nwill have the following packages installed and importable:\\n6 | Chapter 1: \\u2002Preliminaries\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 22, 'page_label': '7'}, page_content='• Scientific Python base: NumPy, SciPy, matplotlib, and IPython. These are all in-\\ncluded in EPDFree.\\n• IPython Notebook dependencies: tornado and pyzmq. These are included in EPD-\\nFree.\\n• pandas (version 0.8.2 or higher).\\nAt some point while reading you may wish to install one or more of the following\\npackages: statsmodels, PyTables, PyQt (or equivalently, PySide), xlrd, lxml, basemap,\\npymongo, and requests. These are used in various examples. Installing these optional\\nlibraries is not necessary, and I would would suggest waiting until you need them. For\\nexample, installing PyQt or PyTables from source on OS X or Linux can be rather\\narduous. For now, it’s most important to get up and running with the bare minimum:\\nEPDFree and pandas.\\nFor information on each Python package and links to binary installers or other help,\\nsee the Python Package Index (PyPI, http://pypi.python.org). This is also an excellent\\nresource for finding new Python packages.\\nTo avoid confusion and to keep things simple, I am avoiding discussion\\nof more complex environment management tools like pip and virtua-\\nlenv. There are many excellent guides available for these tools on the\\nInternet.\\nSome users may be interested in alternate Python implementations, such\\nas IronPython, Jython, or PyPy. To make use of the tools presented in\\nthis book, it is (currently) necessary to use the standard C-based Python\\ninterpreter, known as CPython.\\nWindows\\nTo get started on Windows, download the EPDFree installer from http://www.en\\nthought.com, which should be an MSI installer named like epd_free-7.3-1-win-\\nx86.msi. Run the installer and accept the default installation location C:\\\\Python27. If\\nyou had previously installed Python in this location, you may want to delete it manually\\nfirst (or using Add/Remove Programs).\\nNext, you need to verify that Python has been successfully added to the system path\\nand that there are no conflicts with any prior-installed Python versions. First, open a\\ncommand prompt by going to the Start Menu and starting the Command Prompt ap-\\nplication, also known as cmd.exe. Try starting the Python interpreter by typing\\npython. You should see a message that matches the version of EPDFree you installed:\\nC:\\\\Users\\\\Wes>python\\nPython 2.7.3 |EPD_free 7.3-1 (32-bit)| (default, Apr 12 2012, 14:30:37) on win32\\nType \"credits\", \"demo\" or \"enthought\" for more information.\\n>>>\\nInstallation and Setup | 7\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 23, 'page_label': '8'}, page_content='If you see a message for a different version of EPD or it doesn’t work at all, you will\\nneed to clean up your Windows environment variables. On Windows 7 you can start\\ntyping “environment variables” in the programs search field and select Edit environ\\nment variables for your account . On Windows XP, you will have to go to Control\\nPanel > System > Advanced > Environment Variables . On the window that pops up,\\nyou are looking for the Path variable. It needs to contain the following two directory\\npaths, separated by semicolons:\\nC:\\\\Python27;C:\\\\Python27\\\\Scripts\\nIf you installed other versions of Python, be sure to delete any other Python-related\\ndirectories from both the system and user Path variables. After making a path alterna-\\ntion, you have to restart the command prompt for the changes to take effect.\\nOnce you can launch Python successfully from the command prompt, you need to\\ninstall pandas. The easiest way is to download the appropriate binary installer from\\nhttp://pypi.python.org/pypi/pandas. For EPDFree, this should be pandas-0.9.0.win32-\\npy2.7.exe. After you run this, let’s launch IPython and check that things are installed\\ncorrectly by importing pandas and making a simple matplotlib plot:\\nC:\\\\Users\\\\Wes>ipython --pylab\\nPython 2.7.3 |EPD_free 7.3-1 (32-bit)|\\nType \"copyright\", \"credits\" or \"license\" for more information.\\nIPython 0.12.1 -- An enhanced Interactive Python.\\n?         -> Introduction and overview of IPython\\'s features.\\n%quickref -> Quick reference.\\nhelp      -> Python\\'s own help system.\\nobject?   -> Details about \\'object\\', use \\'object??\\' for extra details.\\nWelcome to pylab, a matplotlib-based Python environment [backend: WXAgg].\\nFor more information, type \\'help(pylab)\\'.\\nIn [1]: import pandas\\nIn [2]: plot(arange(10))\\nIf successful, there should be no error messages and a plot window will appear. You\\ncan also check that the IPython HTML notebook can be successfully run by typing:\\n$ ipython notebook --pylab=inline\\nIf you use the IPython notebook application on Windows and normally\\nuse Internet Explorer, you will likely need to install and run Mozilla\\nFirefox or Google Chrome instead.\\nEPDFree on Windows contains only 32-bit executables. If you want or need a 64-bit\\nsetup on Windows, using EPD Full is the most painless way to accomplish that. If you\\nwould rather install from scratch and not pay for an EPD subscription, Christoph\\nGohlke at the University of California, Irvine, publishes unofficial binary installers for\\n8 | Chapter 1: \\u2002Preliminaries\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 24, 'page_label': '9'}, page_content='all of the book’s necessary packages (http://www.lfd.uci.edu/~gohlke/pythonlibs/) for 32-\\nand 64-bit Windows.\\nApple OS X\\nTo get started on OS X, you must first install Xcode, which includes Apple’s suite of\\nsoftware development tools. The necessary component for our purposes is the gcc C\\nand C++ compiler suite. The Xcode installer can be found on the OS X install DVD\\nthat came with your computer or downloaded from Apple directly.\\nOnce you’ve installed Xcode, launch the terminal (Terminal.app) by navigating to\\nApplications > Utilities. Type gcc and press enter. You should hopefully see some-\\nthing like:\\n$ gcc\\ni686-apple-darwin10-gcc-4.2.1: no input files\\nNow you need to install EPDFree. Download the installer which should be a disk image\\nnamed something like epd_free-7.3-1-macosx-i386.dmg. Double-click the .dmg file to\\nmount it, then double-click the .mpkg file inside to run the installer.\\nWhen the installer runs, it automatically appends the EPDFree executable path to\\nyour .bash_profile file. This is located at /Users/your_uname/.bash_profile:\\n# Setting PATH for EPD_free-7.3-1\\nPATH=\"/Library/Frameworks/Python.framework/Versions/Current/bin:${PATH}\"\\nexport PATH\\nShould you encounter any problems in the following steps, you’ll want to inspect\\nyour .bash_profile and potentially add the above directory to your path.\\nNow, it’s time to install pandas. Execute this command in the terminal:\\n$ sudo easy_install pandas\\nSearching for pandas\\nReading http://pypi.python.org/simple/pandas/\\nReading http://pandas.pydata.org\\nReading http://pandas.sourceforge.net\\nBest match: pandas 0.9.0\\nDownloading http://pypi.python.org/packages/source/p/pandas/pandas-0.9.0.zip\\nProcessing pandas-0.9.0.zip\\nWriting /tmp/easy_install-H5mIX6/pandas-0.9.0/setup.cfg\\nRunning pandas-0.9.0/setup.py -q bdist_egg --dist-dir /tmp/easy_install-H5mIX6/\\npandas-0.9.0/egg-dist-tmp-RhLG0z\\nAdding pandas 0.9.0 to easy-install.pth file\\nInstalled /Library/Frameworks/Python.framework/Versions/7.3/lib/python2.7/\\nsite-packages/pandas-0.9.0-py2.7-macosx-10.5-i386.egg\\nProcessing dependencies for pandas\\nFinished processing dependencies for pandas\\nTo verify everything is working, launch IPython in Pylab mode and test importing pan-\\ndas then making a plot interactively:\\nInstallation and Setup | 9\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 25, 'page_label': '10'}, page_content='$ ipython --pylab\\n22:29 ~/VirtualBox VMs/WindowsXP $ ipython\\nPython 2.7.3 |EPD_free 7.3-1 (32-bit)| (default, Apr 12 2012, 11:28:34)\\nType \"copyright\", \"credits\" or \"license\" for more information.\\nIPython 0.12.1 -- An enhanced Interactive Python.\\n?         -> Introduction and overview of IPython\\'s features.\\n%quickref -> Quick reference.\\nhelp      -> Python\\'s own help system.\\nobject?   -> Details about \\'object\\', use \\'object??\\' for extra details.\\nWelcome to pylab, a matplotlib-based Python environment [backend: WXAgg].\\nFor more information, type \\'help(pylab)\\'.\\nIn [1]: import pandas\\nIn [2]: plot(arange(10))\\nIf this succeeds, a plot window with a straight line should pop up.\\nGNU/Linux\\nSome, but not all, Linux distributions include sufficiently up-to-date\\nversions of all the required Python packages and can be installed using\\nthe built-in package management tool like apt. I detail setup using EPD-\\nFree as it\\'s easily reproducible across distributions.\\nLinux details will vary a bit depending on your Linux flavor, but here I give details for\\nDebian-based GNU/Linux systems like Ubuntu and Mint. Setup is similar to OS X with\\nthe exception of how EPDFree is installed. The installer is a shell script that must be\\nexecuted in the terminal. Depending on whether you have a 32-bit or 64-bit system,\\nyou will either need to install the x86 (32-bit) or x86_64 (64-bit) installer. You will then\\nhave a file named something similar to epd_free-7.3-1-rh5-x86_64.sh. To install it,\\nexecute this script with bash:\\n$ bash epd_free-7.3-1-rh5-x86_64.sh\\nAfter accepting the license, you will be presented with a choice of where to put the\\nEPDFree files. I recommend installing the files in your home directory, say /home/wesm/\\nepd (substituting your own username for wesm).\\nOnce the installer has finished, you need to add EPDFree’s bin directory to your \\n$PATH variable. If you are using the bash shell (the default in Ubuntu, for example), this\\nmeans adding the following path addition in your .bashrc:\\nexport PATH=/home/wesm/epd/bin:$PATH\\nObviously, substitute the installation directory you used for /home/wesm/epd/. After\\ndoing this you can either start a new terminal process or execute your .bashrc again\\nwith source ~/.bashrc.\\n10 | Chapter 1: \\u2002Preliminaries\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 26, 'page_label': '11'}, page_content='You need a C compiler such as gcc to move forward; many Linux distributions include\\ngcc, but others may not. On Debian systems, you can install gcc by executing:\\nsudo apt-get install gcc\\nIf you type gcc on the command line it should say something like:\\n$ gcc\\ngcc: no input files\\nNow, time to install pandas:\\n$ easy_install pandas\\nIf you installed EPDFree as root, you may need to add sudo to the command and enter\\nthe sudo or root password. To verify things are working, perform the same checks as\\nin the OS X section.\\nPython 2 and Python 3\\nThe Python community is currently undergoing a drawn-out transition from the Python\\n2 series of interpreters to the Python 3 series. Until the appearance of Python 3.0, all\\nPython code was backwards compatible. The community decided that in order to move\\nthe language forward, certain backwards incompatible changes were necessary.\\nI am writing this book with Python 2.7 as its basis, as the majority of the scientific\\nPython community has not yet transitioned to Python 3. The good news is that, with\\na few exceptions, you should have no trouble following along with the book if you\\nhappen to be using Python 3.2.\\nIntegrated Development Environments (IDEs)\\nWhen asked about my standard development environment, I almost always say “IPy-\\nthon plus a text editor”. I typically write a program and iteratively test and debug each\\npiece of it in IPython. It is also useful to be able to play around with data interactively\\nand visually verify that a particular set of data manipulations are doing the right thing.\\nLibraries like pandas and NumPy are designed to be easy-to-use in the shell.\\nHowever, some will still prefer to work in an IDE instead of a text editor. They do\\nprovide many nice “code intelligence” features like completion or quickly pulling up\\nthe documentation associated with functions and classes. Here are some that you can\\nexplore:\\n• Eclipse with PyDev Plugin\\n• Python Tools for Visual Studio (for Windows users)\\n• PyCharm\\n• Spyder\\n• Komodo IDE\\nInstallation and Setup | 11\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 27, 'page_label': '12'}, page_content=\"Community and Conferences\\nOutside of an Internet search, the scientific Python mailing lists are generally helpful\\nand responsive to questions. Some ones to take a look at are:\\n• pydata: a Google Group list for questions related to Python for data analysis and\\npandas\\n• pystatsmodels: for statsmodels or pandas-related questions\\n• numpy-discussion: for NumPy-related questions\\n• scipy-user: for general SciPy or scientific Python questions\\nI deliberately did not post URLs for these in case they change. They can be easily located\\nvia Internet search.\\nEach year many conferences are held all over the world for Python programmers. PyCon\\nand EuroPython are the two main general Python conferences in the United States and\\nEurope, respectively. SciPy and EuroSciPy are scientific-oriented Python conferences\\nwhere you will likely find many “birds of a feather” if you become more involved with\\nusing Python for data analysis after reading this book.\\nNavigating This Book\\nIf you have never programmed in Python before, you may actually want to start at the\\nend of the book, where I have placed a condensed tutorial on Python syntax, language\\nfeatures, and built-in data structures like tuples, lists, and dicts. These things are con-\\nsidered prerequisite knowledge for the remainder of the book.\\nThe book starts by introducing you to the IPython environment. Next, I give a short\\nintroduction to the key features of NumPy, leaving more advanced NumPy use for\\nanother chapter at the end of the book. Then, I introduce pandas and devote the rest\\nof the book to data analysis topics applying pandas, NumPy, and matplotlib (for vis-\\nualization). I have structured the material in the most incremental way possible, though\\nthere is occasionally some minor cross-over between chapters.\\nData files and related material for each chapter are hosted as a git repository on GitHub:\\nhttp://github.com/pydata/pydata-book\\nI encourage you to download the data and use it to replicate the book’s code examples\\nand experiment with the tools presented in each chapter. I will happily accept contri-\\nbutions, scripts, IPython notebooks, or any other materials you wish to contribute to\\nthe book's repository for all to enjoy.\\n12 | Chapter 1: \\u2002Preliminaries\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 28, 'page_label': '13'}, page_content='Code Examples\\nMost of the code examples in the book are shown with input and output as it would\\nappear executed in the IPython shell.\\nIn [5]: code\\nOut[5]: output\\nAt times, for clarity, multiple code examples will be shown side by side. These should\\nbe read left to right and executed separately.\\nIn [5]: code         In [6]: code2\\nOut[5]: output       Out[6]: output2\\nData for Examples\\nData sets for the examples in each chapter are hosted in a repository on GitHub: http:\\n//github.com/pydata/pydata-book. You can download this data either by using the git\\nrevision control command-line program or by downloading a zip file of the repository\\nfrom the website.\\nI have made every effort to ensure that it contains everything necessary to reproduce\\nthe examples, but I may have made some mistakes or omissions. If so, please send me\\nan e-mail: wesmckinn@gmail.com.\\nImport Conventions\\nThe Python community has adopted a number of naming conventions for commonly-\\nused modules:\\nimport numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nThis means that when you see np.arange, this is a reference to the arange function in\\nNumPy. This is done as it’s considered bad practice in Python software development\\nto import everything (from numpy import *) from a large package like NumPy.\\nJargon\\nI’ll use some terms common both to programming and data science that you may not\\nbe familiar with. Thus, here are some brief definitions:\\nMunge/Munging/Wrangling\\nDescribes the overall process of manipulating unstructured and/or messy data into\\na structured or clean form. The word has snuck its way into the jargon of many\\nmodern day data hackers. Munge rhymes with “lunge”.\\nNavigating This Book | 13\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 29, 'page_label': '14'}, page_content='Pseudocode\\nA description of an algorithm or process that takes a code-like form while likely\\nnot being actual valid source code.\\nSyntactic sugar\\nProgramming syntax which does not add new features, but makes something more\\nconvenient or easier to type.\\nAcknowledgements\\nIt would have been difficult for me to write this book without the support of a large\\nnumber of people.\\nOn the O’Reilly staff, I’m very grateful for my editors Meghan Blanchette and Julie\\nSteele who guided me through the process. Mike Loukides also worked with me in the\\nproposal stages and helped make the book a reality.\\nI received a wealth of technical review from a large cast of characters. In particular,\\nMartin Blais and Hugh White were incredibly helpful in improving the book’s exam-\\nples, clarity, and organization from cover to cover. James Long, Drew Conway, Fer-\\nnando Pérez, Brian Granger, Thomas Kluyver, Adam Klein, Josh Klein, Chang She, and\\nStéfan van der Walt each reviewed one or more chapters, providing pointed feedback\\nfrom many different perspectives.\\nI got many great ideas for examples and data sets from friends and colleagues in the\\ndata community, among them: Mike Dewar, Jeff Hammerbacher, James Johndrow,\\nKristian Lum, Adam Klein, Hilary Mason, Chang She, and Ashley Williams.\\nI am of course indebted to the many leaders in the open source scientific Python com-\\nmunity who’ve built the foundation for my development work and gave encouragement\\nwhile I was writing this book: the IPython core team (Fernando Pérez, Brian Granger,\\nMin Ragan-Kelly, Thomas Kluyver, and others), John Hunter, Skipper Seabold, Travis\\nOliphant, Peter Wang, Eric Jones, Robert Kern, Josef Perktold, Francesc Alted, Chris\\nFonnesbeck, and too many others to mention. Several other people provided a great\\ndeal of support, ideas, and encouragement along the way: Drew Conway, Sean Taylor,\\nGiuseppe Paleologo, Jared Lander, David Epstein, John Krowas, Joshua Bloom, Den\\nPilsworth, John Myles-White, and many others I’ve forgotten.\\nI’d also like to thank a number of people from my formative years. First, my former\\nAQR colleagues who’ve cheered me on in my pandas work over the years: Alex Reyf-\\nman, Michael Wong, Tim Sargen, Oktay Kurbanov, Matthew Tschantz, Roni Israelov,\\nMichael Katz, Chris Uga, Prasad Ramanan, Ted Square, and Hoon Kim. Lastly, my\\nacademic advisors Haynes Miller (MIT) and Mike West (Duke).\\nOn the personal side, Casey Dinkin provided invaluable day-to-day support during the\\nwriting process, tolerating my highs and lows as I hacked together the final draft on\\n14 | Chapter 1: \\u2002Preliminaries\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 30, 'page_label': '15'}, page_content='top of an already overcommitted schedule. Lastly, my parents, Bill and Kim, taught me\\nto always follow my dreams and to never settle for less.\\nAcknowledgements | 15\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 31, 'page_label': '16'}, page_content='www.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 32, 'page_label': '17'}, page_content='CHAPTER 2\\nIntroductory Examples\\nThis book teaches you the Python tools to work productively with data. While readers\\nmay have many different end goals for their work, the tasks required generally fall into\\na number of different broad groups:\\nInteracting with the outside world\\nReading and writing with a variety of file formats and databases.\\nPreparation\\nCleaning, munging, combining, normalizing, reshaping, slicing and dicing, and\\ntransforming data for analysis.\\nTransformation\\nApplying mathematical and statistical operations to groups of data sets to derive\\nnew data sets. For example, aggregating a large table by group variables.\\nModeling and computation\\nConnecting your data to statistical models, machine learning algorithms, or other\\ncomputational tools\\nPresentation\\nCreating interactive or static graphical visualizations or textual summaries\\nIn this chapter I will show you a few data sets and some things we can do with them.\\nThese examples are just intended to pique your interest and thus will only be explained\\nat a high level. Don’t worry if you have no experience with any of these tools; they will\\nbe discussed in great detail throughout the rest of the book. In the code examples you’ll\\nsee input and output prompts like In [15]:; these are from the IPython shell.\\n1.usa.gov data from bit.ly\\nIn 2011, URL shortening service bit.ly partnered with the United States government\\nwebsite usa.gov to provide a feed of anonymous data gathered from users who shorten\\nlinks ending with .gov or .mil. As of this writing, in addition to providing a live feed,\\nhourly snapshots are available as downloadable text files.1\\n17\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 33, 'page_label': '18'}, page_content='In the case of the hourly snapshots, each line in each file contains a common form of\\nweb data known as JSON, which stands for JavaScript Object Notation. For example,\\nif we read just the first line of a file you may see something like\\nIn [15]: path = \\'ch02/usagov_bitly_data2012-03-16-1331923249.txt\\'\\nIn [16]: open(path).readline()\\nOut[16]: \\'{ \"a\": \"Mozilla\\\\\\\\/5.0 (Windows NT 6.1; WOW64) AppleWebKit\\\\\\\\/535.11\\n(KHTML, like Gecko) Chrome\\\\\\\\/17.0.963.78 Safari\\\\\\\\/535.11\", \"c\": \"US\", \"nk\": 1,\\n\"tz\": \"America\\\\\\\\/New_York\", \"gr\": \"MA\", \"g\": \"A6qOVH\", \"h\": \"wfLQtf\", \"l\":\\n\"orofrog\", \"al\": \"en-US,en;q=0.8\", \"hh\": \"1.usa.gov\", \"r\":\\n\"http:\\\\\\\\/\\\\\\\\/www.facebook.com\\\\\\\\/l\\\\\\\\/7AQEFzjSi\\\\\\\\/1.usa.gov\\\\\\\\/wfLQtf\", \"u\":\\n\"http:\\\\\\\\/\\\\\\\\/www.ncbi.nlm.nih.gov\\\\\\\\/pubmed\\\\\\\\/22415991\", \"t\": 1331923247, \"hc\":\\n1331822918, \"cy\": \"Danvers\", \"ll\": [ 42.576698, -70.954903 ] }\\\\n\\'\\nPython has numerous built-in and 3rd party modules for converting a JSON string into\\na Python dictionary object. Here I’ll use the json module and its loads function invoked\\non each line in the sample file I downloaded:\\nimport json\\npath = \\'ch02/usagov_bitly_data2012-03-16-1331923249.txt\\'\\nrecords = [json.loads(line) for line in open(path)]\\nIf you’ve never programmed in Python before, the last expression here is called a list\\ncomprehension, which is a concise way of applying an operation (like json.loads) to a\\ncollection of strings or other objects. Conveniently, iterating over an open file handle\\ngives you a sequence of its lines. The resulting object records is now a list of Python\\ndicts:\\nIn [18]: records[0]\\nOut[18]:\\n{u\\'a\\': u\\'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like\\nGecko) Chrome/17.0.963.78 Safari/535.11\\',\\n u\\'al\\': u\\'en-US,en;q=0.8\\',\\n u\\'c\\': u\\'US\\',\\n u\\'cy\\': u\\'Danvers\\',\\n u\\'g\\': u\\'A6qOVH\\',\\n u\\'gr\\': u\\'MA\\',\\n u\\'h\\': u\\'wfLQtf\\',\\n u\\'hc\\': 1331822918,\\n u\\'hh\\': u\\'1.usa.gov\\',\\n u\\'l\\': u\\'orofrog\\',\\n u\\'ll\\': [42.576698, -70.954903],\\n u\\'nk\\': 1,\\n u\\'r\\': u\\'http://www.facebook.com/l/7AQEFzjSi/1.usa.gov/wfLQtf\\',\\n u\\'t\\': 1331923247,\\n u\\'tz\\': u\\'America/New_York\\',\\n u\\'u\\': u\\'http://www.ncbi.nlm.nih.gov/pubmed/22415991\\'}\\n1. http://www.usa.gov/About/developer-resources/1usagov.shtml\\n18 | Chapter 2: \\u2002Introductory Examples\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 34, 'page_label': '19'}, page_content=\"Note that Python indices start at 0 and not 1 like some other languages (like R). It’s\\nnow easy to access individual values within records by passing a string for the key you\\nwish to access:\\nIn [19]: records[0]['tz']\\nOut[19]: u'America/New_York'\\nThe u here in front of the quotation stands for unicode, a standard form of string en-\\ncoding. Note that IPython shows the time zone string object representation here rather\\nthan its print equivalent:\\nIn [20]: print records[0]['tz']\\nAmerica/New_York\\nCounting Time Zones in Pure Python\\nSuppose we were interested in the most often-occurring time zones in the data set (the\\ntz field). There are many ways we could do this. First, let’s extract a list of time zones\\nagain using a list comprehension:\\nIn [25]: time_zones = [rec['tz'] for rec in records]\\n---------------------------------------------------------------------------\\nKeyError                                  Traceback (most recent call last)\\n/home/wesm/book_scripts/whetting/<ipython> in <module>()\\n----> 1 time_zones = [rec['tz'] for rec in records]\\nKeyError: 'tz'\\nOops! Turns out that not all of the records have a time zone field. This is easy to handle\\nas we can add the check if 'tz' in rec at the end of the list comprehension:\\nIn [26]: time_zones = [rec['tz'] for rec in records if 'tz' in rec]\\nIn [27]: time_zones[:10]\\nOut[27]:\\n[u'America/New_York',\\n u'America/Denver',\\n u'America/New_York',\\n u'America/Sao_Paulo',\\n u'America/New_York',\\n u'America/New_York',\\n u'Europe/Warsaw',\\n u'',\\n u'',\\n u'']\\nJust looking at the first 10 time zones we see that some of them are unknown (empty).\\nYou can filter these out also but I’ll leave them in for now. Now, to produce counts by\\ntime zone I’ll show two approaches: the harder way (using just the Python standard\\nlibrary) and the easier way (using pandas). One way to do the counting is to use a dict\\nto store counts while we iterate through the time zones:\\ndef get_counts(sequence):\\n    counts = {}\\n1.usa.gov data from bit.ly | 19\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 35, 'page_label': '20'}, page_content=\"for x in sequence:\\n        if x in counts:\\n            counts[x] += 1\\n        else:\\n            counts[x] = 1\\n    return counts\\nIf you know a bit more about the Python standard library, you might prefer to write\\nthe same thing more briefly:\\nfrom collections import defaultdict\\ndef get_counts2(sequence):\\n    counts = defaultdict(int) # values will initialize to 0\\n    for x in sequence:\\n        counts[x] += 1\\n    return counts\\nI put this logic in a function just to make it more reusable. To use it on the time zones,\\njust pass the time_zones list:\\nIn [31]: counts = get_counts(time_zones)\\nIn [32]: counts['America/New_York']\\nOut[32]: 1251\\nIn [33]: len(time_zones)\\nOut[33]: 3440\\nIf we wanted the top 10 time zones and their counts, we have to do a little bit of dic-\\ntionary acrobatics:\\ndef top_counts(count_dict, n=10):\\n    value_key_pairs = [(count, tz) for tz, count in count_dict.items()]\\n    value_key_pairs.sort()\\n    return value_key_pairs[-n:]\\nWe have then:\\nIn [35]: top_counts(counts)\\nOut[35]:\\n[(33, u'America/Sao_Paulo'),\\n (35, u'Europe/Madrid'),\\n (36, u'Pacific/Honolulu'),\\n (37, u'Asia/Tokyo'),\\n (74, u'Europe/London'),\\n (191, u'America/Denver'),\\n (382, u'America/Los_Angeles'),\\n (400, u'America/Chicago'),\\n (521, u''),\\n (1251, u'America/New_York')]\\n20 | Chapter 2: \\u2002Introductory Examples\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 36, 'page_label': '21'}, page_content=\"If you search the Python standard library, you may find the collections.Counter class,\\nwhich makes this task a lot easier:\\nIn [49]: from collections import Counter\\nIn [50]: counts = Counter(time_zones)\\nIn [51]: counts.most_common(10)\\nOut[51]:\\n[(u'America/New_York', 1251),\\n (u'', 521),\\n (u'America/Chicago', 400),\\n (u'America/Los_Angeles', 382),\\n (u'America/Denver', 191),\\n (u'Europe/London', 74),\\n (u'Asia/Tokyo', 37),\\n (u'Pacific/Honolulu', 36),\\n (u'Europe/Madrid', 35),\\n (u'America/Sao_Paulo', 33)]\\nCounting Time Zones with pandas\\nThe main pandas data structure is the DataFrame, which you can think of as repre-\\nsenting a table or spreadsheet of data. Creating a DataFrame from the original set of\\nrecords is simple:\\nIn [289]: from pandas import DataFrame, Series\\nIn [290]: import pandas as pd\\nIn [291]: frame = DataFrame(records)\\nIn [292]: frame\\nOut[292]: \\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 3560 entries, 0 to 3559\\nData columns:\\n_heartbeat_    120  non-null values\\na              3440  non-null values\\nal             3094  non-null values\\nc              2919  non-null values\\ncy             2919  non-null values\\ng              3440  non-null values\\ngr             2919  non-null values\\nh              3440  non-null values\\nhc             3440  non-null values\\nhh             3440  non-null values\\nkw             93  non-null values\\nl              3440  non-null values\\nll             2919  non-null values\\nnk             3440  non-null values\\nr              3440  non-null values\\nt              3440  non-null values\\ntz             3440  non-null values\\n1.usa.gov data from bit.ly | 21\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 37, 'page_label': '22'}, page_content=\"u              3440  non-null values\\ndtypes: float64(4), object(14)\\nIn [293]: frame['tz'][:10]\\nOut[293]: \\n0     America/New_York\\n1       America/Denver\\n2     America/New_York\\n3    America/Sao_Paulo\\n4     America/New_York\\n5     America/New_York\\n6        Europe/Warsaw\\n7                     \\n8                     \\n9                     \\nName: tz\\nThe output shown for the frame is the summary view, shown for large DataFrame ob-\\njects. The Series object returned by frame['tz'] has a method value_counts that gives\\nus what we’re looking for:\\nIn [294]: tz_counts = frame['tz'].value_counts()\\nIn [295]: tz_counts[:10]\\nOut[295]: \\nAmerica/New_York       1251\\n                        521\\nAmerica/Chicago         400\\nAmerica/Los_Angeles     382\\nAmerica/Denver          191\\nEurope/London            74\\nAsia/Tokyo               37\\nPacific/Honolulu         36\\nEurope/Madrid            35\\nAmerica/Sao_Paulo        33\\nThen, we might want to make a plot of this data using plotting library, matplotlib. You\\ncan do a bit of munging to fill in a substitute value for unknown and missing time zone\\ndata in the records. The fillna function can replace missing (NA) values and unknown\\n(empty strings) values can be replaced by boolean array indexing:\\nIn [296]: clean_tz = frame['tz'].fillna('Missing')\\nIn [297]: clean_tz[clean_tz == ''] = 'Unknown'\\nIn [298]: tz_counts = clean_tz.value_counts()\\nIn [299]: tz_counts[:10]\\nOut[299]: \\nAmerica/New_York       1251\\nUnknown                 521\\nAmerica/Chicago         400\\nAmerica/Los_Angeles     382\\nAmerica/Denver          191\\nMissing                 120\\n22 | Chapter 2: \\u2002Introductory Examples\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 38, 'page_label': '23'}, page_content=\"Europe/London            74\\nAsia/Tokyo               37\\nPacific/Honolulu         36\\nEurope/Madrid            35\\nMaking a horizontal bar plot can be accomplished using the plot method on the\\ncounts objects:\\nIn [301]: tz_counts[:10].plot(kind='barh', rot=0)\\nSee Figure 2-1 for the resulting figure. We’ll explore more tools for working with this\\nkind of data. For example, the a field contains information about the browser, device,\\nor application used to perform the URL shortening:\\nIn [302]: frame['a'][1]\\nOut[302]: u'GoogleMaps/RochesterNY'\\nIn [303]: frame['a'][50]\\nOut[303]: u'Mozilla/5.0 (Windows NT 5.1; rv:10.0.2) Gecko/20100101 Firefox/10.0.2'\\nIn [304]: frame['a'][51]\\nOut[304]: u'Mozilla/5.0 (Linux; U; Android 2.2.2; en-us; LG-P925/V10e Build/FRG83G) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1'\\nFigure 2-1. Top time zones in the 1.usa.gov sample data\\nParsing all of the interesting information in these “agent” strings may seem like a\\ndaunting task. Luckily, once you have mastered Python’s built-in string functions and\\nregular expression capabilities, it is really not so bad. For example, we could split off\\nthe first token in the string (corresponding roughly to the browser capability) and make\\nanother summary of the user behavior:\\nIn [305]: results = Series([x.split()[0] for x in frame.a.dropna()])\\nIn [306]: results[:5]\\nOut[306]: \\n0               Mozilla/5.0\\n1    GoogleMaps/RochesterNY\\n2               Mozilla/4.0\\n3               Mozilla/5.0\\n4               Mozilla/5.0\\n1.usa.gov data from bit.ly | 23\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 39, 'page_label': '24'}, page_content=\"In [307]: results.value_counts()[:8]\\nOut[307]: \\nMozilla/5.0                 2594\\nMozilla/4.0                  601\\nGoogleMaps/RochesterNY       121\\nOpera/9.80                    34\\nTEST_INTERNET_AGENT           24\\nGoogleProducer                21\\nMozilla/6.0                    5\\nBlackBerry8520/5.0.0.681       4\\nNow, suppose you wanted to decompose the top time zones into Windows and non-\\nWindows users. As a simplification, let’s say that a user is on Windows if the string\\n'Windows' is in the agent string. Since some of the agents are missing, I’ll exclude these\\nfrom the data:\\nIn [308]: cframe = frame[frame.a.notnull()]\\nWe want to then compute a value whether each row is Windows or not:\\nIn [309]: operating_system = np.where(cframe['a'].str.contains('Windows'),\\n   .....:                             'Windows', 'Not Windows')\\nIn [310]: operating_system[:5]\\nOut[310]: \\n0        Windows\\n1    Not Windows\\n2        Windows\\n3    Not Windows\\n4        Windows\\nName: a\\nThen, you can group the data by its time zone column and this new list of operating\\nsystems:\\nIn [311]: by_tz_os = cframe.groupby(['tz', operating_system])\\nThe group counts, analogous to the value_counts function above, can be computed\\nusing size. This result is then reshaped into a table with unstack:\\nIn [312]: agg_counts = by_tz_os.size().unstack().fillna(0)\\nIn [313]: agg_counts[:10]\\nOut[313]: \\na                               Not Windows  Windows\\ntz                                                  \\n                                        245      276\\nAfrica/Cairo                              0        3\\nAfrica/Casablanca                         0        1\\nAfrica/Ceuta                              0        2\\nAfrica/Johannesburg                       0        1\\nAfrica/Lusaka                             0        1\\nAmerica/Anchorage                         4        1\\nAmerica/Argentina/Buenos_Aires            1        0\\n24 | Chapter 2: \\u2002Introductory Examples\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 40, 'page_label': '25'}, page_content=\"America/Argentina/Cordoba                 0        1\\nAmerica/Argentina/Mendoza                 0        1\\nFinally, let’s select the top overall time zones. To do so, I construct an indirect index\\narray from the row counts in agg_counts:\\n# Use to sort in ascending order\\nIn [314]: indexer = agg_counts.sum(1).argsort()\\nIn [315]: indexer[:10]\\nOut[315]: \\ntz\\n                                  24\\nAfrica/Cairo                      20\\nAfrica/Casablanca                 21\\nAfrica/Ceuta                      92\\nAfrica/Johannesburg               87\\nAfrica/Lusaka                     53\\nAmerica/Anchorage                 54\\nAmerica/Argentina/Buenos_Aires    57\\nAmerica/Argentina/Cordoba         26\\nAmerica/Argentina/Mendoza         55\\nI then use take to select the rows in that order, then slice off the last 10 rows:\\nIn [316]: count_subset = agg_counts.take(indexer)[-10:]\\nIn [317]: count_subset\\nOut[317]: \\na                    Not Windows  Windows\\ntz                                       \\nAmerica/Sao_Paulo             13       20\\nEurope/Madrid                 16       19\\nPacific/Honolulu               0       36\\nAsia/Tokyo                     2       35\\nEurope/London                 43       31\\nAmerica/Denver               132       59\\nAmerica/Los_Angeles          130      252\\nAmerica/Chicago              115      285\\n                             245      276\\nAmerica/New_York             339      912\\nThen, as shown in the preceding code block, this can be plotted in a bar plot; I’ll make\\nit a stacked bar plot by passing stacked=True (see Figure 2-2) :\\nIn [319]: count_subset.plot(kind='barh', stacked=True)\\nThe plot doesn’t make it easy to see the relative percentage of Windows users in the\\nsmaller groups, but the rows can easily be normalized to sum to 1 then plotted again\\n(see Figure 2-3):\\nIn [321]: normed_subset = count_subset.div(count_subset.sum(1), axis=0)\\nIn [322]: normed_subset.plot(kind='barh', stacked=True)\\n1.usa.gov data from bit.ly | 25\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 41, 'page_label': '26'}, page_content='All of the methods employed here will be examined in great detail throughout the rest\\nof the book.\\nMovieLens 1M Data Set\\nGroupLens Research (http://www.grouplens.org/node/73) provides a number of collec-\\ntions of movie ratings data collected from users of MovieLens in the late 1990s and\\nFigure 2-2. Top time zones by Windows and non-Windows users\\nFigure 2-3. Percentage Windows and non-Windows users in top-occurring time zones\\n26 | Chapter 2: \\u2002Introductory Examples\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 42, 'page_label': '27'}, page_content=\"early 2000s. The data provide movie ratings, movie metadata (genres and year), and\\ndemographic data about the users (age, zip code, gender, and occupation). Such data\\nis often of interest in the development of recommendation systems based on machine\\nlearning algorithms. While I will not be exploring machine learning techniques in great\\ndetail in this book, I will show you how to slice and dice data sets like these into the\\nexact form you need.\\nThe MovieLens 1M data set contains 1 million ratings collected from 6000 users on\\n4000 movies. It’s spread across 3 tables: ratings, user information, and movie infor-\\nmation. After extracting the data from the zip file, each table can be loaded into a pandas\\nDataFrame object using pandas.read_table:\\nimport pandas as pd\\nunames = ['user_id', 'gender', 'age', 'occupation', 'zip']\\nusers = pd.read_table('ml-1m/users.dat', sep='::', header=None,\\n                      names=unames)\\nrnames = ['user_id', 'movie_id', 'rating', 'timestamp']\\nratings = pd.read_table('ml-1m/ratings.dat', sep='::', header=None,\\n                        names=rnames)\\nmnames = ['movie_id', 'title', 'genres']\\nmovies = pd.read_table('ml-1m/movies.dat', sep='::', header=None,\\n                        names=mnames)\\nYou can verify that everything succeeded by looking at the first few rows of each Da-\\ntaFrame with Python's slice syntax:\\nIn [334]: users[:5]\\nOut[334]: \\n   user_id gender  age  occupation    zip\\n0        1      F    1          10  48067\\n1        2      M   56          16  70072\\n2        3      M   25          15  55117\\n3        4      M   45           7  02460\\n4        5      M   25          20  55455\\nIn [335]: ratings[:5]\\nOut[335]: \\n   user_id  movie_id  rating  timestamp\\n0        1      1193       5  978300760\\n1        1       661       3  978302109\\n2        1       914       3  978301968\\n3        1      3408       4  978300275\\n4        1      2355       5  978824291\\nIn [336]: movies[:5]\\nOut[336]: \\n   movie_id                               title                        genres\\n0         1                    Toy Story (1995)   Animation|Children's|Comedy\\n1         2                      Jumanji (1995)  Adventure|Children's|Fantasy\\n2         3             Grumpier Old Men (1995)                Comedy|Romance\\n3         4            Waiting to Exhale (1995)                  Comedy|Drama\\nMovieLens 1M Data Set | 27\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 43, 'page_label': '28'}, page_content=\"4         5  Father of the Bride Part II (1995)                        Comedy\\nIn [337]: ratings\\nOut[337]: \\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 1000209 entries, 0 to 1000208\\nData columns:\\nuser_id      1000209  non-null values\\nmovie_id     1000209  non-null values\\nrating       1000209  non-null values\\ntimestamp    1000209  non-null values\\ndtypes: int64(4)\\nNote that ages and occupations are coded as integers indicating groups described in\\nthe data set’s README file. Analyzing the data spread across three tables is not a simple\\ntask; for example, suppose you wanted to compute mean ratings for a particular movie\\nby sex and age. As you will see, this is much easier to do with all of the data merged\\ntogether into a single table. Using pandas’s merge function, we first merge ratings with\\nusers then merging that result with the movies data. pandas infers which columns to\\nuse as the merge (or join) keys based on overlapping names:\\nIn [338]: data = pd.merge(pd.merge(ratings, users), movies)\\nIn [339]: data\\nOut[339]: \\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 1000209 entries, 0 to 1000208\\nData columns:\\nuser_id       1000209  non-null values\\nmovie_id      1000209  non-null values\\nrating        1000209  non-null values\\ntimestamp     1000209  non-null values\\ngender        1000209  non-null values\\nage           1000209  non-null values\\noccupation    1000209  non-null values\\nzip           1000209  non-null values\\ntitle         1000209  non-null values\\ngenres        1000209  non-null values\\ndtypes: int64(6), object(4)\\nIn [340]: data.ix[0]\\nOut[340]: \\nuser_id                                 1\\nmovie_id                                1\\nrating                                  5\\ntimestamp                       978824268\\ngender                                  F\\nage                                     1\\noccupation                             10\\nzip                                 48067\\ntitle                    Toy Story (1995)\\ngenres        Animation|Children's|Comedy\\nName: 0\\n28 | Chapter 2: \\u2002Introductory Examples\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 44, 'page_label': '29'}, page_content=\"In this form, aggregating the ratings grouped by one or more user or movie attributes\\nis straightforward once you build some familiarity with pandas. To get mean movie\\nratings for each film grouped by gender, we can use the pivot_table method:\\nIn [341]: mean_ratings = data.pivot_table('rating', rows='title',\\n   .....:                                 cols='gender', aggfunc='mean')\\nIn [342]: mean_ratings[:5]\\nOut[342]: \\ngender                                F         M\\ntitle                                            \\n$1,000,000 Duck (1971)         3.375000  2.761905\\n'Night Mother (1986)           3.388889  3.352941\\n'Til There Was You (1997)      2.675676  2.733333\\n'burbs, The (1989)             2.793478  2.962085\\n...And Justice for All (1979)  3.828571  3.689024\\nThis produced another DataFrame containing mean ratings with movie totals as row\\nlabels and gender as column labels. First, I’m going to filter down to movies that re-\\nceived at least 250 ratings (a completely arbitrary number); to do this, I group the data\\nby title and use size() to get a Series of group sizes for each title:\\nIn [343]: ratings_by_title = data.groupby('title').size()\\nIn [344]: ratings_by_title[:10]\\nOut[344]: \\ntitle\\n$1,000,000 Duck (1971)                37\\n'Night Mother (1986)                  70\\n'Til There Was You (1997)             52\\n'burbs, The (1989)                   303\\n...And Justice for All (1979)        199\\n1-900 (1994)                           2\\n10 Things I Hate About You (1999)    700\\n101 Dalmatians (1961)                565\\n101 Dalmatians (1996)                364\\n12 Angry Men (1957)                  616\\nIn [345]: active_titles = ratings_by_title.index[ratings_by_title >= 250]\\nIn [346]: active_titles\\nOut[346]: \\nIndex(['burbs, The (1989), 10 Things I Hate About You (1999),\\n       101 Dalmatians (1961), ..., Young Sherlock Holmes (1985),\\n       Zero Effect (1998), eXistenZ (1999)], dtype=object)\\nThe index of titles receiving at least 250 ratings can then be used to select rows from\\nmean_ratings above:\\nIn [347]: mean_ratings = mean_ratings.ix[active_titles]\\nIn [348]: mean_ratings\\nOut[348]: \\n<class 'pandas.core.frame.DataFrame'>\\nIndex: 1216 entries, 'burbs, The (1989) to eXistenZ (1999)\\nMovieLens 1M Data Set | 29\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 45, 'page_label': '30'}, page_content=\"Data columns:\\nF    1216  non-null values\\nM    1216  non-null values\\ndtypes: float64(2)\\nTo see the top films among female viewers, we can sort by the F column in descending\\norder:\\nIn [350]: top_female_ratings = mean_ratings.sort_index(by='F', ascending=False)\\nIn [351]: top_female_ratings[:10]\\nOut[351]: \\ngender                                                         F         M\\nClose Shave, A (1995)                                   4.644444  4.473795\\nWrong Trousers, The (1993)                              4.588235  4.478261\\nSunset Blvd. (a.k.a. Sunset Boulevard) (1950)           4.572650  4.464589\\nWallace & Gromit: The Best of Aardman Animation (1996)  4.563107  4.385075\\nSchindler's List (1993)                                 4.562602  4.491415\\nShawshank Redemption, The (1994)                        4.539075  4.560625\\nGrand Day Out, A (1992)                                 4.537879  4.293255\\nTo Kill a Mockingbird (1962)                            4.536667  4.372611\\nCreature Comforts (1990)                                4.513889  4.272277\\nUsual Suspects, The (1995)                              4.513317  4.518248\\nMeasuring rating disagreement\\nSuppose you wanted to find the movies that are most divisive between male and female\\nviewers. One way is to add a column to mean_ratings containing the difference in\\nmeans, then sort by that:\\nIn [352]: mean_ratings['diff'] = mean_ratings['M'] - mean_ratings['F']\\nSorting by 'diff' gives us the movies with the greatest rating difference and which were\\npreferred by women:\\nIn [353]: sorted_by_diff = mean_ratings.sort_index(by='diff')\\nIn [354]: sorted_by_diff[:15]\\nOut[354]: \\ngender                                        F         M      diff\\nDirty Dancing (1987)                   3.790378  2.959596 -0.830782\\nJumpin' Jack Flash (1986)              3.254717  2.578358 -0.676359\\nGrease (1978)                          3.975265  3.367041 -0.608224\\nLittle Women (1994)                    3.870588  3.321739 -0.548849\\nSteel Magnolias (1989)                 3.901734  3.365957 -0.535777\\nAnastasia (1997)                       3.800000  3.281609 -0.518391\\nRocky Horror Picture Show, The (1975)  3.673016  3.160131 -0.512885\\nColor Purple, The (1985)               4.158192  3.659341 -0.498851\\nAge of Innocence, The (1993)           3.827068  3.339506 -0.487561\\nFree Willy (1993)                      2.921348  2.438776 -0.482573\\nFrench Kiss (1995)                     3.535714  3.056962 -0.478752\\nLittle Shop of Horrors, The (1960)     3.650000  3.179688 -0.470312\\nGuys and Dolls (1955)                  4.051724  3.583333 -0.468391\\nMary Poppins (1964)                    4.197740  3.730594 -0.467147\\nPatch Adams (1998)                     3.473282  3.008746 -0.464536\\n30 | Chapter 2: \\u2002Introductory Examples\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 46, 'page_label': '31'}, page_content=\"Reversing the order of the rows and again slicing off the top 15 rows, we get the movies\\npreferred by men that women didn’t rate as highly:\\n# Reverse order of rows, take first 15 rows\\nIn [355]: sorted_by_diff[::-1][:15]\\nOut[355]: \\ngender                                         F         M      diff\\nGood, The Bad and The Ugly, The (1966)  3.494949  4.221300  0.726351\\nKentucky Fried Movie, The (1977)        2.878788  3.555147  0.676359\\nDumb & Dumber (1994)                    2.697987  3.336595  0.638608\\nLongest Day, The (1962)                 3.411765  4.031447  0.619682\\nCable Guy, The (1996)                   2.250000  2.863787  0.613787\\nEvil Dead II (Dead By Dawn) (1987)      3.297297  3.909283  0.611985\\nHidden, The (1987)                      3.137931  3.745098  0.607167\\nRocky III (1982)                        2.361702  2.943503  0.581801\\nCaddyshack (1980)                       3.396135  3.969737  0.573602\\nFor a Few Dollars More (1965)           3.409091  3.953795  0.544704\\nPorky's (1981)                          2.296875  2.836364  0.539489\\nAnimal House (1978)                     3.628906  4.167192  0.538286\\nExorcist, The (1973)                    3.537634  4.067239  0.529605\\nFright Night (1985)                     2.973684  3.500000  0.526316\\nBarb Wire (1996)                        1.585366  2.100386  0.515020\\nSuppose instead you wanted the movies that elicited the most disagreement among\\nviewers, independent of gender. Disagreement can be measured by the variance or\\nstandard deviation of the ratings:\\n# Standard deviation of rating grouped by title\\nIn [356]: rating_std_by_title = data.groupby('title')['rating'].std()\\n# Filter down to active_titles\\nIn [357]: rating_std_by_title = rating_std_by_title.ix[active_titles]\\n# Order Series by value in descending order\\nIn [358]: rating_std_by_title.order(ascending=False)[:10]\\nOut[358]: \\ntitle\\nDumb & Dumber (1994)                     1.321333\\nBlair Witch Project, The (1999)          1.316368\\nNatural Born Killers (1994)              1.307198\\nTank Girl (1995)                         1.277695\\nRocky Horror Picture Show, The (1975)    1.260177\\nEyes Wide Shut (1999)                    1.259624\\nEvita (1996)                             1.253631\\nBilly Madison (1995)                     1.249970\\nFear and Loathing in Las Vegas (1998)    1.246408\\nBicentennial Man (1999)                  1.245533\\nName: rating\\nYou may have noticed that movie genres are given as a pipe-separated (|) string. If you\\nwanted to do some analysis by genre, more work would be required to transform the\\ngenre information into a more usable form. I will revisit this data later in the book to\\nillustrate such a transformation.\\nMovieLens 1M Data Set | 31\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 47, 'page_label': '32'}, page_content='US Baby Names 1880-2010\\nThe United States Social Security Administration (SSA) has made available data on the\\nfrequency of baby names from 1880 through the present. Hadley Wickham, an author\\nof several popular R packages, has often made use of this data set in illustrating data\\nmanipulation in R.\\nIn [4]: names.head(10)\\nOut[4]:\\n        name sex  births  year\\n0       Mary   F    7065  1880\\n1       Anna   F    2604  1880\\n2       Emma   F    2003  1880\\n3  Elizabeth   F    1939  1880\\n4     Minnie   F    1746  1880\\n5   Margaret   F    1578  1880\\n6        Ida   F    1472  1880\\n7      Alice   F    1414  1880\\n8     Bertha   F    1320  1880\\n9      Sarah   F    1288  1880\\nThere are many things you might want to do with the data set:\\n• Visualize the proportion of babies given a particular name (your own, or another\\nname) over time.\\n• Determine the relative rank of a name.\\n• Determine the most popular names in each year or the names with largest increases\\nor decreases.\\n• Analyze trends in names: vowels, consonants, length, overall diversity, changes in\\nspelling, first and last letters\\n• Analyze external sources of trends: biblical names, celebrities, demographic\\nchanges\\nUsing the tools we’ve looked at so far, most of these kinds of analyses are very straight-\\nforward, so I will walk you through many of them. I encourage you to download and\\nexplore the data yourself. If you find an interesting pattern in the data, I would love to\\nhear about it.\\nAs of this writing, the US Social Security Administration makes available data files, one\\nper year, containing the total number of births for each sex/name combination. The\\nraw archive of these files can be obtained here:\\nhttp://www.ssa.gov/oact/babynames/limits.html\\nIn the event that this page has been moved by the time you’re reading this, it can most\\nlikely be located again by Internet search. After downloading the “National data” file\\nnames.zip and unzipping it, you will have a directory containing a series of files like\\nyob1880.txt. I use the UNIX head command to look at the first 10 lines of one of the\\nfiles (on Windows, you can use the more command or open it in a text editor):\\n32 | Chapter 2: \\u2002Introductory Examples\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 48, 'page_label': '33'}, page_content=\"In [367]: !head -n 10 names/yob1880.txt\\nMary,F,7065\\nAnna,F,2604\\nEmma,F,2003\\nElizabeth,F,1939\\nMinnie,F,1746\\nMargaret,F,1578\\nIda,F,1472\\nAlice,F,1414\\nBertha,F,1320\\nSarah,F,1288\\nAs this is a nicely comma-separated form, it can be loaded into a DataFrame with\\npandas.read_csv:\\nIn [368]: import pandas as pd\\nIn [369]: names1880 = pd.read_csv('names/yob1880.txt', names=['name', 'sex', 'births'])\\nIn [370]: names1880\\nOut[370]: \\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 2000 entries, 0 to 1999\\nData columns:\\nname      2000  non-null values\\nsex       2000  non-null values\\nbirths    2000  non-null values\\ndtypes: int64(1), object(2)\\nThese files only contain names with at least 5 occurrences in each year, so for simplic-\\nity’s sake we can use the sum of the births column by sex as the total number of births\\nin that year:\\nIn [371]: names1880.groupby('sex').births.sum()\\nOut[371]: \\nsex\\nF       90993\\nM      110493\\nName: births\\nSince the data set is split into files by year, one of the first things to do is to assemble\\nall of the data into a single DataFrame and further to add a year field. This is easy to\\ndo using pandas.concat:\\n# 2010 is the last available year right now\\nyears = range(1880, 2011)\\npieces = []\\ncolumns = ['name', 'sex', 'births']\\nfor year in years:\\n    path = 'names/yob%d.txt' % year\\n    frame = pd.read_csv(path, names=columns)\\n    frame['year'] = year\\n    pieces.append(frame)\\nUS Baby Names 1880-2010 | 33\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 49, 'page_label': '34'}, page_content=\"# Concatenate everything into a single DataFrame\\nnames = pd.concat(pieces, ignore_index=True)\\nThere are a couple things to note here. First, remember that concat glues the DataFrame\\nobjects together row-wise by default. Secondly, you have to pass ignore_index=True\\nbecause we’re not interested in preserving the original row numbers returned from\\nread_csv. So we now have a very large DataFrame containing all of the names data:\\nNow the names DataFrame looks like:\\nIn [373]: names\\nOut[373]: \\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 1690784 entries, 0 to 1690783\\nData columns:\\nname      1690784  non-null values\\nsex       1690784  non-null values\\nbirths    1690784  non-null values\\nyear      1690784  non-null values\\ndtypes: int64(2), object(2)\\nWith this data in hand, we can already start aggregating the data at the year and sex\\nlevel using groupby or pivot_table, see Figure 2-4:\\nIn [374]: total_births = names.pivot_table('births', rows='year',\\n   .....:                                  cols='sex', aggfunc=sum)\\nIn [375]: total_births.tail()\\nOut[375]: \\nsex         F        M\\nyear                  \\n2006  1896468  2050234\\n2007  1916888  2069242\\n2008  1883645  2032310\\n2009  1827643  1973359\\n2010  1759010  1898382\\nIn [376]: total_births.plot(title='Total births by sex and year')\\nNext, let’s insert a column prop with the fraction of babies given each name relative to\\nthe total number of births. A prop value of 0.02 would indicate that 2 out of every 100\\nbabies was given a particular name. Thus, we group the data by year and sex, then add\\nthe new column to each group:\\ndef add_prop(group):\\n    # Integer division floors\\n    births = group.births.astype(float)\\n    group['prop'] = births / births.sum()\\n    return group\\nnames = names.groupby(['year', 'sex']).apply(add_prop)\\n34 | Chapter 2: \\u2002Introductory Examples\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 50, 'page_label': '35'}, page_content=\"Remember that because births is of integer type, we have to cast either\\nthe numerator or denominator to floating point to compute a fraction\\n(unless you are using Python 3!).\\nThe resulting complete data set now has the following columns:\\nIn [378]: names\\nOut[378]: \\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 1690784 entries, 0 to 1690783\\nData columns:\\nname      1690784  non-null values\\nsex       1690784  non-null values\\nbirths    1690784  non-null values\\nyear      1690784  non-null values\\nprop      1690784  non-null values\\ndtypes: float64(1), int64(2), object(2)\\nWhen performing a group operation like this, it's often valuable to do a sanity check,\\nlike verifying that the prop column sums to 1 within all the groups. Since this is floating\\npoint data, use np.allclose to check that the group sums are sufficiently close to (but\\nperhaps not exactly equal to) 1:\\nIn [379]: np.allclose(names.groupby(['year', 'sex']).prop.sum(), 1)\\nOut[379]: True\\nNow that this is done, I’m going to extract a subset of the data to facilitate further\\nanalysis: the top 1000 names for each sex/year combination. This is yet another group\\noperation:\\ndef get_top1000(group):\\n    return group.sort_index(by='births', ascending=False)[:1000]\\nFigure 2-4. Total births by sex and year\\nUS Baby Names 1880-2010 | 35\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 51, 'page_label': '36'}, page_content='grouped = names.groupby([\\'year\\', \\'sex\\'])\\ntop1000 = grouped.apply(get_top1000)\\nIf you prefer a do-it-yourself approach, you could also do:\\npieces = []\\nfor year, group in names.groupby([\\'year\\', \\'sex\\']):\\n    pieces.append(group.sort_index(by=\\'births\\', ascending=False)[:1000])\\ntop1000 = pd.concat(pieces, ignore_index=True)\\nThe resulting data set is now quite a bit smaller:\\nIn [382]: top1000\\nOut[382]: \\n<class \\'pandas.core.frame.DataFrame\\'>\\nInt64Index: 261877 entries, 0 to 261876\\nData columns:\\nname      261877  non-null values\\nsex       261877  non-null values\\nbirths    261877  non-null values\\nyear      261877  non-null values\\nprop      261877  non-null values\\ndtypes: float64(1), int64(2), object(2)\\nWe’ll use this Top 1,000 data set in the following investigations into the data.\\nAnalyzing Naming Trends\\nWith the full data set and Top 1,000 data set in hand, we can start analyzing various\\nnaming trends of interest. Splitting the Top 1,000 names into the boy and girl portions\\nis easy to do first:\\nIn [383]: boys = top1000[top1000.sex == \\'M\\']\\nIn [384]: girls = top1000[top1000.sex == \\'F\\']\\nSimple time series, like the number of Johns or Marys for each year can be plotted but\\nrequire a bit of munging to be a bit more useful. Let’s form a pivot table of the total\\nnumber of births by year and name:\\nIn [385]: total_births = top1000.pivot_table(\\'births\\', rows=\\'year\\', cols=\\'name\\',\\n   .....:                                    aggfunc=sum)\\nNow, this can be plotted for a handful of names using DataFrame’s plot method:\\nIn [386]: total_births\\nOut[386]: \\n<class \\'pandas.core.frame.DataFrame\\'>\\nInt64Index: 131 entries, 1880 to 2010\\nColumns: 6865 entries, Aaden to Zuri\\ndtypes: float64(6865)\\nIn [387]: subset = total_births[[\\'John\\', \\'Harry\\', \\'Mary\\', \\'Marilyn\\']]\\nIn [388]: subset.plot(subplots=True, figsize=(12, 10), grid=False,\\n   .....:             title=\"Number of births per year\")\\n36 | Chapter 2: \\u2002Introductory Examples\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 52, 'page_label': '37'}, page_content=\"See Figure 2-5 for the result. On looking at this, you might conclude that these names\\nhave grown out of favor with the American population. But the story is actually more\\ncomplicated than that, as will be explored in the next section.\\nFigure 2-5. A few boy and girl names over time\\nMeasuring the increase in naming diversity\\nOne explanation for the decrease in plots above is that fewer parents are choosing\\ncommon names for their children. This hypothesis can be explored and confirmed in\\nthe data. One measure is the proportion of births represented by the top 1000 most\\npopular names, which I aggregate and plot by year and sex:\\nIn [390]: table = top1000.pivot_table('prop', rows='year',\\n   .....:                             cols='sex', aggfunc=sum)\\nIn [391]: table.plot(title='Sum of table1000.prop by year and sex',\\n   .....:            yticks=np.linspace(0, 1.2, 13), xticks=range(1880, 2020, 10))\\nSee Figure 2-6 for this plot. So you can see that, indeed, there appears to be increasing\\nname diversity (decreasing total proportion in the top 1,000). Another interesting met-\\nric is the number of distinct names, taken in order of popularity from highest to lowest,\\nin the top 50% of births. This number is a bit more tricky to compute. Let’s consider\\njust the boy names from 2010:\\nIn [392]: df = boys[boys.year == 2010]\\nIn [393]: df\\nOut[393]: \\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 1000 entries, 260877 to 261876\\nData columns:\\nUS Baby Names 1880-2010 | 37\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 53, 'page_label': '38'}, page_content=\"name      1000  non-null values\\nsex       1000  non-null values\\nbirths    1000  non-null values\\nyear      1000  non-null values\\nprop      1000  non-null values\\ndtypes: float64(1), int64(2), object(2)\\nFigure 2-6. Proportion of births represented in top 1000 names by sex\\nAfter sorting prop in descending order, we want to know how many of the most popular\\nnames it takes to reach 50%. You could write a for loop to do this, but a vectorized\\nNumPy way is a bit more clever. Taking the cumulative sum, cumsum, of prop then calling\\nthe method searchsorted returns the position in the cumulative sum at which 0.5 would\\nneed to be inserted to keep it in sorted order:\\nIn [394]: prop_cumsum = df.sort_index(by='prop', ascending=False).prop.cumsum()\\nIn [395]: prop_cumsum[:10]\\nOut[395]: \\n260877    0.011523\\n260878    0.020934\\n260879    0.029959\\n260880    0.038930\\n260881    0.047817\\n260882    0.056579\\n260883    0.065155\\n260884    0.073414\\n260885    0.081528\\n260886    0.089621\\nIn [396]: prop_cumsum.searchsorted(0.5)\\nOut[396]: 116\\n38 | Chapter 2: \\u2002Introductory Examples\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 54, 'page_label': '39'}, page_content='Since arrays are zero-indexed, adding 1 to this result gives you a result of 117. By con-\\ntrast, in 1900 this number was much smaller:\\nIn [397]: df = boys[boys.year == 1900]\\nIn [398]: in1900 = df.sort_index(by=\\'prop\\', ascending=False).prop.cumsum()\\nIn [399]: in1900.searchsorted(0.5) + 1\\nOut[399]: 25\\nIt should now be fairly straightforward to apply this operation to each year/sex com-\\nbination; groupby those fields and apply a function returning the count for each group:\\ndef get_quantile_count(group, q=0.5):\\n    group = group.sort_index(by=\\'prop\\', ascending=False)\\n    return group.prop.cumsum().searchsorted(q) + 1\\ndiversity = top1000.groupby([\\'year\\', \\'sex\\']).apply(get_quantile_count)\\ndiversity = diversity.unstack(\\'sex\\')\\nThis resulting DataFrame diversity now has two time series, one for each sex, indexed\\nby year. This can be inspected in IPython and plotted as before (see Figure 2-7):\\nIn [401]: diversity.head()\\nOut[401]: \\nsex    F   M\\nyear        \\n1880  38  14\\n1881  38  14\\n1882  38  15\\n1883  39  15\\n1884  39  16\\nIn [402]: diversity.plot(title=\"Number of popular names in top 50%\")\\nFigure 2-7. Plot of diversity metric by year\\nUS Baby Names 1880-2010 | 39\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 55, 'page_label': '40'}, page_content=\"As you can see, girl names have always been more diverse than boy names, and they\\nhave only become more so over time. Further analysis of what exactly is driving the\\ndiversity, like the increase of alternate spellings, is left to the reader.\\nThe “Last letter” Revolution\\nIn 2007, a baby name researcher Laura Wattenberg pointed out on her website ( http:\\n//www.babynamewizard.com) that the distribution of boy names by final letter has\\nchanged significantly over the last 100 years. To see this, I first aggregate all of the births\\nin the full data set by year, sex, and final letter:\\n# extract last letter from name column\\nget_last_letter = lambda x: x[-1]\\nlast_letters = names.name.map(get_last_letter)\\nlast_letters.name = 'last_letter'\\ntable = names.pivot_table('births', rows=last_letters,\\n                          cols=['sex', 'year'], aggfunc=sum)\\nThen, I select out three representative years spanning the history and print the first few\\nrows:\\nIn [404]: subtable = table.reindex(columns=[1910, 1960, 2010], level='year')\\nIn [405]: subtable.head()\\nOut[405]: \\nsex               F                      M                \\nyear           1910    1960    2010   1910    1960    2010\\nlast_letter                                               \\na            108376  691247  670605    977    5204   28438\\nb               NaN     694     450    411    3912   38859\\nc                 5      49     946    482   15476   23125\\nd              6750    3729    2607  22111  262112   44398\\ne            133569  435013  313833  28655  178823  129012\\nNext, normalize the table by total births to compute a new table containing proportion\\nof total births for each sex ending in each letter:\\nIn [406]: subtable.sum()\\nOut[406]: \\nsex  year\\nF    1910     396416\\n     1960    2022062\\n     2010    1759010\\nM    1910     194198\\n     1960    2132588\\n     2010    1898382\\nIn [407]: letter_prop = subtable / subtable.sum().astype(float)\\nWith the letter proportions now in hand, I can make bar plots for each sex broken\\ndown by year. See Figure 2-8:\\nimport matplotlib.pyplot as plt\\n40 | Chapter 2: \\u2002Introductory Examples\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 56, 'page_label': '41'}, page_content=\"fig, axes = plt.subplots(2, 1, figsize=(10, 8))\\nletter_prop['M'].plot(kind='bar', rot=0, ax=axes[0], title='Male')\\nletter_prop['F'].plot(kind='bar', rot=0, ax=axes[1], title='Female',\\n                      legend=False)\\nFigure 2-8. Proportion of boy and girl names ending in each letter\\nAs you can see, boy names ending in “n” have experienced significant growth since the\\n1960s. Going back to the full table created above, I again normalize by year and sex\\nand select a subset of letters for the boy names, finally transposing to make each column\\na time series:\\nIn [410]: letter_prop = table / table.sum().astype(float)\\nIn [411]: dny_ts = letter_prop.ix[['d', 'n', 'y'], 'M'].T\\nIn [412]: dny_ts.head()\\nOut[412]: \\n             d         n         y\\nyear                              \\n1880  0.083055  0.153213  0.075760\\n1881  0.083247  0.153214  0.077451\\n1882  0.085340  0.149560  0.077537\\n1883  0.084066  0.151646  0.079144\\n1884  0.086120  0.149915  0.080405\\nWith this DataFrame of time series in hand, I can make a plot of the trends over time\\nagain with its plot method (see Figure 2-9):\\nIn [414]: dny_ts.plot()\\nUS Baby Names 1880-2010 | 41\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 57, 'page_label': '42'}, page_content=\"Figure 2-9. Proportion of boys born with names ending in d/n/y over time\\nBoy names that became girl names (and vice versa)\\nAnother fun trend is looking at boy names that were more popular with one sex earlier\\nin the sample but have “changed sexes” in the present. One example is the name Lesley\\nor Leslie. Going back to the top1000 dataset, I compute a list of names occurring in the\\ndataset starting with 'lesl':\\nIn [415]: all_names = top1000.name.unique()\\nIn [416]: mask = np.array(['lesl' in x.lower() for x in all_names])\\nIn [417]: lesley_like = all_names[mask]\\nIn [418]: lesley_like\\nOut[418]: array([Leslie, Lesley, Leslee, Lesli, Lesly], dtype=object)\\nFrom there, we can filter down to just those names and sum births grouped by name\\nto see the relative frequencies:\\nIn [419]: filtered = top1000[top1000.name.isin(lesley_like)]\\nIn [420]: filtered.groupby('name').births.sum()\\nOut[420]: \\nname\\nLeslee      1082\\nLesley     35022\\nLesli        929\\nLeslie    370429\\nLesly      10067\\nName: births\\nNext, let’s aggregate by sex and year and normalize within year:\\n42 | Chapter 2: \\u2002Introductory Examples\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 58, 'page_label': '43'}, page_content=\"In [421]: table = filtered.pivot_table('births', rows='year',\\n   .....:                              cols='sex', aggfunc='sum')\\nIn [422]: table = table.div(table.sum(1), axis=0)\\nIn [423]: table.tail()\\nOut[423]: \\nsex   F   M\\nyear       \\n2006  1 NaN\\n2007  1 NaN\\n2008  1 NaN\\n2009  1 NaN\\n2010  1 NaN\\nLastly, it’s now easy to make a plot of the breakdown by sex over time (Figure 2-10):\\nIn [425]: table.plot(style={'M': 'k-', 'F': 'k--'})\\nFigure 2-10. Proportion of male/female Lesley-like names over time\\nConclusions and The Path Ahead\\nThe examples in this chapter are rather simple, but they’re here to give you a bit of a\\nflavor of what sorts of things you can expect in the upcoming chapters. The focus of\\nthis book is on tools as opposed to presenting more sophisticated analytical methods.\\nMastering the techniques in this book will enable you to implement your own analyses\\n(assuming you know what you want to do!) in short order.\\nConclusions and The Path Ahead | 43\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 59, 'page_label': '44'}, page_content='www.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 60, 'page_label': '45'}, page_content='CHAPTER 3\\nIPython: An Interactive Computing and\\nDevelopment Environment\\nAct without doing; work without effort. Think of the small as large and the few as many.\\nConfront the difficult while it is still easy; accomplish the great task by a series of small\\nacts.\\n—Laozi\\nPeople often ask me, “What is your Python development environment?” My answer is\\nalmost always the same, “IPython and a text editor”. You may choose to substitute an\\nIntegrated Development Environment (IDE) for a text editor in order to take advantage\\nof more advanced graphical tools and code completion capabilities. Even if so, I strongly\\nrecommend making IPython an important part of your workflow. Some IDEs even\\nprovide IPython integration, so it’s possible to get the best of both worlds.\\nThe IPython project began in 2001 as Fernando Pérez’s side project to make a better\\ninteractive Python interpreter. In the subsequent 11 years it has grown into what’s\\nwidely considered one of the most important tools in the modern scientific Python\\ncomputing stack. While it does not provide any computational or data analytical tools\\nby itself, IPython is designed from the ground up to maximize your productivity in both\\ninteractive computing and software development. It encourages an execute-explore\\nworkflow instead of the typical edit-compile-run workflow of many other programming\\nlanguages. It also provides very tight integration with the operating system’s shell and\\nfile system. Since much of data analysis coding involves exploration, trial and error,\\nand iteration, IPython will, in almost all cases, help you get the job done faster.\\nOf course, the IPython project now encompasses a great deal more than just an en-\\nhanced, interactive Python shell. It also includes a rich GUI console with inline plotting,\\na web-based interactive notebook format, and a lightweight, fast parallel computing\\nengine. And, as with so many other tools designed for and by programmers, it is highly\\ncustomizable. I’ll discuss some of these features later in the chapter.\\n45\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 61, 'page_label': '46'}, page_content='Since IPython has interactivity at its core, some of the features in this chapter are dif-\\nficult to fully illustrate without a live console. If this is your first time learning about\\nIPython, I recommend that you follow along with the examples to get a feel for how\\nthings work. As with any keyboard-driven console-like environment, developing mus-\\ncle-memory for the common commands is part of the learning curve.\\nMany parts of this chapter (for example: profiling and debugging) can\\nbe safely omitted on a first reading as they are not necessary for under-\\nstanding the rest of the book. This chapter is intended to provide a\\nstandalone, rich overview of the functionality provided by IPython.\\nIPython Basics\\nYou can launch IPython on the command line just like launching the regular Python\\ninterpreter except with the ipython command:\\n$ ipython\\nPython 2.7.2 (default, May 27 2012, 21:26:12)\\nType \"copyright\", \"credits\" or \"license\" for more information.\\nIPython 0.12 -- An enhanced Interactive Python.\\n?         -> Introduction and overview of IPython\\'s features.\\n%quickref -> Quick reference.\\nhelp      -> Python\\'s own help system.\\nobject?   -> Details about \\'object\\', use \\'object??\\' for extra details.\\nIn [1]: a = 5\\nIn [2]: a\\nOut[2]: 5\\nYou can execute arbitrary Python statements by typing them in and pressing\\n<return>. When typing just a variable into IPython, it renders a string representation\\nof the object:\\nIn [542]: data = {i : randn() for i in range(7)}\\nIn [543]: data\\nOut[543]: \\n{0: 0.6900018528091594,\\n 1: 1.0015434424937888,\\n 2: -0.5030873913603446,\\n 3: -0.6222742250596455,\\n 4: -0.9211686080130108,\\n 5: -0.726213492660829,\\n 6: 0.2228955458351768}\\n46 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 62, 'page_label': '47'}, page_content='Many kinds of Python objects are formatted to be more readable, or pretty-printed,\\nwhich is distinct from normal printing with print. If you printed a dict like the above\\nin the standard Python interpreter, it would be much less readable:\\n>>> from numpy.random import randn\\n>>> data = {i : randn() for i in range(7)}\\n>>> print data\\n{0: -1.5948255432744511, 1: 0.10569006472787983, 2: 1.972367135977295,\\n3: 0.15455217573074576, 4: -0.24058577449429575, 5: -1.2904897053651216,\\n6: 0.3308507317325902}\\nIPython also provides facilities to make it easy to execute arbitrary blocks of code (via\\nsomewhat glorified copy-and-pasting) and whole Python scripts. These will be dis-\\ncussed shortly.\\nTab Completion\\nOn the surface, the IPython shell looks like a cosmetically slightly-different interactive\\nPython interpreter. Users of Mathematica may find the enumerated input and output\\nprompts familiar. One of the major improvements over the standard Python shell is\\ntab completion , a feature common to most interactive data analysis environments.\\nWhile entering expressions in the shell, pressing <Tab> will search the namespace for\\nany variables (objects, functions, etc.) matching the characters you have typed so far:\\nIn [1]: an_apple = 27\\nIn [2]: an_example = 42\\nIn [3]: an<Tab>\\nan_apple    and         an_example  any\\nIn this example, note that IPython displayed both the two variables I defined as well as\\nthe Python keyword and and built-in function any. Naturally, you can also complete\\nmethods and attributes on any object after typing a period:\\nIn [3]: b = [1, 2, 3]\\nIn [4]: b.<Tab>\\nb.append   b.extend   b.insert   b.remove   b.sort\\nb.count    b.index    b.pop      b.reverse\\nThe same goes for modules:\\nIn [1]: import datetime\\nIn [2]: datetime.<Tab>\\ndatetime.date           datetime.MAXYEAR        datetime.timedelta\\ndatetime.datetime       datetime.MINYEAR        datetime.tzinfo\\ndatetime.datetime_CAPI  datetime.time\\nIPython Basics | 47\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 63, 'page_label': '48'}, page_content='Note that IPython by default hides methods and attributes starting with\\nunderscores, such as magic methods and internal “private” methods\\nand attributes, in order to avoid cluttering the display (and confusing\\nnew Python users!). These, too, can be tab-completed but you must first\\ntype an underscore to see them. If you prefer to always see such methods\\nin tab completion, you can change this setting in the IPython configu-\\nration.\\nTab completion works in many contexts outside of searching the interactive namespace\\nand completing object or module attributes.When typing anything that looks like a file\\npath (even in a Python string), pressing <Tab> will complete anything on your com-\\nputer’s file system matching what you’ve typed:\\nIn [3]: book_scripts/<Tab>\\nbook_scripts/cprof_example.py        book_scripts/ipython_script_test.py\\nbook_scripts/ipython_bug.py          book_scripts/prof_mod.py\\nIn [3]: path = \\'book_scripts/<Tab>\\nbook_scripts/cprof_example.py        book_scripts/ipython_script_test.py\\nbook_scripts/ipython_bug.py          book_scripts/prof_mod.py\\nCombined with the %run command (see later section), this functionality will undoubt-\\nedly save you many keystrokes.\\nAnother area where tab completion saves time is in the completion of function keyword\\narguments (including the = sign!).\\nIntrospection\\nUsing a question mark (?) before or after a variable will display some general informa-\\ntion about the object:\\nIn [545]: b?\\nType:       list\\nString Form:[1, 2, 3]\\nLength:     3\\nDocstring:\\nlist() -> new empty list\\nlist(iterable) -> new list initialized from iterable\\'s items\\nThis is referred to as object introspection. If the object is a function or instance method,\\nthe docstring, if defined, will also be shown. Suppose we’d written the following func-\\ntion:\\ndef add_numbers(a, b):\\n    \"\"\"\\n    Add two numbers together\\n    Returns\\n    -------\\n    the_sum : type of arguments\\n48 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 64, 'page_label': '49'}, page_content='\"\"\"\\n    return a + b\\nThen using ? shows us the docstring:\\nIn [547]: add_numbers?\\nType:       function\\nString Form:<function add_numbers at 0x5fad848>\\nFile:       book_scripts/<ipython-input-546-5473012eeb65>\\nDefinition: add_numbers(a, b)\\nDocstring:\\nAdd two numbers together\\nReturns\\n-------\\nthe_sum : type of arguments\\nUsing ?? will also show the function’s source code if possible:\\nIn [548]: add_numbers??\\nType:       function\\nString Form:<function add_numbers at 0x5fad848>\\nFile:       book_scripts/<ipython-input-546-5473012eeb65>\\nDefinition: add_numbers(a, b)\\nSource:\\ndef add_numbers(a, b):\\n    \"\"\"\\n    Add two numbers together\\n    Returns\\n    -------\\n    the_sum : type of arguments\\n    \"\"\"\\n    return a + b\\n? has a final usage, which is for searching the IPython namespace in a manner similar\\nto the standard UNIX or Windows command line. A number of characters combined\\nwith the wildcard ( *) will show all names matching the wildcard expression. For ex-\\nample, we could get a list of all functions in the top level NumPy namespace containing\\nload:\\nIn [549]: np.*load*?\\nnp.load\\nnp.loads\\nnp.loadtxt\\nnp.pkgload\\nThe %run Command\\nAny file can be run as a Python program inside the environment of your IPython session\\nusing the %run command. Suppose you had the following simple script stored in ipy\\nthon_script_test.py:\\ndef f(x, y, z):\\n    return (x + y) / z\\na = 5\\nIPython Basics | 49\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 65, 'page_label': '50'}, page_content='b = 6\\nc = 7.5\\nresult = f(a, b, c)\\nThis can be executed by passing the file name to %run:\\nIn [550]: %run ipython_script_test.py\\nThe script is run in an empty namespace (with no imports or other variables defined)\\nso that the behavior should be identical to running the program on the command line\\nusing python script.py. All of the variables (imports, functions, and globals) defined\\nin the file (up until an exception, if any, is raised) will then be accessible in the IPython\\nshell:\\nIn [551]: c\\nOut[551]: 7.5\\nIn [552]: result\\nOut[552]: 1.4666666666666666\\nIf a Python script expects command line arguments (to be found in sys.argv), these\\ncan be passed after the file path as though run on the command line.\\nShould you wish to give a script access to variables already defined in\\nthe interactive IPython namespace, use %run -i instead of plain %run.\\nInterrupting running code\\nPressing <Ctrl-C> while any code is running, whether a script through %run or a long-\\nrunning command, will cause a KeyboardInterrupt to be raised. This will cause nearly\\nall Python programs to stop immediately except in very exceptional cases.\\nWhen a piece of Python code has called into some compiled extension\\nmodules, pressing <Ctrl-C> will not cause the program execution to stop\\nimmediately in all cases. In such cases, you will have to either wait until\\ncontrol is returned to the Python interpreter, or, in more dire circum-\\nstances, forcibly terminate the Python process via the OS task manager.\\nExecuting Code from the Clipboard\\nA quick-and-dirty way to execute code in IPython is via pasting from the clipboard.\\nThis might seem fairly crude, but in practice it is very useful. For example, while de-\\nveloping a complex or time-consuming application, you may wish to execute a script\\npiece by piece, pausing at each stage to examine the currently loaded data and results.\\nOr, you might find a code snippet on the Internet that you want to run and play around\\nwith, but you’d rather not create a new .py file for it.\\n50 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 66, 'page_label': '51'}, page_content=\"Code snippets can be pasted from the clipboard in many cases by pressing <Ctrl-Shift-\\nV>. Note that it is not completely robust as this mode of pasting mimics typing each\\nline into IPython, and line breaks are treated as <return>. This means that if you paste\\ncode with an indented block and there is a blank line, IPython will think that the in-\\ndented block is over. Once the next line in the block is executed, an IndentationEr\\nror will be raised. For example the following code:\\nx = 5\\ny = 7\\nif x > 5:\\n    x += 1\\n    y = 8\\nwill not work if simply pasted:\\nIn [1]: x = 5\\nIn [2]: y = 7\\nIn [3]: if x > 5:\\n   ...:         x += 1\\n   ...:\\nIn [4]:     y = 8\\nIndentationError: unexpected indent\\nIf you want to paste code into IPython, try the %paste and %cpaste\\nmagic functions.\\nAs the error message suggests, we should instead use the %paste and %cpaste magic\\nfunctions. %paste takes whatever text is in the clipboard and executes it as a single block\\nin the shell:\\nIn [6]: %paste\\nx = 5\\ny = 7\\nif x > 5:\\n    x += 1\\n    y = 8\\n## -- End pasted text --\\nDepending on your platform and how you installed Python, there’s a\\nsmall chance that %paste will not work. Packaged distributions like\\nEPDFree (as described in in the intro) should not be a problem.\\n%cpaste is similar, except that it gives you a special prompt for pasting code into:\\nIn [7]: %cpaste\\nPasting code; enter '--' alone on the line to stop or use Ctrl-D.\\n:x = 5\\n:y = 7\\n:if x > 5:\\nIPython Basics | 51\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 67, 'page_label': '52'}, page_content=':    x += 1\\n:\\n:    y = 8\\n:--\\nWith the %cpaste block, you have the freedom to paste as much code as you like before\\nexecuting it. You might decide to use %cpaste in order to look at the pasted code before\\nexecuting it. If you accidentally paste the wrong code, you can break out of the \\n%cpaste prompt by pressing <Ctrl-C>.\\nLater, I’ll introduce the IPython HTML Notebook which brings a new level of sophis-\\ntication for developing analyses block-by-block in a browser-based notebook format\\nwith executable code cells.\\nIPython interaction with editors and IDEs\\nSome text editors, such as Emacs and vim, have 3rd party extensions enabling blocks\\nof code to be sent directly from the editor to a running IPython shell. Refer to the\\nIPython website or do an Internet search to find out more.\\nSome IDEs, such as the PyDev plugin for Eclipse and Python Tools for Visual Studio\\nfrom Microsoft (and possibly others), have integration with the IPython terminal ap-\\nplication. If you want to work in an IDE but don’t want to give up the IPython console\\nfeatures, this may be a good option for you.\\nKeyboard Shortcuts\\nIPython has many keyboard shortcuts for navigating the prompt (which will be familiar\\nto users of the Emacs text editor or the UNIX bash shell) and interacting with the shell’s\\ncommand history (see later section). Table 3-1 summarizes some of the most commonly\\nused shortcuts. See Figure 3-1 for an illustration of a few of these, such as cursor move-\\nment.\\nFigure 3-1. Illustration of some of IPython’s keyboard shortcuts\\n52 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 68, 'page_label': '53'}, page_content='Table 3-1. Standard IPython Keyboard Shortcuts\\nCommand Description\\nCtrl-P or up-arrow Search backward in command history for commands starting with currently-entered text\\nCtrl-N or down-arrow Search forward in command history for commands starting with currently-entered text\\nCtrl-R Readline-style reverse history search (partial matching)\\nCtrl-Shift-V Paste text from clipboard\\nCtrl-C Interrupt currently-executing code\\nCtrl-A Move cursor to beginning of line\\nCtrl-E Move cursor to end of line\\nCtrl-K Delete text from cursor until end of line\\nCtrl-U Discard all text on current line\\nCtrl-F Move cursor forward one character\\nCtrl-B Move cursor back one character\\nCtrl-L Clear screen\\nExceptions and Tracebacks\\nIf an exception is raised while %run-ing a script or executing any statement, IPython will\\nby default print a full call stack trace (traceback) with a few lines of context around the\\nposition at each point in the stack.\\nIn [553]: %run ch03/ipython_bug.py\\n---------------------------------------------------------------------------\\nAssertionError                            Traceback (most recent call last)\\n/home/wesm/code/ipython/IPython/utils/py3compat.pyc in execfile(fname, *where)\\n    176             else:\\n    177                 filename = fname\\n--> 178             __builtin__.execfile(filename, *where)\\nbook_scripts/ch03/ipython_bug.py in <module>()\\n     13     throws_an_exception()\\n     14 \\n---> 15 calling_things()\\nbook_scripts/ch03/ipython_bug.py in calling_things()\\n     11 def calling_things():\\n     12     works_fine()\\n---> 13     throws_an_exception()\\n     14 \\n     15 calling_things()\\nbook_scripts/ch03/ipython_bug.py in throws_an_exception()\\n      7     a = 5\\n      8     b = 6\\n----> 9     assert(a + b == 10)\\n     10 \\n     11 def calling_things():\\nAssertionError:\\nIPython Basics | 53\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 69, 'page_label': '54'}, page_content=\"Having additional context by itself is a big advantage over the standard Python inter-\\npreter (which does not provide any additional context). The amount of context shown\\ncan be controlled using the %xmode magic command, from minimal (same as the stan-\\ndard Python interpreter) to verbose (which inlines function argument values and more).\\nAs you will see later in the chapter, you can step into the stack (using the %debug or \\n%pdb magics) after an error has occurred for interactive post-mortem debugging.\\nMagic Commands\\nIPython has many special commands, known as “magic” commands, which are de-\\nsigned to faciliate common tasks and enable you to easily control the behavior of the\\nIPython system. A magic command is any command prefixed by the the percent symbol\\n%. For example, you can check the execution time of any Python statement, such as a\\nmatrix multiplication, using the %timeit magic function (which will be discussed in\\nmore detail later):\\nIn [554]: a = np.random.randn(100, 100)\\nIn [555]: %timeit np.dot(a, a)\\n10000 loops, best of 3: 69.1 us per loop\\nMagic commands can be viewed as command line programs to be run within the IPy-\\nthon system. Many of them have additional “command line” options, which can all be\\nviewed (as you might expect) using ?:\\nIn [1]: %reset?\\nResets the namespace by removing all names defined by the user.\\nParameters\\n----------\\n  -f : force reset without asking for confirmation.\\n  -s : 'Soft' reset: Only clears your namespace, leaving history intact.\\n  References to objects may be kept. By default (without this option),\\n  we do a 'hard' reset, giving you a new session and removing all\\n  references to objects from the current session.\\nExamples\\n--------\\nIn [6]: a = 1\\nIn [7]: a\\nOut[7]: 1\\nIn [8]: 'a' in _ip.user_ns\\nOut[8]: True\\nIn [9]: %reset -f\\nIn [1]: 'a' in _ip.user_ns\\nOut[1]: False\\n54 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 70, 'page_label': '55'}, page_content='Magic functions can be used by default without the percent sign, as long as no variable\\nis defined with the same name as the magic function in question. This feature is called\\nautomagic and can be enabled or disabled using %automagic.\\nSince IPython’s documentation is easily accessible from within the system, I encourage\\nyou to explore all of the special commands available by typing %quickref or %magic. I\\nwill highlight a few more of the most critical ones for being productive in interactive\\ncomputing and Python development in IPython.\\nTable 3-2. Frequently-used IPython Magic Commands\\nCommand Description\\n%quickref Display the IPython Quick Reference Card\\n%magic Display detailed documentation for all of the available magic commands\\n%debug Enter the interactive debugger at the bottom of the last exception traceback\\n%hist Print command input (and optionally output) history\\n%pdb Automatically enter debugger after any exception\\n%paste Execute pre-formatted Python code from clipboard\\n%cpaste Open a special prompt for manually pasting Python code to be executed\\n%reset Delete all variables / names defined in interactive namespace\\n%page OBJECT Pretty print the object and display it through a pager\\n%run script.py Run a Python script inside IPython\\n%prun statement Execute statement with cProfile and report the profiler output\\n%time statement Report the execution time of single statement\\n%timeit statement Run a statement multiple times to compute an emsemble average execution time. Useful for\\ntiming code with very short execution time\\n%who, %who_ls, %whos Display variables defined in interactive namespace, with varying levels of information / verbosity\\n%xdel variable Delete a variable and attempt to clear any references to the object in the IPython internals\\nQt-based Rich GUI Console\\nThe IPython team has developed a Qt framework-based GUI console, designed to wed\\nthe features of the terminal-only applications with the features provided by a rich text\\nwidget, like embedded images, multiline editing, and syntax highlighting. If you have\\neither PyQt or PySide installed, the application can be launched with inline plotting by\\nrunning this on the command line:\\nipython qtconsole --pylab=inline\\nThe Qt console can launch multiple IPython processes in tabs, enabling you to switch\\nbetween tasks. It can also share a process with the IPython HTML Notebook applica-\\ntion, which I’ll highlight later.\\nIPython Basics | 55\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 71, 'page_label': '56'}, page_content='Matplotlib Integration and Pylab Mode\\nPart of why IPython is so widely used in scientific computing is that it is designed as a\\ncompanion to libraries like matplotlib and other GUI toolkits. Don’t worry if you have\\nnever used matplotlib before; it will be discussed in much more detail later in this book.\\nIf you create a matplotlib plot window in the regular Python shell, you’ll be sad to find\\nthat the GUI event loop “takes control” of the Python session until the plot window is\\nclosed. That won’t work for interactive data analysis and visualization, so IPython has\\nFigure 3-2. IPython Qt Console\\n56 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 72, 'page_label': '57'}, page_content='implemented special handling for each GUI framework so that it will work seamlessly\\nwith the shell.\\nThe typical way to launch IPython with matplotlib integration is by adding the --\\npylab flag (two dashes).\\n$ ipython --pylab\\nThis will cause several things to happen. First IPython will launch with the default GUI\\nbackend integration enabled so that matplotlib plot windows can be created with no\\nissues. Secondly, most of NumPy and matplotlib will be imported into the top level\\ninteractive namespace to produce an interactive computing environment reminiscent\\nof MATLAB and other domain-specific scientific computing environments. It’s possi-\\nble to do this setup by hand by using %gui, too (try running %gui? to find out how).\\nFigure 3-3. Pylab mode: IPython with matplotlib windows\\nIPython Basics | 57\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 73, 'page_label': '58'}, page_content=\"Using the Command History\\nIPython maintains a small on-disk database containing the text of each command that\\nyou execute. This serves various purposes:\\n• Searching, completing, and executing previously-executed commands with mini-\\nmal typing\\n• Persisting the command history between sessions.\\n• Logging the input/output history to a file\\nSearching and Reusing the Command History\\nBeing able to search and execute previous commands is, for many people, the most\\nuseful feature. Since IPython encourages an iterative, interactive code development\\nworkflow, you may often find yourself repeating the same commands, such as a %run\\ncommand or some other code snippet. Suppose you had run:\\nIn[7]: %run first/second/third/data_script.py\\nand then explored the results of the script (assuming it ran successfully), only to find\\nthat you made an incorrect calculation. After figuring out the problem and modifying\\ndata_script.py, you can start typing a few letters of the %run command then press either\\nthe <Ctrl-P> key combination or the <up arrow> key. This will search the command\\nhistory for the first prior command matching the letters you typed. Pressing either\\n<Ctrl-P> or <up arrow> multiple times will continue to search through the history. If\\nyou pass over the command you wish to execute, fear not. You can move forward\\nthrough the command history by pressing either <Ctrl-N> or <down arrow>. After doing\\nthis a few times you may start pressing these keys without thinking!\\nUsing <Ctrl-R> gives you the same partial incremental searching capability provided\\nby the readline used in UNIX-style shells, such as the bash shell. On Windows, read\\nline functionality is emulated by IPython. To use this, press <Ctrl-R> then type a few\\ncharacters contained in the input line you want to search for:\\nIn [1]: a_command = foo(x, y, z)\\n(reverse-i-search)`com': a_command = foo(x, y, z)\\nPressing <Ctrl-R> will cycle through the history for each line matching the characters\\nyou’ve typed.\\nInput and Output Variables\\nForgetting to assign the result of a function call to a variable can be very annoying.\\nFortunately, IPython stores references to both the input (the text that you type) and\\noutput (the object that is returned) in special variables. The previous two outputs are\\nstored in the _ (one underscore) and __ (two underscores) variables, respectively:\\n58 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 74, 'page_label': '59'}, page_content=\"In [556]: 2 ** 27\\nOut[556]: 134217728\\nIn [557]: _\\nOut[557]: 134217728\\nInput variables are stored in variables named like _iX, where X is the input line number.\\nFor each such input variables there is a corresponding output variable _X. So after input\\nline 27, say, there will be two new variables _27 (for the output) and _i27 for the input.\\nIn [26]: foo = 'bar'\\nIn [27]: foo\\nOut[27]: 'bar'\\nIn [28]: _i27\\nOut[28]: u'foo'\\nIn [29]: _27\\nOut[29]: 'bar'\\nSince the input variables are strings, that can be executed again using the Python \\nexec keyword:\\nIn [30]: exec _i27\\nSeveral magic functions allow you to work with the input and output history. %hist is\\ncapable of printing all or part of the input history, with or without line numbers. \\n%reset is for clearing the interactive namespace and optionally the input and output\\ncaches. The %xdel magic function is intended for removing all references to a particu-\\nlar object from the IPython machinery. See the documentation for both of these magics\\nfor more details.\\nWhen working with very large data sets, keep in mind that IPython’s\\ninput and output history causes any object referenced there to not be\\ngarbage collected (freeing up the memory), even if you delete the vari-\\nables from the interactive namespace using the del keyword. In such\\ncases, careful usage of %xdel and %reset can help you avoid running into\\nmemory problems.\\nLogging the Input and Output\\nIPython is capable of logging the entire console session including input and output.\\nLogging is turned on by typing %logstart:\\nIn [3]: %logstart\\nActivating auto-logging. Current session state plus future input saved.\\nFilename       : ipython_log.py\\nMode           : rotate\\nOutput logging : False\\nRaw input log  : False\\nUsing the Command History | 59\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 75, 'page_label': '60'}, page_content='Timestamping   : False\\nState          : active\\nIPython logging can be enabled at any time and it will record your entire session (in-\\ncluding previous commands). Thus, if you are working on something and you decide\\nyou want to save everything you did, you can simply enable logging. See the docstring\\nof %logstart for more options (including changing the output file path), as well as the\\ncompanion functions %logoff, %logon, %logstate, and %logstop.\\nInteracting with the Operating System\\nAnother important feature of IPython is that it provides very strong integration with\\nthe operating system shell. This means, among other things, that you can perform most\\nstandard command line actions as you would in the Windows or UNIX (Linux, OS X)\\nshell without having to exit IPython. This includes executing shell commands, changing\\ndirectories, and storing the results of a command in a Python object (list or string).\\nThere are also simple shell command aliasing and directory bookmarking features.\\nSee Table 3-3 for a summary of magic functions and syntax for calling shell commands.\\nI’ll briefly visit these features in the next few sections.\\nTable 3-3. IPython system-related commands\\nCommand Description\\n!cmd Execute cmd in the system shell\\noutput = !cmd args Run cmd and store the stdout in output\\n%alias alias_name cmd Define an alias for a system (shell) command\\n%bookmark Utilize IPython’s directory bookmarking system\\n%cd directory Change system working directory to passed directory\\n%pwd Return the current system working directory\\n%pushd directory Place current directory on stack and change to target directory\\n%popd Change to directory popped off the top of the stack\\n%dirs Return a list containing the current directory stack\\n%dhist Print the history of visited directories\\n%env Return the system environment variables as a dict\\nShell Commands and Aliases\\nStarting a line in IPython with an exclamation point !, or bang, tells IPython to execute\\neverything after the bang in the system shell. This means that you can delete files (using\\nrm or del, depending on your OS), change directories, or execute any other process. It’s\\neven possible to start processes that take control away from IPython, even another\\nPython interpreter:\\n60 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 76, 'page_label': '61'}, page_content='In [2]: !python\\nPython 2.7.2 |EPD 7.1-2 (64-bit)| (default, Jul  3 2011, 15:17:51)\\n[GCC 4.1.2 20080704 (Red Hat 4.1.2-44)] on linux2\\nType \"packages\", \"demo\" or \"enthought\" for more information.\\n>>>\\nThe console output of a shell command can be stored in a variable by assigning the !-\\nescaped expression to a variable. For example, on my Linux-based machine connected\\nto the Internet via ethernet, I can get my IP address as a Python variable:\\nIn [1]: ip_info = !ifconfig eth0 | grep \"inet \"\\nIn [2]: ip_info[0].strip()\\nOut[2]: \\'inet addr:192.168.1.137  Bcast:192.168.1.255  Mask:255.255.255.0\\'\\nThe returned Python object ip_info is actually a custom list type containing various\\nversions of the console output.\\nIPython can also substitute in Python values defined in the current environment when\\nusing !. To do this, preface the variable name by the dollar sign $:\\nIn [3]: foo = \\'test*\\'\\nIn [4]: !ls $foo\\ntest4.py  test.py  test.xml\\nThe %alias magic function can define custom shortcuts for shell commands. As a simple\\nexample:\\nIn [1]: %alias ll ls -l\\nIn [2]: ll /usr\\ntotal 332\\ndrwxr-xr-x   2 root root  69632 2012-01-29 20:36 bin/\\ndrwxr-xr-x   2 root root   4096 2010-08-23 12:05 games/\\ndrwxr-xr-x 123 root root  20480 2011-12-26 18:08 include/\\ndrwxr-xr-x 265 root root 126976 2012-01-29 20:36 lib/\\ndrwxr-xr-x  44 root root  69632 2011-12-26 18:08 lib32/\\nlrwxrwxrwx   1 root root      3 2010-08-23 16:02 lib64 -> lib/\\ndrwxr-xr-x  15 root root   4096 2011-10-13 19:03 local/\\ndrwxr-xr-x   2 root root  12288 2012-01-12 09:32 sbin/\\ndrwxr-xr-x 387 root root  12288 2011-11-04 22:53 share/\\ndrwxrwsr-x  24 root src    4096 2011-07-17 18:38 src/\\nMultiple commands can be executed just as on the command line by separating them\\nwith semicolons:\\nIn [558]: %alias test_alias (cd ch08; ls; cd ..)\\nIn [559]: test_alias\\nmacrodata.csv  spx.csv    tips.csv\\nYou’ll notice that IPython “forgets” any aliases you define interactively as soon as the\\nsession is closed. To create permanent aliases, you will need to use the configuration\\nsystem. See later in the chapter.\\nInteracting with the Operating System | 61\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 77, 'page_label': '62'}, page_content='Directory Bookmark System\\nIPython has a simple directory bookmarking system to enable you to save aliases for\\ncommon directories so that you can jump around very easily. For example, I’m an avid\\nuser of Dropbox, so I can define a bookmark to make it easy to change directories to\\nmy Dropbox:\\nIn [6]: %bookmark db /home/wesm/Dropbox/\\nOnce I’ve done this, when I use the %cd magic, I can use any bookmarks I’ve defined\\nIn [7]: cd db\\n(bookmark:db) -> /home/wesm/Dropbox/\\n/home/wesm/Dropbox\\nIf a bookmark name conflicts with a directory name in your current working directory,\\nyou can use the -b flag to override and use the bookmark location. Using the -l option\\nwith %bookmark lists all of your bookmarks:\\nIn [8]: %bookmark -l\\nCurrent bookmarks:\\ndb -> /home/wesm/Dropbox/\\nBookmarks, unlike aliases, are automatically persisted between IPython sessions.\\nSoftware Development Tools\\nIn addition to being a comfortable environment for interactive computing and data\\nexploration, IPython is well suited as a software development environment. In data\\nanalysis applications, it’s important first to have correct code. Fortunately, IPython has\\nclosely integrated and enhanced the built-in Python pdb debugger. Secondly you want\\nyour code to be fast. For this IPython has easy-to-use code timing and profiling tools.\\nI will give an overview of these tools in detail here.\\nInteractive Debugger\\nIPython’s debugger enhances pdb with tab completion, syntax highlighting, and context\\nfor each line in exception tracebacks. One of the best times to debug code is right after\\nan error has occurred. The %debug command, when entered immediately after an ex-\\nception, invokes the “post-mortem” debugger and drops you into the stack frame where\\nthe exception was raised:\\nIn [2]: run ch03/ipython_bug.py\\n---------------------------------------------------------------------------\\nAssertionError                            Traceback (most recent call last)\\n/home/wesm/book_scripts/ch03/ipython_bug.py in <module>()\\n     13     throws_an_exception()\\n     14\\n---> 15 calling_things()\\n/home/wesm/book_scripts/ch03/ipython_bug.py in calling_things()\\n62 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 78, 'page_label': '63'}, page_content=\"11 def calling_things():\\n     12     works_fine()\\n---> 13     throws_an_exception()\\n     14\\n     15 calling_things()\\n/home/wesm/book_scripts/ch03/ipython_bug.py in throws_an_exception()\\n      7     a = 5\\n      8     b = 6\\n----> 9     assert(a + b == 10)\\n     10\\n     11 def calling_things():\\nAssertionError:\\nIn [3]: %debug\\n> /home/wesm/book_scripts/ch03/ipython_bug.py(9)throws_an_exception()\\n      8     b = 6\\n----> 9     assert(a + b == 10)\\n     10\\nipdb>\\nOnce inside the debugger, you can execute arbitrary Python code and explore all of the\\nobjects and data (which have been “kept alive” by the interpreter) inside each stack\\nframe. By default you start in the lowest level, where the error occurred. By pressing\\nu (up) and d (down), you can switch between the levels of the stack trace:\\nipdb> u\\n> /home/wesm/book_scripts/ch03/ipython_bug.py(13)calling_things()\\n     12     works_fine()\\n---> 13     throws_an_exception()\\n     14\\nExecuting the %pdb command makes it so that IPython automatically invokes the de-\\nbugger after any exception, a mode that many users will find especially useful.\\nIt’s also easy to use the debugger to help develop code, especially when you wish to set\\nbreakpoints or step through the execution of a function or script to examine the state\\nat each stage. There are several ways to accomplish this. The first is by using %run with\\nthe -d flag, which invokes the debugger before executing any code in the passed script.\\nYou must immediately press s (step) to enter the script:\\nIn [5]: run -d ch03/ipython_bug.py\\nBreakpoint 1 at /home/wesm/book_scripts/ch03/ipython_bug.py:1\\nNOTE: Enter 'c' at the ipdb>  prompt to start your script.\\n> <string>(1)<module>()\\nipdb> s\\n--Call--\\n> /home/wesm/book_scripts/ch03/ipython_bug.py(1)<module>()\\n1---> 1 def works_fine():\\n      2     a = 5\\n      3     b = 6\\nSoftware Development Tools | 63\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 79, 'page_label': '64'}, page_content='After this point, it’s up to you how you want to work your way through the file. For\\nexample, in the above exception, we could set a breakpoint right before calling the\\nworks_fine method and run the script until we reach the breakpoint by pressing c\\n(continue):\\nipdb> b 12\\nipdb> c\\n> /home/wesm/book_scripts/ch03/ipython_bug.py(12)calling_things()\\n     11 def calling_things():\\n2--> 12     works_fine()\\n     13     throws_an_exception()\\nAt this point, you can step into works_fine() or execute works_fine() by pressing n\\n(next) to advance to the next line:\\nipdb> n\\n> /home/wesm/book_scripts/ch03/ipython_bug.py(13)calling_things()\\n2    12     works_fine()\\n---> 13     throws_an_exception()\\n     14\\nThen, we could step into throws_an_exception and advance to the line where the error\\noccurs and look at the variables in the scope. Note that debugger commands take\\nprecedence over variable names; in such cases preface the variables with ! to examine\\ntheir contents.\\nipdb> s\\n--Call--\\n> /home/wesm/book_scripts/ch03/ipython_bug.py(6)throws_an_exception()\\n      5\\n----> 6 def throws_an_exception():\\n      7     a = 5\\nipdb> n\\n> /home/wesm/book_scripts/ch03/ipython_bug.py(7)throws_an_exception()\\n      6 def throws_an_exception():\\n----> 7     a = 5\\n      8     b = 6\\nipdb> n\\n> /home/wesm/book_scripts/ch03/ipython_bug.py(8)throws_an_exception()\\n      7     a = 5\\n----> 8     b = 6\\n      9     assert(a + b == 10)\\nipdb> n\\n> /home/wesm/book_scripts/ch03/ipython_bug.py(9)throws_an_exception()\\n      8     b = 6\\n----> 9     assert(a + b == 10)\\n     10\\nipdb> !a\\n5\\nipdb> !b\\n6\\n64 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 80, 'page_label': '65'}, page_content=\"Becoming proficient in the interactive debugger is largely a matter of practice and ex-\\nperience. See Table 3-3 for a full catalogue of the debugger commands. If you are used\\nto an IDE, you might find the terminal-driven debugger to be a bit bewildering at first,\\nbut that will improve in time. Most of the Python IDEs have excellent GUI debuggers,\\nbut it is usually a significant productivity gain to remain in IPython for your debugging.\\nTable 3-4. (I)Python debugger commands\\nCommand Action\\nh(elp) Display command list\\nhelp command Show documentation for command\\nc(ontinue) Resume program execution\\nq(uit) Exit debugger without executing any more code\\nb(reak) number Set breakpoint at number in current file\\nb path/to/file.py:number Set breakpoint at line number in specified file\\ns(tep) Step into function call\\nn(ext) Execute current line and advance to next line at current level\\nu(p) / d(own) Move up/down in function call stack\\na(rgs) Show arguments for current function\\ndebug statement Invoke statement statement in new (recursive) debugger\\nl(ist) statement Show current position and context at current level of stack\\nw(here) Print full stack trace with context at current position\\nOther ways to make use of the debugger\\nThere are a couple of other useful ways to invoke the debugger. The first is by using a\\nspecial set_trace function (named after pdb.set_trace), which is basically a “poor\\nman’s breakpoint”. Here are two small recipes you might want to put somewhere for\\nyour general use (potentially adding them to your IPython profile as I do):\\ndef set_trace():\\n    from IPython.core.debugger import Pdb\\n    Pdb(color_scheme='Linux').set_trace(sys._getframe().f_back)\\ndef debug(f, *args, **kwargs):\\n    from IPython.core.debugger import Pdb\\n    pdb = Pdb(color_scheme='Linux')\\n    return pdb.runcall(f, *args, **kwargs)\\nThe first function, set_trace, is very simple. Put set_trace() anywhere in your code\\nthat you want to stop and take a look around (for example, right before an exception\\noccurs):\\nIn [7]: run ch03/ipython_bug.py\\n> /home/wesm/book_scripts/ch03/ipython_bug.py(16)calling_things()\\n     15     set_trace()\\nSoftware Development Tools | 65\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 81, 'page_label': '66'}, page_content=\"---> 16     throws_an_exception()\\n     17\\nPressing c (continue) will cause the code to resume normally with no harm done.\\nThe debug function above enables you to invoke the interactive debugger easily on an\\narbitrary function call. Suppose we had written a function like\\ndef f(x, y, z=1):\\n    tmp = x + y\\n    return tmp / z\\nand we wished to step through its logic. Ordinarily using f would look like f(1, 2,\\nz=3). To instead step into f, pass f as the first argument to debug followed by the po-\\nsitional and keyword arguments to be passed to f:\\nIn [6]: debug(f, 1, 2, z=3)\\n> <ipython-input>(2)f()\\n      1 def f(x, y, z):\\n----> 2     tmp = x + y\\n      3     return tmp / z\\nipdb>\\nI find that these two simple recipes save me a lot of time on a day-to-day basis.\\nLastly, the debugger can be used in conjunction with %run. By running a script with\\n%run -d, you will be dropped directly into the debugger, ready to set any breakpoints\\nand start the script:\\nIn [1]: %run -d ch03/ipython_bug.py\\nBreakpoint 1 at /home/wesm/book_scripts/ch03/ipython_bug.py:1\\nNOTE: Enter 'c' at the ipdb>  prompt to start your script.\\n> <string>(1)<module>()\\nipdb>\\nAdding -b with a line number starts the debugger with a breakpoint set already:\\nIn [2]: %run -d -b2 ch03/ipython_bug.py\\nBreakpoint 1 at /home/wesm/book_scripts/ch03/ipython_bug.py:2\\nNOTE: Enter 'c' at the ipdb>  prompt to start your script.\\n> <string>(1)<module>()\\nipdb> c\\n> /home/wesm/book_scripts/ch03/ipython_bug.py(2)works_fine()\\n      1 def works_fine():\\n1---> 2     a = 5\\n      3     b = 6\\nipdb>\\n66 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 82, 'page_label': '67'}, page_content=\"Timing Code: %time and %timeit\\nFor larger-scale or longer-running data analysis applications, you may wish to measure\\nthe execution time of various components or of individual statements or function calls.\\nYou may want a report of which functions are taking up the most time in a complex\\nprocess. Fortunately, IPython enables you to get this information very easily while you\\nare developing and testing your code.\\nTiming code by hand using the built-in time module and its functions time.clock and\\ntime.time is often tedious and repetitive, as you must write the same uninteresting\\nboilerplate code:\\nimport time\\nstart = time.time()\\nfor i in range(iterations):\\n    # some code to run here\\nelapsed_per = (time.time() - start) / iterations\\nSince this is such a common operation, IPython has two magic functions %time and \\n%timeit to automate this process for you. %time runs a statement once, reporting the\\ntotal execution time. Suppose we had a large list of strings and we wanted to compare\\ndifferent methods of selecting all strings starting with a particular prefix. Here is a\\nsimple list of 700,000 strings and two identical methods of selecting only the ones that\\nstart with 'foo':\\n# a very large list of strings\\nstrings = ['foo', 'foobar', 'baz', 'qux',\\n           'python', 'Guido Van Rossum'] * 100000\\nmethod1 = [x for x in strings if x.startswith('foo')]\\nmethod2 = [x for x in strings if x[:3] == 'foo']\\nIt looks like they should be about the same performance-wise, right? We can check for\\nsure using %time:\\nIn [561]: %time method1 = [x for x in strings if x.startswith('foo')]\\nCPU times: user 0.19 s, sys: 0.00 s, total: 0.19 s\\nWall time: 0.19 s\\nIn [562]: %time method2 = [x for x in strings if x[:3] == 'foo']\\nCPU times: user 0.09 s, sys: 0.00 s, total: 0.09 s\\nWall time: 0.09 s\\nThe Wall time is the main number of interest. So, it looks like the first method takes\\nmore than twice as long, but it’s not a very precise measurement. If you try %time-ing\\nthose statements multiple times yourself, you’ll find that the results are somewhat\\nvariable. To get a more precise measurement, use the %timeit magic function. Given\\nan arbitrary statement, it has a heuristic to run a statement multiple times to produce\\na fairly accurate average runtime.\\nIn [563]: %timeit [x for x in strings if x.startswith('foo')]\\n10 loops, best of 3: 159 ms per loop\\nSoftware Development Tools | 67\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 83, 'page_label': '68'}, page_content=\"In [564]: %timeit [x for x in strings if x[:3] == 'foo']\\n10 loops, best of 3: 59.3 ms per loop\\nThis seemingly innocuous example illustrates that it is worth understanding the per-\\nformance characteristics of the Python standard library, NumPy, pandas, and other\\nlibraries used in this book. In larger-scale data analysis applications, those milliseconds\\nwill start to add up!\\n%timeit is especially useful for analyzing statements and functions with very short ex-\\necution times, even at the level of microseconds (1e-6 seconds) or nanoseconds (1e-9\\nseconds). These may seem like insignificant amounts of time, but of course a 20 mi-\\ncrosecond function invoked 1 million times takes 15 seconds longer than a 5 micro-\\nsecond function. In the above example, we could very directly compare the two string\\noperations to understand their performance characteristics:\\nIn [565]: x = 'foobar'\\nIn [566]: y = 'foo'\\nIn [567]: %timeit x.startswith(y)\\n1000000 loops, best of 3: 267 ns per loop\\nIn [568]: %timeit x[:3] == y\\n10000000 loops, best of 3: 147 ns per loop\\nBasic Profiling: %prun and %run -p\\nProfiling code is closely related to timing code, except it is concerned with determining\\nwhere time is spent. The main Python profiling tool is the cProfile module, which is\\nnot specific to IPython at all. cProfile executes a program or any arbitrary block of\\ncode while keeping track of how much time is spent in each function.\\nA common way to use cProfile is on the command line, running an entire program\\nand outputting the aggregated time per function. Suppose we had a simple script which\\ndoes some linear algebra in a loop (computing the maximum absolute eigenvalues of\\na series of 100 x 100 matrices):\\nimport numpy as np\\nfrom numpy.linalg import eigvals\\ndef run_experiment(niter=100):\\n    K = 100\\n    results = []\\n    for _ in xrange(niter):\\n        mat = np.random.randn(K, K)\\n        max_eigenvalue = np.abs(eigvals(mat)).max()\\n        results.append(max_eigenvalue)\\n    return results\\nsome_results = run_experiment()\\nprint 'Largest one we saw: %s' % np.max(some_results)\\n68 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 84, 'page_label': '69'}, page_content=\"Don’t worry if you are not familiar with NumPy. You can run this script through\\ncProfile by running the following in the command line:\\npython -m cProfile cprof_example.py\\nIf you try that, you’ll find that the results are outputted sorted by function name. This\\nmakes it a bit hard to get an idea of where the most time is spent, so it’s very common\\nto specify a sort order using the -s flag:\\n$ python -m cProfile -s cumulative cprof_example.py\\nLargest one we saw: 11.923204422\\n    15116 function calls (14927 primitive calls) in 0.720 seconds\\nOrdered by: cumulative time\\nncalls  tottime  percall  cumtime  percall filename:lineno(function)\\n     1    0.001    0.001    0.721    0.721 cprof_example.py:1(<module>)\\n   100    0.003    0.000    0.586    0.006 linalg.py:702(eigvals)\\n   200    0.572    0.003    0.572    0.003 {numpy.linalg.lapack_lite.dgeev}\\n     1    0.002    0.002    0.075    0.075 __init__.py:106(<module>)\\n   100    0.059    0.001    0.059    0.001 {method 'randn')\\n     1    0.000    0.000    0.044    0.044 add_newdocs.py:9(<module>)\\n     2    0.001    0.001    0.037    0.019 __init__.py:1(<module>)\\n     2    0.003    0.002    0.030    0.015 __init__.py:2(<module>)\\n     1    0.000    0.000    0.030    0.030 type_check.py:3(<module>)\\n     1    0.001    0.001    0.021    0.021 __init__.py:15(<module>)\\n     1    0.013    0.013    0.013    0.013 numeric.py:1(<module>)\\n     1    0.000    0.000    0.009    0.009 __init__.py:6(<module>)\\n     1    0.001    0.001    0.008    0.008 __init__.py:45(<module>)\\n   262    0.005    0.000    0.007    0.000 function_base.py:3178(add_newdoc)\\n   100    0.003    0.000    0.005    0.000 linalg.py:162(_assertFinite)\\n   ...\\nOnly the first 15 rows of the output are shown. It’s easiest to read by scanning down\\nthe cumtime column to see how much total time was spent inside each function. Note\\nthat if a function calls some other function, the clock does not stop running . cProfile\\nrecords the start and end time of each function call and uses that to produce the timing.\\nIn addition to the above command-line usage, cProfile can also be used programmat-\\nically to profile arbitrary blocks of code without having to run a new process. IPython\\nhas a convenient interface to this capability using the %prun command and the -p option\\nto %run. %prun takes the same “command line options” as cProfile but will profile an\\narbitrary Python statement instead of a while .py file:\\nIn [4]: %prun -l 7 -s cumulative run_experiment()\\n         4203 function calls in 0.643 seconds\\nOrdered by: cumulative time\\nList reduced from 32 to 7 due to restriction <7>\\nncalls  tottime  percall  cumtime  percall filename:lineno(function)\\n     1    0.000    0.000    0.643    0.643 <string>:1(<module>)\\n     1    0.001    0.001    0.643    0.643 cprof_example.py:4(run_experiment)\\n   100    0.003    0.000    0.583    0.006 linalg.py:702(eigvals)\\nSoftware Development Tools | 69\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 85, 'page_label': '70'}, page_content=\"200    0.569    0.003    0.569    0.003 {numpy.linalg.lapack_lite.dgeev}\\n   100    0.058    0.001    0.058    0.001 {method 'randn'}\\n   100    0.003    0.000    0.005    0.000 linalg.py:162(_assertFinite)\\n   200    0.002    0.000    0.002    0.000 {method 'all' of 'numpy.ndarray' objects}\\nSimilarly, calling %run -p -s cumulative cprof_example.py has the same effect as the\\ncommand-line approach above, except you never have to leave IPython.\\nProfiling a Function Line-by-Line\\nIn some cases the information you obtain from %prun (or another cProfile-based profile\\nmethod) may not tell the whole story about a function’s execution time, or it may be\\nso complex that the results, aggregated by function name, are hard to interpret. For\\nthis case, there is a small library called line_profiler (obtainable via PyPI or one of the\\npackage management tools). It contains an IPython extension enabling a new magic\\nfunction %lprun that computes a line-by-line-profiling of one or more functions. You\\ncan enable this extension by modifying your IPython configuration (see the IPython\\ndocumentation or the section on configuration later in this chapter) to include the\\nfollowing line:\\n# A list of dotted module names of IPython extensions to load.\\nc.TerminalIPythonApp.extensions = ['line_profiler']\\nline_profiler can be used programmatically (see the full documentation), but it is\\nperhaps most powerful when used interactively in IPython. Suppose you had a module\\nprof_mod with the following code doing some NumPy array operations:\\nfrom numpy.random import randn\\ndef add_and_sum(x, y):\\n    added = x + y\\n    summed = added.sum(axis=1)\\n    return summed\\ndef call_function():\\n    x = randn(1000, 1000)\\n    y = randn(1000, 1000)\\n    return add_and_sum(x, y)\\nIf we wanted to understand the performance of the add_and_sum function, %prun gives\\nus the following:\\nIn [569]: %run prof_mod\\nIn [570]: x = randn(3000, 3000)\\nIn [571]: y = randn(3000, 3000)\\nIn [572]: %prun add_and_sum(x, y)\\n         4 function calls in 0.049 seconds\\n   Ordered by: internal time\\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\\n        1    0.036    0.036    0.046    0.046 prof_mod.py:3(add_and_sum)\\n70 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 86, 'page_label': '71'}, page_content=\"1    0.009    0.009    0.009    0.009 {method 'sum' of 'numpy.ndarray' objects}\\n        1    0.003    0.003    0.049    0.049 <string>:1(<module>)\\n        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\\nThis is not especially enlightening. With the line_profiler IPython extension activa-\\nted, a new command %lprun is available. The only difference in usage is that we must\\ninstruct %lprun which function or functions we wish to profile. The general syntax is:\\n%lprun -f func1 -f func2 statement_to_profile\\nIn this case, we want to profile add_and_sum, so we run:\\nIn [573]: %lprun -f add_and_sum add_and_sum(x, y)\\nTimer unit: 1e-06 s\\nFile: book_scripts/prof_mod.py\\nFunction: add_and_sum at line 3\\nTotal time: 0.045936 s\\nLine #      Hits         Time  Per Hit   % Time  Line Contents\\n==============================================================\\n     3                                           def add_and_sum(x, y):\\n     4         1        36510  36510.0     79.5      added = x + y\\n     5         1         9425   9425.0     20.5      summed = added.sum(axis=1)\\n     6         1            1      1.0      0.0      return summed\\nYou’ll probably agree this is much easier to interpret. In this case we profiled the same\\nfunction we used in the statement. Looking at the module code above, we could call\\ncall_function and profile that as well as add_and_sum, thus getting a full picture of the\\nperformance of the code:\\nIn [574]: %lprun -f add_and_sum -f call_function call_function()\\nTimer unit: 1e-06 s\\nFile: book_scripts/prof_mod.py\\nFunction: add_and_sum at line 3\\nTotal time: 0.005526 s\\nLine #      Hits         Time  Per Hit   % Time  Line Contents\\n==============================================================\\n     3                                           def add_and_sum(x, y):\\n     4         1         4375   4375.0     79.2      added = x + y\\n     5         1         1149   1149.0     20.8      summed = added.sum(axis=1)\\n     6         1            2      2.0      0.0      return summed\\nFile: book_scripts/prof_mod.py\\nFunction: call_function at line 8\\nTotal time: 0.121016 s\\nLine #      Hits         Time  Per Hit   % Time  Line Contents\\n==============================================================\\n     8                                           def call_function():\\n     9         1        57169  57169.0     47.2      x = randn(1000, 1000)\\n    10         1        58304  58304.0     48.2      y = randn(1000, 1000)\\n    11         1         5543   5543.0      4.6      return add_and_sum(x, y)\\nAs a general rule of thumb, I tend to prefer %prun (cProfile) for “macro” profiling and\\n%lprun (line_profiler) for “micro” profiling. It’s worthwhile to have a good under-\\nstanding of both tools.\\nSoftware Development Tools | 71\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 87, 'page_label': '72'}, page_content=\"The reason that you have to specify explicitly the names of the functions\\nyou want to profile with %lprun is that the overhead of “tracing” the\\nexecution time of each line is significant. Tracing functions that are not\\nof interest would potentially significantly alter the profile results.\\nIPython HTML Notebook\\nStarting in 2011, the IPython team, led by Brian Granger, built a web technology−based\\ninteractive computational document format that is commonly known as the IPython\\nNotebook. It has grown into a wonderful tool for interactive computing and an ideal\\nmedium for reproducible research and teaching. I’ve used it while writing most of the\\nexamples in the book; I encourage you to make use of it, too.\\nIt has a JSON-based .ipynb document format that enables easy sharing of code, output,\\nand figures. Recently in Python conferences, a popular approach for demonstrations\\nhas been to use the notebook and post the .ipynb files online afterward for everyone\\nto play with.\\nThe notebook application runs as a lightweight server process on the command line.\\nIt can be started by running:\\n$ ipython notebook --pylab=inline\\n[NotebookApp] Using existing profile dir: u'/home/wesm/.config/ipython/profile_default'\\n[NotebookApp] Serving notebooks from /home/wesm/book_scripts\\n[NotebookApp] The IPython Notebook is running at: http://127.0.0.1:8888/\\n[NotebookApp] Use Control-C to stop this server and shut down all kernels.\\nOn most platforms, your primary web browser will automatically open up to the note-\\nbook dashboard. In some cases you may have to navigate to the listed URL. From there,\\nyou can create a new notebook and start exploring.\\nSince you use the notebook inside a web browser, the server process can run anywhere.\\nYou can even securely connect to notebooks running on cloud service providers like\\nAmazon EC2. As of this writing, a new project NotebookCloud (http://notebookcloud\\n.appspot.com) makes it easy to launch notebooks on EC2.\\nTips for Productive Code Development Using IPython\\nWriting code in a way that makes it easy to develop, debug, and ultimately use inter-\\nactively may be a paradigm shift for many users. There are procedural details like code\\nreloading that may require some adjustment as well as coding style concerns.\\nAs such, most of this section is more of an art than a science and will require some\\nexperimentation on your part to determine a way to write your Python code that is\\neffective and productive for you. Ultimately you want to structure your code in a way\\nthat makes it easy to use iteratively and to be able to explore the results of running a\\nprogram or function as effortlessly as possible. I have found software designed with\\n72 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 88, 'page_label': '73'}, page_content='IPython in mind to be easier to work with than code intended only to be run as as\\nstandalone command-line application. This becomes especially important when some-\\nthing goes wrong and you have to diagnose an error in code that you or someone else\\nmight have written months or years beforehand.\\nFigure 3-4. IPython Notebook\\nTips for Productive Code Development Using IPython | 73\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 89, 'page_label': '74'}, page_content=\"Reloading Module Dependencies\\nIn Python, when you type import some_lib, the code in some_lib is executed and all the\\nvariables, functions, and imports defined within are stored in the newly created\\nsome_lib module namespace. The next time you type import some_lib, you will get a\\nreference to the existing module namespace. The potential difficulty in interactive code\\ndevelopment in IPython comes when you, say, %run a script that depends on some other\\nmodule where you may have made changes. Suppose I had the following code in\\ntest_script.py:\\nimport some_lib\\nx = 5\\ny = [1, 2, 3, 4]\\nresult = some_lib.get_answer(x, y)\\nIf you were to execute %run test_script.py then modify some_lib.py, the next time you\\nexecute %run test_script.py you will still get the old version of some_lib because of\\nPython’s “load-once” module system. This behavior differs from some other data anal-\\nysis environments, like MATLAB, which automatically propagate code changes. 1 To\\ncope with this, you have a couple of options. The first way is to use Python's built-in \\nreload function, altering test_script.py to look like the following:\\nimport some_lib\\nreload(some_lib)\\nx = 5\\ny = [1, 2, 3, 4]\\nresult = some_lib.get_answer(x, y)\\nThis guarantees that you will get a fresh copy of some_lib every time you run\\ntest_script.py. Obviously, if the dependencies go deeper, it might be a bit tricky to be\\ninserting usages of reload all over the place. For this problem, IPython has a special \\ndreload function (not a magic function) for “deep” (recursive) reloading of modules. If\\nI were to run import some_lib then type dreload(some_lib), it will attempt to reload\\nsome_lib as well as all of its dependencies. This will not work in all cases, unfortunately,\\nbut when it does it beats having to restart IPython.\\nCode Design Tips\\nThere’s no simple recipe for this, but here are some high-level principles I have found\\neffective in my own work.\\n1. Since a module or package may be imported in many different places in a particular program, Python\\ncaches a module’s code the first time it is imported rather than executing the code in the module every\\ntime. Otherwise, modularity and good code organization could potentially cause inefficiency in an\\napplication.\\n74 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 90, 'page_label': '75'}, page_content=\"Keep relevant objects and data alive\\nIt’s not unusual to see a program written for the command line with a structure some-\\nwhat like the following trivial example:\\nfrom my_functions import g\\ndef f(x, y):\\n    return g(x + y)\\ndef main():\\n    x = 6\\n    y = 7.5\\n    result = x + y\\nif __name__ == '__main__':\\n    main()\\nDo you see what might be wrong with this program if we were to run it in IPython?\\nAfter it’s done, none of the results or objects defined in the main function willl be ac-\\ncessible in the IPython shell. A better way is to have whatever code is in main execute\\ndirectly in the module’s global namespace (or in the if __name__ == '__main__': block,\\nif you want the module to also be importable). That way, when you %run the code,\\nyou’ll be able to look at all of the variables defined in main. It’s less meaningful in this\\nsimple example, but in this book we’ll be looking at some complex data analysis prob-\\nlems involving large data sets that you will want to be able to play with in IPython.\\nFlat is better than nested\\nDeeply nested code makes me think about the many layers of an onion. When testing\\nor debugging a function, how many layers of the onion must you peel back in order to\\nreach the code of interest? The idea that “flat is better than nested” is a part of the Zen\\nof Python, and it applies generally to developing code for interactive use as well. Making\\nfunctions and classes as decoupled and modular as possible makes them easier to test\\n(if you are writing unit tests), debug, and use interactively.\\nOvercome a fear of longer files\\nIf you come from a Java (or another such language) background, you may have been\\ntold to keep files short. In many languages, this is sound advice; long length is usually\\na bad “code smell”, indicating refactoring or reorganization may be necessary. How-\\never, while developing code using IPython, working with 10 small, but interconnected\\nfiles (under, say, 100 lines each) is likely to cause you more headache in general than a\\nsingle large file or two or three longer files. Fewer files means fewer modules to reload\\nand less jumping between files while editing, too. I have found maintaining larger\\nmodules, each with high internal cohesion, to be much more useful and pythonic. After\\niterating toward a solution, it sometimes will make sense to refactor larger files into\\nsmaller ones.\\nTips for Productive Code Development Using IPython | 75\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 91, 'page_label': '76'}, page_content=\"Obviously, I don’t support taking this argument to the extreme, which would to be to\\nput all of your code in a single monstrous file. Finding a sensible and intuitive module\\nand package structure for a large codebase often takes a bit of work, but it is especially\\nimportant to get right in teams. Each module should be internally cohesive, and it\\nshould be as obvious as possible where to find functions and classes responsible for\\neach area of functionality.\\nAdvanced IPython Features\\nMaking Your Own Classes IPython-friendly\\nIPython makes every effort to display a console-friendly string representation of any\\nobject that you inspect. For many objects, like dicts, lists, and tuples, the built-in \\npprint module is used to do the nice formatting. In user-defined classes, however, you\\nhave to generate the desired string output yourself. Suppose we had the following sim-\\nple class:\\nclass Message:\\n    def __init__(self, msg):\\n        self.msg = msg\\nIf you wrote this, you would be disappointed to discover that the default output for\\nyour class isn’t very nice:\\nIn [576]: x = Message('I have a secret')\\nIn [577]: x\\nOut[577]: <__main__.Message instance at 0x60ebbd8>\\nIPython takes the string returned by the __repr__ magic method (by doing output =\\nrepr(obj)) and prints that to the console. Thus, we can add a simple __repr__ method\\nto the above class to get a more helpful output:\\nclass Message:\\n    def __init__(self, msg):\\n        self.msg = msg\\n    def __repr__(self):\\n        return 'Message: %s' % self.msg\\nIn [579]: x = Message('I have a secret')\\nIn [580]: x\\nOut[580]: Message: I have a secret\\n76 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 92, 'page_label': '77'}, page_content='Profiles and Configuration\\nMost aspects of the appearance (colors, prompt, spacing between lines, etc.) and be-\\nhavior of the IPython shell are configurable through an extensive configuration system.\\nHere are some of the things you can do via configuration:\\n• Change the color scheme\\n• Change how the input and output prompts look, or remove the blank line after\\nOut and before the next In prompt\\n• Change how the input and output prompts look\\n• Execute an arbitrary list of Python statements. These could be imports that you\\nuse all the time or anything else you want to happen each time you launch IPython\\n• Enable IPython extensions, like the %lprun magic in line_profiler\\n• Define your own magics or system aliases\\nAll of these configuration options are specified in a special ipython_config.py file which\\nwill be found in the ~/.config/ipython/ directory on UNIX-like systems and %HOME\\n%/.ipython/ directory on Windows. Where your home directory is depends on your\\nsystem. Configuration is performed based on a particular profile. When you start IPy-\\nthon normally, you load up, by default, the default profile , stored in the pro\\nfile_default directory. Thus, on my Linux OS the full path to my default IPython\\nconfiguration file is:\\n/home/wesm/.config/ipython/profile_default/ipython_config.py\\nI’ll spare you the gory details of what’s in this file. Fortunately it has comments de-\\nscribing what each configuration option is for, so I will leave it to the reader to tinker\\nand customize. One additional useful feature is that it’s possible to have multiple pro-\\nfiles. Suppose you wanted to have an alternate IPython configuration tailored for a\\nparticular application or project. Creating a new profile is as simple is typing something\\nlike\\nipython profile create secret_project\\nOnce you’ve done this, edit the config files in the newly-created pro\\nfile_secret_project directory then launch IPython like so\\n$ ipython --profile=secret_project\\nPython 2.7.2 |EPD 7.1-2 (64-bit)| (default, Jul  3 2011, 15:17:51)\\nType \"copyright\", \"credits\" or \"license\" for more information.\\nIPython 0.13 -- An enhanced Interactive Python.\\n?         -> Introduction and overview of IPython\\'s features.\\n%quickref -> Quick reference.\\nhelp      -> Python\\'s own help system.\\nobject?   -> Details about \\'object\\', use \\'object??\\' for extra details.\\nIPython profile: secret_project\\nAdvanced IPython Features | 77\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 93, 'page_label': '78'}, page_content='In [1]:\\nAs always, the online IPython documentation is an excellent resource for more on\\nprofiles and configuration.\\nCredits\\nParts of this chapter were derived from the wonderful documentation put together by\\nthe IPython Development Team. I can’t thank them enough for all of their work build-\\ning this amazing set of tools.\\n78 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 94, 'page_label': '79'}, page_content='CHAPTER 4\\nNumPy Basics: Arrays and Vectorized\\nComputation\\nNumPy, short for Numerical Python, is the fundamental package required for high\\nperformance scientific computing and data analysis. It is the foundation on which\\nnearly all of the higher-level tools in this book are built. Here are some of the things it\\nprovides:\\n• ndarray, a fast and space-efficient multidimensional array providing vectorized\\narithmetic operations and sophisticated broadcasting capabilities\\n• Standard mathematical functions for fast operations on entire arrays of data\\nwithout having to write loops\\n• Tools for reading / writing array data to disk and working with memory-mapped\\nfiles\\n• Linear algebra, random number generation, and Fourier transform capabilities\\n• Tools for integrating code written in C, C++, and Fortran\\nThe last bullet point is also one of the most important ones from an ecosystem point\\nof view. Because NumPy provides an easy-to-use C API, it is very easy to pass data to\\nexternal libraries written in a low-level language and also for external libraries to return\\ndata to Python as NumPy arrays. This feature has made Python a language of choice\\nfor wrapping legacy C/C++/Fortran codebases and giving them a dynamic and easy-\\nto-use interface.\\nWhile NumPy by itself does not provide very much high-level data analytical func-\\ntionality, having an understanding of NumPy arrays and array-oriented computing will\\nhelp you use tools like pandas much more effectively. If you’re new to Python and just\\nlooking to get your hands dirty working with data using pandas, feel free to give this\\nchapter a skim. For more on advanced NumPy features like broadcasting, see Chap-\\nter 12.\\n79\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 95, 'page_label': '80'}, page_content='For most data analysis applications, the main areas of functionality I’ll focus on are:\\n• Fast vectorized array operations for data munging and cleaning, subsetting and\\nfiltering, transformation, and any other kinds of computations\\n• Common array algorithms like sorting, unique, and set operations\\n• Efficient descriptive statistics and aggregating/summarizing data\\n• Data alignment and relational data manipulations for merging and joining together\\nheterogeneous data sets\\n• Expressing conditional logic as array expressions instead of loops with if-elif-\\nelse branches\\n• Group-wise data manipulations (aggregation, transformation, function applica-\\ntion). Much more on this in Chapter 5\\nWhile NumPy provides the computational foundation for these operations, you will\\nlikely want to use pandas as your basis for most kinds of data analysis (especially for\\nstructured or tabular data) as it provides a rich, high-level interface making most com-\\nmon data tasks very concise and simple. pandas also provides some more domain-\\nspecific functionality like time series manipulation, which is not present in NumPy.\\nIn this chapter and throughout the book, I use the standard NumPy\\nconvention of always using import numpy as np . You are, of course,\\nwelcome to put from numpy import * in your code to avoid having to\\nwrite np., but I would caution you against making a habit of this.\\nThe NumPy ndarray: A Multidimensional Array Object\\nOne of the key features of NumPy is its N-dimensional array object, or ndarray, which\\nis a fast, flexible container for large data sets in Python. Arrays enable you to perform\\nmathematical operations on whole blocks of data using similar syntax to the equivalent\\noperations between scalar elements:\\nIn [8]: data\\nOut[8]: \\narray([[ 0.9526, -0.246 , -0.8856],\\n       [ 0.5639,  0.2379,  0.9104]])\\nIn [9]: data * 10                         In [10]: data + data                \\nOut[9]:                                   Out[10]:                            \\narray([[ 9.5256, -2.4601, -8.8565],       array([[ 1.9051, -0.492 , -1.7713], \\n       [ 5.6385,  2.3794,  9.104 ]])             [ 1.1277,  0.4759,  1.8208]])\\nAn ndarray is a generic multidimensional container for homogeneous data; that is, all\\nof the elements must be the same type. Every array has a shape, a tuple indicating the\\nsize of each dimension, and a dtype, an object describing the data type of the array:\\nIn [11]: data.shape\\nOut[11]: (2, 3)\\n80 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 96, 'page_label': '81'}, page_content=\"In [12]: data.dtype\\nOut[12]: dtype('float64')\\nThis chapter will introduce you to the basics of using NumPy arrays, and should be\\nsufficient for following along with the rest of the book. While it’s not necessary to have\\na deep understanding of NumPy for many data analytical applications, becoming pro-\\nficient in array-oriented programming and thinking is a key step along the way to be-\\ncoming a scientific Python guru.\\nWhenever you see “array”, “NumPy array”, or “ndarray” in the text,\\nwith few exceptions they all refer to the same thing: the ndarray object.\\nCreating ndarrays\\nThe easiest way to create an array is to use the array function. This accepts any se-\\nquence-like object (including other arrays) and produces a new NumPy array contain-\\ning the passed data. For example, a list is a good candidate for conversion:\\nIn [13]: data1 = [6, 7.5, 8, 0, 1]\\nIn [14]: arr1 = np.array(data1)\\nIn [15]: arr1\\nOut[15]: array([ 6. ,  7.5,  8. ,  0. ,  1. ])\\nNested sequences, like a list of equal-length lists, will be converted into a multidimen-\\nsional array:\\nIn [16]: data2 = [[1, 2, 3, 4], [5, 6, 7, 8]]\\nIn [17]: arr2 = np.array(data2)\\nIn [18]: arr2\\nOut[18]: \\narray([[1, 2, 3, 4],\\n       [5, 6, 7, 8]])\\nIn [19]: arr2.ndim\\nOut[19]: 2\\nIn [20]: arr2.shape\\nOut[20]: (2, 4)\\nUnless explicitly specified (more on this later), np.array tries to infer a good data type\\nfor the array that it creates. The data type is stored in a special dtype object; for example,\\nin the above two examples we have:\\nIn [21]: arr1.dtype\\nOut[21]: dtype('float64')\\nThe NumPy ndarray: A Multidimensional Array Object | 81\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 97, 'page_label': '82'}, page_content=\"In [22]: arr2.dtype\\nOut[22]: dtype('int64')\\nIn addition to np.array, there are a number of other functions for creating new arrays.\\nAs examples, zeros and ones create arrays of 0’s or 1’s, respectively, with a given length\\nor shape. empty creates an array without initializing its values to any particular value.\\nTo create a higher dimensional array with these methods, pass a tuple for the shape:\\nIn [23]: np.zeros(10)\\nOut[23]: array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])\\nIn [24]: np.zeros((3, 6))                    \\nOut[24]:                                                         \\narray([[ 0.,  0.,  0.,  0.,  0.,  0.],                \\n       [ 0.,  0.,  0.,  0.,  0.,  0.],                \\n       [ 0.,  0.,  0.,  0.,  0.,  0.]])              \\nIn [25]: np.empty((2, 3, 2))\\nOut[25]:\\narray([[[  4.94065646e-324,   4.94065646e-324],\\n        [  3.87491056e-297,   2.46845796e-130],\\n        [  4.94065646e-324,   4.94065646e-324]],\\n       [[  1.90723115e+083,   5.73293533e-053],\\n        [ -2.33568637e+124,  -6.70608105e-012],\\n        [  4.42786966e+160,   1.27100354e+025]]])\\nIt’s not safe to assume that np.empty will return an array of all zeros. In\\nmany cases, as previously shown, it will return uninitialized garbage\\nvalues.\\narange is an array-valued version of the built-in Python range function:\\nIn [26]: np.arange(15)\\nOut[26]: array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])\\nSee Table 4-1 for a short list of standard array creation functions. Since NumPy is\\nfocused on numerical computing, the data type, if not specified, will in many cases be\\nfloat64 (floating point).\\nTable 4-1. Array creation functions\\nFunction Description\\narray Convert input data (list, tuple, array, or other sequence type) to an ndarray either by\\ninferring a dtype or explicitly specifying a dtype. Copies the input data by default.\\nasarray Convert input to ndarray, but do not copy if the input is already an ndarray\\narange Like the built-in range but returns an ndarray instead of a list.\\nones, ones_like Produce an array of all 1’s with the given shape and dtype. ones_like takes another\\narray and produces a ones array of the same shape and dtype.\\nzeros, zeros_like Like ones and ones_like but producing arrays of 0’s instead\\n82 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 98, 'page_label': '83'}, page_content=\"Function Description\\nempty, empty_like Create new arrays by allocating new memory, but do not populate with any values like \\nones and zeros\\neye, identity Create a square N x N identity matrix (1’s on the diagonal and 0’s elsewhere)\\nData Types for ndarrays\\nThe data type or dtype is a special object containing the information the ndarray needs\\nto interpret a chunk of memory as a particular type of data:\\nIn [27]: arr1 = np.array([1, 2, 3], dtype=np.float64)\\nIn [28]: arr2 = np.array([1, 2, 3], dtype=np.int32)\\nIn [29]: arr1.dtype            In [30]: arr2.dtype    \\nOut[29]: dtype('float64')      Out[30]: dtype('int32')\\nDtypes are part of what make NumPy so powerful and flexible. In most cases they map\\ndirectly onto an underlying machine representation, which makes it easy to read and\\nwrite binary streams of data to disk and also to connect to code written in a low-level\\nlanguage like C or Fortran. The numerical dtypes are named the same way: a type name,\\nlike float or int, followed by a number indicating the number of bits per element. A\\nstandard double-precision floating point value (what’s used under the hood in Python’s\\nfloat object) takes up 8 bytes or 64 bits. Thus, this type is known in NumPy as\\nfloat64. See Table 4-2 for a full listing of NumPy’s supported data types.\\nDon’t worry about memorizing the NumPy dtypes, especially if you’re\\na new user. It’s often only necessary to care about the general kind of\\ndata you’re dealing with, whether floating point, complex, integer,\\nboolean, string, or general Python object. When you need more control\\nover how data are stored in memory and on disk, especially large data\\nsets, it is good to know that you have control over the storage type.\\nTable 4-2. NumPy data types\\nType Type Code Description\\nint8, uint8 i1, u1 Signed and unsigned 8-bit (1 byte) integer types\\nint16, uint16 i2, u2 Signed and unsigned 16-bit integer types\\nint32, uint32 i4, u4 Signed and unsigned 32-bit integer types\\nint64, uint64 i8, u8 Signed and unsigned 32-bit integer types\\nfloat16 f2 Half-precision floating point\\nfloat32 f4 or f Standard single-precision floating point. Compatible with C float\\nfloat64, float128 f8 or d Standard double-precision floating point. Compatible with C double\\nand Python float object\\nThe NumPy ndarray: A Multidimensional Array Object | 83\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 99, 'page_label': '84'}, page_content=\"Type Type Code Description\\nfloat128 f16 or g Extended-precision floating point\\ncomplex64, complex128,\\ncomplex256\\nc8, c16,\\nc32\\nComplex numbers represented by two 32, 64, or 128 floats, respectively\\nbool ? Boolean type storing True and False values\\nobject O Python object type\\nstring_ S Fixed-length string type (1 byte per character). For example, to create\\na string dtype with length 10, use 'S10'.\\nunicode_ U Fixed-length unicode type (number of bytes platform specific). Same\\nspecification semantics as string_ (e.g. 'U10').\\nYou can explicitly convert or cast an array from one dtype to another using ndarray’s \\nastype method:\\nIn [31]: arr = np.array([1, 2, 3, 4, 5])\\nIn [32]: arr.dtype\\nOut[32]: dtype('int64')\\nIn [33]: float_arr = arr.astype(np.float64)\\nIn [34]: float_arr.dtype\\nOut[34]: dtype('float64')\\nIn this example, integers were cast to floating point. If I cast some floating point num-\\nbers to be of integer dtype, the decimal part will be truncated:\\nIn [35]: arr = np.array([3.7, -1.2, -2.6, 0.5, 12.9, 10.1])\\nIn [36]: arr\\nOut[36]: array([  3.7,  -1.2,  -2.6,   0.5,  12.9,  10.1])\\nIn [37]: arr.astype(np.int32)\\nOut[37]: array([ 3, -1, -2,  0, 12, 10], dtype=int32)\\nShould you have an array of strings representing numbers, you can use astype to convert\\nthem to numeric form:\\nIn [38]: numeric_strings = np.array(['1.25', '-9.6', '42'], dtype=np.string_)\\nIn [39]: numeric_strings.astype(float)\\nOut[39]: array([  1.25,  -9.6 ,  42.  ])\\nIf casting were to fail for some reason (like a string that cannot be converted to\\nfloat64), a TypeError will be raised. See that I was a bit lazy and wrote float instead of\\nnp.float64; NumPy is smart enough to alias the Python types to the equivalent dtypes.\\nYou can also use another array’s dtype attribute:\\nIn [40]: int_array = np.arange(10)\\n84 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 100, 'page_label': '85'}, page_content=\"In [41]: calibers = np.array([.22, .270, .357, .380, .44, .50], dtype=np.float64)\\nIn [42]: int_array.astype(calibers.dtype)\\nOut[42]: array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.])\\nThere are shorthand type code strings you can also use to refer to a dtype:\\nIn [43]: empty_uint32 = np.empty(8, dtype='u4')\\nIn [44]: empty_uint32\\nOut[44]: \\narray([       0,        0, 65904672,        0, 64856792,        0,\\n       39438163,        0], dtype=uint32)\\nCalling astype always creates a new array (a copy of the data), even if\\nthe new dtype is the same as the old dtype.\\nIt’s worth keeping in mind that floating point numbers, such as those\\nin float64 and float32 arrays, are only capable of approximating frac-\\ntional quantities. In complex computations, you may accrue some\\nfloating point error, making comparisons only valid up to a certain num-\\nber of decimal places.\\nOperations between Arrays and Scalars\\nArrays are important because they enable you to express batch operations on data\\nwithout writing any for loops. This is usually called vectorization. Any arithmetic op-\\nerations between equal-size arrays applies the operation elementwise:\\nIn [45]: arr = np.array([[1., 2., 3.], [4., 5., 6.]])\\nIn [46]: arr\\nOut[46]: \\narray([[ 1.,  2.,  3.],\\n       [ 4.,  5.,  6.]])\\nIn [47]: arr * arr                 In [48]: arr - arr      \\nOut[47]:                           Out[48]:                \\narray([[  1.,   4.,   9.],         array([[ 0.,  0.,  0.], \\n       [ 16.,  25.,  36.]])               [ 0.,  0.,  0.]])\\nArithmetic operations with scalars are as you would expect, propagating the value to\\neach element:\\nIn [49]: 1 / arr                            In [50]: arr ** 0.5                 \\nOut[49]:                                    Out[50]:                            \\narray([[ 1.    ,  0.5   ,  0.3333],         array([[ 1.    ,  1.4142,  1.7321], \\n       [ 0.25  ,  0.2   ,  0.1667]])               [ 2.    ,  2.2361,  2.4495]])\\nThe NumPy ndarray: A Multidimensional Array Object | 85\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 101, 'page_label': '86'}, page_content='Operations between differently sized arrays is called broadcasting and will be discussed\\nin more detail in Chapter 12. Having a deep understanding of broadcasting is not nec-\\nessary for most of this book.\\nBasic Indexing and Slicing\\nNumPy array indexing is a rich topic, as there are many ways you may want to select\\na subset of your data or individual elements. One-dimensional arrays are simple; on\\nthe surface they act similarly to Python lists:\\nIn [51]: arr = np.arange(10)\\nIn [52]: arr\\nOut[52]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\\nIn [53]: arr[5]\\nOut[53]: 5\\nIn [54]: arr[5:8]\\nOut[54]: array([5, 6, 7])\\nIn [55]: arr[5:8] = 12\\nIn [56]: arr\\nOut[56]: array([ 0,  1,  2,  3,  4, 12, 12, 12,  8,  9])\\nAs you can see, if you assign a scalar value to a slice, as in arr[5:8] = 12, the value is\\npropagated (or broadcasted henceforth) to the entire selection. An important first dis-\\ntinction from lists is that array slices are views on the original array. This means that\\nthe data is not copied, and any modifications to the view will be reflected in the source\\narray:\\nIn [57]: arr_slice = arr[5:8]\\nIn [58]: arr_slice[1] = 12345\\nIn [59]: arr\\nOut[59]: array([    0,     1,     2,     3,     4,    12, 12345,    12,     8,     9])\\nIn [60]: arr_slice[:] = 64\\nIn [61]: arr\\nOut[61]: array([ 0,  1,  2,  3,  4, 64, 64, 64,  8,  9])\\nIf you are new to NumPy, you might be surprised by this, especially if they have used\\nother array programming languages which copy data more zealously. As NumPy has\\nbeen designed with large data use cases in mind, you could imagine performance and\\nmemory problems if NumPy insisted on copying data left and right.\\n86 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 102, 'page_label': '87'}, page_content='If you want a copy of a slice of an ndarray instead of a view, you will\\nneed to explicitly copy the array; for example arr[5:8].copy().\\nWith higher dimensional arrays, you have many more options. In a two-dimensional\\narray, the elements at each index are no longer scalars but rather one-dimensional\\narrays:\\nIn [62]: arr2d = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\\nIn [63]: arr2d[2]\\nOut[63]: array([7, 8, 9])\\nThus, individual elements can be accessed recursively. But that is a bit too much work,\\nso you can pass a comma-separated list of indices to select individual elements. So these\\nare equivalent:\\nIn [64]: arr2d[0][2]\\nOut[64]: 3\\nIn [65]: arr2d[0, 2]\\nOut[65]: 3\\nSee Figure 4-1 for an illustration of indexing on a 2D array.\\nFigure 4-1. Indexing elements in a NumPy array\\nIn multidimensional arrays, if you omit later indices, the returned object will be a lower-\\ndimensional ndarray consisting of all the data along the higher dimensions. So in the\\n2 × 2 × 3 array arr3d\\nIn [66]: arr3d = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\\nIn [67]: arr3d\\nOut[67]: \\narray([[[ 1,  2,  3],\\nThe NumPy ndarray: A Multidimensional Array Object | 87\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 103, 'page_label': '88'}, page_content='[ 4,  5,  6]],\\n       [[ 7,  8,  9],\\n        [10, 11, 12]]])\\narr3d[0] is a 2 × 3 array:\\nIn [68]: arr3d[0]\\nOut[68]: \\narray([[1, 2, 3],\\n       [4, 5, 6]])\\nBoth scalar values and arrays can be assigned to arr3d[0]:\\nIn [69]: old_values = arr3d[0].copy()\\nIn [70]: arr3d[0] = 42\\nIn [71]: arr3d\\nOut[71]: \\narray([[[42, 42, 42],\\n        [42, 42, 42]],\\n       [[ 7,  8,  9],\\n        [10, 11, 12]]])\\nIn [72]: arr3d[0] = old_values\\nIn [73]: arr3d\\nOut[73]: \\narray([[[ 1,  2,  3],\\n        [ 4,  5,  6]],\\n       [[ 7,  8,  9],\\n        [10, 11, 12]]])\\nSimilarly, arr3d[1, 0] gives you all of the values whose indices start with (1, 0), form-\\ning a 1-dimensional array:\\nIn [74]: arr3d[1, 0]\\nOut[74]: array([7, 8, 9])\\nNote that in all of these cases where subsections of the array have been selected, the\\nreturned arrays are views.\\nIndexing with slices\\nLike one-dimensional objects such as Python lists, ndarrays can be sliced using the\\nfamiliar syntax:\\nIn [75]: arr[1:6]\\nOut[75]: array([ 1,  2,  3,  4, 64])\\nHigher dimensional objects give you more options as you can slice one or more axes\\nand also mix integers. Consider the 2D array above, arr2d. Slicing this array is a bit\\ndifferent:\\nIn [76]: arr2d            In [77]: arr2d[:2]\\nOut[76]:                  Out[77]:          \\n88 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 104, 'page_label': '89'}, page_content=\"array([[1, 2, 3],         array([[1, 2, 3], \\n       [4, 5, 6],                [4, 5, 6]])\\n       [7, 8, 9]])\\nAs you can see, it has sliced along axis 0, the first axis. A slice, therefore, selects a range\\nof elements along an axis. You can pass multiple slices just like you can pass multiple\\nindexes:\\nIn [78]: arr2d[:2, 1:]\\nOut[78]: \\narray([[2, 3],\\n       [5, 6]])\\nWhen slicing like this, you always obtain array views of the same number of dimensions.\\nBy mixing integer indexes and slices, you get lower dimensional slices:\\nIn [79]: arr2d[1, :2]         In [80]: arr2d[2, :1]\\nOut[79]: array([4, 5])        Out[80]: array([7])\\nSee Figure 4-2 for an illustration. Note that a colon by itself means to take the entire\\naxis, so you can slice only higher dimensional axes by doing:\\nIn [81]: arr2d[:, :1]\\nOut[81]: \\narray([[1],\\n       [4],\\n       [7]])\\nOf course, assigning to a slice expression assigns to the whole selection:\\nIn [82]: arr2d[:2, 1:] = 0\\nBoolean Indexing\\nLet’s consider an example where we have some data in an array and an array of names\\nwith duplicates. I’m going to use here the randn function in numpy.random to generate\\nsome random normally distributed data:\\nIn [83]: names = np.array(['Bob', 'Joe', 'Will', 'Bob', 'Will', 'Joe', 'Joe'])\\nIn [84]: data = randn(7, 4)\\nIn [85]: names\\nOut[85]: \\narray(['Bob', 'Joe', 'Will', 'Bob', 'Will', 'Joe', 'Joe'], \\n      dtype='|S4')\\nIn [86]: data\\nOut[86]: \\narray([[-0.048 ,  0.5433, -0.2349,  1.2792],\\n       [-0.268 ,  0.5465,  0.0939, -2.0445],\\n       [-0.047 , -2.026 ,  0.7719,  0.3103],\\n       [ 2.1452,  0.8799, -0.0523,  0.0672],\\n       [-1.0023, -0.1698,  1.1503,  1.7289],\\nThe NumPy ndarray: A Multidimensional Array Object | 89\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 105, 'page_label': '90'}, page_content=\"[ 0.1913,  0.4544,  0.4519,  0.5535],\\n       [ 0.5994,  0.8174, -0.9297, -1.2564]])\\nFigure 4-2. Two-dimensional array slicing\\nSuppose each name corresponds to a row in the data array. If we wanted to select all\\nthe rows with corresponding name 'Bob'. Like arithmetic operations, comparisons\\n(such as ==) with arrays are also vectorized. Thus, comparing names with the string\\n'Bob' yields a boolean array:\\nIn [87]: names == 'Bob'\\nOut[87]: array([ True, False, False, True, False, False, False], dtype=bool)\\nThis boolean array can be passed when indexing the array:\\nIn [88]: data[names == 'Bob']\\nOut[88]: \\narray([[-0.048 ,  0.5433, -0.2349,  1.2792],\\n       [ 2.1452,  0.8799, -0.0523,  0.0672]])\\nThe boolean array must be of the same length as the axis it’s indexing. You can even\\nmix and match boolean arrays with slices or integers (or sequences of integers, more\\non this later):\\nIn [89]: data[names == 'Bob', 2:]\\nOut[89]: \\narray([[-0.2349,  1.2792],\\n90 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 106, 'page_label': '91'}, page_content=\"[-0.0523,  0.0672]])\\nIn [90]: data[names == 'Bob', 3]\\nOut[90]: array([ 1.2792,  0.0672])\\nTo select everything but 'Bob', you can either use != or negate the condition using -:\\nIn [91]: names != 'Bob'\\nOut[91]: array([False, True, True, False, True, True, True], dtype=bool)\\nIn [92]: data[-(names == 'Bob')]\\nOut[92]: \\narray([[-0.268 ,  0.5465,  0.0939, -2.0445],\\n       [-0.047 , -2.026 ,  0.7719,  0.3103],\\n       [-1.0023, -0.1698,  1.1503,  1.7289],\\n       [ 0.1913,  0.4544,  0.4519,  0.5535],\\n       [ 0.5994,  0.8174, -0.9297, -1.2564]])\\nSelecting two of the three names to combine multiple boolean conditions, use boolean\\narithmetic operators like & (and) and | (or):\\nIn [93]: mask = (names == 'Bob') | (names == 'Will')\\nIn [94]: mask\\nOut[94]: array([True, False, True, True, True, False, False], dtype=bool)\\nIn [95]: data[mask]\\nOut[95]: \\narray([[-0.048 ,  0.5433, -0.2349,  1.2792],\\n       [-0.047 , -2.026 ,  0.7719,  0.3103],\\n       [ 2.1452,  0.8799, -0.0523,  0.0672],\\n       [-1.0023, -0.1698,  1.1503,  1.7289]])\\nSelecting data from an array by boolean indexing always creates a copy of the data,\\neven if the returned array is unchanged.\\nThe Python keywords and and or do not work with boolean arrays.\\nSetting values with boolean arrays works in a common-sense way. To set all of the\\nnegative values in data to 0 we need only do:\\nIn [96]: data[data < 0] = 0\\nIn [97]: data\\nOut[97]: \\narray([[ 0.    ,  0.5433,  0.    ,  1.2792],\\n       [ 0.    ,  0.5465,  0.0939,  0.    ],\\n       [ 0.    ,  0.    ,  0.7719,  0.3103],\\n       [ 2.1452,  0.8799,  0.    ,  0.0672],\\n       [ 0.    ,  0.    ,  1.1503,  1.7289],\\n       [ 0.1913,  0.4544,  0.4519,  0.5535],\\n       [ 0.5994,  0.8174,  0.    ,  0.    ]])\\nThe NumPy ndarray: A Multidimensional Array Object | 91\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 107, 'page_label': '92'}, page_content=\"Setting whole rows or columns using a 1D boolean array is also easy:\\nIn [98]: data[names != 'Joe'] = 7\\nIn [99]: data\\nOut[99]: \\narray([[ 7.    ,  7.    ,  7.    ,  7.    ],\\n       [ 0.    ,  0.5465,  0.0939,  0.    ],\\n       [ 7.    ,  7.    ,  7.    ,  7.    ],\\n       [ 7.    ,  7.    ,  7.    ,  7.    ],\\n       [ 7.    ,  7.    ,  7.    ,  7.    ],\\n       [ 0.1913,  0.4544,  0.4519,  0.5535],\\n       [ 0.5994,  0.8174,  0.    ,  0.    ]])\\nFancy Indexing\\nFancy indexing is a term adopted by NumPy to describe indexing using integer arrays.\\nSuppose we had a 8 × 4 array:\\nIn [100]: arr = np.empty((8, 4))\\nIn [101]: for i in range(8):\\n   .....:     arr[i] = i\\nIn [102]: arr\\nOut[102]: \\narray([[ 0.,  0.,  0.,  0.],\\n       [ 1.,  1.,  1.,  1.],\\n       [ 2.,  2.,  2.,  2.],\\n       [ 3.,  3.,  3.,  3.],\\n       [ 4.,  4.,  4.,  4.],\\n       [ 5.,  5.,  5.,  5.],\\n       [ 6.,  6.,  6.,  6.],\\n       [ 7.,  7.,  7.,  7.]])\\nTo select out a subset of the rows in a particular order, you can simply pass a list or\\nndarray of integers specifying the desired order:\\nIn [103]: arr[[4, 3, 0, 6]]\\nOut[103]: \\narray([[ 4.,  4.,  4.,  4.],\\n       [ 3.,  3.,  3.,  3.],\\n       [ 0.,  0.,  0.,  0.],\\n       [ 6.,  6.,  6.,  6.]])\\nHopefully this code did what you expected! Using negative indices select rows from\\nthe end:\\nIn [104]: arr[[-3, -5, -7]]\\nOut[104]: \\narray([[ 5.,  5.,  5.,  5.],\\n       [ 3.,  3.,  3.,  3.],\\n       [ 1.,  1.,  1.,  1.]])\\n92 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 108, 'page_label': '93'}, page_content='Passing multiple index arrays does something slightly different; it selects a 1D array of\\nelements corresponding to each tuple of indices:\\n# more on reshape in Chapter 12\\nIn [105]: arr = np.arange(32).reshape((8, 4))\\nIn [106]: arr\\nOut[106]: \\narray([[ 0,  1,  2,  3],\\n       [ 4,  5,  6,  7],\\n       [ 8,  9, 10, 11],\\n       [12, 13, 14, 15],\\n       [16, 17, 18, 19],\\n       [20, 21, 22, 23],\\n       [24, 25, 26, 27],\\n       [28, 29, 30, 31]])\\nIn [107]: arr[[1, 5, 7, 2], [0, 3, 1, 2]]\\nOut[107]: array([ 4, 23, 29, 10])\\nTake a moment to understand what just happened: the elements (1, 0), (5, 3), (7,\\n1), and (2, 2) were selected. The behavior of fancy indexing in this case is a bit different\\nfrom what some users might have expected (myself included), which is the rectangular\\nregion formed by selecting a subset of the matrix’s rows and columns. Here is one way\\nto get that:\\nIn [108]: arr[[1, 5, 7, 2]][:, [0, 3, 1, 2]]\\nOut[108]: \\narray([[ 4,  7,  5,  6],\\n       [20, 23, 21, 22],\\n       [28, 31, 29, 30],\\n       [ 8, 11,  9, 10]])\\nAnother way is to use the np.ix_ function, which converts two 1D integer arrays to an\\nindexer that selects the square region:\\nIn [109]: arr[np.ix_([1, 5, 7, 2], [0, 3, 1, 2])]\\nOut[109]: \\narray([[ 4,  7,  5,  6],\\n       [20, 23, 21, 22],\\n       [28, 31, 29, 30],\\n       [ 8, 11,  9, 10]])\\nKeep in mind that fancy indexing, unlike slicing, always copies the data into a new array.\\nTransposing Arrays and Swapping Axes\\nTransposing is a special form of reshaping which similarly returns a view on the un-\\nderlying data without copying anything. Arrays have the transpose method and also\\nthe special T attribute:\\nIn [110]: arr = np.arange(15).reshape((3, 5))\\nIn [111]: arr                        In [112]: arr.T      \\nThe NumPy ndarray: A Multidimensional Array Object | 93\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 109, 'page_label': '94'}, page_content='Out[111]:                            Out[112]:            \\narray([[ 0,  1,  2,  3,  4],         array([[ 0,  5, 10], \\n       [ 5,  6,  7,  8,  9],                [ 1,  6, 11], \\n       [10, 11, 12, 13, 14]])               [ 2,  7, 12], \\n                                            [ 3,  8, 13], \\n                                            [ 4,  9, 14]])\\nWhen doing matrix computations, you will do this very often, like for example com-\\nputing the inner matrix product XTX using np.dot:\\nIn [113]: arr = np.random.randn(6, 3)\\nIn [114]: np.dot(arr.T, arr)\\nOut[114]: \\narray([[ 2.584 ,  1.8753,  0.8888],\\n       [ 1.8753,  6.6636,  0.3884],\\n       [ 0.8888,  0.3884,  3.9781]])\\nFor higher dimensional arrays, transpose will accept a tuple of axis numbers to permute\\nthe axes (for extra mind bending):\\nIn [115]: arr = np.arange(16).reshape((2, 2, 4))\\nIn [116]: arr\\nOut[116]: \\narray([[[ 0,  1,  2,  3],\\n        [ 4,  5,  6,  7]],\\n       [[ 8,  9, 10, 11],\\n        [12, 13, 14, 15]]])\\nIn [117]: arr.transpose((1, 0, 2))\\nOut[117]: \\narray([[[ 0,  1,  2,  3],\\n        [ 8,  9, 10, 11]],\\n       [[ 4,  5,  6,  7],\\n        [12, 13, 14, 15]]])\\nSimple transposing with .T is just a special case of swapping axes. ndarray has the\\nmethod swapaxes which takes a pair of axis numbers:\\nIn [118]: arr                      In [119]: arr.swapaxes(1, 2)\\nOut[118]:                          Out[119]:                   \\narray([[[ 0,  1,  2,  3],          array([[[ 0,  4],           \\n        [ 4,  5,  6,  7]],                 [ 1,  5],           \\n                                           [ 2,  6],           \\n       [[ 8,  9, 10, 11],                  [ 3,  7]],          \\n        [12, 13, 14, 15]]])                                    \\n                                          [[ 8, 12],           \\n                                           [ 9, 13],           \\n                                           [10, 14],           \\n                                           [11, 15]]])\\nswapaxes similarly returns a view on the data without making a copy.\\n94 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 110, 'page_label': '95'}, page_content='Universal Functions: Fast Element-wise Array Functions\\nA universal function, or ufunc, is a function that performs elementwise operations on\\ndata in ndarrays. You can think of them as fast vectorized wrappers for simple functions\\nthat take one or more scalar values and produce one or more scalar results.\\nMany ufuncs are simple elementwise transformations, like sqrt or exp:\\nIn [120]: arr = np.arange(10)\\nIn [121]: np.sqrt(arr)\\nOut[121]: \\narray([ 0.    ,  1.    ,  1.4142,  1.7321,  2.    ,  2.2361,  2.4495,\\n        2.6458,  2.8284,  3.    ])\\nIn [122]: np.exp(arr)\\nOut[122]: \\narray([    1.    ,     2.7183,     7.3891,    20.0855,    54.5982,\\n         148.4132,   403.4288,  1096.6332,  2980.958 ,  8103.0839])\\nThese are referred to as unary ufuncs. Others, such as add or maximum, take 2 arrays\\n(thus, binary ufuncs) and return a single array as the result:\\nIn [123]: x = randn(8)\\nIn [124]: y = randn(8)\\nIn [125]: x\\nOut[125]: \\narray([ 0.0749,  0.0974,  0.2002, -0.2551,  0.4655,  0.9222,  0.446 ,\\n       -0.9337])\\nIn [126]: y\\nOut[126]: \\narray([ 0.267 , -1.1131, -0.3361,  0.6117, -1.2323,  0.4788,  0.4315,\\n       -0.7147])\\nIn [127]: np.maximum(x, y) # element-wise maximum\\nOut[127]: \\narray([ 0.267 ,  0.0974,  0.2002,  0.6117,  0.4655,  0.9222,  0.446 ,\\n       -0.7147])\\nWhile not common, a ufunc can return multiple arrays. modf is one example, a vector-\\nized version of the built-in Python divmod: it returns the fractional and integral parts of\\na floating point array:\\nIn [128]: arr = randn(7) * 5\\nIn [129]: np.modf(arr)\\nOut[129]: \\n(array([-0.6808,  0.0636, -0.386 ,  0.1393, -0.8806,  0.9363, -0.883 ]),\\n array([-2.,  4., -3.,  5., -3.,  3., -6.]))\\nUniversal Functions: Fast Element-wise Array Functions | 95\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 111, 'page_label': '96'}, page_content='See Table 4-3 and Table 4-4 for a listing of available ufuncs.\\nTable 4-3. Unary ufuncs\\nFunction Description\\nabs, fabs Compute the absolute value element-wise for integer, floating point, or complex values.\\nUse fabs as a faster alternative for non-complex-valued data\\nsqrt Compute the square root of each element. Equivalent to arr ** 0.5\\nsquare Compute the square of each element. Equivalent to arr ** 2\\nexp Compute the exponent ex of each element\\nlog, log10, log2, log1p Natural logarithm (base e), log base 10, log base 2, and log(1 + x), respectively\\nsign Compute the sign of each element: 1 (positive), 0 (zero), or -1 (negative)\\nceil Compute the ceiling of each element, i.e. the smallest integer greater than or equal to\\neach element\\nfloor Compute the floor of each element, i.e. the largest integer less than or equal to each\\nelement\\nrint Round elements to the nearest integer, preserving the dtype\\nmodf Return fractional and integral parts of array as separate array\\nisnan Return boolean array indicating whether each value is NaN (Not a Number)\\nisfinite, isinf Return boolean array indicating whether each element is finite (non-inf, non-NaN) or\\ninfinite, respectively\\ncos, cosh, sin, sinh,\\ntan, tanh\\nRegular and hyperbolic trigonometric functions\\narccos, arccosh, arcsin,\\narcsinh, arctan, arctanh\\nInverse trigonometric functions\\nlogical_not Compute truth value of not x element-wise. Equivalent to -arr.\\nTable 4-4. Binary universal functions\\nFunction Description\\nadd Add corresponding elements in arrays\\nsubtract Subtract elements in second array from first array\\nmultiply Multiply array elements\\ndivide, floor_divide Divide or floor divide (truncating the remainder)\\npower Raise elements in first array to powers indicated in second array\\nmaximum, fmax Element-wise maximum. fmax ignores NaN\\nminimum, fmin Element-wise minimum. fmin ignores NaN\\nmod Element-wise modulus (remainder of division)\\ncopysign Copy sign of values in second argument to values in first argument\\n96 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 112, 'page_label': '97'}, page_content='Function Description\\ngreater, greater_equal,\\nless, less_equal, equal,\\nnot_equal\\nPerform element-wise comparison, yielding boolean array. Equivalent to infix operators\\n>, >=, <, <=, ==, !=\\nlogical_and,\\nlogical_or, logical_xor\\nCompute element-wise truth value of logical operation. Equivalent to infix operators &\\n|, ^\\nData Processing Using Arrays\\nUsing NumPy arrays enables you to express many kinds of data processing tasks as\\nconcise array expressions that might otherwise require writing loops. This practice of\\nreplacing explicit loops with array expressions is commonly referred to as vectoriza-\\ntion. In general, vectorized array operations will often be one or two (or more) orders\\nof magnitude faster than their pure Python equivalents, with the biggest impact in any\\nkind of numerical computations. Later, in Chapter 12, I will explain broadcasting, a\\npowerful method for vectorizing computations.\\nAs a simple example, suppose we wished to evaluate the function sqrt(x^2 + y^2)\\nacross a regular grid of values. The np.meshgrid function takes two 1D arrays and pro-\\nduces two 2D matrices corresponding to all pairs of (x, y) in the two arrays:\\nIn [130]: points = np.arange(-5, 5, 0.01) # 1000 equally spaced points\\nIn [131]: xs, ys = np.meshgrid(points, points)\\nIn [132]: ys\\nOut[132]: \\narray([[-5.  , -5.  , -5.  , ..., -5.  , -5.  , -5.  ],\\n       [-4.99, -4.99, -4.99, ..., -4.99, -4.99, -4.99],\\n       [-4.98, -4.98, -4.98, ..., -4.98, -4.98, -4.98],\\n       ..., \\n       [ 4.97,  4.97,  4.97, ...,  4.97,  4.97,  4.97],\\n       [ 4.98,  4.98,  4.98, ...,  4.98,  4.98,  4.98],\\n       [ 4.99,  4.99,  4.99, ...,  4.99,  4.99,  4.99]])\\nNow, evaluating the function is a simple matter of writing the same expression you\\nwould write with two points:\\nIn [134]: import matplotlib.pyplot as plt\\nIn [135]: z = np.sqrt(xs ** 2 + ys ** 2)\\nIn [136]: z\\nOut[136]: \\narray([[ 7.0711,  7.064 ,  7.0569, ...,  7.0499,  7.0569,  7.064 ],\\n       [ 7.064 ,  7.0569,  7.0499, ...,  7.0428,  7.0499,  7.0569],\\n       [ 7.0569,  7.0499,  7.0428, ...,  7.0357,  7.0428,  7.0499],\\n       ..., \\n       [ 7.0499,  7.0428,  7.0357, ...,  7.0286,  7.0357,  7.0428],\\n       [ 7.0569,  7.0499,  7.0428, ...,  7.0357,  7.0428,  7.0499],\\n       [ 7.064 ,  7.0569,  7.0499, ...,  7.0428,  7.0499,  7.0569]])\\nData Processing Using Arrays | 97\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 113, 'page_label': '98'}, page_content='In [137]: plt.imshow(z, cmap=plt.cm.gray); plt.colorbar()\\nOut[137]: <matplotlib.colorbar.Colorbar instance at 0x4e46d40>\\nIn [138]: plt.title(\"Image plot of $\\\\sqrt{x^2 + y^2}$ for a grid of values\")\\nOut[138]: <matplotlib.text.Text at 0x4565790>\\nSee Figure 4-3. Here I used the matplotlib function imshow to create an image plot from\\na 2D array of function values.\\nFigure 4-3. Plot of function evaluated on grid\\nExpressing Conditional Logic as Array Operations\\nThe numpy.where function is a vectorized version of the ternary expression x if condi\\ntion else y. Suppose we had a boolean array and two arrays of values:\\nIn [140]: xarr = np.array([1.1, 1.2, 1.3, 1.4, 1.5])\\nIn [141]: yarr = np.array([2.1, 2.2, 2.3, 2.4, 2.5])\\nIn [142]: cond = np.array([True, False, True, True, False])\\nSuppose we wanted to take a value from xarr whenever the corresponding value in\\ncond is True otherwise take the value from yarr. A list comprehension doing this might\\nlook like:\\nIn [143]: result = [(x if c else y)\\n   .....:           for x, y, c in zip(xarr, yarr, cond)]\\nIn [144]: result\\nOut[144]: [1.1000000000000001, 2.2000000000000002, 1.3, 1.3999999999999999, 2.5]\\n98 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 114, 'page_label': '99'}, page_content='This has multiple problems. First, it will not be very fast for large arrays (because all\\nthe work is being done in pure Python). Secondly, it will not work with multidimen-\\nsional arrays. With np.where you can write this very concisely:\\nIn [145]: result = np.where(cond, xarr, yarr)\\nIn [146]: result\\nOut[146]: array([ 1.1,  2.2,  1.3,  1.4,  2.5])\\nThe second and third arguments to np.where don’t need to be arrays; one or both of\\nthem can be scalars. A typical use of where in data analysis is to produce a new array of\\nvalues based on another array. Suppose you had a matrix of randomly generated data\\nand you wanted to replace all positive values with 2 and all negative values with -2.\\nThis is very easy to do with np.where:\\nIn [147]: arr = randn(4, 4)\\nIn [148]: arr\\nOut[148]: \\narray([[ 0.6372,  2.2043,  1.7904,  0.0752],\\n       [-1.5926, -1.1536,  0.4413,  0.3483],\\n       [-0.1798,  0.3299,  0.7827, -0.7585],\\n       [ 0.5857,  0.1619,  1.3583, -1.3865]])\\nIn [149]: np.where(arr > 0, 2, -2)\\nOut[149]: \\narray([[ 2,  2,  2,  2],\\n       [-2, -2,  2,  2],\\n       [-2,  2,  2, -2],\\n       [ 2,  2,  2, -2]])\\nIn [150]: np.where(arr > 0, 2, arr) # set only positive values to 2\\nOut[150]: \\narray([[ 2.    ,  2.    ,  2.    ,  2.    ],\\n       [-1.5926, -1.1536,  2.    ,  2.    ],\\n       [-0.1798,  2.    ,  2.    , -0.7585],\\n       [ 2.    ,  2.    ,  2.    , -1.3865]])\\nThe arrays passed to where can be more than just equal sizes array or scalers.\\nWith some cleverness you can use where to express more complicated logic; consider\\nthis example where I have two boolean arrays, cond1 and cond2, and wish to assign a\\ndifferent value for each of the 4 possible pairs of boolean values:\\nresult = []\\nfor i in range(n):\\n    if cond1[i] and cond2[i]:\\n        result.append(0)\\n    elif cond1[i]:\\n        result.append(1)\\n    elif cond2[i]:\\n        result.append(2)\\n    else:\\n        result.append(3)\\nData Processing Using Arrays | 99\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 115, 'page_label': '100'}, page_content='While perhaps not immediately obvious, this for loop can be converted into a nested\\nwhere expression:\\nnp.where(cond1 & cond2, 0,\\n         np.where(cond1, 1,\\n                  np.where(cond2, 2, 3)))\\nIn this particular example, we can also take advantage of the fact that boolean values\\nare treated as 0 or 1 in calculations, so this could alternatively be expressed (though a\\nbit more cryptically) as an arithmetic operation:\\nresult = 1 * cond1 + 2 * cond2 + 3 * -(cond1 | cond2)\\nMathematical and Statistical Methods\\nA set of mathematical functions which compute statistics about an entire array or about\\nthe data along an axis are accessible as array methods. Aggregations (often called\\nreductions) like sum, mean, and standard deviation std can either be used by calling the\\narray instance method or using the top level NumPy function:\\nIn [151]: arr = np.random.randn(5, 4) # normally-distributed data\\nIn [152]: arr.mean()\\nOut[152]: 0.062814911084854597\\nIn [153]: np.mean(arr)\\nOut[153]: 0.062814911084854597\\nIn [154]: arr.sum()\\nOut[154]: 1.2562982216970919\\nFunctions like mean and sum take an optional axis argument which computes the statistic\\nover the given axis, resulting in an array with one fewer dimension:\\nIn [155]: arr.mean(axis=1)\\nOut[155]: array([-1.2833,  0.2844,  0.6574,  0.6743, -0.0187])\\nIn [156]: arr.sum(0)\\nOut[156]: array([-3.1003, -1.6189,  1.4044,  4.5712])\\nOther methods like cumsum and cumprod do not aggregate, instead producing an array\\nof the intermediate results:\\nIn [157]: arr = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8]])\\nIn [158]: arr.cumsum(0)        In [159]: arr.cumprod(1)\\nOut[158]:                      Out[159]:               \\narray([[ 0,  1,  2],           array([[  0,   0,   0], \\n       [ 3,  5,  7],                  [  3,  12,  60], \\n       [ 9, 12, 15]])                 [  6,  42, 336]])\\nSee Table 4-5 for a full listing. We’ll see many examples of these methods in action in\\nlater chapters.\\n100 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 116, 'page_label': '101'}, page_content='Table 4-5. Basic array statistical methods\\nMethod Description\\nsum Sum of all the elements in the array or along an axis. Zero-length arrays have sum 0.\\nmean Arithmetic mean. Zero-length arrays have NaN mean.\\nstd, var Standard deviation and variance, respectively, with optional degrees of freedom adjust-\\nment (default denominator n).\\nmin, max Minimum and maximum.\\nargmin, argmax Indices of minimum and maximum elements, respectively.\\ncumsum Cumulative sum of elements starting from 0\\ncumprod Cumulative product of elements starting from 1\\nMethods for Boolean Arrays\\nBoolean values are coerced to 1 (True) and 0 (False) in the above methods. Thus, sum\\nis often used as a means of counting True values in a boolean array:\\nIn [160]: arr = randn(100)\\nIn [161]: (arr > 0).sum() # Number of positive values\\nOut[161]: 44\\nThere are two additional methods, any and all, useful especially for boolean arrays. \\nany tests whether one or more values in an array is True, while all checks if every value\\nis True:\\nIn [162]: bools = np.array([False, False, True, False])\\nIn [163]: bools.any()\\nOut[163]: True\\nIn [164]: bools.all()\\nOut[164]: False\\nThese methods also work with non-boolean arrays, where non-zero elements evaluate\\nto True.\\nSorting\\nLike Python’s built-in list type, NumPy arrays can be sorted in-place using the sort\\nmethod:\\nIn [165]: arr = randn(8)\\nIn [166]: arr\\nOut[166]: \\narray([ 0.6903,  0.4678,  0.0968, -0.1349,  0.9879,  0.0185, -1.3147,\\n       -0.5425])\\nIn [167]: arr.sort()\\nData Processing Using Arrays | 101\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 117, 'page_label': '102'}, page_content=\"In [168]: arr\\nOut[168]: \\narray([-1.3147, -0.5425, -0.1349,  0.0185,  0.0968,  0.4678,  0.6903,\\n        0.9879])\\nMultidimensional arrays can have each 1D section of values sorted in-place along an\\naxis by passing the axis number to sort:\\nIn [169]: arr = randn(5, 3)\\nIn [170]: arr\\nOut[170]: \\narray([[-0.7139, -1.6331, -0.4959],\\n       [ 0.8236, -1.3132, -0.1935],\\n       [-1.6748,  3.0336, -0.863 ],\\n       [-0.3161,  0.5362, -2.468 ],\\n       [ 0.9058,  1.1184, -1.0516]])\\nIn [171]: arr.sort(1)\\nIn [172]: arr\\nOut[172]: \\narray([[-1.6331, -0.7139, -0.4959],\\n       [-1.3132, -0.1935,  0.8236],\\n       [-1.6748, -0.863 ,  3.0336],\\n       [-2.468 , -0.3161,  0.5362],\\n       [-1.0516,  0.9058,  1.1184]])\\nThe top level method np.sort returns a sorted copy of an array instead of modifying\\nthe array in place. A quick-and-dirty way to compute the quantiles of an array is to sort\\nit and select the value at a particular rank:\\nIn [173]: large_arr = randn(1000)\\nIn [174]: large_arr.sort()\\nIn [175]: large_arr[int(0.05 * len(large_arr))] # 5% quantile\\nOut[175]: -1.5791023260896004\\nFor more details on using NumPy’s sorting methods, and more advanced techniques\\nlike indirect sorts, see Chapter 12. Several other kinds of data manipulations related to\\nsorting (for example, sorting a table of data by one or more columns) are also to be\\nfound in pandas.\\nUnique and Other Set Logic\\nNumPy has some basic set operations for one-dimensional ndarrays. Probably the most\\ncommonly used one is np.unique, which returns the sorted unique values in an array:\\nIn [176]: names = np.array(['Bob', 'Joe', 'Will', 'Bob', 'Will', 'Joe', 'Joe'])\\nIn [177]: np.unique(names)\\nOut[177]: \\n102 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 118, 'page_label': '103'}, page_content=\"array(['Bob', 'Joe', 'Will'], \\n      dtype='|S4')\\nIn [178]: ints = np.array([3, 3, 3, 2, 2, 1, 1, 4, 4])\\nIn [179]: np.unique(ints)\\nOut[179]: array([1, 2, 3, 4])\\nContrast np.unique with the pure Python alternative:\\nIn [180]: sorted(set(names))\\nOut[180]: ['Bob', 'Joe', 'Will']\\nAnother function, np.in1d, tests membership of the values in one array in another,\\nreturning a boolean array:\\nIn [181]: values = np.array([6, 0, 0, 3, 2, 5, 6])\\nIn [182]: np.in1d(values, [2, 3, 6])\\nOut[182]: array([ True, False, False,  True,  True, False,  True], dtype=bool)\\nSee Table 4-6 for a listing of set functions in NumPy.\\nTable 4-6. Array set operations\\nMethod Description\\nunique(x) Compute the sorted, unique elements in x\\nintersect1d(x, y) Compute the sorted, common elements in x and y\\nunion1d(x, y) Compute the sorted union of elements\\nin1d(x, y) Compute a boolean array indicating whether each element of x is contained in y\\nsetdiff1d(x, y) Set difference, elements in x that are not in y\\nsetxor1d(x, y) Set symmetric differences; elements that are in either of the arrays, but not both\\nFile Input and Output with Arrays\\nNumPy is able to save and load data to and from disk either in text or binary format.\\nIn later chapters you will learn about tools in pandas for reading tabular data into\\nmemory.\\nStoring Arrays on Disk in Binary Format\\nnp.save and np.load are the two workhorse functions for efficiently saving and loading\\narray data on disk. Arrays are saved by default in an uncompressed raw binary format\\nwith file extension .npy.\\nIn [183]: arr = np.arange(10)\\nIn [184]: np.save('some_array', arr)\\nFile Input and Output with Arrays | 103\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 119, 'page_label': '104'}, page_content=\"If the file path does not already end in .npy, the extension will be appended. The array\\non disk can then be loaded using np.load:\\nIn [185]: np.load('some_array.npy')\\nOut[185]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\\nYou save multiple arrays in a zip archive using np.savez and passing the arrays as key-\\nword arguments:\\nIn [186]: np.savez('array_archive.npz', a=arr, b=arr)\\nWhen loading an .npz file, you get back a dict-like object which loads the individual\\narrays lazily:\\nIn [187]: arch = np.load('array_archive.npz')\\nIn [188]: arch['b']\\nOut[188]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\\nSaving and Loading Text Files\\nLoading text from files is a fairly standard task. The landscape of file reading and writing\\nfunctions in Python can be a bit confusing for a newcomer, so I will focus mainly on\\nthe read_csv and read_table functions in pandas. It will at times be useful to load data\\ninto vanilla NumPy arrays using np.loadtxt or the more specialized np.genfromtxt.\\nThese functions have many options allowing you to specify different delimiters, con-\\nverter functions for certain columns, skipping rows, and other things. Take a simple\\ncase of a comma-separated file (CSV) like this:\\nIn [191]: !cat array_ex.txt\\n0.580052,0.186730,1.040717,1.134411\\n0.194163,-0.636917,-0.938659,0.124094\\n-0.126410,0.268607,-0.695724,0.047428\\n-1.484413,0.004176,-0.744203,0.005487\\n2.302869,0.200131,1.670238,-1.881090\\n-0.193230,1.047233,0.482803,0.960334\\nThis can be loaded into a 2D array like so:\\nIn [192]: arr = np.loadtxt('array_ex.txt', delimiter=',')\\nIn [193]: arr\\nOut[193]: \\narray([[ 0.5801,  0.1867,  1.0407,  1.1344],\\n       [ 0.1942, -0.6369, -0.9387,  0.1241],\\n       [-0.1264,  0.2686, -0.6957,  0.0474],\\n       [-1.4844,  0.0042, -0.7442,  0.0055],\\n       [ 2.3029,  0.2001,  1.6702, -1.8811],\\n       [-0.1932,  1.0472,  0.4828,  0.9603]])\\nnp.savetxt performs the inverse operation: writing an array to a delimited text file.\\ngenfromtxt is similar to loadtxt but is geared for structured arrays and missing data\\nhandling; see Chapter 12 for more on structured arrays.\\n104 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 120, 'page_label': '105'}, page_content='For more on file reading and writing, especially tabular or spreadsheet-\\nlike data, see the later chapters involving pandas and DataFrame objects.\\nLinear Algebra\\nLinear algebra, like matrix multiplication, decompositions, determinants, and other\\nsquare matrix math, is an important part of any array library. Unlike some languages\\nlike MATLAB, multiplying two two-dimensional arrays with * is an element-wise\\nproduct instead of a matrix dot product. As such, there is a function dot, both an array\\nmethod, and a function in the numpy namespace, for matrix multiplication:\\nIn [194]: x = np.array([[1., 2., 3.], [4., 5., 6.]])\\nIn [195]: y = np.array([[6., 23.], [-1, 7], [8, 9]])\\nIn [196]: x                   In [197]: y          \\nOut[196]:                     Out[197]:            \\narray([[ 1.,  2.,  3.],       array([[  6.,  23.], \\n       [ 4.,  5.,  6.]])             [ -1.,   7.], \\n                                     [  8.,   9.]])\\n                                                   \\nIn [198]: x.dot(y)  # equivalently np.dot(x, y)\\nOut[198]: \\narray([[  28.,   64.],\\n       [  67.,  181.]])\\nA matrix product between a 2D array and a suitably sized 1D array results in a 1D array:\\nIn [199]: np.dot(x, np.ones(3))\\nOut[199]: array([  6.,  15.])\\nnumpy.linalg has a standard set of matrix decompositions and things like inverse and\\ndeterminant. These are implemented under the hood using the same industry-standard\\nFortran libraries used in other languages like MATLAB and R, such as like BLAS, LA-\\nPACK, or possibly (depending on your NumPy build) the Intel MKL:\\nIn [201]: from numpy.linalg import inv, qr\\nIn [202]: X = randn(5, 5)\\nIn [203]: mat = X.T.dot(X)\\nIn [204]: inv(mat)\\nOut[204]: \\narray([[ 3.0361, -0.1808, -0.6878, -2.8285, -1.1911],\\n       [-0.1808,  0.5035,  0.1215,  0.6702,  0.0956],\\n       [-0.6878,  0.1215,  0.2904,  0.8081,  0.3049],\\n       [-2.8285,  0.6702,  0.8081,  3.4152,  1.1557],\\n       [-1.1911,  0.0956,  0.3049,  1.1557,  0.6051]])\\nIn [205]: mat.dot(inv(mat))\\nLinear Algebra | 105\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 121, 'page_label': '106'}, page_content='Out[205]: \\narray([[ 1.,  0.,  0.,  0., -0.],\\n       [ 0.,  1., -0.,  0.,  0.],\\n       [ 0., -0.,  1.,  0.,  0.],\\n       [ 0., -0., -0.,  1., -0.],\\n       [ 0.,  0.,  0.,  0.,  1.]])\\nIn [206]: q, r = qr(mat)\\nIn [207]: r\\nOut[207]: \\narray([[ -6.9271,   7.389 ,   6.1227,  -7.1163,  -4.9215],\\n       [  0.    ,  -3.9735,  -0.8671,   2.9747,  -5.7402],\\n       [  0.    ,   0.    , -10.2681,   1.8909,   1.6079],\\n       [  0.    ,   0.    ,   0.    ,  -1.2996,   3.3577],\\n       [  0.    ,   0.    ,   0.    ,   0.    ,   0.5571]])\\nSee Table 4-7 for a list of some of the most commonly-used linear algebra functions.\\nThe scientific Python community is hopeful that there may be a matrix\\nmultiplication infix operator implemented someday, providing syntac-\\ntically nicer alternative to using np.dot. But for now this is the way.\\nTable 4-7. Commonly-used numpy.linalg functions\\nFunction Description\\ndiag Return the diagonal (or off-diagonal) elements of a square matrix as a 1D array, or convert a 1D array into a square\\nmatrix with zeros on the off-diagonal\\ndot Matrix multiplication\\ntrace Compute the sum of the diagonal elements\\ndet Compute the matrix determinant\\neig Compute the eigenvalues and eigenvectors of a square matrix\\ninv Compute the inverse of a square matrix\\npinv Compute the Moore-Penrose pseudo-inverse inverse of a square matrix\\nqr Compute the QR decomposition\\nsvd Compute the singular value decomposition (SVD)\\nsolve Solve the linear system Ax = b for x, where A is a square matrix\\nlstsq Compute the least-squares solution to y = Xb\\nRandom Number Generation\\nThe numpy.random module supplements the built-in Python random with functions for\\nefficiently generating whole arrays of sample values from many kinds of probability\\n106 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 122, 'page_label': '107'}, page_content='distributions. For example, you can get a 4 by 4 array of samples from the standard\\nnormal distribution using normal:\\nIn [208]: samples = np.random.normal(size=(4, 4))\\nIn [209]: samples\\nOut[209]: \\narray([[ 0.1241,  0.3026,  0.5238,  0.0009],\\n       [ 1.3438, -0.7135, -0.8312, -2.3702],\\n       [-1.8608, -0.8608,  0.5601, -1.2659],\\n       [ 0.1198, -1.0635,  0.3329, -2.3594]])\\nPython’s built-in random module, by contrast, only samples one value at a time. As you\\ncan see from this benchmark, numpy.random is well over an order of magnitude faster\\nfor generating very large samples:\\nIn [210]: from random import normalvariate\\nIn [211]: N = 1000000\\nIn [212]: %timeit samples = [normalvariate(0, 1) for _ in xrange(N)]\\n1 loops, best of 3: 1.33 s per loop\\nIn [213]: %timeit np.random.normal(size=N)\\n10 loops, best of 3: 57.7 ms per loop\\nSee table Table 4-8 for a partial list of functions available in numpy.random. I’ll give some\\nexamples of leveraging these functions’ ability to generate large arrays of samples all at\\nonce in the next section.\\nTable 4-8. Partial list of numpy.random functions\\nFunction Description\\nseed Seed the random number generator\\npermutation Return a random permutation of a sequence, or return a permuted range\\nshuffle Randomly permute a sequence in place\\nrand Draw samples from a uniform distribution\\nrandint Draw random integers from a given low-to-high range\\nrandn Draw samples from a normal distribution with mean 0 and standard deviation 1 (MATLAB-like interface)\\nbinomial Draw samples a binomial distribution\\nnormal Draw samples from a normal (Gaussian) distribution\\nbeta Draw samples from a beta distribution\\nchisquare Draw samples from a chi-square distribution\\ngamma Draw samples from a gamma distribution\\nuniform Draw samples from a uniform [0, 1) distribution\\nRandom Number Generation | 107\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 123, 'page_label': '108'}, page_content='Example: Random Walks\\nAn illustrative application of utilizing array operations is in the simulation of random\\nwalks. Let’s first consider a simple random walk starting at 0 with steps of 1 and -1\\noccurring with equal probability. A pure Python way to implement a single random\\nwalk with 1,000 steps using the built-in random module:\\nimport random\\nposition = 0\\nwalk = [position]\\nsteps = 1000\\nfor i in xrange(steps):\\n    step = 1 if random.randint(0, 1) else -1\\n    position += step\\n    walk.append(position)\\nSee Figure 4-4 for an example plot of the first 100 values on one of these random walks.\\nFigure 4-4. A simple random walk\\nYou might make the observation that walk is simply the cumulative sum of the random\\nsteps and could be evaluated as an array expression. Thus, I use the np.random module\\nto draw 1,000 coin flips at once, set these to 1 and -1, and compute the cumulative sum:\\nIn [215]: nsteps = 1000\\nIn [216]: draws = np.random.randint(0, 2, size=nsteps)\\nIn [217]: steps = np.where(draws > 0, 1, -1)\\nIn [218]: walk = steps.cumsum()\\n108 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 124, 'page_label': '109'}, page_content='From this we can begin to extract statistics like the minimum and maximum value along\\nthe walk’s trajectory:\\nIn [219]: walk.min()        In [220]: walk.max()\\nOut[219]: -3                Out[220]: 31\\nA more complicated statistic is the first crossing time , the step at which the random\\nwalk reaches a particular value. Here we might want to know how long it took the\\nrandom walk to get at least 10 steps away from the origin 0 in either direction.\\nnp.abs(walk) >= 10 gives us a boolean array indicating where the walk has reached or\\nexceeded 10, but we want the index of the first 10 or -10. Turns out this can be com-\\nputed using argmax, which returns the first index of the maximum value in the boolean\\narray (True is the maximum value):\\nIn [221]: (np.abs(walk) >= 10).argmax()\\nOut[221]: 37\\nNote that using argmax here is not always efficient because it always makes a full scan\\nof the array. In this special case once a True is observed we know it to be the maximum\\nvalue.\\nSimulating Many Random Walks at Once\\nIf your goal was to simulate many random walks, say 5,000 of them, you can generate\\nall of the random walks with minor modifications to the above code. The numpy.ran\\ndom functions if passed a 2-tuple will generate a 2D array of draws, and we can compute\\nthe cumulative sum across the rows to compute all 5,000 random walks in one shot:\\nIn [222]: nwalks = 5000\\nIn [223]: nsteps = 1000\\nIn [224]: draws = np.random.randint(0, 2, size=(nwalks, nsteps)) # 0 or 1\\nIn [225]: steps = np.where(draws > 0, 1, -1)\\nIn [226]: walks = steps.cumsum(1)\\nIn [227]: walks\\nOut[227]: \\narray([[  1,   0,   1, ...,   8,   7,   8],\\n       [  1,   0,  -1, ...,  34,  33,  32],\\n       [  1,   0,  -1, ...,   4,   5,   4],\\n       ..., \\n       [  1,   2,   1, ...,  24,  25,  26],\\n       [  1,   2,   3, ...,  14,  13,  14],\\n       [ -1,  -2,  -3, ..., -24, -23, -22]])\\nNow, we can compute the maximum and minimum values obtained over all of the\\nwalks:\\nIn [228]: walks.max()        In [229]: walks.min()\\nOut[228]: 138                Out[229]: -133\\nExample: Random Walks | 109\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 125, 'page_label': '110'}, page_content='Out of these walks, let’s compute the minimum crossing time to 30 or -30. This is\\nslightly tricky because not all 5,000 of them reach 30. We can check this using the \\nany method:\\nIn [230]: hits30 = (np.abs(walks) >= 30).any(1)\\nIn [231]: hits30\\nOut[231]: array([False, True, False, ..., False, True, False], dtype=bool)\\nIn [232]: hits30.sum() # Number that hit 30 or -30\\nOut[232]: 3410\\nWe can use this boolean array to select out the rows of walks that actually cross the\\nabsolute 30 level and call argmax across axis 1 to get the crossing times:\\nIn [233]: crossing_times = (np.abs(walks[hits30]) >= 30).argmax(1)\\nIn [234]: crossing_times.mean()\\nOut[234]: 498.88973607038122\\nFeel free to experiment with other distributions for the steps other than equal sized\\ncoin flips. You need only use a different random number generation function, like \\nnormal to generate normally distributed steps with some mean and standard deviation:\\nIn [235]: steps = np.random.normal(loc=0, scale=0.25,\\n   .....:                          size=(nwalks, nsteps))\\n110 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 126, 'page_label': '111'}, page_content='CHAPTER 5\\nGetting Started with pandas\\npandas will be the primary library of interest throughout much of the rest of the book.\\nIt contains high-level data structures and manipulation tools designed to make data\\nanalysis fast and easy in Python. pandas is built on top of NumPy and makes it easy to\\nuse in NumPy-centric applications.\\nAs a bit of background, I started building pandas in early 2008 during my tenure at\\nAQR, a quantitative investment management firm. At the time, I had a distinct set of\\nrequirements that were not well-addressed by any single tool at my disposal:\\n• Data structures with labeled axes supporting automatic or explicit data alignment.\\nThis prevents common errors resulting from misaligned data and working with\\ndifferently-indexed data coming from different sources.\\n• Integrated time series functionality.\\n• The same data structures handle both time series data and non-time series data.\\n• Arithmetic operations and reductions (like summing across an axis) would pass\\non the metadata (axis labels).\\n• Flexible handling of missing data.\\n• Merge and other relational operations found in popular database databases (SQL-\\nbased, for example).\\nI wanted to be able to do all of these things in one place, preferably in a language well-\\nsuited to general purpose software development. Python was a good candidate lan-\\nguage for this, but at that time there was not an integrated set of data structures and\\ntools providing this functionality.\\nOver the last four years, pandas has matured into a quite large library capable of solving\\na much broader set of data handling problems than I ever anticipated, but it has ex-\\npanded in its scope without compromising the simplicity and ease-of-use that I desired\\nfrom the very beginning. I hope that after reading this book, you will find it to be just\\nas much of an indispensable tool as I do.\\nThroughout the rest of the book, I use the following import conventions for pandas:\\n111\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 127, 'page_label': '112'}, page_content=\"In [1]: from pandas import Series, DataFrame\\nIn [2]: import pandas as pd\\nThus, whenever you see pd. in code, it’s referring to pandas. Series and DataFrame are\\nused so much that I find it easier to import them into the local namespace.\\nIntroduction to pandas Data Structures\\nTo get started with pandas, you will need to get comfortable with its two workhorse\\ndata structures: Series and DataFrame. While they are not a universal solution for every\\nproblem, they provide a solid, easy-to-use basis for most applications.\\nSeries\\nA Series is a one-dimensional array-like object containing an array of data (of any\\nNumPy data type) and an associated array of data labels, called its index. The simplest\\nSeries is formed from only an array of data:\\nIn [4]: obj = Series([4, 7, -5, 3])\\nIn [5]: obj\\nOut[5]: \\n0    4\\n1    7\\n2   -5\\n3    3\\nThe string representation of a Series displayed interactively shows the index on the left\\nand the values on the right. Since we did not specify an index for the data, a default\\none consisting of the integers 0 through N - 1 (where N is the length of the data) is\\ncreated. You can get the array representation and index object of the Series via its values\\nand index attributes, respectively:\\nIn [6]: obj.values\\nOut[6]: array([ 4,  7, -5,  3])\\nIn [7]: obj.index\\nOut[7]: Int64Index([0, 1, 2, 3])\\nOften it will be desirable to create a Series with an index identifying each data point:\\nIn [8]: obj2 = Series([4, 7, -5, 3], index=['d', 'b', 'a', 'c'])\\nIn [9]: obj2\\nOut[9]: \\nd    4\\nb    7\\na   -5\\nc    3\\n112 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 128, 'page_label': '113'}, page_content=\"In [10]: obj2.index\\nOut[10]: Index([d, b, a, c], dtype=object)\\nCompared with a regular NumPy array, you can use values in the index when selecting\\nsingle values or a set of values:\\nIn [11]: obj2['a']\\nOut[11]: -5\\nIn [12]: obj2['d'] = 6\\nIn [13]: obj2[['c', 'a', 'd']]\\nOut[13]: \\nc    3\\na   -5\\nd    6\\nNumPy array operations, such as filtering with a boolean array, scalar multiplication,\\nor applying math functions, will preserve the index-value link:\\nIn [14]: obj2\\nOut[14]: \\nd    6\\nb    7\\na   -5\\nc    3\\nIn [15]: obj2[obj2 > 0]      In [16]: obj2 * 2      In [17]: np.exp(obj2)\\nOut[15]:                     Out[16]:               Out[17]:             \\nd    6                       d    12                d     403.428793     \\nb    7                       b    14                b    1096.633158     \\nc    3                       a   -10                a       0.006738     \\n                             c     6                c      20.085537\\nAnother way to think about a Series is as a fixed-length, ordered dict, as it is a mapping\\nof index values to data values. It can be substituted into many functions that expect a\\ndict:\\nIn [18]: 'b' in obj2\\nOut[18]: True\\nIn [19]: 'e' in obj2\\nOut[19]: False\\nShould you have data contained in a Python dict, you can create a Series from it by\\npassing the dict:\\nIn [20]: sdata = {'Ohio': 35000, 'Texas': 71000, 'Oregon': 16000, 'Utah': 5000}\\nIn [21]: obj3 = Series(sdata)\\nIn [22]: obj3\\nOut[22]: \\nOhio      35000\\nOregon    16000\\nIntroduction to pandas Data Structures | 113\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 129, 'page_label': '114'}, page_content=\"Texas     71000\\nUtah       5000\\nWhen only passing a dict, the index in the resulting Series will have the dict’s keys in\\nsorted order.\\nIn [23]: states = ['California', 'Ohio', 'Oregon', 'Texas']\\nIn [24]: obj4 = Series(sdata, index=states)\\nIn [25]: obj4\\nOut[25]: \\nCalifornia      NaN\\nOhio          35000\\nOregon        16000\\nTexas         71000\\nIn this case, 3 values found in sdata were placed in the appropriate locations, but since\\nno value for 'California' was found, it appears as NaN (not a number) which is con-\\nsidered in pandas to mark missing or NA values. I will use the terms “missing” or “NA”\\nto refer to missing data. The isnull and notnull functions in pandas should be used to\\ndetect missing data:\\nIn [26]: pd.isnull(obj4)      In [27]: pd.notnull(obj4)\\nOut[26]:                      Out[27]:                 \\nCalifornia     True           California    False      \\nOhio          False           Ohio           True      \\nOregon        False           Oregon         True      \\nTexas         False           Texas          True\\nSeries also has these as instance methods:\\nIn [28]: obj4.isnull()\\nOut[28]: \\nCalifornia     True\\nOhio          False\\nOregon        False\\nTexas         False\\nI discuss working with missing data in more detail later in this chapter.\\nA critical Series feature for many applications is that it automatically aligns differently-\\nindexed data in arithmetic operations:\\nIn [29]: obj3          In [30]: obj4      \\nOut[29]:               Out[30]:           \\nOhio      35000        California      NaN\\nOregon    16000        Ohio          35000\\nTexas     71000        Oregon        16000\\nUtah       5000        Texas         71000\\n                                          \\nIn [31]: obj3 + obj4\\nOut[31]: \\nCalifornia       NaN\\nOhio           70000\\nOregon         32000\\n114 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 130, 'page_label': '115'}, page_content=\"Texas         142000\\nUtah             NaN\\nData alignment features are addressed as a separate topic.\\nBoth the Series object itself and its index have a name attribute, which integrates with\\nother key areas of pandas functionality:\\nIn [32]: obj4.name = 'population'\\nIn [33]: obj4.index.name = 'state'\\nIn [34]: obj4\\nOut[34]: \\nstate\\nCalifornia      NaN\\nOhio          35000\\nOregon        16000\\nTexas         71000\\nName: population\\nA Series’s index can be altered in place by assignment:\\nIn [35]: obj.index = ['Bob', 'Steve', 'Jeff', 'Ryan']\\nIn [36]: obj\\nOut[36]: \\nBob      4\\nSteve    7\\nJeff    -5\\nRyan     3\\nDataFrame\\nA DataFrame represents a tabular, spreadsheet-like data structure containing an or-\\ndered collection of columns, each of which can be a different value type (numeric,\\nstring, boolean, etc.). The DataFrame has both a row and column index; it can be\\nthought of as a dict of Series (one for all sharing the same index). Compared with other\\nsuch DataFrame-like structures you may have used before (like R’s data.frame), row-\\noriented and column-oriented operations in DataFrame are treated roughly symmet-\\nrically. Under the hood, the data is stored as one or more two-dimensional blocks rather\\nthan a list, dict, or some other collection of one-dimensional arrays. The exact details\\nof DataFrame’s internals are far outside the scope of this book.\\nWhile DataFrame stores the data internally in a two-dimensional for-\\nmat, you can easily represent much higher-dimensional data in a tabular\\nformat using hierarchical indexing, a subject of a later section and a key\\ningredient in many of the more advanced data-handling features in pan-\\ndas.\\nIntroduction to pandas Data Structures | 115\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 131, 'page_label': '116'}, page_content=\"There are numerous ways to construct a DataFrame, though one of the most common\\nis from a dict of equal-length lists or NumPy arrays\\ndata = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'],\\n        'year': [2000, 2001, 2002, 2001, 2002],\\n        'pop': [1.5, 1.7, 3.6, 2.4, 2.9]}\\nframe = DataFrame(data)\\nThe resulting DataFrame will have its index assigned automatically as with Series, and\\nthe columns are placed in sorted order:\\nIn [38]: frame\\nOut[38]: \\n   pop   state  year\\n0  1.5    Ohio  2000\\n1  1.7    Ohio  2001\\n2  3.6    Ohio  2002\\n3  2.4  Nevada  2001\\n4  2.9  Nevada  2002\\nIf you specify a sequence of columns, the DataFrame’s columns will be exactly what\\nyou pass:\\nIn [39]: DataFrame(data, columns=['year', 'state', 'pop'])\\nOut[39]: \\n   year   state  pop\\n0  2000    Ohio  1.5\\n1  2001    Ohio  1.7\\n2  2002    Ohio  3.6\\n3  2001  Nevada  2.4\\n4  2002  Nevada  2.9\\nAs with Series, if you pass a column that isn’t contained in data, it will appear with NA\\nvalues in the result:\\nIn [40]: frame2 = DataFrame(data, columns=['year', 'state', 'pop', 'debt'],\\n   ....:                    index=['one', 'two', 'three', 'four', 'five'])\\nIn [41]: frame2\\nOut[41]: \\n       year   state  pop  debt\\none    2000    Ohio  1.5   NaN\\ntwo    2001    Ohio  1.7   NaN\\nthree  2002    Ohio  3.6   NaN\\nfour   2001  Nevada  2.4   NaN\\nfive   2002  Nevada  2.9   NaN\\nIn [42]: frame2.columns\\nOut[42]: Index([year, state, pop, debt], dtype=object)\\nA column in a DataFrame can be retrieved as a Series either by dict-like notation or by\\nattribute:\\nIn [43]: frame2['state']        In [44]: frame2.year\\nOut[43]:                        Out[44]:            \\none        Ohio                 one      2000       \\n116 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 132, 'page_label': '117'}, page_content=\"two        Ohio                 two      2001       \\nthree      Ohio                 three    2002       \\nfour     Nevada                 four     2001       \\nfive     Nevada                 five     2002       \\nName: state                     Name: year\\nNote that the returned Series have the same index as the DataFrame, and their name\\nattribute has been appropriately set.\\nRows can also be retrieved by position or name by a couple of methods, such as the\\nix indexing field (much more on this later):\\nIn [45]: frame2.ix['three']\\nOut[45]: \\nyear     2002\\nstate    Ohio\\npop       3.6\\ndebt      NaN\\nName: three\\nColumns can be modified by assignment. For example, the empty 'debt' column could\\nbe assigned a scalar value or an array of values:\\nIn [46]: frame2['debt'] = 16.5\\nIn [47]: frame2\\nOut[47]: \\n       year   state  pop  debt\\none    2000    Ohio  1.5  16.5\\ntwo    2001    Ohio  1.7  16.5\\nthree  2002    Ohio  3.6  16.5\\nfour   2001  Nevada  2.4  16.5\\nfive   2002  Nevada  2.9  16.5\\nIn [48]: frame2['debt'] = np.arange(5.)\\nIn [49]: frame2\\nOut[49]: \\n       year   state  pop  debt\\none    2000    Ohio  1.5     0\\ntwo    2001    Ohio  1.7     1\\nthree  2002    Ohio  3.6     2\\nfour   2001  Nevada  2.4     3\\nfive   2002  Nevada  2.9     4\\nWhen assigning lists or arrays to a column, the value’s length must match the length\\nof the DataFrame. If you assign a Series, it will be instead conformed exactly to the\\nDataFrame’s index, inserting missing values in any holes:\\nIn [50]: val = Series([-1.2, -1.5, -1.7], index=['two', 'four', 'five'])\\nIn [51]: frame2['debt'] = val\\nIn [52]: frame2\\nOut[52]: \\n       year   state  pop  debt\\nIntroduction to pandas Data Structures | 117\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 133, 'page_label': '118'}, page_content=\"one    2000    Ohio  1.5   NaN\\ntwo    2001    Ohio  1.7  -1.2\\nthree  2002    Ohio  3.6   NaN\\nfour   2001  Nevada  2.4  -1.5\\nfive   2002  Nevada  2.9  -1.7\\nAssigning a column that doesn’t exist will create a new column. The del keyword will\\ndelete columns as with a dict:\\nIn [53]: frame2['eastern'] = frame2.state == 'Ohio'\\nIn [54]: frame2\\nOut[54]: \\n       year   state  pop  debt eastern\\none    2000    Ohio  1.5   NaN    True\\ntwo    2001    Ohio  1.7  -1.2    True\\nthree  2002    Ohio  3.6   NaN    True\\nfour   2001  Nevada  2.4  -1.5   False\\nfive   2002  Nevada  2.9  -1.7   False\\nIn [55]: del frame2['eastern']\\nIn [56]: frame2.columns\\nOut[56]: Index([year, state, pop, debt], dtype=object)\\nThe column returned when indexing a DataFrame is a view on the un-\\nderlying data, not a copy. Thus, any in-place modifications to the Series\\nwill be reflected in the DataFrame. The column can be explicitly copied\\nusing the Series’s copy method.\\nAnother common form of data is a nested dict of dicts format:\\nIn [57]: pop = {'Nevada': {2001: 2.4, 2002: 2.9},\\n   ....:        'Ohio': {2000: 1.5, 2001: 1.7, 2002: 3.6}}\\nIf passed to DataFrame, it will interpret the outer dict keys as the columns and the inner\\nkeys as the row indices:\\nIn [58]: frame3 = DataFrame(pop)\\nIn [59]: frame3\\nOut[59]: \\n      Nevada  Ohio\\n2000     NaN   1.5\\n2001     2.4   1.7\\n2002     2.9   3.6\\nOf course you can always transpose the result:\\nIn [60]: frame3.T\\nOut[60]: \\n        2000  2001  2002\\nNevada   NaN   2.4   2.9\\nOhio     1.5   1.7   3.6\\n118 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 134, 'page_label': '119'}, page_content=\"The keys in the inner dicts are unioned and sorted to form the index in the result. This\\nisn’t true if an explicit index is specified:\\nIn [61]: DataFrame(pop, index=[2001, 2002, 2003])\\nOut[61]: \\n      Nevada  Ohio\\n2001     2.4   1.7\\n2002     2.9   3.6\\n2003     NaN   NaN\\nDicts of Series are treated much in the same way:\\nIn [62]: pdata = {'Ohio': frame3['Ohio'][:-1],\\n   ....:          'Nevada': frame3['Nevada'][:2]}\\nIn [63]: DataFrame(pdata)\\nOut[63]: \\n      Nevada  Ohio\\n2000     NaN   1.5\\n2001     2.4   1.7\\nFor a complete list of things you can pass the DataFrame constructor, see Table 5-1.\\nIf a DataFrame’s index and columns have their name attributes set, these will also be\\ndisplayed:\\nIn [64]: frame3.index.name = 'year'; frame3.columns.name = 'state'\\nIn [65]: frame3\\nOut[65]: \\nstate  Nevada  Ohio\\nyear               \\n2000      NaN   1.5\\n2001      2.4   1.7\\n2002      2.9   3.6\\nLike Series, the values attribute returns the data contained in the DataFrame as a 2D\\nndarray:\\nIn [66]: frame3.values\\nOut[66]: \\narray([[ nan,  1.5],\\n       [ 2.4,  1.7],\\n       [ 2.9,  3.6]])\\nIf the DataFrame’s columns are different dtypes, the dtype of the values array will be\\nchosen to accomodate all of the columns:\\nIn [67]: frame2.values\\nOut[67]: \\narray([[2000, Ohio, 1.5, nan],\\n       [2001, Ohio, 1.7, -1.2],\\n       [2002, Ohio, 3.6, nan],\\n       [2001, Nevada, 2.4, -1.5],\\n       [2002, Nevada, 2.9, -1.7]], dtype=object)\\nIntroduction to pandas Data Structures | 119\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 135, 'page_label': '120'}, page_content='Table 5-1. Possible data inputs to DataFrame constructor\\nType Notes\\n2D ndarray A matrix of data, passing optional row and column labels\\ndict of arrays, lists, or tuples Each sequence becomes a column in the DataFrame. All sequences must be the same length.\\nNumPy structured/record array Treated as the “dict of arrays” case\\ndict of Series Each value becomes a column. Indexes from each Series are unioned together to form the\\nresult’s row index if no explicit index is passed.\\ndict of dicts Each inner dict becomes a column. Keys are unioned to form the row index as in the “dict of\\nSeries” case.\\nlist of dicts or Series Each item becomes a row in the DataFrame. Union of dict keys or Series indexes become the\\nDataFrame’s column labels\\nList of lists or tuples Treated as the “2D ndarray” case\\nAnother DataFrame The DataFrame’s indexes are used unless different ones are passed\\nNumPy MaskedArray\\nLike the “2D ndarray” case except masked values become NA/missing in the DataFrame result\\nIndex Objects\\npandas’s \\nIndex objects are responsible for holding the axis labels and other metadata\\n(like the axis name or names). Any array or other sequence of labels used when con-\\nstructing a Series or DataFrame is internally converted to an Index:\\nIn [68]: obj = Series(range(3), index=[\\'a\\', \\'b\\', \\'c\\'])\\nIn [69]: index = obj.index\\nIn [70]: index\\nOut[70]: Index([a, b, c], dtype=object)\\nIn [71]: index[1:]\\nOut[71]: Index([b, c], dtype=object)\\nIndex objects are immutable and thus can’t be modified by the user:\\nIn [72]: index[1] = \\'d\\'\\n---------------------------------------------------------------------------\\nException                                 Traceback (most recent call last)\\n<ipython-input-72-676fdeb26a68> in <module>()\\n----> 1 index[1] = \\'d\\'\\n/Users/wesm/code/pandas/pandas/core/index.pyc in __setitem__(self, key, value)\\n    302     def __setitem__(self, key, value):\\n    303         \"\"\"Disable the setting of values.\"\"\"\\n--> 304         raise Exception(str(self.__class__) + \\' object is immutable\\')\\n    305 \\n    306     def __getitem__(self, key):\\nException: <class \\'pandas.core.index.Index\\'> object is immutable\\n120 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 136, 'page_label': '121'}, page_content=\"Immutability is important so that Index objects can be safely shared among data\\nstructures:\\nIn [73]: index = pd.Index(np.arange(3))\\nIn [74]: obj2 = Series([1.5, -2.5, 0], index=index)\\nIn [75]: obj2.index is index\\nOut[75]: True\\nTable 5-2 has a list of built-in Index classes in the library. With some development\\neffort, Index can even be subclassed to implement specialized axis indexing function-\\nality.\\nMany users will not need to know much about Index objects, but they’re\\nnonetheless an important part of pandas’s data model.\\nTable 5-2. Main Index objects in pandas\\nClass Description\\nIndex The most general Index object, representing axis labels in a NumPy array of Python objects.\\nInt64Index Specialized Index for integer values.\\nMultiIndex “Hierarchical” index object representing multiple levels of indexing on a single axis. Can be thought of\\nas similar to an array of tuples.\\nDatetimeIndex Stores nanosecond timestamps (represented using NumPy’s datetime64 dtype).\\nPeriodIndex Specialized Index for Period data (timespans).\\nIn addition to being array-like, an Index also functions as a fixed-size set:\\nIn [76]: frame3\\nOut[76]: \\nstate  Nevada  Ohio\\nyear               \\n2000      NaN   1.5\\n2001      2.4   1.7\\n2002      2.9   3.6\\nIn [77]: 'Ohio' in frame3.columns\\nOut[77]: True\\nIn [78]: 2003 in frame3.index\\nOut[78]: False\\nEach Index has a number of methods and properties for set logic and answering other\\ncommon questions about the data it contains. These are summarized in Table 5-3.\\nIntroduction to pandas Data Structures | 121\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 137, 'page_label': '122'}, page_content=\"Table 5-3. Index methods and properties\\nMethod Description\\nappend Concatenate with additional Index objects, producing a new Index\\ndiff Compute set difference as an Index\\nintersection Compute set intersection\\nunion Compute set union\\nisin Compute boolean array indicating whether each value is contained in the passed collection\\ndelete Compute new Index with element at index i deleted\\ndrop Compute new index by deleting passed values\\ninsert Compute new Index by inserting element at index i\\nis_monotonic Returns True if each element is greater than or equal to the previous element\\nis_unique Returns True if the Index has no duplicate values\\nunique Compute the array of unique values in the Index\\nEssential Functionality\\nIn this section, I’ll walk you through the fundamental mechanics of interacting with\\nthe data contained in a Series or DataFrame. Upcoming chapters will delve more deeply\\ninto data analysis and manipulation topics using pandas. This book is not intended to\\nserve as exhaustive documentation for the pandas library; I instead focus on the most\\nimportant features, leaving the less common (that is, more esoteric) things for you to\\nexplore on your own.\\nReindexing\\nA critical method on pandas objects is reindex, which means to create a new object\\nwith the data conformed to a new index. Consider a simple example from above:\\nIn [79]: obj = Series([4.5, 7.2, -5.3, 3.6], index=['d', 'b', 'a', 'c'])\\nIn [80]: obj\\nOut[80]: \\nd    4.5\\nb    7.2\\na   -5.3\\nc    3.6\\nCalling reindex on this Series rearranges the data according to the new index, intro-\\nducing missing values if any index values were not already present:\\nIn [81]: obj2 = obj.reindex(['a', 'b', 'c', 'd', 'e'])\\nIn [82]: obj2\\nOut[82]: \\na   -5.3\\n122 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 138, 'page_label': '123'}, page_content=\"b    7.2\\nc    3.6\\nd    4.5\\ne    NaN\\nIn [83]: obj.reindex(['a', 'b', 'c', 'd', 'e'], fill_value=0)\\nOut[83]: \\na   -5.3\\nb    7.2\\nc    3.6\\nd    4.5\\ne    0.0\\nFor ordered data like time series, it may be desirable to do some interpolation or filling\\nof values when reindexing. The method option allows us to do this, using a method such\\nas ffill which forward fills the values:\\nIn [84]: obj3 = Series(['blue', 'purple', 'yellow'], index=[0, 2, 4])\\nIn [85]: obj3.reindex(range(6), method='ffill')\\nOut[85]: \\n0      blue\\n1      blue\\n2    purple\\n3    purple\\n4    yellow\\n5    yellow\\nTable 5-4 lists available method options. At this time, interpolation more sophisticated\\nthan forward- and backfilling would need to be applied after the fact.\\nTable 5-4. reindex method (interpolation) options\\nArgument Description\\nffill or pad Fill (or carry) values forward\\nbfill or backfill Fill (or carry) values backward\\nWith DataFrame, reindex can alter either the (row) index, columns, or both. When\\npassed just a sequence, the rows are reindexed in the result:\\nIn [86]: frame = DataFrame(np.arange(9).reshape((3, 3)), index=['a', 'c', 'd'],\\n   ....:                   columns=['Ohio', 'Texas', 'California'])\\nIn [87]: frame\\nOut[87]: \\n   Ohio  Texas  California\\na     0      1           2\\nc     3      4           5\\nd     6      7           8\\nIn [88]: frame2 = frame.reindex(['a', 'b', 'c', 'd'])\\nIn [89]: frame2\\nOut[89]: \\nEssential Functionality | 123\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 139, 'page_label': '124'}, page_content=\"Ohio  Texas  California\\na     0      1           2\\nb   NaN    NaN         NaN\\nc     3      4           5\\nd     6      7           8\\nThe columns can be reindexed using the columns keyword:\\nIn [90]: states = ['Texas', 'Utah', 'California']\\nIn [91]: frame.reindex(columns=states)\\nOut[91]: \\n   Texas  Utah  California\\na      1   NaN           2\\nc      4   NaN           5\\nd      7   NaN           8\\nBoth can be reindexed in one shot, though interpolation will only apply row-wise (axis\\n0):\\nIn [92]: frame.reindex(index=['a', 'b', 'c', 'd'], method='ffill',\\n   ....:               columns=states)\\nOut[92]: \\n   Texas  Utah  California\\na      1   NaN           2\\nb      1   NaN           2\\nc      4   NaN           5\\nd      7   NaN           8\\nAs you’ll see soon, reindexing can be done more succinctly by label-indexing with ix:\\nIn [93]: frame.ix[['a', 'b', 'c', 'd'], states]\\nOut[93]: \\n   Texas  Utah  California\\na      1   NaN           2\\nb    NaN   NaN         NaN\\nc      4   NaN           5\\nd      7   NaN           8\\nTable 5-5. reindex function arguments\\nArgument Description\\nindex New sequence to use as index. Can be Index instance or any other sequence-like Python data structure. An\\nIndex will be used exactly as is without any copying\\nmethod Interpolation (fill) method, see Table 5-4 for options.\\nfill_value Substitute value to use when introducing missing data by reindexing\\nlimit When forward- or backfilling, maximum size gap to fill\\nlevel Match simple Index on level of MultiIndex, otherwise select subset of\\ncopy Do not copy underlying data if new index is equivalent to old index. True by default (i.e. always copy data).\\n124 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 140, 'page_label': '125'}, page_content=\"Dropping entries from an axis\\nDropping one or more entries from an axis is easy if you have an index array or list\\nwithout those entries. As that can require a bit of munging and set logic, the drop\\nmethod will return a new object with the indicated value or values deleted from an axis:\\nIn [94]: obj = Series(np.arange(5.), index=['a', 'b', 'c', 'd', 'e'])\\nIn [95]: new_obj = obj.drop('c')\\nIn [96]: new_obj\\nOut[96]: \\na    0\\nb    1\\nd    3\\ne    4\\nIn [97]: obj.drop(['d', 'c'])\\nOut[97]: \\na    0\\nb    1\\ne    4\\nWith DataFrame, index values can be deleted from either axis:\\nIn [98]: data = DataFrame(np.arange(16).reshape((4, 4)),\\n   ....:                  index=['Ohio', 'Colorado', 'Utah', 'New York'],\\n   ....:                  columns=['one', 'two', 'three', 'four'])\\nIn [99]: data.drop(['Colorado', 'Ohio'])\\nOut[99]: \\n          one  two  three  four\\nUtah        8    9     10    11\\nNew York   12   13     14    15\\nIn [100]: data.drop('two', axis=1)      In [101]: data.drop(['two', 'four'], axis=1)\\nOut[100]:                               Out[101]:                                   \\n          one  three  four                        one  three                        \\nOhio        0      2     3              Ohio        0      2                        \\nColorado    4      6     7              Colorado    4      6                        \\nUtah        8     10    11              Utah        8     10                        \\nNew York   12     14    15              New York   12     14\\nIndexing, selection, and filtering\\nSeries indexing (obj[...]) works analogously to NumPy array indexing, except you can\\nuse the Series’s index values instead of only integers. Here are some examples this:\\nIn [102]: obj = Series(np.arange(4.), index=['a', 'b', 'c', 'd'])\\nIn [103]: obj['b']          In [104]: obj[1]\\nOut[103]: 1.0               Out[104]: 1.0   \\n                                            \\nIn [105]: obj[2:4]          In [106]: obj[['b', 'a', 'd']]\\nOut[105]:                   Out[106]:                     \\nEssential Functionality | 125\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 141, 'page_label': '126'}, page_content=\"c    2                      b    1                        \\nd    3                      a    0                        \\n                            d    3                        \\n                                                          \\nIn [107]: obj[[1, 3]]       In [108]: obj[obj < 2]\\nOut[107]:                   Out[108]:             \\nb    1                      a    0                \\nd    3                      b    1\\nSlicing with labels behaves differently than normal Python slicing in that the endpoint\\nis inclusive:\\nIn [109]: obj['b':'c']\\nOut[109]: \\nb    1\\nc    2\\nSetting using these methods works just as you would expect:\\nIn [110]: obj['b':'c'] = 5\\nIn [111]: obj\\nOut[111]: \\na    0\\nb    5\\nc    5\\nd    3\\nAs you’ve seen above, indexing into a DataFrame is for retrieving one or more columns\\neither with a single value or sequence:\\nIn [112]: data = DataFrame(np.arange(16).reshape((4, 4)),\\n   .....:                  index=['Ohio', 'Colorado', 'Utah', 'New York'],\\n   .....:                  columns=['one', 'two', 'three', 'four'])\\nIn [113]: data\\nOut[113]: \\n          one  two  three  four\\nOhio        0    1      2     3\\nColorado    4    5      6     7\\nUtah        8    9     10    11\\nNew York   12   13     14    15\\nIn [114]: data['two']        In [115]: data[['three', 'one']]\\nOut[114]:                    Out[115]:                       \\nOhio         1                         three  one            \\nColorado     5               Ohio          2    0            \\nUtah         9               Colorado      6    4            \\nNew York    13               Utah         10    8            \\nName: two                    New York     14   12\\nIndexing like this has a few special cases. First selecting rows by slicing or a boolean\\narray:\\nIn [116]: data[:2]                     In [117]: data[data['three'] > 5]\\nOut[116]:                              Out[117]:                        \\n          one  two  three  four                  one  two  three  four  \\n126 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 142, 'page_label': '127'}, page_content=\"Ohio        0    1      2     3        Colorado    4    5      6     7  \\nColorado    4    5      6     7        Utah        8    9     10    11  \\n                                       New York   12   13     14    15\\nThis might seem inconsistent to some readers, but this syntax arose out of practicality\\nand nothing more. Another use case is in indexing with a boolean DataFrame, such as\\none produced by a scalar comparison:\\nIn [118]: data < 5\\nOut[118]: \\n            one    two  three   four\\nOhio       True   True   True   True\\nColorado   True  False  False  False\\nUtah      False  False  False  False\\nNew York  False  False  False  False\\nIn [119]: data[data < 5] = 0\\nIn [120]: data\\nOut[120]: \\n          one  two  three  four\\nOhio        0    0      0     0\\nColorado    0    5      6     7\\nUtah        8    9     10    11\\nNew York   12   13     14    15\\nThis is intended to make DataFrame syntactically more like an ndarray in this case.\\nFor DataFrame label-indexing on the rows, I introduce the special indexing field ix. It\\nenables you to select a subset of the rows and columns from a DataFrame with NumPy-\\nlike notation plus axis labels. As I mentioned earlier, this is also a less verbose way to\\ndo reindexing:\\nIn [121]: data.ix['Colorado', ['two', 'three']]\\nOut[121]: \\ntwo      5\\nthree    6\\nName: Colorado\\nIn [122]: data.ix[['Colorado', 'Utah'], [3, 0, 1]]\\nOut[122]: \\n          four  one  two\\nColorado     7    0    5\\nUtah        11    8    9\\nIn [123]: data.ix[2]        In [124]: data.ix[:'Utah', 'two']\\nOut[123]:                   Out[124]:                        \\none       8                 Ohio        0                    \\ntwo       9                 Colorado    5                    \\nthree    10                 Utah        9                    \\nfour     11                 Name: two                        \\nName: Utah                                                   \\n                                                             \\nIn [125]: data.ix[data.three > 5, :3]\\nOut[125]: \\nEssential Functionality | 127\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 143, 'page_label': '128'}, page_content=\"one  two  three\\nColorado    0    5      6\\nUtah        8    9     10\\nNew York   12   13     14\\nSo there are many ways to select and rearrange the data contained in a pandas object.\\nFor DataFrame, there is a short summary of many of them in Table 5-6. You have a\\nnumber of additional options when working with hierarchical indexes as you’ll later\\nsee.\\nWhen designing pandas, I felt that having to type frame[:, col] to select\\na column was too verbose (and error-prone), since column selection is\\none of the most common operations. Thus I made the design trade-off\\nto push all of the rich label-indexing into ix.\\nTable 5-6. Indexing options with DataFrame\\nType Notes\\nobj[val] Select single column or sequence of columns from the DataFrame. Special case con-\\nveniences: boolean array (filter rows), slice (slice rows), or boolean DataFrame (set\\nvalues based on some criterion).\\nobj.ix[val] Selects single row of subset of rows from the DataFrame.\\nobj.ix[:, val] Selects single column of subset of columns.\\nobj.ix[val1, val2] Select both rows and columns.\\nreindex method Conform one or more axes to new indexes.\\nxs method Select single row or column as a Series by label.\\nicol, irow methods Select single column or row, respectively, as a Series by integer location.\\nget_value, set_value methods Select single value by row and column label.\\nArithmetic and data alignment\\nOne of the most important pandas features is the behavior of arithmetic between ob-\\njects with different indexes. When adding together objects, if any index pairs are not\\nthe same, the respective index in the result will be the union of the index pairs. Let’s\\nlook at a simple example:\\nIn [126]: s1 = Series([7.3, -2.5, 3.4, 1.5], index=['a', 'c', 'd', 'e'])\\nIn [127]: s2 = Series([-2.1, 3.6, -1.5, 4, 3.1], index=['a', 'c', 'e', 'f', 'g'])\\nIn [128]: s1        In [129]: s2\\nOut[128]:           Out[129]:   \\na    7.3            a   -2.1    \\nc   -2.5            c    3.6    \\nd    3.4            e   -1.5    \\n128 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 144, 'page_label': '129'}, page_content=\"e    1.5            f    4.0    \\n                    g    3.1\\nAdding these together yields:\\nIn [130]: s1 + s2\\nOut[130]: \\na    5.2\\nc    1.1\\nd    NaN\\ne    0.0\\nf    NaN\\ng    NaN\\nThe internal data alignment introduces NA values in the indices that don’t overlap.\\nMissing values propagate in arithmetic computations.\\nIn the case of DataFrame, alignment is performed on both the rows and the columns:\\nIn [131]: df1 = DataFrame(np.arange(9.).reshape((3, 3)), columns=list('bcd'),\\n   .....:                 index=['Ohio', 'Texas', 'Colorado'])\\nIn [132]: df2 = DataFrame(np.arange(12.).reshape((4, 3)), columns=list('bde'),\\n   .....:                 index=['Utah', 'Ohio', 'Texas', 'Oregon'])\\nIn [133]: df1            In [134]: df2    \\nOut[133]:                Out[134]:        \\n          b  c  d                b   d   e\\nOhio      0  1  2        Utah    0   1   2\\nTexas     3  4  5        Ohio    3   4   5\\nColorado  6  7  8        Texas   6   7   8\\n                         Oregon  9  10  11\\nAdding these together returns a DataFrame whose index and columns are the unions\\nof the ones in each DataFrame:\\nIn [135]: df1 + df2\\nOut[135]: \\n           b   c   d   e\\nColorado NaN NaN NaN NaN\\nOhio       3 NaN   6 NaN\\nOregon   NaN NaN NaN NaN\\nTexas      9 NaN  12 NaN\\nUtah     NaN NaN NaN NaN\\nArithmetic methods with fill values\\nIn arithmetic operations between differently-indexed objects, you might want to fill\\nwith a special value, like 0, when an axis label is found in one object but not the other:\\nIn [136]: df1 = DataFrame(np.arange(12.).reshape((3, 4)), columns=list('abcd'))\\nIn [137]: df2 = DataFrame(np.arange(20.).reshape((4, 5)), columns=list('abcde'))\\nIn [138]: df1          In [139]: df2        \\nOut[138]:              Out[139]:            \\n   a  b   c   d            a   b   c   d   e\\nEssential Functionality | 129\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 145, 'page_label': '130'}, page_content='0  0  1   2   3        0   0   1   2   3   4\\n1  4  5   6   7        1   5   6   7   8   9\\n2  8  9  10  11        2  10  11  12  13  14\\n                       3  15  16  17  18  19\\nAdding these together results in NA values in the locations that don’t overlap:\\nIn [140]: df1 + df2\\nOut[140]: \\n    a   b   c   d   e\\n0   0   2   4   6 NaN\\n1   9  11  13  15 NaN\\n2  18  20  22  24 NaN\\n3 NaN NaN NaN NaN NaN\\nUsing the add method on df1, I pass df2 and an argument to fill_value:\\nIn [141]: df1.add(df2, fill_value=0)\\nOut[141]: \\n    a   b   c   d   e\\n0   0   2   4   6   4\\n1   9  11  13  15   9\\n2  18  20  22  24  14\\n3  15  16  17  18  19\\nRelatedly, when reindexing a Series or DataFrame, you can also specify a different fill\\nvalue:\\nIn [142]: df1.reindex(columns=df2.columns, fill_value=0)\\nOut[142]: \\n   a  b   c   d  e\\n0  0  1   2   3  0\\n1  4  5   6   7  0\\n2  8  9  10  11  0\\nTable 5-7. Flexible arithmetic methods\\nMethod Description\\nadd Method for addition (+)\\nsub Method for subtraction (-)\\ndiv Method for division (/)\\nmul Method for multiplication (*)\\nOperations between DataFrame and Series\\nAs with NumPy arrays, arithmetic between DataFrame and Series is well-defined. First,\\nas a motivating example, consider the difference between a 2D array and one of its rows:\\nIn [143]: arr = np.arange(12.).reshape((3, 4))\\nIn [144]: arr\\nOut[144]: \\narray([[  0.,   1.,   2.,   3.],\\n       [  4.,   5.,   6.,   7.],\\n130 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 146, 'page_label': '131'}, page_content=\"[  8.,   9.,  10.,  11.]])\\nIn [145]: arr[0]\\nOut[145]: array([ 0.,  1.,  2.,  3.])\\nIn [146]: arr - arr[0]\\nOut[146]: \\narray([[ 0.,  0.,  0.,  0.],\\n       [ 4.,  4.,  4.,  4.],\\n       [ 8.,  8.,  8.,  8.]])\\nThis is referred to as broadcasting and is explained in more detail in Chapter 12. Op-\\nerations between a DataFrame and a Series are similar:\\nIn [147]: frame = DataFrame(np.arange(12.).reshape((4, 3)), columns=list('bde'),\\n   .....:                   index=['Utah', 'Ohio', 'Texas', 'Oregon'])\\nIn [148]: series = frame.ix[0]\\nIn [149]: frame          In [150]: series\\nOut[149]:                Out[150]:       \\n        b   d   e        b    0          \\nUtah    0   1   2        d    1          \\nOhio    3   4   5        e    2          \\nTexas   6   7   8        Name: Utah      \\nOregon  9  10  11\\nBy default, arithmetic between DataFrame and Series matches the index of the Series\\non the DataFrame's columns, broadcasting down the rows:\\nIn [151]: frame - series\\nOut[151]: \\n        b  d  e\\nUtah    0  0  0\\nOhio    3  3  3\\nTexas   6  6  6\\nOregon  9  9  9\\nIf an index value is not found in either the DataFrame’s columns or the Series’s index,\\nthe objects will be reindexed to form the union:\\nIn [152]: series2 = Series(range(3), index=['b', 'e', 'f'])\\nIn [153]: frame + series2\\nOut[153]: \\n        b   d   e   f\\nUtah    0 NaN   3 NaN\\nOhio    3 NaN   6 NaN\\nTexas   6 NaN   9 NaN\\nOregon  9 NaN  12 NaN\\nIf you want to instead broadcast over the columns, matching on the rows, you have to\\nuse one of the arithmetic methods. For example:\\nIn [154]: series3 = frame['d']\\nIn [155]: frame      In [156]: series3\\nEssential Functionality | 131\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 147, 'page_label': '132'}, page_content=\"Out[155]:            Out[156]:        \\n        b   d   e    Utah       1     \\nUtah    0   1   2    Ohio       4     \\nOhio    3   4   5    Texas      7     \\nTexas   6   7   8    Oregon    10     \\nOregon  9  10  11    Name: d          \\n                                      \\nIn [157]: frame.sub(series3, axis=0)\\nOut[157]: \\n        b  d  e\\nUtah   -1  0  1\\nOhio   -1  0  1\\nTexas  -1  0  1\\nOregon -1  0  1\\nThe axis number that you pass is the axis to match on. In this case we mean to match\\non the DataFrame’s row index and broadcast across.\\nFunction application and mapping\\nNumPy ufuncs (element-wise array methods) work fine with pandas objects:\\nIn [158]: frame = DataFrame(np.random.randn(4, 3), columns=list('bde'),\\n   .....:                   index=['Utah', 'Ohio', 'Texas', 'Oregon'])\\nIn [159]: frame                           In [160]: np.abs(frame)             \\nOut[159]:                                 Out[160]:                           \\n               b         d         e                     b         d         e\\nUtah   -0.204708  0.478943 -0.519439      Utah    0.204708  0.478943  0.519439\\nOhio   -0.555730  1.965781  1.393406      Ohio    0.555730  1.965781  1.393406\\nTexas   0.092908  0.281746  0.769023      Texas   0.092908  0.281746  0.769023\\nOregon  1.246435  1.007189 -1.296221      Oregon  1.246435  1.007189  1.296221\\nAnother frequent operation is applying a function on 1D arrays to each column or row.\\nDataFrame’s apply method does exactly this:\\nIn [161]: f = lambda x: x.max() - x.min()\\nIn [162]: frame.apply(f)        In [163]: frame.apply(f, axis=1)\\nOut[162]:                       Out[163]:                       \\nb    1.802165                   Utah      0.998382              \\nd    1.684034                   Ohio      2.521511              \\ne    2.689627                   Texas     0.676115              \\n                                Oregon    2.542656\\nMany of the most common array statistics (like sum and mean) are DataFrame methods,\\nso using apply is not necessary.\\nThe function passed to apply need not return a scalar value, it can also return a Series\\nwith multiple values:\\nIn [164]: def f(x):\\n   .....:     return Series([x.min(), x.max()], index=['min', 'max'])\\nIn [165]: frame.apply(f)\\n132 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 148, 'page_label': '133'}, page_content=\"Out[165]: \\n            b         d         e\\nmin -0.555730  0.281746 -1.296221\\nmax  1.246435  1.965781  1.393406\\nElement-wise Python functions can be used, too. Suppose you wanted to compute a\\nformatted string from each floating point value in frame. You can do this with applymap:\\nIn [166]: format = lambda x: '%.2f' % x\\nIn [167]: frame.applymap(format)\\nOut[167]: \\n            b     d      e\\nUtah    -0.20  0.48  -0.52\\nOhio    -0.56  1.97   1.39\\nTexas    0.09  0.28   0.77\\nOregon   1.25  1.01  -1.30\\nThe reason for the name applymap is that Series has a map method for applying an ele-\\nment-wise function:\\nIn [168]: frame['e'].map(format)\\nOut[168]: \\nUtah      -0.52\\nOhio       1.39\\nTexas      0.77\\nOregon    -1.30\\nName: e\\nSorting and ranking\\nSorting a data set by some criterion is another important built-in operation. To sort\\nlexicographically by row or column index, use the sort_index method, which returns\\na new, sorted object:\\nIn [169]: obj = Series(range(4), index=['d', 'a', 'b', 'c'])\\nIn [170]: obj.sort_index()\\nOut[170]: \\na    1\\nb    2\\nc    3\\nd    0\\nWith a DataFrame, you can sort by index on either axis:\\nIn [171]: frame = DataFrame(np.arange(8).reshape((2, 4)), index=['three', 'one'],\\n   .....:                   columns=['d', 'a', 'b', 'c'])\\nIn [172]: frame.sort_index()        In [173]: frame.sort_index(axis=1)\\nOut[172]:                           Out[173]:                         \\n       d  a  b  c                          a  b  c  d                 \\none    4  5  6  7                   three  1  2  3  0                 \\nthree  0  1  2  3                   one    5  6  7  4\\nEssential Functionality | 133\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 149, 'page_label': '134'}, page_content=\"The data is sorted in ascending order by default, but can be sorted in descending order,\\ntoo:\\nIn [174]: frame.sort_index(axis=1, ascending=False)\\nOut[174]: \\n       d  c  b  a\\nthree  0  3  2  1\\none    4  7  6  5\\nTo sort a Series by its values, use its order method:\\nIn [175]: obj = Series([4, 7, -3, 2])\\nIn [176]: obj.order()\\nOut[176]: \\n2   -3\\n3    2\\n0    4\\n1    7\\nAny missing values are sorted to the end of the Series by default:\\nIn [177]: obj = Series([4, np.nan, 7, np.nan, -3, 2])\\nIn [178]: obj.order()\\nOut[178]: \\n4    -3\\n5     2\\n0     4\\n2     7\\n1   NaN\\n3   NaN\\nOn DataFrame, you may want to sort by the values in one or more columns. To do so,\\npass one or more column names to the by option:\\nIn [179]: frame = DataFrame({'b': [4, 7, -3, 2], 'a': [0, 1, 0, 1]})\\nIn [180]: frame        In [181]: frame.sort_index(by='b')\\nOut[180]:              Out[181]:                         \\n   a  b                   a  b                           \\n0  0  4                2  0 -3                           \\n1  1  7                3  1  2                           \\n2  0 -3                0  0  4                           \\n3  1  2                1  1  7\\nTo sort by multiple columns, pass a list of names:\\nIn [182]: frame.sort_index(by=['a', 'b'])\\nOut[182]: \\n   a  b\\n2  0 -3\\n0  0  4\\n3  1  2\\n1  1  7\\n134 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 150, 'page_label': '135'}, page_content=\"Ranking is closely related to sorting, assigning ranks from one through the number of\\nvalid data points in an array. It is similar to the indirect sort indices produced by \\nnumpy.argsort, except that ties are broken according to a rule. The rank methods for\\nSeries and DataFrame are the place to look; by default rank breaks ties by assigning\\neach group the mean rank:\\nIn [183]: obj = Series([7, -5, 7, 4, 2, 0, 4])\\nIn [184]: obj.rank()\\nOut[184]: \\n0    6.5\\n1    1.0\\n2    6.5\\n3    4.5\\n4    3.0\\n5    2.0\\n6    4.5\\nRanks can also be assigned according to the order they’re observed in the data:\\nIn [185]: obj.rank(method='first')\\nOut[185]: \\n0    6\\n1    1\\n2    7\\n3    4\\n4    3\\n5    2\\n6    5\\nNaturally, you can rank in descending order, too:\\nIn [186]: obj.rank(ascending=False, method='max')\\nOut[186]: \\n0    2\\n1    7\\n2    2\\n3    4\\n4    5\\n5    6\\n6    4\\nSee Table 5-8 for a list of tie-breaking methods available. DataFrame can compute ranks\\nover the rows or the columns:\\nIn [187]: frame = DataFrame({'b': [4.3, 7, -3, 2], 'a': [0, 1, 0, 1],\\n   .....:                    'c': [-2, 5, 8, -2.5]})\\nIn [188]: frame        In [189]: frame.rank(axis=1)\\nOut[188]:              Out[189]:                   \\n   a    b    c            a  b  c                  \\n0  0  4.3 -2.0         0  2  3  1                  \\n1  1  7.0  5.0         1  1  3  2                  \\n2  0 -3.0  8.0         2  2  1  3                  \\n3  1  2.0 -2.5         3  2  3  1\\nEssential Functionality | 135\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 151, 'page_label': '136'}, page_content=\"Table 5-8. Tie-breaking methods with rank\\nMethod Description\\n'average' Default: assign the average rank to each entry in the equal group.\\n'min' Use the minimum rank for the whole group.\\n'max' Use the maximum rank for the whole group.\\n'first' Assign ranks in the order the values appear in the data.\\nAxis indexes with duplicate values\\nUp until now all of the examples I’ve showed you have had unique axis labels (index\\nvalues). While many pandas functions (like reindex) require that the labels be unique,\\nit’s not mandatory. Let’s consider a small Series with duplicate indices:\\nIn [190]: obj = Series(range(5), index=['a', 'a', 'b', 'b', 'c'])\\nIn [191]: obj\\nOut[191]: \\na    0\\na    1\\nb    2\\nb    3\\nc    4\\nThe index’s is_unique property can tell you whether its values are unique or not:\\nIn [192]: obj.index.is_unique\\nOut[192]: False\\nData selection is one of the main things that behaves differently with duplicates. In-\\ndexing a value with multiple entries returns a Series while single entries return a scalar\\nvalue:\\nIn [193]: obj['a']    In [194]: obj['c']\\nOut[193]:             Out[194]: 4       \\na    0                                  \\na    1\\nThe same logic extends to indexing rows in a DataFrame:\\nIn [195]: df = DataFrame(np.random.randn(4, 3), index=['a', 'a', 'b', 'b'])\\nIn [196]: df\\nOut[196]: \\n          0         1         2\\na  0.274992  0.228913  1.352917\\na  0.886429 -2.001637 -0.371843\\nb  1.669025 -0.438570 -0.539741\\nb  0.476985  3.248944 -1.021228\\nIn [197]: df.ix['b']\\nOut[197]: \\n          0         1         2\\n136 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 152, 'page_label': '137'}, page_content=\"b  1.669025 -0.438570 -0.539741\\nb  0.476985  3.248944 -1.021228\\nSummarizing and Computing Descriptive Statistics\\npandas objects are equipped with a set of common mathematical and statistical meth-\\nods. Most of these fall into the category of reductions or summary statistics, methods\\nthat extract a single value (like the sum or mean) from a Series or a Series of values from\\nthe rows or columns of a DataFrame. Compared with the equivalent methods of vanilla\\nNumPy arrays, they are all built from the ground up to exclude missing data. Consider\\na small DataFrame:\\nIn [198]: df = DataFrame([[1.4, np.nan], [7.1, -4.5],\\n   .....:                 [np.nan, np.nan], [0.75, -1.3]],\\n   .....:                index=['a', 'b', 'c', 'd'],\\n   .....:                columns=['one', 'two'])\\nIn [199]: df\\nOut[199]: \\n    one  two\\na  1.40  NaN\\nb  7.10 -4.5\\nc   NaN  NaN\\nd  0.75 -1.3\\nCalling DataFrame’s sum method returns a Series containing column sums:\\nIn [200]: df.sum()\\nOut[200]: \\none    9.25\\ntwo   -5.80\\nPassing axis=1 sums over the rows instead:\\nIn [201]: df.sum(axis=1)\\nOut[201]: \\na    1.40\\nb    2.60\\nc     NaN\\nd   -0.55\\nNA values are excluded unless the entire slice (row or column in this case) is NA. This\\ncan be disabled using the skipna option:\\nIn [202]: df.mean(axis=1, skipna=False)\\nOut[202]: \\na      NaN\\nb    1.300\\nc      NaN\\nd   -0.275\\nSee Table 5-9 for a list of common options for each reduction method options.\\nSummarizing and Computing Descriptive Statistics | 137\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 153, 'page_label': '138'}, page_content=\"Table 5-9. Options for reduction methods\\nMethod Description\\naxis Axis to reduce over. 0 for DataFrame’s rows and 1 for columns.\\nskipna Exclude missing values, True by default.\\nlevel Reduce grouped by level if the axis is hierarchically-indexed (MultiIndex).\\nSome methods, like idxmin and idxmax, return indirect statistics like the index value\\nwhere the minimum or maximum values are attained:\\nIn [203]: df.idxmax()\\nOut[203]: \\none    b\\ntwo    d\\nOther methods are accumulations:\\nIn [204]: df.cumsum()\\nOut[204]: \\n    one  two\\na  1.40  NaN\\nb  8.50 -4.5\\nc   NaN  NaN\\nd  9.25 -5.8\\nAnother type of method is neither a reduction nor an accumulation. describe is one\\nsuch example, producing multiple summary statistics in one shot:\\nIn [205]: df.describe()\\nOut[205]: \\n            one       two\\ncount  3.000000  2.000000\\nmean   3.083333 -2.900000\\nstd    3.493685  2.262742\\nmin    0.750000 -4.500000\\n25%    1.075000 -3.700000\\n50%    1.400000 -2.900000\\n75%    4.250000 -2.100000\\nmax    7.100000 -1.300000\\nOn non-numeric data, describe produces alternate summary statistics:\\nIn [206]: obj = Series(['a', 'a', 'b', 'c'] * 4)\\nIn [207]: obj.describe()\\nOut[207]: \\ncount     16\\nunique     3\\ntop        a\\nfreq       8\\nSee Table 5-10 for a full list of summary statistics and related methods.\\n138 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 154, 'page_label': '139'}, page_content=\"Table 5-10. Descriptive and summary statistics\\nMethod Description\\ncount Number of non-NA values\\ndescribe Compute set of summary statistics for Series or each DataFrame column\\nmin, max Compute minimum and maximum values\\nargmin, argmax Compute index locations (integers) at which minimum or maximum value obtained, respectively\\nidxmin, idxmax Compute index values at which minimum or maximum value obtained, respectively\\nquantile Compute sample quantile ranging from 0 to 1\\nsum Sum of values\\nmean Mean of values\\nmedian Arithmetic median (50% quantile) of values\\nmad Mean absolute deviation from mean value\\nvar Sample variance of values\\nstd Sample standard deviation of values\\nskew Sample skewness (3rd moment) of values\\nkurt Sample kurtosis (4th moment) of values\\ncumsum Cumulative sum of values\\ncummin, cummax Cumulative minimum or maximum of values, respectively\\ncumprod Cumulative product of values\\ndiff Compute 1st arithmetic difference (useful for time series)\\npct_change Compute percent changes\\nCorrelation and Covariance\\nSome summary statistics, like correlation and covariance, are computed from pairs of\\narguments. Let’s consider some DataFrames of stock prices and volumes obtained from\\nYahoo! Finance:\\nimport pandas.io.data as web\\nall_data = {}\\nfor ticker in ['AAPL', 'IBM', 'MSFT', 'GOOG']:\\n    all_data[ticker] = web.get_data_yahoo(ticker, '1/1/2000', '1/1/2010')\\nprice = DataFrame({tic: data['Adj Close']\\n                   for tic, data in all_data.iteritems()})\\nvolume = DataFrame({tic: data['Volume']\\n                    for tic, data in all_data.iteritems()})\\nI now compute percent changes of the prices:\\nIn [209]: returns = price.pct_change()\\nIn [210]: returns.tail()\\nSummarizing and Computing Descriptive Statistics | 139\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 155, 'page_label': '140'}, page_content='Out[210]: \\n                AAPL      GOOG       IBM      MSFT\\nDate                                              \\n2009-12-24  0.034339  0.011117  0.004420  0.002747\\n2009-12-28  0.012294  0.007098  0.013282  0.005479\\n2009-12-29 -0.011861 -0.005571 -0.003474  0.006812\\n2009-12-30  0.012147  0.005376  0.005468 -0.013532\\n2009-12-31 -0.004300 -0.004416 -0.012609 -0.015432\\nThe corr method of Series computes the correlation of the overlapping, non-NA,\\naligned-by-index values in two Series. Relatedly, cov computes the covariance:\\nIn [211]: returns.MSFT.corr(returns.IBM)\\nOut[211]: 0.49609291822168838\\nIn [212]: returns.MSFT.cov(returns.IBM)\\nOut[212]: 0.00021600332437329015\\nDataFrame’s corr and cov methods, on the other hand, return a full correlation or\\ncovariance matrix as a DataFrame, respectively:\\nIn [213]: returns.corr()\\nOut[213]: \\n          AAPL      GOOG       IBM      MSFT\\nAAPL  1.000000  0.470660  0.410648  0.424550\\nGOOG  0.470660  1.000000  0.390692  0.443334\\nIBM   0.410648  0.390692  1.000000  0.496093\\nMSFT  0.424550  0.443334  0.496093  1.000000\\nIn [214]: returns.cov()\\nOut[214]: \\n          AAPL      GOOG       IBM      MSFT\\nAAPL  0.001028  0.000303  0.000252  0.000309\\nGOOG  0.000303  0.000580  0.000142  0.000205\\nIBM   0.000252  0.000142  0.000367  0.000216\\nMSFT  0.000309  0.000205  0.000216  0.000516\\nUsing DataFrame’s corrwith method, you can compute pairwise correlations between\\na DataFrame’s columns or rows with another Series or DataFrame. Passing a Series\\nreturns a Series with the correlation value computed for each column:\\nIn [215]: returns.corrwith(returns.IBM)\\nOut[215]: \\nAAPL    0.410648\\nGOOG    0.390692\\nIBM     1.000000\\nMSFT    0.496093\\nPassing a DataFrame computes the correlations of matching column names. Here I\\ncompute correlations of percent changes with volume:\\nIn [216]: returns.corrwith(volume)\\nOut[216]: \\nAAPL   -0.057461\\nGOOG    0.062644\\n140 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 156, 'page_label': '141'}, page_content=\"IBM    -0.007900\\nMSFT   -0.014175\\nPassing axis=1 does things row-wise instead. In all cases, the data points are aligned by\\nlabel before computing the correlation.\\nUnique Values, Value Counts, and Membership\\nAnother class of related methods extracts information about the values contained in a\\none-dimensional Series. To illustrate these, consider this example:\\nIn [217]: obj = Series(['c', 'a', 'd', 'a', 'a', 'b', 'b', 'c', 'c'])\\nThe first function is unique, which gives you an array of the unique values in a Series:\\nIn [218]: uniques = obj.unique()\\nIn [219]: uniques\\nOut[219]: array([c, a, d, b], dtype=object)\\nThe unique values are not necessarily returned in sorted order, but could be sorted after\\nthe fact if needed ( uniques.sort()). Relatedly, value_counts computes a Series con-\\ntaining value frequencies:\\nIn [220]: obj.value_counts()\\nOut[220]: \\nc    3\\na    3\\nb    2\\nd    1\\nThe Series is sorted by value in descending order as a convenience. value_counts is also\\navailable as a top-level pandas method that can be used with any array or sequence:\\nIn [221]: pd.value_counts(obj.values, sort=False)\\nOut[221]: \\na    3\\nb    2\\nc    3\\nd    1\\nLastly, isin is responsible for vectorized set membership and can be very useful in\\nfiltering a data set down to a subset of values in a Series or column in a DataFrame:\\nIn [222]: mask = obj.isin(['b', 'c'])\\nIn [223]: mask        In [224]: obj[mask]\\nOut[223]:             Out[224]:          \\n0     True            0    c             \\n1    False            5    b             \\n2    False            6    b             \\n3    False            7    c             \\n4    False            8    c             \\n5     True                               \\n6     True                               \\nSummarizing and Computing Descriptive Statistics | 141\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 157, 'page_label': '142'}, page_content=\"7     True                               \\n8     True\\nSee Table 5-11 for a reference on these methods.\\nTable 5-11. Unique, value counts, and binning methods\\nMethod Description\\nisin Compute boolean array indicating whether each Series value is contained in the passed sequence of values.\\nunique Compute array of unique values in a Series, returned in the order observed.\\nvalue_counts Return a Series containing unique values as its index and frequencies as its values, ordered count in\\ndescending order.\\nIn some cases, you may want to compute a histogram on multiple related columns in\\na DataFrame. Here’s an example:\\nIn [225]: data = DataFrame({'Qu1': [1, 3, 4, 3, 4],\\n   .....:                   'Qu2': [2, 3, 1, 2, 3],\\n   .....:                   'Qu3': [1, 5, 2, 4, 4]})\\nIn [226]: data\\nOut[226]: \\n   Qu1  Qu2  Qu3\\n0    1    2    1\\n1    3    3    5\\n2    4    1    2\\n3    3    2    4\\n4    4    3    4\\nPassing pandas.value_counts to this DataFrame’s apply function gives:\\nIn [227]: result = data.apply(pd.value_counts).fillna(0)\\nIn [228]: result\\nOut[228]: \\n   Qu1  Qu2  Qu3\\n1    1    1    1\\n2    0    2    1\\n3    2    2    0\\n4    2    0    2\\n5    0    0    1\\nHandling Missing Data\\nMissing data is common in most data analysis applications. One of the goals in de-\\nsigning pandas was to make working with missing data as painless as possible. For\\nexample, all of the descriptive statistics on pandas objects exclude missing data as\\nyou’ve seen earlier in the chapter.\\n142 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 158, 'page_label': '143'}, page_content=\"pandas uses the floating point value NaN (Not a Number) to represent missing data in\\nboth floating as well as in non-floating point arrays. It is just used as a sentinel that can\\nbe easily detected:\\nIn [229]: string_data = Series(['aardvark', 'artichoke', np.nan, 'avocado'])\\nIn [230]: string_data        In [231]: string_data.isnull()\\nOut[230]:                    Out[231]:                     \\n0     aardvark               0    False                    \\n1    artichoke               1    False                    \\n2          NaN               2     True                    \\n3      avocado               3    False\\nThe built-in Python None value is also treated as NA in object arrays:\\nIn [232]: string_data[0] = None\\nIn [233]: string_data.isnull()\\nOut[233]: \\n0     True\\n1    False\\n2     True\\n3    False\\nI do not claim that pandas’s NA representation is optimal, but it is simple and reason-\\nably consistent. It’s the best solution, with good all-around performance characteristics\\nand a simple API, that I could concoct in the absence of a true NA data type or bit\\npattern in NumPy’s data types. Ongoing development work in NumPy may change this\\nin the future.\\nTable 5-12. NA handling methods\\nArgument Description\\ndropna Filter axis labels based on whether values for each label have missing data, with varying thresholds for how much\\nmissing data to tolerate.\\nfillna Fill in missing data with some value or using an interpolation method such as 'ffill' or 'bfill'.\\nisnull Return like-type object containing boolean values indicating which values are missing / NA.\\nnotnull Negation of isnull.\\nFiltering Out Missing Data\\nYou have a number of options for filtering out missing data. While doing it by hand is\\nalways an option, dropna can be very helpful. On a Series, it returns the Series with only\\nthe non-null data and index values:\\nIn [234]: from numpy import nan as NA\\nIn [235]: data = Series([1, NA, 3.5, NA, 7])\\nIn [236]: data.dropna()\\nOut[236]: \\nHandling Missing Data | 143\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 159, 'page_label': '144'}, page_content=\"0    1.0\\n2    3.5\\n4    7.0\\nNaturally, you could have computed this yourself by boolean indexing:\\nIn [237]: data[data.notnull()]\\nOut[237]: \\n0    1.0\\n2    3.5\\n4    7.0\\nWith DataFrame objects, these are a bit more complex. You may want to drop rows\\nor columns which are all NA or just those containing any NAs. dropna by default drops\\nany row containing a missing value:\\nIn [238]: data = DataFrame([[1., 6.5, 3.], [1., NA, NA],\\n   .....:                   [NA, NA, NA], [NA, 6.5, 3.]])\\nIn [239]: cleaned = data.dropna()\\nIn [240]: data        In [241]: cleaned\\nOut[240]:             Out[241]:        \\n    0    1   2           0    1  2     \\n0   1  6.5   3        0  1  6.5  3     \\n1   1  NaN NaN                         \\n2 NaN  NaN NaN                         \\n3 NaN  6.5   3\\nPassing how='all' will only drop rows that are all NA:\\nIn [242]: data.dropna(how='all')\\nOut[242]: \\n    0    1   2\\n0   1  6.5   3\\n1   1  NaN NaN\\n3 NaN  6.5   3\\nDropping columns in the same way is only a matter of passing axis=1:\\nIn [243]: data[4] = NA\\nIn [244]: data            In [245]: data.dropna(axis=1, how='all')\\nOut[244]:                 Out[245]:                               \\n    0    1   2   4            0    1   2                          \\n0   1  6.5   3 NaN        0   1  6.5   3                          \\n1   1  NaN NaN NaN        1   1  NaN NaN                          \\n2 NaN  NaN NaN NaN        2 NaN  NaN NaN                          \\n3 NaN  6.5   3 NaN        3 NaN  6.5   3\\nA related way to filter out DataFrame rows tends to concern time series data. Suppose\\nyou want to keep only rows containing a certain number of observations. You can\\nindicate this with the thresh argument:\\nIn [246]: df = DataFrame(np.random.randn(7, 3))\\nIn [247]: df.ix[:4, 1] = NA; df.ix[:2, 2] = NA\\n144 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 160, 'page_label': '145'}, page_content='In [248]: df                           In [249]: df.dropna(thresh=3)  \\nOut[248]:                              Out[249]:                      \\n          0         1         2                  0         1         2\\n0 -0.577087       NaN       NaN        5  0.332883 -2.359419 -0.199543\\n1  0.523772       NaN       NaN        6 -1.541996 -0.970736 -1.307030\\n2 -0.713544       NaN       NaN                                       \\n3 -1.860761       NaN  0.560145                                       \\n4 -1.265934       NaN -1.063512                                       \\n5  0.332883 -2.359419 -0.199543                                       \\n6 -1.541996 -0.970736 -1.307030\\nFilling in Missing Data\\nRather than filtering out missing data (and potentially discarding other data along with\\nit), you may want to fill in the “holes” in any number of ways. For most purposes, the \\nfillna method is the workhorse function to use. Calling fillna with a constant replaces\\nmissing values with that value:\\nIn [250]: df.fillna(0)\\nOut[250]: \\n          0         1         2\\n0 -0.577087  0.000000  0.000000\\n1  0.523772  0.000000  0.000000\\n2 -0.713544  0.000000  0.000000\\n3 -1.860761  0.000000  0.560145\\n4 -1.265934  0.000000 -1.063512\\n5  0.332883 -2.359419 -0.199543\\n6 -1.541996 -0.970736 -1.307030\\nCalling fillna with a dict you can use a different fill value for each column:\\nIn [251]: df.fillna({1: 0.5, 3: -1})\\nOut[251]: \\n          0         1         2\\n0 -0.577087  0.500000       NaN\\n1  0.523772  0.500000       NaN\\n2 -0.713544  0.500000       NaN\\n3 -1.860761  0.500000  0.560145\\n4 -1.265934  0.500000 -1.063512\\n5  0.332883 -2.359419 -0.199543\\n6 -1.541996 -0.970736 -1.307030\\nfillna returns a new object, but you can modify the existing object in place:\\n# always returns a reference to the filled object\\nIn [252]: _ = df.fillna(0, inplace=True)\\nIn [253]: df\\nOut[253]: \\n          0         1         2\\n0 -0.577087  0.000000  0.000000\\n1  0.523772  0.000000  0.000000\\n2 -0.713544  0.000000  0.000000\\n3 -1.860761  0.000000  0.560145\\nHandling Missing Data | 145\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 161, 'page_label': '146'}, page_content=\"4 -1.265934  0.000000 -1.063512\\n5  0.332883 -2.359419 -0.199543\\n6 -1.541996 -0.970736 -1.307030\\nThe same interpolation methods available for reindexing can be used with fillna:\\nIn [254]: df = DataFrame(np.random.randn(6, 3))\\nIn [255]: df.ix[2:, 1] = NA; df.ix[4:, 2] = NA\\nIn [256]: df\\nOut[256]: \\n          0         1         2\\n0  0.286350  0.377984 -0.753887\\n1  0.331286  1.349742  0.069877\\n2  0.246674       NaN  1.004812\\n3  1.327195       NaN -1.549106\\n4  0.022185       NaN       NaN\\n5  0.862580       NaN       NaN\\nIn [257]: df.fillna(method='ffill')      In [258]: df.fillna(method='ffill', limit=2)\\nOut[257]:                                Out[258]:                                   \\n          0         1         2                    0         1         2             \\n0  0.286350  0.377984 -0.753887          0  0.286350  0.377984 -0.753887             \\n1  0.331286  1.349742  0.069877          1  0.331286  1.349742  0.069877             \\n2  0.246674  1.349742  1.004812          2  0.246674  1.349742  1.004812             \\n3  1.327195  1.349742 -1.549106          3  1.327195  1.349742 -1.549106             \\n4  0.022185  1.349742 -1.549106          4  0.022185       NaN -1.549106             \\n5  0.862580  1.349742 -1.549106          5  0.862580       NaN -1.549106\\nWith fillna you can do lots of other things with a little creativity. For example, you\\nmight pass the mean or median value of a Series:\\nIn [259]: data = Series([1., NA, 3.5, NA, 7])\\nIn [260]: data.fillna(data.mean())\\nOut[260]: \\n0    1.000000\\n1    3.833333\\n2    3.500000\\n3    3.833333\\n4    7.000000\\nSee Table 5-13 for a reference on fillna.\\nTable 5-13. fillna function arguments\\nArgument Description\\nvalue Scalar value or dict-like object to use to fill missing values\\nmethod Interpolation, by default 'ffill' if function called with no other arguments\\naxis Axis to fill on, default axis=0\\ninplace Modify the calling object without producing a copy\\nlimit For forward and backward filling, maximum number of consecutive periods to fill\\n146 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 162, 'page_label': '147'}, page_content=\"Hierarchical Indexing\\nHierarchical indexing is an important feature of pandas enabling you to have multiple\\n(two or more) index levels on an axis. Somewhat abstractly, it provides a way for you\\nto work with higher dimensional data in a lower dimensional form. Let’s start with a\\nsimple example; create a Series with a list of lists or arrays as the index:\\nIn [261]: data = Series(np.random.randn(10),\\n   .....:               index=[['a', 'a', 'a', 'b', 'b', 'b', 'c', 'c', 'd', 'd'],\\n   .....:                      [1, 2, 3, 1, 2, 3, 1, 2, 2, 3]])\\nIn [262]: data\\nOut[262]: \\na  1    0.670216\\n   2    0.852965\\n   3   -0.955869\\nb  1   -0.023493\\n   2   -2.304234\\n   3   -0.652469\\nc  1   -1.218302\\n   2   -1.332610\\nd  2    1.074623\\n   3    0.723642\\nWhat you’re seeing is a prettified view of a Series with a MultiIndex as its index. The\\n“gaps” in the index display mean “use the label directly above”:\\nIn [263]: data.index\\nOut[263]: \\nMultiIndex\\n[('a', 1) ('a', 2) ('a', 3) ('b', 1) ('b', 2) ('b', 3) ('c', 1)\\n ('c', 2) ('d', 2) ('d', 3)]\\nWith a hierarchically-indexed object, so-called partial indexing is possible, enabling\\nyou to concisely select subsets of the data:\\nIn [264]: data['b']\\nOut[264]: \\n1   -0.023493\\n2   -2.304234\\n3   -0.652469\\nIn [265]: data['b':'c']        In [266]: data.ix[['b', 'd']]\\nOut[265]:                      Out[266]:                    \\nb  1   -0.023493               b  1   -0.023493             \\n   2   -2.304234                  2   -2.304234             \\n   3   -0.652469                  3   -0.652469             \\nc  1   -1.218302               d  2    1.074623             \\n   2   -1.332610                  3    0.723642\\nSelection is even possible in some cases from an “inner” level:\\nIn [267]: data[:, 2]\\nOut[267]: \\na    0.852965\\nHierarchical Indexing | 147\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 163, 'page_label': '148'}, page_content=\"b   -2.304234\\nc   -1.332610\\nd    1.074623\\nHierarchical indexing plays a critical role in reshaping data and group-based operations\\nlike forming a pivot table. For example, this data could be rearranged into a DataFrame\\nusing its unstack method:\\nIn [268]: data.unstack()\\nOut[268]: \\n          1         2         3\\na  0.670216  0.852965 -0.955869\\nb -0.023493 -2.304234 -0.652469\\nc -1.218302 -1.332610       NaN\\nd       NaN  1.074623  0.723642\\nThe inverse operation of unstack is stack:\\nIn [269]: data.unstack().stack()\\nOut[269]: \\na  1    0.670216\\n   2    0.852965\\n   3   -0.955869\\nb  1   -0.023493\\n   2   -2.304234\\n   3   -0.652469\\nc  1   -1.218302\\n   2   -1.332610\\nd  2    1.074623\\n   3    0.723642\\nstack and unstack will be explored in more detail in Chapter 7.\\nWith a DataFrame, either axis can have a hierarchical index:\\nIn [270]: frame = DataFrame(np.arange(12).reshape((4, 3)),\\n   .....:                   index=[['a', 'a', 'b', 'b'], [1, 2, 1, 2]],\\n   .....:                   columns=[['Ohio', 'Ohio', 'Colorado'],\\n   .....:                            ['Green', 'Red', 'Green']])\\nIn [271]: frame\\nOut[271]: \\n      Ohio       Colorado\\n     Green  Red     Green\\na 1      0    1         2\\n  2      3    4         5\\nb 1      6    7         8\\n  2      9   10        11\\nThe hierarchical levels can have names (as strings or any Python objects). If so, these\\nwill show up in the console output (don’t confuse the index names with the axis labels!):\\nIn [272]: frame.index.names = ['key1', 'key2']\\nIn [273]: frame.columns.names = ['state', 'color']\\nIn [274]: frame\\n148 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 164, 'page_label': '149'}, page_content=\"Out[274]: \\nstate       Ohio       Colorado\\ncolor      Green  Red     Green\\nkey1 key2                      \\na    1         0    1         2\\n     2         3    4         5\\nb    1         6    7         8\\n     2         9   10        11\\nWith partial column indexing you can similarly select groups of columns:\\nIn [275]: frame['Ohio']\\nOut[275]: \\ncolor      Green  Red\\nkey1 key2            \\na    1         0    1\\n     2         3    4\\nb    1         6    7\\n     2         9   10\\nA MultiIndex can be created by itself and then reused; the columns in the above Data-\\nFrame with level names could be created like this:\\nMultiIndex.from_arrays([['Ohio', 'Ohio', 'Colorado'], ['Green', 'Red', 'Green']],\\n                       names=['state', 'color'])\\nReordering and Sorting Levels\\nAt times you will need to rearrange the order of the levels on an axis or sort the data\\nby the values in one specific level. The swaplevel takes two level numbers or names and\\nreturns a new object with the levels interchanged (but the data is otherwise unaltered):\\nIn [276]: frame.swaplevel('key1', 'key2')\\nOut[276]: \\nstate       Ohio       Colorado\\ncolor      Green  Red     Green\\nkey2 key1                      \\n1    a         0    1         2\\n2    a         3    4         5\\n1    b         6    7         8\\n2    b         9   10        11\\nsortlevel, on the other hand, sorts the data (stably) using only the values in a single\\nlevel. When swapping levels, it’s not uncommon to also use sortlevel so that the result\\nis lexicographically sorted:\\nIn [277]: frame.sortlevel(1)           In [278]: frame.swaplevel(0, 1).sortlevel(0)\\nOut[277]:                              Out[278]:                                   \\nstate       Ohio       Colorado        state       Ohio       Colorado             \\ncolor      Green  Red     Green        color      Green  Red     Green             \\nkey1 key2                              key2 key1                                   \\na    1         0    1         2        1    a         0    1         2             \\nb    1         6    7         8             b         6    7         8             \\na    2         3    4         5        2    a         3    4         5             \\nb    2         9   10        11             b         9   10        11\\nHierarchical Indexing | 149\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 165, 'page_label': '150'}, page_content=\"Data selection performance is much better on hierarchically indexed\\nobjects if the index is lexicographically sorted starting with the outer-\\nmost level, that is, the result of calling sortlevel(0) or sort_index().\\nSummary Statistics by Level\\nMany descriptive and summary statistics on DataFrame and Series have a level option\\nin which you can specify the level you want to sum by on a particular axis. Consider\\nthe above DataFrame; we can sum by level on either the rows or columns like so:\\nIn [279]: frame.sum(level='key2')\\nOut[279]: \\nstate   Ohio       Colorado\\ncolor  Green  Red     Green\\nkey2                       \\n1          6    8        10\\n2         12   14        16\\nIn [280]: frame.sum(level='color', axis=1)\\nOut[280]: \\ncolor      Green  Red\\nkey1 key2            \\na    1         2    1\\n     2         8    4\\nb    1        14    7\\n     2        20   10\\nUnder the hood, this utilizes pandas’s groupby machinery which will be discussed in\\nmore detail later in the book.\\nUsing a DataFrame’s Columns\\nIt’s not unusual to want to use one or more columns from a DataFrame as the row\\nindex; alternatively, you may wish to move the row index into the DataFrame’s col-\\numns. Here’s an example DataFrame:\\nIn [281]: frame = DataFrame({'a': range(7), 'b': range(7, 0, -1),\\n   .....:                    'c': ['one', 'one', 'one', 'two', 'two', 'two', 'two'],\\n   .....:                    'd': [0, 1, 2, 0, 1, 2, 3]})\\nIn [282]: frame\\nOut[282]: \\n   a  b    c  d\\n0  0  7  one  0\\n1  1  6  one  1\\n2  2  5  one  2\\n3  3  4  two  0\\n4  4  3  two  1\\n5  5  2  two  2\\n6  6  1  two  3\\n150 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 166, 'page_label': '151'}, page_content=\"DataFrame’s set_index function will create a new DataFrame using one or more of its\\ncolumns as the index:\\nIn [283]: frame2 = frame.set_index(['c', 'd'])\\nIn [284]: frame2\\nOut[284]: \\n       a  b\\nc   d      \\none 0  0  7\\n    1  1  6\\n    2  2  5\\ntwo 0  3  4\\n    1  4  3\\n    2  5  2\\n    3  6  1\\nBy default the columns are removed from the DataFrame, though you can leave them in:\\nIn [285]: frame.set_index(['c', 'd'], drop=False)\\nOut[285]: \\n       a  b    c  d\\nc   d              \\none 0  0  7  one  0\\n    1  1  6  one  1\\n    2  2  5  one  2\\ntwo 0  3  4  two  0\\n    1  4  3  two  1\\n    2  5  2  two  2\\n    3  6  1  two  3\\nreset_index, on the other hand, does the opposite of set_index; the hierarchical index\\nlevels are are moved into the columns:\\nIn [286]: frame2.reset_index()\\nOut[286]: \\n     c  d  a  b\\n0  one  0  0  7\\n1  one  1  1  6\\n2  one  2  2  5\\n3  two  0  3  4\\n4  two  1  4  3\\n5  two  2  5  2\\n6  two  3  6  1\\nOther pandas Topics\\nHere are some additional topics that may be of use to you in your data travels.\\nInteger Indexing\\nWorking with pandas objects indexed by integers is something that often trips up new\\nusers due to some differences with indexing semantics on built-in Python data\\nOther pandas Topics | 151\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 167, 'page_label': '152'}, page_content=\"structures like lists and tuples. For example, you would not expect the following code\\nto generate an error:\\nser = Series(np.arange(3.))\\nser[-1]\\nIn this case, pandas could “fall back” on integer indexing, but there’s not a safe and\\ngeneral way (that I know of) to do this without introducing subtle bugs. Here we have\\nan index containing 0, 1, 2, but inferring what the user wants (label-based indexing or\\nposition-based) is difficult::\\nIn [288]: ser\\nOut[288]: \\n0    0\\n1    1\\n2    2\\nOn the other hand, with a non-integer index, there is no potential for ambiguity:\\nIn [289]: ser2 = Series(np.arange(3.), index=['a', 'b', 'c'])\\nIn [290]: ser2[-1]\\nOut[290]: 2.0\\nTo keep things consistent, if you have an axis index containing indexers, data selection\\nwith integers will always be label-oriented. This includes slicing with ix, too:\\nIn [291]: ser.ix[:1]\\nOut[291]: \\n0    0\\n1    1\\nIn cases where you need reliable position-based indexing regardless of the index type,\\nyou can use the iget_value method from Series and irow and icol methods from Da-\\ntaFrame:\\nIn [292]: ser3 = Series(range(3), index=[-5, 1, 3])\\nIn [293]: ser3.iget_value(2)\\nOut[293]: 2\\nIn [294]: frame = DataFrame(np.arange(6).reshape(3, 2)), index=[2, 0, 1])\\nIn [295]: frame.irow(0)\\nOut[295]: \\n0    0\\n1    1\\nName: 2\\nPanel Data\\nWhile not a major topic of this book, pandas has a Panel data structure, which you can\\nthink of as a three-dimensional analogue of DataFrame. Much of the development focus\\nof pandas has been in tabular data manipulations as these are easier to reason about,\\n152 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 168, 'page_label': '153'}, page_content=\"and hierarchical indexing makes using truly N-dimensional arrays unnecessary in a lot\\nof cases.\\nTo create a Panel, you can use a dict of DataFrame objects or a three-dimensional\\nndarray:\\nimport pandas.io.data as web\\npdata = pd.Panel(dict((stk, web.get_data_yahoo(stk, '1/1/2009', '6/1/2012'))\\n                       for stk in ['AAPL', 'GOOG', 'MSFT', 'DELL']))\\nEach item (the analogue of columns in a DataFrame) in the Panel is a DataFrame:\\nIn [297]: pdata\\nOut[297]: \\n<class 'pandas.core.panel.Panel'>\\nDimensions: 4 (items) x 861 (major) x 6 (minor)\\nItems: AAPL to MSFT\\nMajor axis: 2009-01-02 00:00:00 to 2012-06-01 00:00:00\\nMinor axis: Open to Adj Close\\nIn [298]: pdata = pdata.swapaxes('items', 'minor')\\nIn [299]: pdata['Adj Close']\\nOut[299]: \\n<class 'pandas.core.frame.DataFrame'>\\nDatetimeIndex: 861 entries, 2009-01-02 00:00:00 to 2012-06-01 00:00:00\\nData columns:\\nAAPL    861  non-null values\\nDELL    861  non-null values\\nGOOG    861  non-null values\\nMSFT    861  non-null values\\ndtypes: float64(4)\\nix-based label indexing generalizes to three dimensions, so we can select all data at a\\nparticular date or a range of dates like so:\\nIn [300]: pdata.ix[:, '6/1/2012', :]\\nOut[300]: \\n        Open    High     Low   Close    Volume  Adj Close\\nAAPL  569.16  572.65  560.52  560.99  18606700     560.99\\nDELL   12.15   12.30   12.05   12.07  19396700      12.07\\nGOOG  571.79  572.65  568.35  570.98   3057900     570.98\\nMSFT   28.76   28.96   28.44   28.45  56634300      28.45\\nIn [301]: pdata.ix['Adj Close', '5/22/2012':, :]\\nOut[301]: \\n              AAPL   DELL    GOOG   MSFT\\nDate                                    \\n2012-05-22  556.97  15.08  600.80  29.76\\n2012-05-23  570.56  12.49  609.46  29.11\\n2012-05-24  565.32  12.45  603.66  29.07\\n2012-05-25  562.29  12.46  591.53  29.06\\n2012-05-29  572.27  12.66  594.34  29.56\\n2012-05-30  579.17  12.56  588.23  29.34\\nOther pandas Topics | 153\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 169, 'page_label': '154'}, page_content=\"2012-05-31  577.73  12.33  580.86  29.19\\n2012-06-01  560.99  12.07  570.98  28.45\\nAn alternate way to represent panel data, especially for fitting statistical models, is in\\n“stacked” DataFrame form:\\nIn [302]: stacked = pdata.ix[:, '5/30/2012':, :].to_frame()\\nIn [303]: stacked\\nOut[303]: \\n                    Open    High     Low   Close    Volume  Adj Close\\nmajor      minor                                                     \\n2012-05-30 AAPL   569.20  579.99  566.56  579.17  18908200     579.17\\n           DELL    12.59   12.70   12.46   12.56  19787800      12.56\\n           GOOG   588.16  591.90  583.53  588.23   1906700     588.23\\n           MSFT    29.35   29.48   29.12   29.34  41585500      29.34\\n2012-05-31 AAPL   580.74  581.50  571.46  577.73  17559800     577.73\\n           DELL    12.53   12.54   12.33   12.33  19955500      12.33\\n           GOOG   588.72  590.00  579.00  580.86   2968300     580.86\\n           MSFT    29.30   29.42   28.94   29.19  39134000      29.19\\n2012-06-01 AAPL   569.16  572.65  560.52  560.99  18606700     560.99\\n           DELL    12.15   12.30   12.05   12.07  19396700      12.07\\n           GOOG   571.79  572.65  568.35  570.98   3057900     570.98\\n           MSFT    28.76   28.96   28.44   28.45  56634300      28.45\\nDataFrame has a related to_panel method, the inverse of to_frame:\\nIn [304]: stacked.to_panel()\\nOut[304]: \\n<class 'pandas.core.panel.Panel'>\\nDimensions: 6 (items) x 3 (major) x 4 (minor)\\nItems: Open to Adj Close\\nMajor axis: 2012-05-30 00:00:00 to 2012-06-01 00:00:00\\nMinor axis: AAPL to MSFT\\n154 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 170, 'page_label': '155'}, page_content=\"CHAPTER 6\\nData Loading, Storage, and File\\nFormats\\nThe tools in this book are of little use if you can’t easily import and export data in\\nPython. I’m going to be focused on input and output with pandas objects, though there\\nare of course numerous tools in other libraries to aid in this process. NumPy, for ex-\\nample, features low-level but extremely fast binary data loading and storage, including\\nsupport for memory-mapped array. See Chapter 12 for more on those.\\nInput and output typically falls into a few main categories: reading text files and other\\nmore efficient on-disk formats, loading data from databases, and interacting with net-\\nwork sources like web APIs.\\nReading and Writing Data in Text Format\\nPython has become a beloved language for text and file munging due to its simple syntax\\nfor interacting with files, intuitive data structures, and convenient features like tuple\\npacking and unpacking.\\npandas features a number of functions for reading tabular data as a DataFrame object.\\nTable 6-1 has a summary of all of them, though read_csv and read_table are likely the\\nones you’ll use the most.\\nTable 6-1. Parsing functions in pandas\\nFunction Description\\nread_csv Load delimited data from a file, URL, or file-like object. Use comma as default delimiter\\nread_table Load delimited data from a file, URL, or file-like object. Use tab ('\\\\t') as default delimiter\\nread_fwf Read data in fixed-width column format (that is, no delimiters)\\nread_clipboard Version of read_table that reads data from the clipboard. Useful for converting tables from web pages\\n155\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 171, 'page_label': '156'}, page_content=\"I’ll give an overview of the mechanics of these functions, which are meant to convert\\ntext data into a DataFrame. The options for these functions fall into a few categories:\\n• Indexing: can treat one or more columns as the returned DataFrame, and whether\\nto get column names from the file, the user, or not at all.\\n• Type inference and data conversion: this includes the user-defined value conver-\\nsions and custom list of missing value markers.\\n• Datetime parsing: includes combining capability, including combining date and\\ntime information spread over multiple columns into a single column in the result.\\n• Iterating: support for iterating over chunks of very large files.\\n• Unclean data issues: skipping rows or a footer, comments, or other minor things\\nlike numeric data with thousands separated by commas.\\nType inference is one of the more important features of these functions; that means you\\ndon’t have to specify which columns are numeric, integer, boolean, or string. Handling\\ndates and other custom types requires a bit more effort, though. Let’s start with a small\\ncomma-separated (CSV) text file:\\nIn [846]: !cat ch06/ex1.csv\\na,b,c,d,message\\n1,2,3,4,hello\\n5,6,7,8,world\\n9,10,11,12,foo\\nSince this is comma-delimited, we can use read_csv to read it into a DataFrame:\\nIn [847]: df = pd.read_csv('ch06/ex1.csv')\\nIn [848]: df\\nOut[848]: \\n   a   b   c   d message\\n0  1   2   3   4   hello\\n1  5   6   7   8   world\\n2  9  10  11  12     foo\\nWe could also have used read_table and specifying the delimiter:\\nIn [849]: pd.read_table('ch06/ex1.csv', sep=',')\\nOut[849]: \\n   a   b   c   d message\\n0  1   2   3   4   hello\\n1  5   6   7   8   world\\n2  9  10  11  12     foo\\nHere I used the Unix cat shell command to print the raw contents of\\nthe file to the screen. If you’re on Windows, you can use type instead\\nof cat to achieve the same effect.\\n156 | Chapter 6: \\u2002Data Loading, Storage, and File Formats\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 172, 'page_label': '157'}, page_content=\"A file will not always have a header row. Consider this file:\\nIn [850]: !cat ch06/ex2.csv\\n1,2,3,4,hello\\n5,6,7,8,world\\n9,10,11,12,foo\\nTo read this in, you have a couple of options. You can allow pandas to assign default\\ncolumn names, or you can specify names yourself:\\nIn [851]: pd.read_csv('ch06/ex2.csv', header=None)\\nOut[851]: \\n   X.1  X.2  X.3  X.4    X.5\\n0    1    2    3    4  hello\\n1    5    6    7    8  world\\n2    9   10   11   12    foo\\nIn [852]: pd.read_csv('ch06/ex2.csv', names=['a', 'b', 'c', 'd', 'message'])\\nOut[852]: \\n   a   b   c   d message\\n0  1   2   3   4   hello\\n1  5   6   7   8   world\\n2  9  10  11  12     foo\\nSuppose you wanted the message column to be the index of the returned DataFrame.\\nYou can either indicate you want the column at index 4 or named 'message' using the\\nindex_col argument:\\nIn [853]: names = ['a', 'b', 'c', 'd', 'message']\\nIn [854]: pd.read_csv('ch06/ex2.csv', names=names, index_col='message')\\nOut[854]: \\n         a   b   c   d\\nmessage               \\nhello    1   2   3   4\\nworld    5   6   7   8\\nfoo      9  10  11  12\\nIn the event that you want to form a hierarchical index from multiple columns, just\\npass a list of column numbers or names:\\nIn [855]: !cat ch06/csv_mindex.csv\\nkey1,key2,value1,value2\\none,a,1,2\\none,b,3,4\\none,c,5,6\\none,d,7,8\\ntwo,a,9,10\\ntwo,b,11,12\\ntwo,c,13,14\\ntwo,d,15,16\\nIn [856]: parsed = pd.read_csv('ch06/csv_mindex.csv', index_col=['key1', 'key2'])\\nIn [857]: parsed\\nOut[857]: \\nReading and Writing Data in Text Format | 157\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 173, 'page_label': '158'}, page_content=\"value1  value2\\nkey1 key2                \\none  a          1       2\\n     b          3       4\\n     c          5       6\\n     d          7       8\\ntwo  a          9      10\\n     b         11      12\\n     c         13      14\\n     d         15      16\\nIn some cases, a table might not have a fixed delimiter, using whitespace or some other\\npattern to separate fields. In these cases, you can pass a regular expression as a delimiter\\nfor read_table. Consider a text file that looks like this:\\nIn [858]: list(open('ch06/ex3.txt'))\\nOut[858]: \\n['            A         B         C\\\\n',\\n 'aaa -0.264438 -1.026059 -0.619500\\\\n',\\n 'bbb  0.927272  0.302904 -0.032399\\\\n',\\n 'ccc -0.264273 -0.386314 -0.217601\\\\n',\\n 'ddd -0.871858 -0.348382  1.100491\\\\n']\\nWhile you could do some munging by hand, in this case fields are separated by a variable\\namount of whitespace. This can be expressed by the regular expression \\\\s+, so we have\\nthen:\\nIn [859]: result = pd.read_table('ch06/ex3.txt', sep='\\\\s+')\\nIn [860]: result\\nOut[860]: \\n            A         B         C\\naaa -0.264438 -1.026059 -0.619500\\nbbb  0.927272  0.302904 -0.032399\\nccc -0.264273 -0.386314 -0.217601\\nddd -0.871858 -0.348382  1.100491\\nBecause there was one fewer column name than the number of data rows, read_table\\ninfers that the first column should be the DataFrame’s index in this special case.\\nThe parser functions have many additional arguments to help you handle the wide\\nvariety of exception file formats that occur (see Table 6-2). For example, you can skip\\nthe first, third, and fourth rows of a file with skiprows:\\nIn [861]: !cat ch06/ex4.csv\\n# hey!\\na,b,c,d,message\\n# just wanted to make things more difficult for you\\n# who reads CSV files with computers, anyway?\\n1,2,3,4,hello\\n5,6,7,8,world\\n9,10,11,12,foo\\nIn [862]: pd.read_csv('ch06/ex4.csv', skiprows=[0, 2, 3])\\nOut[862]: \\n   a   b   c   d message\\n158 | Chapter 6: \\u2002Data Loading, Storage, and File Formats\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 174, 'page_label': '159'}, page_content=\"0  1   2   3   4   hello\\n1  5   6   7   8   world\\n2  9  10  11  12     foo\\nHandling missing values is an important and frequently nuanced part of the file parsing\\nprocess. Missing data is usually either not present (empty string) or marked by some \\nsentinel value. By default, pandas uses a set of commonly occurring sentinels, such as\\nNA, -1.#IND, and NULL:\\nIn [863]: !cat ch06/ex5.csv\\nsomething,a,b,c,d,message\\none,1,2,3,4,NA\\ntwo,5,6,,8,world\\nthree,9,10,11,12,foo\\nIn [864]: result = pd.read_csv('ch06/ex5.csv')\\nIn [865]: result\\nOut[865]: \\n  something  a   b   c   d message\\n0       one  1   2   3   4     NaN\\n1       two  5   6 NaN   8   world\\n2     three  9  10  11  12     foo\\nIn [866]: pd.isnull(result)\\nOut[866]: \\n  something      a      b      c      d message\\n0     False  False  False  False  False    True\\n1     False  False  False   True  False   False\\n2     False  False  False  False  False   False\\nThe na_values option can take either a list or set of strings to consider missing values:\\nIn [867]: result = pd.read_csv('ch06/ex5.csv', na_values=['NULL'])\\nIn [868]: result\\nOut[868]: \\n  something  a   b   c   d message\\n0       one  1   2   3   4     NaN\\n1       two  5   6 NaN   8   world\\n2     three  9  10  11  12     foo\\nDifferent NA sentinels can be specified for each column in a dict:\\nIn [869]: sentinels = {'message': ['foo', 'NA'], 'something': ['two']}\\nIn [870]: pd.read_csv('ch06/ex5.csv', na_values=sentinels)\\nOut[870]: \\n  something  a   b   c   d message\\n0       one  1   2   3   4     NaN\\n1       NaN  5   6 NaN   8   world\\n2     three  9  10  11  12     NaN\\nReading and Writing Data in Text Format | 159\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 175, 'page_label': '160'}, page_content=\"Table 6-2. read_csv /read_table function arguments\\nArgument Description\\npath String indicating filesystem location, URL, or file-like object\\nsep or delimiter Character sequence or regular expression to use to split fields in each row\\nheader Row number to use as column names. Defaults to 0 (first row), but should be None if there is no header\\nrow\\nindex_col Column numbers or names to use as the row index in the result. Can be a single name/number or a list\\nof them for a hierarchical index\\nnames List of column names for result, combine with header=None\\nskiprows Number of rows at beginning of file to ignore or list of row numbers (starting from 0) to skip\\nna_values Sequence of values to replace with NA\\ncomment Character or characters to split comments off the end of lines\\nparse_dates Attempt to parse data to datetime; False by default. If True, will attempt to parse all columns. Otherwise\\ncan specify a list of column numbers or name to parse. If element of list is tuple or list, will combine\\nmultiple columns together and parse to date (for example if date/time split across two columns)\\nkeep_date_col If joining columns to parse date, drop the joined columns. Default True\\nconverters Dict containing column number of name mapping to functions. For example {'foo': f} would apply\\nthe function f to all values in the 'foo' column\\ndayfirst When parsing potentially ambiguous dates, treat as international format (e.g. 7/6/2012 -> June 7,\\n2012). Default False\\ndate_parser Function to use to parse dates\\nnrows Number of rows to read from beginning of file\\niterator Return a TextParser object for reading file piecemeal\\nchunksize For iteration, size of file chunks\\nskip_footer Number of lines to ignore at end of file\\nverbose Print various parser output information, like the number of missing values placed in non-numeric\\ncolumns\\nencoding Text encoding for unicode. For example 'utf-8' for UTF-8 encoded text\\nsqueeze If the parsed data only contains one column return a Series\\nthousands Separator for thousands, e.g. ',' or '.'\\nReading Text Files in Pieces\\nWhen processing very large files or figuring out the right set of arguments to correctly\\nprocess a large file, you may only want to read in a small piece of a file or iterate through\\nsmaller chunks of the file.\\nIn [871]: result = pd.read_csv('ch06/ex6.csv')\\nIn [872]: result\\nOut[872]: \\n160 | Chapter 6: \\u2002Data Loading, Storage, and File Formats\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 176, 'page_label': '161'}, page_content=\"<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 10000 entries, 0 to 9999\\nData columns:\\none      10000  non-null values\\ntwo      10000  non-null values\\nthree    10000  non-null values\\nfour     10000  non-null values\\nkey      10000  non-null values\\ndtypes: float64(4), object(1)\\nIf you want to only read out a small number of rows (avoiding reading the entire file),\\nspecify that with nrows:\\nIn [873]: pd.read_csv('ch06/ex6.csv', nrows=5)\\nOut[873]: \\n        one       two     three      four key\\n0  0.467976 -0.038649 -0.295344 -1.824726   L\\n1 -0.358893  1.404453  0.704965 -0.200638   B\\n2 -0.501840  0.659254 -0.421691 -0.057688   G\\n3  0.204886  1.074134  1.388361 -0.982404   R\\n4  0.354628 -0.133116  0.283763 -0.837063   Q\\nTo read out a file in pieces, specify a chunksize as a number of rows:\\nIn [874]: chunker = pd.read_csv('ch06/ex6.csv', chunksize=1000)\\nIn [875]: chunker\\nOut[875]: <pandas.io.parsers.TextParser at 0x8398150>\\nThe TextParser object returned by read_csv allows you to iterate over the parts of the\\nfile according to the chunksize. For example, we can iterate over ex6.csv, aggregating\\nthe value counts in the 'key' column like so:\\nchunker = pd.read_csv('ch06/ex6.csv', chunksize=1000)\\ntot = Series([])\\nfor piece in chunker:\\n    tot = tot.add(piece['key'].value_counts(), fill_value=0)\\ntot = tot.order(ascending=False)\\nWe have then:\\nIn [877]: tot[:10]\\nOut[877]: \\nE    368\\nX    364\\nL    346\\nO    343\\nQ    340\\nM    338\\nJ    337\\nF    335\\nK    334\\nH    330\\nReading and Writing Data in Text Format | 161\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 177, 'page_label': '162'}, page_content=\"TextParser is also equipped with a get_chunk method which enables you to read pieces\\nof an arbitrary size.\\nWriting Data Out to Text Format\\nData can also be exported to delimited format. Let’s consider one of the CSV files read\\nabove:\\nIn [878]: data = pd.read_csv('ch06/ex5.csv')\\nIn [879]: data\\nOut[879]: \\n  something  a   b   c   d message\\n0       one  1   2   3   4     NaN\\n1       two  5   6 NaN   8   world\\n2     three  9  10  11  12     foo\\nUsing DataFrame’s to_csv method, we can write the data out to a comma-separated file:\\nIn [880]: data.to_csv('ch06/out.csv')\\nIn [881]: !cat ch06/out.csv\\n,something,a,b,c,d,message\\n0,one,1,2,3.0,4,\\n1,two,5,6,,8,world\\n2,three,9,10,11.0,12,foo\\nOther delimiters can be used, of course (writing to sys.stdout so it just prints the text\\nresult):\\nIn [882]: data.to_csv(sys.stdout, sep='|')\\n|something|a|b|c|d|message\\n0|one|1|2|3.0|4|\\n1|two|5|6||8|world\\n2|three|9|10|11.0|12|foo\\nMissing values appear as empty strings in the output. You might want to denote them\\nby some other sentinel value:\\nIn [883]: data.to_csv(sys.stdout, na_rep='NULL')\\n,something,a,b,c,d,message\\n0,one,1,2,3.0,4,NULL\\n1,two,5,6,NULL,8,world\\n2,three,9,10,11.0,12,foo\\nWith no other options specified, both the row and column labels are written. Both of\\nthese can be disabled:\\nIn [884]: data.to_csv(sys.stdout, index=False, header=False)\\none,1,2,3.0,4,\\ntwo,5,6,,8,world\\nthree,9,10,11.0,12,foo\\nYou can also write only a subset of the columns, and in an order of your choosing:\\n162 | Chapter 6: \\u2002Data Loading, Storage, and File Formats\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 178, 'page_label': '163'}, page_content='In [885]: data.to_csv(sys.stdout, index=False, cols=[\\'a\\', \\'b\\', \\'c\\'])\\na,b,c\\n1,2,3.0\\n5,6,\\n9,10,11.0\\nSeries also has a to_csv method:\\nIn [886]: dates = pd.date_range(\\'1/1/2000\\', periods=7)\\nIn [887]: ts = Series(np.arange(7), index=dates)\\nIn [888]: ts.to_csv(\\'ch06/tseries.csv\\')\\nIn [889]: !cat ch06/tseries.csv\\n2000-01-01 00:00:00,0\\n2000-01-02 00:00:00,1\\n2000-01-03 00:00:00,2\\n2000-01-04 00:00:00,3\\n2000-01-05 00:00:00,4\\n2000-01-06 00:00:00,5\\n2000-01-07 00:00:00,6\\nWith a bit of wrangling (no header, first column as index), you can read a CSV version\\nof a Series with read_csv, but there is also a from_csv convenience method that makes\\nit a bit simpler:\\nIn [890]: Series.from_csv(\\'ch06/tseries.csv\\', parse_dates=True)\\nOut[890]: \\n2000-01-01    0\\n2000-01-02    1\\n2000-01-03    2\\n2000-01-04    3\\n2000-01-05    4\\n2000-01-06    5\\n2000-01-07    6\\nSee the docstrings for to_csv and from_csv in IPython for more information.\\nManually Working with Delimited Formats\\nMost forms of tabular data can be loaded from disk using functions like pan\\ndas.read_table. In some cases, however, some manual processing may be necessary.\\nIt’s not uncommon to receive a file with one or more malformed lines that trip up \\nread_table. To illustrate the basic tools, consider a small CSV file:\\nIn [891]: !cat ch06/ex7.csv\\n\"a\",\"b\",\"c\"\\n\"1\",\"2\",\"3\"\\n\"1\",\"2\",\"3\",\"4\"\\nFor any file with a single-character delimiter, you can use Python’s built-in csv module.\\nTo use it, pass any open file or file-like object to csv.reader:\\nReading and Writing Data in Text Format | 163\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 179, 'page_label': '164'}, page_content='import csv\\nf = open(\\'ch06/ex7.csv\\')\\nreader = csv.reader(f)\\nIterating through the reader like a file yields tuples of values in each like with any quote\\ncharacters removed:\\nIn [893]: for line in reader:\\n   .....:     print line\\n[\\'a\\', \\'b\\', \\'c\\']\\n[\\'1\\', \\'2\\', \\'3\\']\\n[\\'1\\', \\'2\\', \\'3\\', \\'4\\']\\nFrom there, it’s up to you to do the wrangling necessary to put the data in the form\\nthat you need it. For example:\\nIn [894]: lines = list(csv.reader(open(\\'ch06/ex7.csv\\')))\\nIn [895]: header, values = lines[0], lines[1:]\\nIn [896]: data_dict = {h: v for h, v in zip(header, zip(*values))}\\nIn [897]: data_dict\\nOut[897]: {\\'a\\': (\\'1\\', \\'1\\'), \\'b\\': (\\'2\\', \\'2\\'), \\'c\\': (\\'3\\', \\'3\\')}\\nCSV files come in many different flavors. Defining a new format with a different de-\\nlimiter, string quoting convention, or line terminator is done by defining a simple sub-\\nclass of csv.Dialect:\\nclass my_dialect(csv.Dialect):\\n    lineterminator = \\'\\\\n\\'\\n    delimiter = \\';\\'\\n    quotechar = \\'\"\\'\\nreader = csv.reader(f, dialect=my_dialect)\\nIndividual CSV dialect parameters can also be given as keywords to csv.reader without\\nhaving to define a subclass:\\nreader = csv.reader(f, delimiter=\\'|\\')\\nThe possible options (attributes of csv.Dialect) and what they do can be found in\\nTable 6-3.\\nTable 6-3. CSV dialect options\\nArgument Description\\ndelimiter One-character string to separate fields. Defaults to \\',\\'.\\nlineterminator Line terminator for writing, defaults to \\'\\\\r\\\\n\\'. Reader ignores this and recognizes\\ncross-platform line terminators.\\nquotechar Quote character for fields with special characters (like a delimiter). Default is \\'\"\\'.\\nquoting Quoting convention. Options include csv.QUOTE_ALL (quote all fields),\\ncsv.QUOTE_MINIMAL (only fields with special characters like the delimiter),\\n164 | Chapter 6: \\u2002Data Loading, Storage, and File Formats\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 180, 'page_label': '165'}, page_content='Argument Description\\ncsv.QUOTE_NONNUMERIC, and csv.QUOTE_NON (no quoting). See Python’s\\ndocumentation for full details. Defaults to QUOTE_MINIMAL.\\nskipinitialspace Ignore whitespace after each delimiter. Default False.\\ndoublequote How to handle quoting character inside a field. If True, it is doubled. See online\\ndocumentation for full detail and behavior.\\nescapechar String to escape the delimiter if quoting is set to csv.QUOTE_NONE. Disabled by\\ndefault\\nFor files with more complicated or fixed multicharacter delimiters, you\\nwill not be able to use the csv module. In those cases, you’ll have to do\\nthe line splitting and other cleanup using string’s split method or the\\nregular expression method re.split.\\nTo write delimited files manually, you can use csv.writer. It accepts an open, writable\\nfile object and the same dialect and format options as csv.reader:\\nwith open(\\'mydata.csv\\', \\'w\\') as f:\\n    writer = csv.writer(f, dialect=my_dialect)\\n    writer.writerow((\\'one\\', \\'two\\', \\'three\\'))\\n    writer.writerow((\\'1\\', \\'2\\', \\'3\\'))\\n    writer.writerow((\\'4\\', \\'5\\', \\'6\\'))\\n    writer.writerow((\\'7\\', \\'8\\', \\'9\\'))\\nJSON Data\\nJSON (short for JavaScript Object Notation) has become one of the standard formats\\nfor sending data by HTTP request between web browsers and other applications. It is\\na much more flexible data format than a tabular text form like CSV. Here is an example:\\nobj = \"\"\"\\n{\"name\": \"Wes\",\\n \"places_lived\": [\"United States\", \"Spain\", \"Germany\"],\\n \"pet\": null,\\n \"siblings\": [{\"name\": \"Scott\", \"age\": 25, \"pet\": \"Zuko\"},\\n              {\"name\": \"Katie\", \"age\": 33, \"pet\": \"Cisco\"}]\\n}\\n\"\"\"\\nJSON is very nearly valid Python code with the exception of its null value null and\\nsome other nuances (such as disallowing trailing commas at the end of lists). The basic\\ntypes are objects (dicts), arrays (lists), strings, numbers, booleans, and nulls. All of the\\nkeys in an object must be strings. There are several Python libraries for reading and\\nwriting JSON data. I’ll use json here as it is built into the Python standard library. To\\nconvert a JSON string to Python form, use json.loads:\\nIn [899]: import json\\nReading and Writing Data in Text Format | 165\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 181, 'page_label': '166'}, page_content=\"In [900]: result = json.loads(obj)\\nIn [901]: result\\nOut[901]: \\n{u'name': u'Wes',\\n u'pet': None,\\n u'places_lived': [u'United States', u'Spain', u'Germany'],\\n u'siblings': [{u'age': 25, u'name': u'Scott', u'pet': u'Zuko'},\\n  {u'age': 33, u'name': u'Katie', u'pet': u'Cisco'}]}\\njson.dumps on the other hand converts a Python object back to JSON:\\nIn [902]: asjson = json.dumps(result)\\nHow you convert a JSON object or list of objects to a DataFrame or some other data\\nstructure for analysis will be up to you. Conveniently, you can pass a list of JSON objects\\nto the DataFrame constructor and select a subset of the data fields:\\nIn [903]: siblings = DataFrame(result['siblings'], columns=['name', 'age'])\\nIn [904]: siblings\\nOut[904]: \\n    name  age\\n0  Scott   25\\n1  Katie   33\\nFor an extended example of reading and manipulating JSON data (including nested\\nrecords), see the USDA Food Database example in the next chapter.\\nAn effort is underway to add fast native JSON export ( to_json) and\\ndecoding (from_json) to pandas. This was not ready at the time of writ-\\ning.\\nXML and HTML: Web Scraping\\nPython has many libraries for reading and writing data in the ubiquitous HTML and\\nXML formats. lxml (http://lxml.de) is one that has consistently strong performance in\\nparsing very large files. lxml has multiple programmer interfaces; first I’ll show using \\nlxml.html for HTML, then parse some XML using lxml.objectify.\\nMany websites make data available in HTML tables for viewing in a browser, but not\\ndownloadable as an easily machine-readable format like JSON, HTML, or XML. I no-\\nticed that this was the case with Yahoo! Finance’s stock options data. If you aren’t\\nfamiliar with this data; options are derivative contracts giving you the right to buy\\n(call option) or sell ( put option) a company’s stock at some particular price (the\\nstrike) between now and some fixed point in the future (the expiry). People trade both\\ncall and put options across many strikes and expiries; this data can all be found together\\nin tables on Yahoo! Finance.\\n166 | Chapter 6: \\u2002Data Loading, Storage, and File Formats\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 182, 'page_label': '167'}, page_content=\"To get started, find the URL you want to extract data from, open it with urllib2 and\\nparse the stream with lxml like so:\\nfrom lxml.html import parse\\nfrom urllib2 import urlopen\\nparsed = parse(urlopen('http://finance.yahoo.com/q/op?s=AAPL+Options'))\\ndoc = parsed.getroot()\\nUsing this object, you can extract all HTML tags of a particular type, such as table tags\\ncontaining the data of interest. As a simple motivating example, suppose you wanted\\nto get a list of every URL linked to in the document; links are a tags in HTML. Using\\nthe document root’s findall method along with an XPath (a means of expressing\\n“queries” on the document):\\nIn [906]: links = doc.findall('.//a')\\nIn [907]: links[15:20]\\nOut[907]: \\n[<Element a at 0x6c488f0>,\\n <Element a at 0x6c48950>,\\n <Element a at 0x6c489b0>,\\n <Element a at 0x6c48a10>,\\n <Element a at 0x6c48a70>]\\nBut these are objects representing HTML elements; to get the URL and link text you\\nhave to use each element’s get method (for the URL) and text_content method (for\\nthe display text):\\nIn [908]: lnk = links[28]\\nIn [909]: lnk\\nOut[909]: <Element a at 0x6c48dd0>\\nIn [910]: lnk.get('href')\\nOut[910]: 'http://biz.yahoo.com/special.html'\\nIn [911]: lnk.text_content()\\nOut[911]: 'Special Editions'\\nThus, getting a list of all URLs in the document is a matter of writing this list compre-\\nhension:\\nIn [912]: urls = [lnk.get('href') for lnk in doc.findall('.//a')]\\nIn [913]: urls[-10:]\\nOut[913]: \\n['http://info.yahoo.com/privacy/us/yahoo/finance/details.html',\\n 'http://info.yahoo.com/relevantads/',\\n 'http://docs.yahoo.com/info/terms/',\\n 'http://docs.yahoo.com/info/copyright/copyright.html',\\n 'http://help.yahoo.com/l/us/yahoo/finance/forms_index.html',\\n 'http://help.yahoo.com/l/us/yahoo/finance/quotes/fitadelay.html',\\n 'http://help.yahoo.com/l/us/yahoo/finance/quotes/fitadelay.html',\\nReading and Writing Data in Text Format | 167\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 183, 'page_label': '168'}, page_content=\"'http://www.capitaliq.com',\\n 'http://www.csidata.com',\\n 'http://www.morningstar.com/']\\nNow, finding the right tables in the document can be a matter of trial and error; some\\nwebsites make it easier by giving a table of interest an id attribute. I determined that\\nthese were the two tables containing the call data and put data, respectively:\\ntables = doc.findall('.//table')\\ncalls = tables[9]\\nputs = tables[13]\\nEach table has a header row followed by each of the data rows:\\nIn [915]: rows = calls.findall('.//tr')\\nFor the header as well as the data rows, we want to extract the text from each cell; in\\nthe case of the header these are th cells and td cells for the data:\\ndef _unpack(row, kind='td'):\\n    elts = row.findall('.//%s' % kind)\\n    return [val.text_content() for val in elts]\\nThus, we obtain:\\nIn [917]: _unpack(rows[0], kind='th')\\nOut[917]: ['Strike', 'Symbol', 'Last', 'Chg', 'Bid', 'Ask', 'Vol', 'Open Int']\\nIn [918]: _unpack(rows[1], kind='td')\\nOut[918]: \\n['295.00',\\n 'AAPL120818C00295000',\\n '310.40',\\n ' 0.00',\\n '289.80',\\n '290.80',\\n '1',\\n '169']\\nNow, it’s a matter of combining all of these steps together to convert this data into a\\nDataFrame. Since the numerical data is still in string format, we want to convert some,\\nbut perhaps not all of the columns to floating point format. You could do this by hand,\\nbut, luckily, pandas has a class TextParser that is used internally in the read_csv and\\nother parsing functions to do the appropriate automatic type conversion:\\nfrom pandas.io.parsers import TextParser\\ndef parse_options_data(table):\\n    rows = table.findall('.//tr')\\n    header = _unpack(rows[0], kind='th')\\n    data = [_unpack(r) for r in rows[1:]]\\n    return TextParser(data, names=header).get_chunk()\\nFinally, we invoke this parsing function on the lxml table objects and get DataFrame\\nresults:\\n168 | Chapter 6: \\u2002Data Loading, Storage, and File Formats\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 184, 'page_label': '169'}, page_content='In [920]: call_data = parse_options_data(calls)\\nIn [921]: put_data = parse_options_data(puts)\\nIn [922]: call_data[:10]\\nOut[922]: \\n   Strike               Symbol    Last  Chg     Bid     Ask  Vol Open Int\\n0     295  AAPL120818C00295000  310.40  0.0  289.80  290.80    1      169\\n1     300  AAPL120818C00300000  277.10  1.7  284.80  285.60    2      478\\n2     305  AAPL120818C00305000  300.97  0.0  279.80  280.80   10      316\\n3     310  AAPL120818C00310000  267.05  0.0  274.80  275.65    6      239\\n4     315  AAPL120818C00315000  296.54  0.0  269.80  270.80   22       88\\n5     320  AAPL120818C00320000  291.63  0.0  264.80  265.80   96      173\\n6     325  AAPL120818C00325000  261.34  0.0  259.80  260.80  N/A      108\\n7     330  AAPL120818C00330000  230.25  0.0  254.80  255.80  N/A       21\\n8     335  AAPL120818C00335000  266.03  0.0  249.80  250.65    4       46\\n9     340  AAPL120818C00340000  272.58  0.0  244.80  245.80    4       30\\nParsing XML with lxml.objectify\\nXML (extensible markup language) is another common structured data format sup-\\nporting hierarchical, nested data with metadata. The files that generate the book you\\nare reading actually form a series of large XML documents.\\nAbove, I showed the lxml library and its lxml.html interface. Here I show an alternate\\ninterface that’s convenient for XML data, lxml.objectify.\\nThe New York Metropolitan Transportation Authority (MTA) publishes a number of\\ndata series about its bus and train services ( http://www.mta.info/developers/download\\n.html). Here we’ll look at the performance data which is contained in a set of XML files.\\nEach train or bus service has a different file (like Performance_MNR.xml for the Metro-\\nNorth Railroad) containing monthly data as a series of XML records that look like this:\\n<INDICATOR>\\n  <INDICATOR_SEQ>373889</INDICATOR_SEQ>\\n  <PARENT_SEQ></PARENT_SEQ>\\n  <AGENCY_NAME>Metro-North Railroad</AGENCY_NAME>\\n  <INDICATOR_NAME>Escalator Availability</INDICATOR_NAME>\\n  <DESCRIPTION>Percent of the time that escalators are operational\\n  systemwide. The availability rate is based on physical observations performed\\n  the morning of regular business days only. This is a new indicator the agency\\n  began reporting in 2009.</DESCRIPTION>\\n  <PERIOD_YEAR>2011</PERIOD_YEAR>\\n  <PERIOD_MONTH>12</PERIOD_MONTH>\\n  <CATEGORY>Service Indicators</CATEGORY>\\n  <FREQUENCY>M</FREQUENCY>\\n  <DESIRED_CHANGE>U</DESIRED_CHANGE>\\n  <INDICATOR_UNIT>%</INDICATOR_UNIT>\\n  <DECIMAL_PLACES>1</DECIMAL_PLACES>\\n  <YTD_TARGET>97.00</YTD_TARGET>\\n  <YTD_ACTUAL></YTD_ACTUAL>\\n  <MONTHLY_TARGET>97.00</MONTHLY_TARGET>\\n  <MONTHLY_ACTUAL></MONTHLY_ACTUAL>\\n</INDICATOR>\\nReading and Writing Data in Text Format | 169\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 185, 'page_label': '170'}, page_content='Using lxml.objectify, we parse the file and get a reference to the root node of the XML\\nfile with getroot:\\nfrom lxml import objectify\\npath = \\'Performance_MNR.xml\\'\\nparsed = objectify.parse(open(path))\\nroot = parsed.getroot()\\nroot.INDICATOR return a generator yielding each <INDICATOR> XML element. For each\\nrecord, we can populate a dict of tag names (like YTD_ACTUAL) to data values (excluding\\na few tags):\\ndata = []\\nskip_fields = [\\'PARENT_SEQ\\', \\'INDICATOR_SEQ\\',\\n               \\'DESIRED_CHANGE\\', \\'DECIMAL_PLACES\\']\\nfor elt in root.INDICATOR:\\n    el_data = {}\\n    for child in elt.getchildren():\\n        if child.tag in skip_fields:\\n            continue\\n        el_data[child.tag] = child.pyval\\n    data.append(el_data)\\nLastly, convert this list of dicts into a DataFrame:\\nIn [927]: perf = DataFrame(data)\\nIn [928]: perf\\nOut[928]: \\nEmpty DataFrame\\nColumns: array([], dtype=int64)\\nIndex: array([], dtype=int64)\\nXML data can get much more complicated than this example. Each tag can have met-\\nadata, too. Consider an HTML link tag which is also valid XML:\\nfrom StringIO import StringIO\\ntag = \\'<a href=\"http://www.google.com\">Google</a>\\'\\nroot = objectify.parse(StringIO(tag)).getroot()\\nYou can now access any of the fields (like href) in the tag or the link text:\\nIn [930]: root\\nOut[930]: <Element a at 0x88bd4b0>\\nIn [931]: root.get(\\'href\\')\\nOut[931]: \\'http://www.google.com\\'\\nIn [932]: root.text\\nOut[932]: \\'Google\\'\\n170 | Chapter 6: \\u2002Data Loading, Storage, and File Formats\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 186, 'page_label': '171'}, page_content=\"Binary Data Formats\\nOne of the easiest ways to store data efficiently in binary format is using Python’s built-\\nin pickle serialization. Conveniently, pandas objects all have a save method which\\nwrites the data to disk as a pickle:\\nIn [933]: frame = pd.read_csv('ch06/ex1.csv')\\nIn [934]: frame\\nOut[934]: \\n   a   b   c   d message\\n0  1   2   3   4   hello\\n1  5   6   7   8   world\\n2  9  10  11  12     foo\\nIn [935]: frame.save('ch06/frame_pickle')\\nYou read the data back into Python with pandas.load, another pickle convenience\\nfunction:\\nIn [936]: pd.load('ch06/frame_pickle')\\nOut[936]: \\n   a   b   c   d message\\n0  1   2   3   4   hello\\n1  5   6   7   8   world\\n2  9  10  11  12     foo\\npickle is only recommended as a short-term storage format. The prob-\\nlem is that it is hard to guarantee that the format will be stable over time;\\nan object pickled today may not unpickle with a later version of a library.\\nI have made every effort to ensure that this does not occur with pandas,\\nbut at some point in the future it may be necessary to “break” the pickle\\nformat.\\nUsing HDF5 Format\\nThere are a number of tools that facilitate efficiently reading and writing large amounts\\nof scientific data in binary format on disk. A popular industry-grade library for this is\\nHDF5, which is a C library with interfaces in many other languages like Java, Python,\\nand MATLAB. The “HDF” in HDF5 stands for hierarchical data format. Each HDF5\\nfile contains an internal file system-like node structure enabling you to store multiple\\ndatasets and supporting metadata. Compared with simpler formats, HDF5 supports\\non-the-fly compression with a variety of compressors, enabling data with repeated pat-\\nterns to be stored more efficiently. For very large datasets that don’t fit into memory,\\nHDF5 is a good choice as you can efficiently read and write small sections of much\\nlarger arrays.\\nThere are not one but two interfaces to the HDF5 library in Python, PyTables and h5py,\\neach of which takes a different approach to the problem. h5py provides a direct, but\\nhigh-level interface to the HDF5 API, while PyTables abstracts many of the details of\\nBinary Data Formats | 171\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 187, 'page_label': '172'}, page_content=\"HDF5 to provide multiple flexible data containers, table indexing, querying capability,\\nand some support for out-of-core computations.\\npandas has a minimal dict-like HDFStore class, which uses PyTables to store pandas\\nobjects:\\nIn [937]: store = pd.HDFStore('mydata.h5')\\nIn [938]: store['obj1'] = frame\\nIn [939]: store['obj1_col'] = frame['a']\\nIn [940]: store\\nOut[940]: \\n<class 'pandas.io.pytables.HDFStore'>\\nFile path: mydata.h5\\nobj1         DataFrame\\nobj1_col     Series\\nObjects contained in the HDF5 file can be retrieved in a dict-like fashion:\\nIn [941]: store['obj1']\\nOut[941]: \\n   a   b   c   d message\\n0  1   2   3   4   hello\\n1  5   6   7   8   world\\n2  9  10  11  12     foo\\nIf you work with huge quantities of data, I would encourage you to explore PyTables\\nand h5py to see how they can suit your needs. Since many data analysis problems are\\nIO-bound (rather than CPU-bound), using a tool like HDF5 can massively accelerate\\nyour applications.\\nHDF5 is not a database. It is best suited for write-once, read-many da-\\ntasets. While data can be added to a file at any time, if multiple writers\\ndo so simultaneously, the file can become corrupted.\\nReading Microsoft Excel Files\\npandas also supports reading tabular data stored in Excel 2003 (and higher) files using\\nthe ExcelFile class. Interally ExcelFile uses the xlrd and openpyxl packages, so you\\nmay have to install them first. To use ExcelFile, create an instance by passing a path\\nto an xls or xlsx file:\\nxls_file = pd.ExcelFile('data.xls')\\nData stored in a sheet can then be read into DataFrame using parse:\\ntable = xls_file.parse('Sheet1')\\n172 | Chapter 6: \\u2002Data Loading, Storage, and File Formats\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 188, 'page_label': '173'}, page_content='Interacting with HTML and Web APIs\\nMany websites have public APIs providing data feeds via JSON or some other format.\\nThere are a number of ways to access these APIs from Python; one easy-to-use method\\nthat I recommend is the requests package ( http://docs.python-requests.org). To search\\nfor the words “python pandas” on Twitter, we can make an HTTP GET request like so:\\nIn [944]: import requests\\nIn [945]: url = \\'http://search.twitter.com/search.json?q=python%20pandas\\'\\nIn [946]: resp = requests.get(url)\\nIn [947]: resp\\nOut[947]: <Response [200]>\\nThe Response object’s text attribute contains the content of the GET query. Many web\\nAPIs will return a JSON string that must be loaded into a Python object:\\nIn [948]: import json\\nIn [949]: data = json.loads(resp.text)\\nIn [950]: data.keys()\\nOut[950]: \\n[u\\'next_page\\',\\n u\\'completed_in\\',\\n u\\'max_id_str\\',\\n u\\'since_id_str\\',\\n u\\'refresh_url\\',\\n u\\'results\\',\\n u\\'since_id\\',\\n u\\'results_per_page\\',\\n u\\'query\\',\\n u\\'max_id\\',\\n u\\'page\\']\\nThe results field in the response contains a list of tweets, each of which is represented\\nas a Python dict that looks like:\\n{u\\'created_at\\': u\\'Mon, 25 Jun 2012 17:50:33 +0000\\',\\n u\\'from_user\\': u\\'wesmckinn\\',\\n u\\'from_user_id\\': 115494880,\\n u\\'from_user_id_str\\': u\\'115494880\\',\\n u\\'from_user_name\\': u\\'Wes McKinney\\',\\n u\\'geo\\': None,\\n u\\'id\\': 217313849177686018,\\n u\\'id_str\\': u\\'217313849177686018\\',\\n u\\'iso_language_code\\': u\\'pt\\',\\n u\\'metadata\\': {u\\'result_type\\': u\\'recent\\'},\\n u\\'source\\': u\\'<a href=\"http://twitter.com/\">web</a>\\',\\n u\\'text\\': u\\'Lunchtime pandas-fu http://t.co/SI70xZZQ #pydata\\',\\n u\\'to_user\\': None,\\n u\\'to_user_id\\': 0,\\nInteracting with HTML and Web APIs | 173\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 189, 'page_label': '174'}, page_content='u\\'to_user_id_str\\': u\\'0\\',\\n u\\'to_user_name\\': None}\\nWe can then make a list of the tweet fields of interest then pass the results list to Da-\\ntaFrame:\\nIn [951]: tweet_fields = [\\'created_at\\', \\'from_user\\', \\'id\\', \\'text\\']\\nIn [952]: tweets = DataFrame(data[\\'results\\'], columns=tweet_fields)\\nIn [953]: tweets\\nOut[953]: \\n<class \\'pandas.core.frame.DataFrame\\'>\\nInt64Index: 15 entries, 0 to 14\\nData columns:\\ncreated_at    15  non-null values\\nfrom_user     15  non-null values\\nid            15  non-null values\\ntext          15  non-null values\\ndtypes: int64(1), object(3)\\nEach row in the DataFrame now has the extracted data from each tweet:\\nIn [121]: tweets.ix[7]\\nOut[121]:\\ncreated_at                  Thu, 23 Jul 2012 09:54:00 +0000\\nfrom_user                                           deblike\\nid                                       227419585803059201\\ntext          pandas: powerful Python data analysis toolkit\\nName: 7\\nWith a bit of elbow grease, you can create some higher-level interfaces to common web\\nAPIs that return DataFrame objects for easy analysis.\\nInteracting with Databases\\nIn many applications data rarely comes from text files, that being a fairly inefficient\\nway to store large amounts of data. SQL-based relational databases (such as SQL Server,\\nPostgreSQL, and MySQL) are in wide use, and many alternative non-SQL (so-called\\nNoSQL) databases have become quite popular. The choice of database is usually de-\\npendent on the performance, data integrity, and scalability needs of an application.\\nLoading data from SQL into a DataFrame is fairly straightforward, and pandas has\\nsome functions to simplify the process. As an example, I’ll use an in-memory SQLite\\ndatabase using Python’s built-in sqlite3 driver:\\nimport sqlite3\\nquery = \"\"\"\\nCREATE TABLE test\\n(a VARCHAR(20), b VARCHAR(20),\\n c REAL,        d INTEGER\\n);\"\"\"\\n174 | Chapter 6: \\u2002Data Loading, Storage, and File Formats\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 190, 'page_label': '175'}, page_content='con = sqlite3.connect(\\':memory:\\')\\ncon.execute(query)\\ncon.commit()\\nThen, insert a few rows of data:\\ndata = [(\\'Atlanta\\', \\'Georgia\\', 1.25, 6),\\n        (\\'Tallahassee\\', \\'Florida\\', 2.6, 3),\\n        (\\'Sacramento\\', \\'California\\', 1.7, 5)]\\nstmt = \"INSERT INTO test VALUES(?, ?, ?, ?)\"\\ncon.executemany(stmt, data)\\ncon.commit()\\nMost Python SQL drivers (PyODBC, psycopg2, MySQLdb, pymssql, etc.) return a list\\nof tuples when selecting data from a table:\\nIn [956]: cursor = con.execute(\\'select * from test\\')\\nIn [957]: rows = cursor.fetchall()\\nIn [958]: rows\\nOut[958]: \\n[(u\\'Atlanta\\', u\\'Georgia\\', 1.25, 6),\\n (u\\'Tallahassee\\', u\\'Florida\\', 2.6, 3),\\n (u\\'Sacramento\\', u\\'California\\', 1.7, 5)]\\nYou can pass the list of tuples to the DataFrame constructor, but you also need the\\ncolumn names, contained in the cursor’s description attribute:\\nIn [959]: cursor.description\\nOut[959]: \\n((\\'a\\', None, None, None, None, None, None),\\n (\\'b\\', None, None, None, None, None, None),\\n (\\'c\\', None, None, None, None, None, None),\\n (\\'d\\', None, None, None, None, None, None))\\nIn [960]: DataFrame(rows, columns=zip(*cursor.description)[0])\\nOut[960]: \\n             a           b     c  d\\n0      Atlanta     Georgia  1.25  6\\n1  Tallahassee     Florida  2.60  3\\n2   Sacramento  California  1.70  5\\nThis is quite a bit of munging that you’d rather not repeat each time you query the\\ndatabase. pandas has a read_frame function in its pandas.io.sql module that simplifies\\nthe process. Just pass the select statement and the connection object:\\nIn [961]: import pandas.io.sql as sql\\nIn [962]: sql.read_frame(\\'select * from test\\', con)\\nOut[962]: \\n             a           b     c  d\\n0      Atlanta     Georgia  1.25  6\\n1  Tallahassee     Florida  2.60  3\\n2   Sacramento  California  1.70  5\\nInteracting with Databases | 175\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 191, 'page_label': '176'}, page_content=\"Storing and Loading Data in MongoDB\\nNoSQL databases take many different forms. Some are simple dict-like key-value stores\\nlike BerkeleyDB or Tokyo Cabinet, while others are document-based, with a dict-like\\nobject being the basic unit of storage. I've chosen MongoDB ( http://mongodb.org) for\\nmy example. I started a MongoDB instance locally on my machine, and connect to it\\non the default port using pymongo, the official driver for MongoDB:\\nimport pymongo\\ncon = pymongo.Connection('localhost', port=27017)\\nDocuments stored in MongoDB are found in collections inside databases. Each running\\ninstance of the MongoDB server can have multiple databases, and each database can\\nhave multiple collections. Suppose I wanted to store the Twitter API data from earlier\\nin the chapter. First, I can access the (currently empty) tweets collection:\\ntweets = con.db.tweets\\nThen, I load the list of tweets and write each of them to the collection using\\ntweets.save (which writes the Python dict to MongoDB):\\nimport requests, json\\nurl = 'http://search.twitter.com/search.json?q=python%20pandas'\\ndata = json.loads(requests.get(url).text)\\nfor tweet in data['results']:\\n    tweets.save(tweet)\\nNow, if I wanted to get all of my tweets (if any) from the collection, I can query the\\ncollection with the following syntax:\\ncursor = tweets.find({'from_user': 'wesmckinn'})\\nThe cursor returned is an iterator that yields each document as a dict. As above I can\\nconvert this into a DataFrame, optionally extracting a subset of the data fields in each\\ntweet:\\ntweet_fields = ['created_at', 'from_user', 'id', 'text']\\nresult = DataFrame(list(cursor), columns=tweet_fields)\\n176 | Chapter 6: \\u2002Data Loading, Storage, and File Formats\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 192, 'page_label': '177'}, page_content='CHAPTER 7\\nData Wrangling: Clean, Transform,\\nMerge, Reshape\\nMuch of the programming work in data analysis and modeling is spent on data prep-\\naration: loading, cleaning, transforming, and rearranging. Sometimes the way that data\\nis stored in files or databases is not the way you need it for a data processing application.\\nMany people choose to do ad hoc processing of data from one form to another using\\na general purpose programming, like Python, Perl, R, or Java, or UNIX text processing\\ntools like sed or awk. Fortunately, pandas along with the Python standard library pro-\\nvide you with a high-level, flexible, and high-performance set of core manipulations\\nand algorithms to enable you to wrangle data into the right form without much trouble.\\nIf you identify a type of data manipulation that isn’t anywhere in this book or elsewhere\\nin the pandas library, feel free to suggest it on the mailing list or GitHub site. Indeed,\\nmuch of the design and implementation of pandas has been driven by the needs of real\\nworld applications.\\nCombining and Merging Data Sets\\nData contained in pandas objects can be combined together in a number of built-in\\nways:\\n• pandas.merge connects rows in DataFrames based on one or more keys. This will\\nbe familiar to users of SQL or other relational databases, as it implements database\\njoin operations.\\n• pandas.concat glues or stacks together objects along an axis.\\n• combine_first instance method enables splicing together overlapping data to fill\\nin missing values in one object with values from another.\\nI will address each of these and give a number of examples. They’ll be utilized in ex-\\namples throughout the rest of the book.\\n177\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 193, 'page_label': '178'}, page_content=\"Database-style DataFrame Merges\\nMerge or join operations combine data sets by linking rows using one or more keys.\\nThese operations are central to relational databases. The merge function in pandas is\\nthe main entry point for using these algorithms on your data.\\nLet’s start with a simple example:\\nIn [15]: df1 = DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'a', 'b'],\\n   ....:                  'data1': range(7)})\\nIn [16]: df2 = DataFrame({'key': ['a', 'b', 'd'],\\n   ....:                  'data2': range(3)})\\nIn [17]: df1        In [18]: df2\\nOut[17]:            Out[18]:\\n   data1 key           data2 key\\n0      0   b        0      0   a\\n1      1   b        1      1   b\\n2      2   a        2      2   d\\n3      3   c\\n4      4   a\\n5      5   a\\n6      6   b\\nThis is an example of a many-to-one merge situation; the data in df1 has multiple rows\\nlabeled a and b, whereas df2 has only one row for each value in the key column. Calling\\nmerge with these objects we obtain:\\nIn [19]: pd.merge(df1, df2)\\nOut[19]:\\n   data1 key  data2\\n0      2   a      0\\n1      4   a      0\\n2      5   a      0\\n3      0   b      1\\n4      1   b      1\\n5      6   b      1\\nNote that I didn’t specify which column to join on. If not specified, merge uses the\\noverlapping column names as the keys. It’s a good practice to specify explicitly, though:\\nIn [20]: pd.merge(df1, df2, on='key')\\nOut[20]:\\n   data1 key  data2\\n0      2   a      0\\n1      4   a      0\\n2      5   a      0\\n3      0   b      1\\n4      1   b      1\\n5      6   b      1\\nIf the column names are different in each object, you can specify them separately:\\nIn [21]: df3 = DataFrame({'lkey': ['b', 'b', 'a', 'c', 'a', 'a', 'b'],\\n   ....:                  'data1': range(7)})\\n178 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 194, 'page_label': '179'}, page_content=\"In [22]: df4 = DataFrame({'rkey': ['a', 'b', 'd'],\\n   ....:                  'data2': range(3)})\\nIn [23]: pd.merge(df3, df4, left_on='lkey', right_on='rkey')\\nOut[23]:\\n   data1 lkey  data2 rkey\\n0      2    a      0    a\\n1      4    a      0    a\\n2      5    a      0    a\\n3      0    b      1    b\\n4      1    b      1    b\\n5      6    b      1    b\\nYou probably noticed that the 'c' and 'd' values and associated data are missing from\\nthe result. By default merge does an 'inner' join; the keys in the result are the intersec-\\ntion. Other possible options are 'left', 'right', and 'outer'. The outer join takes the\\nunion of the keys, combining the effect of applying both left and right joins:\\nIn [24]: pd.merge(df1, df2, how='outer')\\nOut[24]:\\n   data1 key  data2\\n0      2   a      0\\n1      4   a      0\\n2      5   a      0\\n3      0   b      1\\n4      1   b      1\\n5      6   b      1\\n6      3   c    NaN\\n7    NaN   d      2\\nMany-to-many merges have well-defined though not necessarily intuitive behavior.\\nHere’s an example:\\nIn [25]: df1 = DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'b'],\\n   ....:                  'data1': range(6)})\\nIn [26]: df2 = DataFrame({'key': ['a', 'b', 'a', 'b', 'd'],\\n   ....:                  'data2': range(5)})\\nIn [27]: df1        In [28]: df2\\nOut[27]:            Out[28]:\\n   data1 key           data2 key\\n0      0   b        0      0   a\\n1      1   b        1      1   b\\n2      2   a        2      2   a\\n3      3   c        3      3   b\\n4      4   a        4      4   d\\n5      5   b\\nIn [29]: pd.merge(df1, df2, on='key', how='left')\\nOut[29]:\\n    data1 key  data2\\n0       2   a      0\\n1       2   a      2\\nCombining and Merging Data Sets | 179\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 195, 'page_label': '180'}, page_content=\"2       4   a      0\\n3       4   a      2\\n4       0   b      1\\n5       0   b      3\\n6       1   b      1\\n7       1   b      3\\n8       5   b      1\\n9       5   b      3\\n10      3   c    NaN\\nMany-to-many joins form the Cartesian product of the rows. Since there were 3 'b'\\nrows in the left DataFrame and 2 in the right one, there are 6 'b' rows in the result.\\nThe join method only affects the distinct key values appearing in the result:\\nIn [30]: pd.merge(df1, df2, how='inner')\\nOut[30]:\\n   data1 key  data2\\n0      2   a      0\\n1      2   a      2\\n2      4   a      0\\n3      4   a      2\\n4      0   b      1\\n5      0   b      3\\n6      1   b      1\\n7      1   b      3\\n8      5   b      1\\n9      5   b      3\\nTo merge with multiple keys, pass a list of column names:\\nIn [31]: left = DataFrame({'key1': ['foo', 'foo', 'bar'],\\n   ....:                   'key2': ['one', 'two', 'one'],\\n   ....:                   'lval': [1, 2, 3]})\\nIn [32]: right = DataFrame({'key1': ['foo', 'foo', 'bar', 'bar'],\\n   ....:                    'key2': ['one', 'one', 'one', 'two'],\\n   ....:                    'rval': [4, 5, 6, 7]})\\nIn [33]: pd.merge(left, right, on=['key1', 'key2'], how='outer')\\nOut[33]:\\n  key1 key2  lval  rval\\n0  bar  one     3     6\\n1  bar  two   NaN     7\\n2  foo  one     1     4\\n3  foo  one     1     5\\n4  foo  two     2   NaN\\nTo determine which key combinations will appear in the result depending on the choice\\nof merge method, think of the multiple keys as forming an array of tuples to be used\\nas a single join key (even though it’s not actually implemented that way).\\nWhen joining columns-on-columns, the indexes on the passed Data-\\nFrame objects are discarded.\\n180 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 196, 'page_label': '181'}, page_content=\"A last issue to consider in merge operations is the treatment of overlapping column\\nnames. While you can address the overlap manually (see the later section on renaming\\naxis labels), merge has a suffixes option for specifying strings to append to overlapping\\nnames in the left and right DataFrame objects:\\nIn [34]: pd.merge(left, right, on='key1')\\nOut[34]:\\n  key1 key2_x  lval key2_y  rval\\n0  bar    one     3    one     6\\n1  bar    one     3    two     7\\n2  foo    one     1    one     4\\n3  foo    one     1    one     5\\n4  foo    two     2    one     4\\n5  foo    two     2    one     5\\nIn [35]: pd.merge(left, right, on='key1', suffixes=('_left', '_right'))\\nOut[35]:\\n  key1 key2_left  lval key2_right  rval\\n0  bar       one     3        one     6\\n1  bar       one     3        two     7\\n2  foo       one     1        one     4\\n3  foo       one     1        one     5\\n4  foo       two     2        one     4\\n5  foo       two     2        one     5\\nSee Table 7-1 for an argument reference on merge. Joining on index is the subject of the\\nnext section.\\nTable 7-1. merge function arguments\\nArgument Description\\nleft DataFrame to be merged on the left side\\nright DataFrame to be merged on the right side\\nhow One of 'inner', 'outer', 'left' or 'right'. 'inner' by default\\non Column names to join on. Must be found in both DataFrame objects. If not specified and no other join keys\\ngiven, will use the intersection of the column names in left and right as the join keys\\nleft_on Columns in left DataFrame to use as join keys\\nright_on Analogous to left_on for left DataFrame\\nleft_index Use row index in left as its join key (or keys, if a MultiIndex)\\nright_index Analogous to left_index\\nsort Sort merged data lexicographically by join keys; True by default. Disable to get better performance in some\\ncases on large datasets\\nsuffixes Tuple of string values to append to column names in case of overlap; defaults to ('_x', '_y'). For\\nexample, if 'data' in both DataFrame objects, would appear as 'data_x' and 'data_y' in result\\ncopy If False, avoid copying data into resulting data structure in some exceptional cases. By default always copies\\nCombining and Merging Data Sets | 181\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 197, 'page_label': '182'}, page_content=\"Merging on Index\\nIn some cases, the merge key or keys in a DataFrame will be found in its index. In this\\ncase, you can pass left_index=True or right_index=True (or both) to indicate that the\\nindex should be used as the merge key:\\nIn [36]: left1 = DataFrame({'key': ['a', 'b', 'a', 'a', 'b', 'c'],\\n   ....:                   'value': range(6)})\\nIn [37]: right1 = DataFrame({'group_val': [3.5, 7]}, index=['a', 'b'])\\nIn [38]: left1        In [39]: right1\\nOut[38]:              Out[39]:\\n  key  value             group_val\\n0   a      0          a        3.5\\n1   b      1          b        7.0\\n2   a      2\\n3   a      3\\n4   b      4\\n5   c      5\\nIn [40]: pd.merge(left1, right1, left_on='key', right_index=True)\\nOut[40]:\\n  key  value  group_val\\n0   a      0        3.5\\n2   a      2        3.5\\n3   a      3        3.5\\n1   b      1        7.0\\n4   b      4        7.0\\nSince the default merge method is to intersect the join keys, you can instead form the\\nunion of them with an outer join:\\nIn [41]: pd.merge(left1, right1, left_on='key', right_index=True, how='outer')\\nOut[41]:\\n  key  value  group_val\\n0   a      0        3.5\\n2   a      2        3.5\\n3   a      3        3.5\\n1   b      1        7.0\\n4   b      4        7.0\\n5   c      5        NaN\\nWith hierarchically-indexed data, things are a bit more complicated:\\nIn [42]: lefth = DataFrame({'key1': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'],\\n   ....:                    'key2': [2000, 2001, 2002, 2001, 2002],\\n   ....:                    'data': np.arange(5.)})\\nIn [43]: righth = DataFrame(np.arange(12).reshape((6, 2)),\\n   ....:                    index=[['Nevada', 'Nevada', 'Ohio', 'Ohio', 'Ohio', 'Ohio'],\\n   ....:                           [2001, 2000, 2000, 2000, 2001, 2002]],\\n   ....:                    columns=['event1', 'event2'])\\nIn [44]: lefth               In [45]: righth\\nOut[44]:                     Out[45]:\\n182 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 198, 'page_label': '183'}, page_content=\"data    key1  key2                     event1  event2\\n0     0    Ohio  2000        Nevada 2001       0       1\\n1     1    Ohio  2001               2000       2       3\\n2     2    Ohio  2002        Ohio   2000       4       5\\n3     3  Nevada  2001               2000       6       7\\n4     4  Nevada  2002               2001       8       9\\n                                    2002      10      11\\nIn this case, you have to indicate multiple columns to merge on as a list (pay attention\\nto the handling of duplicate index values):\\nIn [46]: pd.merge(lefth, righth, left_on=['key1', 'key2'], right_index=True)\\nOut[46]:\\n   data    key1  key2  event1  event2\\n3     3  Nevada  2001       0       1\\n0     0    Ohio  2000       4       5\\n0     0    Ohio  2000       6       7\\n1     1    Ohio  2001       8       9\\n2     2    Ohio  2002      10      11\\nIn [47]: pd.merge(lefth, righth, left_on=['key1', 'key2'],\\n   ....:          right_index=True, how='outer')\\nOut[47]:\\n   data    key1  key2  event1  event2\\n4   NaN  Nevada  2000       2       3\\n3     3  Nevada  2001       0       1\\n4     4  Nevada  2002     NaN     NaN\\n0     0    Ohio  2000       4       5\\n0     0    Ohio  2000       6       7\\n1     1    Ohio  2001       8       9\\n2     2    Ohio  2002      10      11\\nUsing the indexes of both sides of the merge is also not an issue:\\nIn [48]: left2 = DataFrame([[1., 2.], [3., 4.], [5., 6.]], index=['a', 'c', 'e'],\\n   ....:                  columns=['Ohio', 'Nevada'])\\nIn [49]: right2 = DataFrame([[7., 8.], [9., 10.], [11., 12.], [13, 14]],\\n   ....:                    index=['b', 'c', 'd', 'e'], columns=['Missouri', 'Alabama'])\\nIn [50]: left2         In [51]: right2\\nOut[50]:               Out[51]:\\n   Ohio  Nevada           Missouri  Alabama\\na     1       2        b         7        8\\nc     3       4        c         9       10\\ne     5       6        d        11       12\\n                       e        13       14\\nIn [52]: pd.merge(left2, right2, how='outer', left_index=True, right_index=True)\\nOut[52]:\\n   Ohio  Nevada  Missouri  Alabama\\na     1       2       NaN      NaN\\nb   NaN     NaN         7        8\\nc     3       4         9       10\\nd   NaN     NaN        11       12\\ne     5       6        13       14\\nCombining and Merging Data Sets | 183\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 199, 'page_label': '184'}, page_content=\"DataFrame has a more convenient join instance for merging by index. It can also be\\nused to combine together many DataFrame objects having the same or similar indexes\\nbut non-overlapping columns. In the prior example, we could have written:\\nIn [53]: left2.join(right2, how='outer')\\nOut[53]:\\n   Ohio  Nevada  Missouri  Alabama\\na     1       2       NaN      NaN\\nb   NaN     NaN         7        8\\nc     3       4         9       10\\nd   NaN     NaN        11       12\\ne     5       6        13       14\\nIn part for legacy reasons (much earlier versions of pandas), DataFrame’s join method\\nperforms a left join on the join keys. It also supports joining the index of the passed\\nDataFrame on one of the columns of the calling DataFrame:\\nIn [54]: left1.join(right1, on='key')\\nOut[54]:\\n  key  value  group_val\\n0   a      0        3.5\\n1   b      1        7.0\\n2   a      2        3.5\\n3   a      3        3.5\\n4   b      4        7.0\\n5   c      5        NaN\\nLastly, for simple index-on-index merges, you can pass a list of DataFrames to join as\\nan alternative to using the more general concat function described below:\\nIn [55]: another = DataFrame([[7., 8.], [9., 10.], [11., 12.], [16., 17.]],\\n   ....:                     index=['a', 'c', 'e', 'f'], columns=['New York', 'Oregon'])\\nIn [56]: left2.join([right2, another])\\nOut[56]:\\n   Ohio  Nevada  Missouri  Alabama  New York  Oregon\\na     1       2       NaN      NaN         7       8\\nc     3       4         9       10         9      10\\ne     5       6        13       14        11      12\\nIn [57]: left2.join([right2, another], how='outer')\\nOut[57]:\\n   Ohio  Nevada  Missouri  Alabama  New York  Oregon\\na     1       2       NaN      NaN         7       8\\nb   NaN     NaN         7        8       NaN     NaN\\nc     3       4         9       10         9      10\\nd   NaN     NaN        11       12       NaN     NaN\\ne     5       6        13       14        11      12\\nf   NaN     NaN       NaN      NaN        16      17\\n184 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 200, 'page_label': '185'}, page_content=\"Concatenating Along an Axis\\nAnother kind of data combination operation is alternatively referred to as concatena-\\ntion, binding, or stacking. NumPy has a concatenate function for doing this with raw\\nNumPy arrays:\\nIn [58]: arr = np.arange(12).reshape((3, 4))\\nIn [59]: arr\\nOut[59]:\\narray([[ 0,  1,  2,  3],\\n       [ 4,  5,  6,  7],\\n       [ 8,  9, 10, 11]])\\nIn [60]: np.concatenate([arr, arr], axis=1)\\nOut[60]:\\narray([[ 0,  1,  2,  3,  0,  1,  2,  3],\\n       [ 4,  5,  6,  7,  4,  5,  6,  7],\\n       [ 8,  9, 10, 11,  8,  9, 10, 11]])\\nIn the context of pandas objects such as Series and DataFrame, having labeled axes\\nenable you to further generalize array concatenation. In particular, you have a number\\nof additional things to think about:\\n• If the objects are indexed differently on the other axes, should the collection of\\naxes be unioned or intersected?\\n• Do the groups need to be identifiable in the resulting object?\\n• Does the concatenation axis matter at all?\\nThe concat function in pandas provides a consistent way to address each of these con-\\ncerns. I’ll give a number of examples to illustrate how it works. Suppose we have three\\nSeries with no index overlap:\\nIn [61]: s1 = Series([0, 1], index=['a', 'b'])\\nIn [62]: s2 = Series([2, 3, 4], index=['c', 'd', 'e'])\\nIn [63]: s3 = Series([5, 6], index=['f', 'g'])\\nCalling concat with these object in a list glues together the values and indexes:\\nIn [64]: pd.concat([s1, s2, s3])\\nOut[64]:\\na    0\\nb    1\\nc    2\\nd    3\\ne    4\\nf    5\\ng    6\\nCombining and Merging Data Sets | 185\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 201, 'page_label': '186'}, page_content=\"By default concat works along axis=0, producing another Series. If you pass axis=1, the\\nresult will instead be a DataFrame (axis=1 is the columns):\\nIn [65]: pd.concat([s1, s2, s3], axis=1)\\nOut[65]:\\n    0   1   2\\na   0 NaN NaN\\nb   1 NaN NaN\\nc NaN   2 NaN\\nd NaN   3 NaN\\ne NaN   4 NaN\\nf NaN NaN   5\\ng NaN NaN   6\\nIn this case there is no overlap on the other axis, which as you can see is the sorted\\nunion (the 'outer' join) of the indexes. You can instead intersect them by passing\\njoin='inner':\\nIn [66]: s4 = pd.concat([s1 * 5, s3])\\nIn [67]: pd.concat([s1, s4], axis=1)      In [68]: pd.concat([s1, s4], axis=1, join='inner')\\nOut[67]:                                  Out[68]:\\n    0  1                                     0  1\\na   0  0                                  a  0  0\\nb   1  5                                  b  1  5\\nf NaN  5\\ng NaN  6\\nYou can even specify the axes to be used on the other axes with join_axes:\\nIn [69]: pd.concat([s1, s4], axis=1, join_axes=[['a', 'c', 'b', 'e']])\\nOut[69]:\\n    0   1\\na   0   0\\nc NaN NaN\\nb   1   5\\ne NaN NaN\\nOne issue is that the concatenated pieces are not identifiable in the result. Suppose\\ninstead you wanted to create a hierarchical index on the concatenation axis. To do this,\\nuse the keys argument:\\nIn [70]: result = pd.concat([s1, s1, s3], keys=['one', 'two', 'three'])\\nIn [71]: result\\nOut[71]:\\none    a    0\\n       b    1\\ntwo    a    0\\n       b    1\\nthree  f    5\\n       g    6\\n# Much more on the unstack function later\\nIn [72]: result.unstack()\\nOut[72]:\\n186 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 202, 'page_label': '187'}, page_content=\"a   b   f   g\\none     0   1 NaN NaN\\ntwo     0   1 NaN NaN\\nthree NaN NaN   5   6\\nIn the case of combining Series along axis=1, the keys become the DataFrame column\\nheaders:\\nIn [73]: pd.concat([s1, s2, s3], axis=1, keys=['one', 'two', 'three'])\\nOut[73]:\\n   one  two  three\\na    0  NaN    NaN\\nb    1  NaN    NaN\\nc  NaN    2    NaN\\nd  NaN    3    NaN\\ne  NaN    4    NaN\\nf  NaN  NaN      5\\ng  NaN  NaN      6\\nThe same logic extends to DataFrame objects:\\nIn [74]: df1 = DataFrame(np.arange(6).reshape(3, 2), index=['a', 'b', 'c'],\\n   ....:                 columns=['one', 'two'])\\nIn [75]: df2 = DataFrame(5 + np.arange(4).reshape(2, 2), index=['a', 'c'],\\n   ....:                 columns=['three', 'four'])\\nIn [76]: pd.concat([df1, df2], axis=1, keys=['level1', 'level2'])\\nOut[76]:\\n   level1       level2\\n      one  two   three  four\\na       0    1       5     6\\nb       2    3     NaN   NaN\\nc       4    5       7     8\\nIf you pass a dict of objects instead of a list, the dict’s keys will be used for the keys\\noption:\\nIn [77]: pd.concat({'level1': df1, 'level2': df2}, axis=1)\\nOut[77]:\\n   level1       level2\\n      one  two   three  four\\na       0    1       5     6\\nb       2    3     NaN   NaN\\nc       4    5       7     8\\nThere are a couple of additional arguments governing how the hierarchical index is\\ncreated (see Table 7-2):\\nIn [78]: pd.concat([df1, df2], axis=1, keys=['level1', 'level2'],\\n   ....:           names=['upper', 'lower'])\\nOut[78]:\\nupper  level1       level2\\nlower     one  two   three  four\\na           0    1       5     6\\nb           2    3     NaN   NaN\\nc           4    5       7     8\\nCombining and Merging Data Sets | 187\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 203, 'page_label': '188'}, page_content=\"A last consideration concerns DataFrames in which the row index is not meaningful in\\nthe context of the analysis:\\nIn [79]: df1 = DataFrame(np.random.randn(3, 4), columns=['a', 'b', 'c', 'd'])\\nIn [80]: df2 = DataFrame(np.random.randn(2, 3), columns=['b', 'd', 'a'])\\nIn [81]: df1                                     In [82]: df2\\nOut[81]:                                         Out[82]:\\n          a         b         c         d                  b         d         a\\n0 -0.204708  0.478943 -0.519439 -0.555730        0  0.274992  0.228913  1.352917\\n1  1.965781  1.393406  0.092908  0.281746        1  0.886429 -2.001637 -0.371843\\n2  0.769023  1.246435  1.007189 -1.296221\\nIn this case, you can pass ignore_index=True:\\nIn [83]: pd.concat([df1, df2], ignore_index=True)\\nOut[83]:\\n          a         b         c         d\\n0 -0.204708  0.478943 -0.519439 -0.555730\\n1  1.965781  1.393406  0.092908  0.281746\\n2  0.769023  1.246435  1.007189 -1.296221\\n3  1.352917  0.274992       NaN  0.228913\\n4 -0.371843  0.886429       NaN -2.001637\\nTable 7-2. concat function arguments\\nArgument Description\\nobjs List or dict of pandas objects to be concatenated. The only required argument\\naxis Axis to concatenate along; defaults to 0\\njoin One of 'inner', 'outer', defaulting to 'outer'; whether to intersection (inner) or union\\n(outer) together indexes along the other axes\\njoin_axes Specific indexes to use for the other n-1 axes instead of performing union/intersection logic\\nkeys Values to associate with objects being concatenated, forming a hierarchical index along the\\nconcatenation axis. Can either be a list or array of arbitrary values, an array of tuples, or a list of\\narrays (if multiple level arrays passed in levels)\\nlevels Specific indexes to use as hierarchical index level or levels if keys passed\\nnames Names for created hierarchical levels if keys and / or levels passed\\nverify_integrity Check new axis in concatenated object for duplicates and raise exception if so. By default\\n(False) allows duplicates\\nignore_index Do not preserve indexes along concatenation axis, instead producing a new\\nrange(total_length) index\\nCombining Data with Overlap\\nAnother data combination situation can’t be expressed as either a merge or concate-\\nnation operation. You may have two datasets whose indexes overlap in full or part. As\\na motivating example, consider NumPy’s where function, which expressed a vectorized\\nif-else:\\n188 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 204, 'page_label': '189'}, page_content=\"In [84]: a = Series([np.nan, 2.5, np.nan, 3.5, 4.5, np.nan],\\n   ....:            index=['f', 'e', 'd', 'c', 'b', 'a'])\\nIn [85]: b = Series(np.arange(len(a), dtype=np.float64),\\n   ....:            index=['f', 'e', 'd', 'c', 'b', 'a'])\\nIn [86]: b[-1] = np.nan\\nIn [87]: a        In [88]: b        In [89]: np.where(pd.isnull(a), b, a)\\nOut[87]:          Out[88]:          Out[89]:\\nf    NaN          f     0           f    0.0\\ne    2.5          e     1           e    2.5\\nd    NaN          d     2           d    2.0\\nc    3.5          c     3           c    3.5\\nb    4.5          b     4           b    4.5\\na    NaN          a   NaN           a    NaN\\nSeries has a combine_first method, which performs the equivalent of this operation\\nplus data alignment:\\nIn [90]: b[:-2].combine_first(a[2:])\\nOut[90]:\\na    NaN\\nb    4.5\\nc    3.0\\nd    2.0\\ne    1.0\\nf    0.0\\nWith DataFrames, combine_first naturally does the same thing column by column, so\\nyou can think of it as “patching” missing data in the calling object with data from the\\nobject you pass:\\nIn [91]: df1 = DataFrame({'a': [1., np.nan, 5., np.nan],\\n   ....:                  'b': [np.nan, 2., np.nan, 6.],\\n   ....:                  'c': range(2, 18, 4)})\\nIn [92]: df2 = DataFrame({'a': [5., 4., np.nan, 3., 7.],\\n   ....:                  'b': [np.nan, 3., 4., 6., 8.]})\\nIn [93]: df1.combine_first(df2)\\nOut[93]:\\n   a   b   c\\n0  1 NaN   2\\n1  4   2   6\\n2  5   4  10\\n3  3   6  14\\n4  7   8 NaN\\nReshaping and Pivoting\\nThere are a number of fundamental operations for rearranging tabular data. These are\\nalternatingly referred to as reshape or pivot operations.\\nReshaping and Pivoting | 189\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 205, 'page_label': '190'}, page_content=\"Reshaping with Hierarchical Indexing\\nHierarchical indexing provides a consistent way to rearrange data in a DataFrame.\\nThere are two primary actions:\\n• stack: this “rotates” or pivots from the columns in the data to the rows\\n• unstack: this pivots from the rows into the columns\\nI’ll illustrate these operations through a series of examples. Consider a small DataFrame\\nwith string arrays as row and column indexes:\\nIn [94]: data = DataFrame(np.arange(6).reshape((2, 3)),\\n   ....:                  index=pd.Index(['Ohio', 'Colorado'], name='state'),\\n   ....:                  columns=pd.Index(['one', 'two', 'three'], name='number'))\\nIn [95]: data\\nOut[95]:\\nnumber    one  two  three\\nstate\\nOhio        0    1      2\\nColorado    3    4      5\\nUsing the stack method on this data pivots the columns into the rows, producing a\\nSeries:\\nIn [96]: result = data.stack()\\nIn [97]: result\\nOut[97]:\\nstate     number\\nOhio      one       0\\n          two       1\\n          three     2\\nColorado  one       3\\n          two       4\\n          three     5\\nFrom a hierarchically-indexed Series, you can rearrange the data back into a DataFrame\\nwith unstack:\\nIn [98]: result.unstack()\\nOut[98]:\\nnumber    one  two  three\\nstate\\nOhio        0    1      2\\nColorado    3    4      5\\nBy default the innermost level is unstacked (same with stack). You can unstack a dif-\\nferent level by passing a level number or name:\\nIn [99]: result.unstack(0)        In [100]: result.unstack('state')\\nOut[99]:                          Out[100]:\\nstate   Ohio  Colorado            state   Ohio  Colorado\\nnumber                            number\\none        0         3            one        0         3\\n190 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 206, 'page_label': '191'}, page_content=\"two        1         4            two        1         4\\nthree      2         5            three      2         5\\nUnstacking might introduce missing data if all of the values in the level aren’t found in\\neach of the subgroups:\\nIn [101]: s1 = Series([0, 1, 2, 3], index=['a', 'b', 'c', 'd'])\\nIn [102]: s2 = Series([4, 5, 6], index=['c', 'd', 'e'])\\nIn [103]: data2 = pd.concat([s1, s2], keys=['one', 'two'])\\nIn [104]: data2.unstack()\\nOut[104]:\\n      a   b  c  d   e\\none   0   1  2  3 NaN\\ntwo NaN NaN  4  5   6\\nStacking filters out missing data by default, so the operation is easily invertible:\\nIn [105]: data2.unstack().stack()      In [106]: data2.unstack().stack(dropna=False)\\nOut[105]:                              Out[106]:\\none  a    0                            one  a     0\\n     b    1                                 b     1\\n     c    2                                 c     2\\n     d    3                                 d     3\\ntwo  c    4                                 e   NaN\\n     d    5                            two  a   NaN\\n     e    6                                 b   NaN\\n                                            c     4\\n                                            d     5\\n                                            e     6\\nWhen unstacking in a DataFrame, the level unstacked becomes the lowest level in the\\nresult:\\nIn [107]: df = DataFrame({'left': result, 'right': result + 5},\\n   .....:                columns=pd.Index(['left', 'right'], name='side'))\\nIn [108]: df\\nOut[108]:\\nside             left  right\\nstate    number\\nOhio     one        0      5\\n         two        1      6\\n         three      2      7\\nColorado one        3      8\\n         two        4      9\\n         three      5     10\\nIn [109]: df.unstack('state')                In [110]: df.unstack('state').stack('side')\\nOut[109]:                                    Out[110]:\\nside    left            right                state         Ohio  Colorado\\nstate   Ohio  Colorado   Ohio  Colorado      number side\\nnumber                                       one    left      0         3\\none        0         3      5         8             right     5         8\\ntwo        1         4      6         9      two    left      1         4\\nReshaping and Pivoting | 191\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 207, 'page_label': '192'}, page_content=\"three      2         5      7        10             right     6         9\\n                                             three  left      2         5\\n                                                    right     7        10\\nPivoting “long” to “wide” Format\\nA common way to store multiple time series in databases and CSV is in so-called long\\nor stacked format:\\nIn [116]: ldata[:10]\\nOut[116]:\\n                 date     item     value\\n0 1959-03-31 00:00:00  realgdp  2710.349\\n1 1959-03-31 00:00:00     infl     0.000\\n2 1959-03-31 00:00:00    unemp     5.800\\n3 1959-06-30 00:00:00  realgdp  2778.801\\n4 1959-06-30 00:00:00     infl     2.340\\n5 1959-06-30 00:00:00    unemp     5.100\\n6 1959-09-30 00:00:00  realgdp  2775.488\\n7 1959-09-30 00:00:00     infl     2.740\\n8 1959-09-30 00:00:00    unemp     5.300\\n9 1959-12-31 00:00:00  realgdp  2785.204\\nData is frequently stored this way in relational databases like MySQL as a fixed schema\\n(column names and data types) allows the number of distinct values in the item column\\nto increase or decrease as data is added or deleted in the table. In the above example\\ndate and item would usually be the primary keys (in relational database parlance),\\noffering both relational integrity and easier joins and programmatic queries in many\\ncases. The downside, of course, is that the data may not be easy to work with in long\\nformat; you might prefer to have a DataFrame containing one column per distinct\\nitem value indexed by timestamps in the date column. DataFrame’s pivot method per-\\nforms exactly this transformation:\\nIn [117]: pivoted = ldata.pivot('date', 'item', 'value')\\nIn [118]: pivoted.head()\\nOut[118]:\\nitem        infl   realgdp  unemp\\ndate\\n1959-03-31  0.00  2710.349    5.8\\n1959-06-30  2.34  2778.801    5.1\\n1959-09-30  2.74  2775.488    5.3\\n1959-12-31  0.27  2785.204    5.6\\n1960-03-31  2.31  2847.699    5.2\\nThe first two values passed are the columns to be used as the row and column index,\\nand finally an optional value column to fill the DataFrame. Suppose you had two value\\ncolumns that you wanted to reshape simultaneously:\\nIn [119]: ldata['value2'] = np.random.randn(len(ldata))\\nIn [120]: ldata[:10]\\nOut[120]:\\n192 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 208, 'page_label': '193'}, page_content=\"date     item     value    value2\\n0 1959-03-31 00:00:00  realgdp  2710.349  1.669025\\n1 1959-03-31 00:00:00     infl     0.000 -0.438570\\n2 1959-03-31 00:00:00    unemp     5.800 -0.539741\\n3 1959-06-30 00:00:00  realgdp  2778.801  0.476985\\n4 1959-06-30 00:00:00     infl     2.340  3.248944\\n5 1959-06-30 00:00:00    unemp     5.100 -1.021228\\n6 1959-09-30 00:00:00  realgdp  2775.488 -0.577087\\n7 1959-09-30 00:00:00     infl     2.740  0.124121\\n8 1959-09-30 00:00:00    unemp     5.300  0.302614\\n9 1959-12-31 00:00:00  realgdp  2785.204  0.523772\\nBy omitting the last argument, you obtain a DataFrame with hierarchical columns:\\nIn [121]: pivoted = ldata.pivot('date', 'item')\\nIn [122]: pivoted[:5]\\nOut[122]:\\n            value                     value2\\nitem         infl   realgdp  unemp      infl   realgdp     unemp\\ndate\\n1959-03-31   0.00  2710.349    5.8 -0.438570  1.669025 -0.539741\\n1959-06-30   2.34  2778.801    5.1  3.248944  0.476985 -1.021228\\n1959-09-30   2.74  2775.488    5.3  0.124121 -0.577087  0.302614\\n1959-12-31   0.27  2785.204    5.6  0.000940  0.523772  1.343810\\n1960-03-31   2.31  2847.699    5.2 -0.831154 -0.713544 -2.370232\\nIn [123]: pivoted['value'][:5]\\nOut[123]:\\nitem        infl   realgdp  unemp\\ndate\\n1959-03-31  0.00  2710.349    5.8\\n1959-06-30  2.34  2778.801    5.1\\n1959-09-30  2.74  2775.488    5.3\\n1959-12-31  0.27  2785.204    5.6\\n1960-03-31  2.31  2847.699    5.2\\nNote that pivot is just a shortcut for creating a hierarchical index using set_index and\\nreshaping with unstack:\\nIn [124]: unstacked = ldata.set_index(['date', 'item']).unstack('item')\\nIn [125]: unstacked[:7]\\nOut[125]:\\n            value                     value2\\nitem         infl   realgdp  unemp      infl   realgdp     unemp\\ndate\\n1959-03-31   0.00  2710.349    5.8 -0.438570  1.669025 -0.539741\\n1959-06-30   2.34  2778.801    5.1  3.248944  0.476985 -1.021228\\n1959-09-30   2.74  2775.488    5.3  0.124121 -0.577087  0.302614\\n1959-12-31   0.27  2785.204    5.6  0.000940  0.523772  1.343810\\n1960-03-31   2.31  2847.699    5.2 -0.831154 -0.713544 -2.370232\\n1960-06-30   0.14  2834.390    5.2 -0.860757 -1.860761  0.560145\\n1960-09-30   2.70  2839.022    5.6  0.119827 -1.265934 -1.063512\\nReshaping and Pivoting | 193\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 209, 'page_label': '194'}, page_content=\"Data Transformation\\nSo far in this chapter we’ve been concerned with rearranging data. Filtering, cleaning,\\nand other tranformations are another class of important operations.\\nRemoving Duplicates\\nDuplicate rows may be found in a DataFrame for any number of reasons. Here is an\\nexample:\\nIn [126]: data = DataFrame({'k1': ['one'] * 3 + ['two'] * 4,\\n   .....:                   'k2': [1, 1, 2, 3, 3, 4, 4]})\\nIn [127]: data\\nOut[127]:\\n    k1  k2\\n0  one   1\\n1  one   1\\n2  one   2\\n3  two   3\\n4  two   3\\n5  two   4\\n6  two   4\\nThe DataFrame method duplicated returns a boolean Series indicating whether each\\nrow is a duplicate or not:\\nIn [128]: data.duplicated()\\nOut[128]:\\n0    False\\n1     True\\n2    False\\n3    False\\n4     True\\n5    False\\n6     True\\nRelatedly, drop_duplicates returns a DataFrame where the duplicated array is True:\\nIn [129]: data.drop_duplicates()\\nOut[129]:\\n    k1  k2\\n0  one   1\\n2  one   2\\n3  two   3\\n5  two   4\\nBoth of these methods by default consider all of the columns; alternatively you can\\nspecify any subset of them to detect duplicates. Suppose we had an additional column\\nof values and wanted to filter duplicates only based on the 'k1' column:\\nIn [130]: data['v1'] = range(7)\\nIn [131]: data.drop_duplicates(['k1'])\\n194 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 210, 'page_label': '195'}, page_content=\"Out[131]:\\n    k1  k2  v1\\n0  one   1   0\\n3  two   3   3\\nduplicated and drop_duplicates by default keep the first observed value combination.\\nPassing take_last=True will return the last one:\\nIn [132]: data.drop_duplicates(['k1', 'k2'], take_last=True)\\nOut[132]:\\n    k1  k2  v1\\n1  one   1   1\\n2  one   2   2\\n4  two   3   4\\n6  two   4   6\\nTransforming Data Using a Function or Mapping\\nFor many data sets, you may wish to perform some transformation based on the values\\nin an array, Series, or column in a DataFrame. Consider the following hypothetical data\\ncollected about some kinds of meat:\\nIn [133]: data = DataFrame({'food': ['bacon', 'pulled pork', 'bacon', 'Pastrami',\\n   .....:                            'corned beef', 'Bacon', 'pastrami', 'honey ham',\\n   .....:                            'nova lox'],\\n   .....:                   'ounces': [4, 3, 12, 6, 7.5, 8, 3, 5, 6]})\\nIn [134]: data\\nOut[134]:\\n          food  ounces\\n0        bacon     4.0\\n1  pulled pork     3.0\\n2        bacon    12.0\\n3     Pastrami     6.0\\n4  corned beef     7.5\\n5        Bacon     8.0\\n6     pastrami     3.0\\n7    honey ham     5.0\\n8     nova lox     6.0\\nSuppose you wanted to add a column indicating the type of animal that each food came\\nfrom. Let’s write down a mapping of each distinct meat type to the kind of animal:\\nmeat_to_animal = {\\n  'bacon': 'pig',\\n  'pulled pork': 'pig',\\n  'pastrami': 'cow',\\n  'corned beef': 'cow',\\n  'honey ham': 'pig',\\n  'nova lox': 'salmon'\\n}\\nData Transformation | 195\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 211, 'page_label': '196'}, page_content=\"The map method on a Series accepts a function or dict-like object containing a mapping,\\nbut here we have a small problem in that some of the meats above are capitalized and\\nothers are not. Thus, we also need to convert each value to lower case:\\nIn [136]: data['animal'] = data['food'].map(str.lower).map(meat_to_animal)\\nIn [137]: data\\nOut[137]:\\n          food  ounces  animal\\n0        bacon     4.0     pig\\n1  pulled pork     3.0     pig\\n2        bacon    12.0     pig\\n3     Pastrami     6.0     cow\\n4  corned beef     7.5     cow\\n5        Bacon     8.0     pig\\n6     pastrami     3.0     cow\\n7    honey ham     5.0     pig\\n8     nova lox     6.0  salmon\\nWe could also have passed a function that does all the work:\\nIn [138]: data['food'].map(lambda x: meat_to_animal[x.lower()])\\nOut[138]:\\n0       pig\\n1       pig\\n2       pig\\n3       cow\\n4       cow\\n5       pig\\n6       cow\\n7       pig\\n8    salmon\\nName: food\\nUsing map is a convenient way to perform element-wise transformations and other data\\ncleaning-related operations.\\nReplacing Values\\nFilling in missing data with the fillna method can be thought of as a special case of\\nmore general value replacement. While map, as you’ve seen above, can be used to modify\\na subset of values in an object, replace provides a simpler and more flexible way to do\\nso. Let’s consider this Series:\\nIn [139]: data = Series([1., -999., 2., -999., -1000., 3.])\\nIn [140]: data\\nOut[140]:\\n0       1\\n1    -999\\n2       2\\n3    -999\\n4   -1000\\n5       3\\n196 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 212, 'page_label': '197'}, page_content=\"The -999 values might be sentinel values for missing data. To replace these with NA\\nvalues that pandas understands, we can use replace, producing a new Series:\\nIn [141]: data.replace(-999, np.nan)\\nOut[141]:\\n0       1\\n1     NaN\\n2       2\\n3     NaN\\n4   -1000\\n5       3\\nIf you want to replace multiple values at once, you instead pass a list then the substitute\\nvalue:\\nIn [142]: data.replace([-999, -1000], np.nan)\\nOut[142]:\\n0     1\\n1   NaN\\n2     2\\n3   NaN\\n4   NaN\\n5     3\\nTo use a different replacement for each value, pass a list of substitutes:\\nIn [143]: data.replace([-999, -1000], [np.nan, 0])\\nOut[143]:\\n0     1\\n1   NaN\\n2     2\\n3   NaN\\n4     0\\n5     3\\nThe argument passed can also be a dict:\\nIn [144]: data.replace({-999: np.nan, -1000: 0})\\nOut[144]:\\n0     1\\n1   NaN\\n2     2\\n3   NaN\\n4     0\\n5     3\\nRenaming Axis Indexes\\nLike values in a Series, axis labels can be similarly transformed by a function or mapping\\nof some form to produce new, differently labeled objects. The axes can also be modified\\nin place without creating a new data structure. Here’s a simple example:\\nIn [145]: data = DataFrame(np.arange(12).reshape((3, 4)),\\n   .....:                  index=['Ohio', 'Colorado', 'New York'],\\n   .....:                  columns=['one', 'two', 'three', 'four'])\\nData Transformation | 197\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 213, 'page_label': '198'}, page_content=\"Like a Series, the axis indexes have a map method:\\nIn [146]: data.index.map(str.upper)\\nOut[146]: array([OHIO, COLORADO, NEW YORK], dtype=object)\\nYou can assign to index, modifying the DataFrame in place:\\nIn [147]: data.index = data.index.map(str.upper)\\nIn [148]: data\\nOut[148]:\\n          one  two  three  four\\nOHIO        0    1      2     3\\nCOLORADO    4    5      6     7\\nNEW YORK    8    9     10    11\\nIf you want to create a transformed version of a data set without modifying the original,\\na useful method is rename:\\nIn [149]: data.rename(index=str.title, columns=str.upper)\\nOut[149]:\\n          ONE  TWO  THREE  FOUR\\nOhio        0    1      2     3\\nColorado    4    5      6     7\\nNew York    8    9     10    11\\nNotably, rename can be used in conjunction with a dict-like object providing new values\\nfor a subset of the axis labels:\\nIn [150]: data.rename(index={'OHIO': 'INDIANA'},\\n   .....:             columns={'three': 'peekaboo'})\\nOut[150]:\\n          one  two  peekaboo  four\\nINDIANA     0    1         2     3\\nCOLORADO    4    5         6     7\\nNEW YORK    8    9        10    11\\nrename saves having to copy the DataFrame manually and assign to its index and col\\numns attributes. Should you wish to modify a data set in place, pass inplace=True:\\n# Always returns a reference to a DataFrame\\nIn [151]: _ = data.rename(index={'OHIO': 'INDIANA'}, inplace=True)\\nIn [152]: data\\nOut[152]:\\n          one  two  three  four\\nINDIANA     0    1      2     3\\nCOLORADO    4    5      6     7\\nNEW YORK    8    9     10    11\\n198 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 214, 'page_label': '199'}, page_content=\"Discretization and Binning\\nContinuous data is often discretized or otherwised separated into “bins” for analysis.\\nSuppose you have data about a group of people in a study, and you want to group them\\ninto discrete age buckets:\\nIn [153]: ages = [20, 22, 25, 27, 21, 23, 37, 31, 61, 45, 41, 32]\\nLet’s divide these into bins of 18 to 25, 26 to 35, 35 to 60, and finally 60 and older. To\\ndo so, you have to use cut, a function in pandas:\\nIn [154]: bins = [18, 25, 35, 60, 100]\\nIn [155]: cats = pd.cut(ages, bins)\\nIn [156]: cats\\nOut[156]:\\nCategorical:\\narray([(18, 25], (18, 25], (18, 25], (25, 35], (18, 25], (18, 25],\\n       (35, 60], (25, 35], (60, 100], (35, 60], (35, 60], (25, 35]], dtype=object)\\nLevels (4): Index([(18, 25], (25, 35], (35, 60], (60, 100]], dtype=object)\\nThe object pandas returns is a special Categorical object. You can treat it like an array\\nof strings indicating the bin name; internally it contains a levels array indicating the\\ndistinct category names along with a labeling for the ages data in the labels attribute:\\nIn [157]: cats.labels\\nOut[157]: array([0, 0, 0, 1, 0, 0, 2, 1, 3, 2, 2, 1])\\nIn [158]: cats.levels\\nOut[158]: Index([(18, 25], (25, 35], (35, 60], (60, 100]], dtype=object)\\nIn [159]: pd.value_counts(cats)\\nOut[159]:\\n(18, 25]     5\\n(35, 60]     3\\n(25, 35]     3\\n(60, 100]    1\\nConsistent with mathematical notation for intervals, a parenthesis means that the side\\nis open while the square bracket means it is closed (inclusive). Which side is closed can\\nbe changed by passing right=False:\\nIn [160]: pd.cut(ages, [18, 26, 36, 61, 100], right=False)\\nOut[160]:\\nCategorical:\\narray([[18, 26), [18, 26), [18, 26), [26, 36), [18, 26), [18, 26),\\n       [36, 61), [26, 36), [61, 100), [36, 61), [36, 61), [26, 36)], dtype=object)\\nLevels (4): Index([[18, 26), [26, 36), [36, 61), [61, 100)], dtype=object)\\nYou can also pass your own bin names by passing a list or array to the labels option:\\nIn [161]: group_names = ['Youth', 'YoungAdult', 'MiddleAged', 'Senior']\\nIn [162]: pd.cut(ages, bins, labels=group_names)\\nOut[162]:\\nData Transformation | 199\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 215, 'page_label': '200'}, page_content='Categorical:\\narray([Youth, Youth, Youth, YoungAdult, Youth, Youth, MiddleAged,\\n       YoungAdult, Senior, MiddleAged, MiddleAged, YoungAdult], dtype=object)\\nLevels (4): Index([Youth, YoungAdult, MiddleAged, Senior], dtype=object)\\nIf you pass cut a integer number of bins instead of explicit bin edges, it will compute\\nequal-length bins based on the minimum and maximum values in the data. Consider\\nthe case of some uniformly distributed data chopped into fourths:\\nIn [163]: data = np.random.rand(20)\\nIn [164]: pd.cut(data, 4, precision=2)\\nOut[164]:\\nCategorical:\\narray([(0.45, 0.67], (0.23, 0.45], (0.0037, 0.23], (0.45, 0.67],\\n       (0.67, 0.9], (0.45, 0.67], (0.67, 0.9], (0.23, 0.45], (0.23, 0.45],\\n       (0.67, 0.9], (0.67, 0.9], (0.67, 0.9], (0.23, 0.45], (0.23, 0.45],\\n       (0.23, 0.45], (0.67, 0.9], (0.0037, 0.23], (0.0037, 0.23],\\n       (0.23, 0.45], (0.23, 0.45]], dtype=object)\\nLevels (4): Index([(0.0037, 0.23], (0.23, 0.45], (0.45, 0.67],\\n                   (0.67, 0.9]], dtype=object)\\nA closely related function, qcut, bins the data based on sample quantiles. Depending\\non the distribution of the data, using cut will not usually result in each bin having the\\nsame number of data points. Since qcut uses sample quantiles instead, by definition\\nyou will obtain roughly equal-size bins:\\nIn [165]: data = np.random.randn(1000) # Normally distributed\\nIn [166]: cats = pd.qcut(data, 4) # Cut into quartiles\\nIn [167]: cats\\nOut[167]:\\nCategorical:\\narray([(-0.022, 0.641], [-3.745, -0.635], (0.641, 3.26], ...,\\n       (-0.635, -0.022], (0.641, 3.26], (-0.635, -0.022]], dtype=object)\\nLevels (4): Index([[-3.745, -0.635], (-0.635, -0.022], (-0.022, 0.641],\\n                   (0.641, 3.26]], dtype=object)\\nIn [168]: pd.value_counts(cats)\\nOut[168]:\\n[-3.745, -0.635]    250\\n(0.641, 3.26]       250\\n(-0.635, -0.022]    250\\n(-0.022, 0.641]     250\\nSimilar to cut you can pass your own quantiles (numbers between 0 and 1, inclusive):\\nIn [169]: pd.qcut(data, [0, 0.1, 0.5, 0.9, 1.])\\nOut[169]:\\nCategorical:\\narray([(-0.022, 1.302], (-1.266, -0.022], (-0.022, 1.302], ...,\\n       (-1.266, -0.022], (-0.022, 1.302], (-1.266, -0.022]], dtype=object)\\nLevels (4): Index([[-3.745, -1.266], (-1.266, -0.022], (-0.022, 1.302],\\n                   (1.302, 3.26]], dtype=object)\\n200 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 216, 'page_label': '201'}, page_content='We’ll return to cut and qcut later in the chapter on aggregation and group operations,\\nas these discretization functions are especially useful for quantile and group analysis.\\nDetecting and Filtering Outliers\\nFiltering or transforming outliers is largely a matter of applying array operations. Con-\\nsider a DataFrame with some normally distributed data:\\nIn [170]: np.random.seed(12345)\\nIn [171]: data = DataFrame(np.random.randn(1000, 4))\\nIn [172]: data.describe()\\nOut[172]:\\n                 0            1            2            3\\ncount  1000.000000  1000.000000  1000.000000  1000.000000\\nmean     -0.067684     0.067924     0.025598    -0.002298\\nstd       0.998035     0.992106     1.006835     0.996794\\nmin      -3.428254    -3.548824    -3.184377    -3.745356\\n25%      -0.774890    -0.591841    -0.641675    -0.644144\\n50%      -0.116401     0.101143     0.002073    -0.013611\\n75%       0.616366     0.780282     0.680391     0.654328\\nmax       3.366626     2.653656     3.260383     3.927528\\nSuppose you wanted to find values in one of the columns exceeding three in magnitude:\\nIn [173]: col = data[3]\\nIn [174]: col[np.abs(col) > 3]\\nOut[174]:\\n97     3.927528\\n305   -3.399312\\n400   -3.745356\\nName: 3\\nTo select all rows having a value exceeding 3 or -3, you can use the any method on a\\nboolean DataFrame:\\nIn [175]: data[(np.abs(data) > 3).any(1)]\\nOut[175]:\\n            0         1         2         3\\n5   -0.539741  0.476985  3.248944 -1.021228\\n97  -0.774363  0.552936  0.106061  3.927528\\n102 -0.655054 -0.565230  3.176873  0.959533\\n305 -2.315555  0.457246 -0.025907 -3.399312\\n324  0.050188  1.951312  3.260383  0.963301\\n400  0.146326  0.508391 -0.196713 -3.745356\\n499 -0.293333 -0.242459 -3.056990  1.918403\\n523 -3.428254 -0.296336 -0.439938 -0.867165\\n586  0.275144  1.179227 -3.184377  1.369891\\n808 -0.362528 -3.548824  1.553205 -2.186301\\n900  3.366626 -2.372214  0.851010  1.332846\\nValues can just as easily be set based on these criteria. Here is code to cap values outside\\nthe interval -3 to 3:\\nData Transformation | 201\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 217, 'page_label': '202'}, page_content='In [176]: data[np.abs(data) > 3] = np.sign(data) * 3\\nIn [177]: data.describe()\\nOut[177]:\\n                 0            1            2            3\\ncount  1000.000000  1000.000000  1000.000000  1000.000000\\nmean     -0.067623     0.068473     0.025153    -0.002081\\nstd       0.995485     0.990253     1.003977     0.989736\\nmin      -3.000000    -3.000000    -3.000000    -3.000000\\n25%      -0.774890    -0.591841    -0.641675    -0.644144\\n50%      -0.116401     0.101143     0.002073    -0.013611\\n75%       0.616366     0.780282     0.680391     0.654328\\nmax       3.000000     2.653656     3.000000     3.000000\\nThe ufunc np.sign returns an array of 1 and -1 depending on the sign of the values.\\nPermutation and Random Sampling\\nPermuting (randomly reordering) a Series or the rows in a DataFrame is easy to do using\\nthe numpy.random.permutation function. Calling permutation with the length of the axis\\nyou want to permute produces an array of integers indicating the new ordering:\\nIn [178]: df = DataFrame(np.arange(5 * 4).reshape(5, 4))\\nIn [179]: sampler = np.random.permutation(5)\\nIn [180]: sampler\\nOut[180]: array([1, 0, 2, 3, 4])\\nThat array can then be used in ix-based indexing or the take function:\\nIn [181]: df             In [182]: df.take(sampler)\\nOut[181]:                Out[182]:\\n    0   1   2   3            0   1   2   3\\n0   0   1   2   3        1   4   5   6   7\\n1   4   5   6   7        0   0   1   2   3\\n2   8   9  10  11        2   8   9  10  11\\n3  12  13  14  15        3  12  13  14  15\\n4  16  17  18  19        4  16  17  18  19\\nTo select a random subset without replacement, one way is to slice off the first k ele-\\nments of the array returned by permutation, where k is the desired subset size. There\\nare much more efficient sampling-without-replacement algorithms, but this is an easy\\nstrategy that uses readily available tools:\\nIn [183]: df.take(np.random.permutation(len(df))[:3])\\nOut[183]:\\n    0   1   2   3\\n1   4   5   6   7\\n3  12  13  14  15\\n4  16  17  18  19\\nTo generate a sample with replacement, the fastest way is to use np.random.randint to\\ndraw random integers:\\n202 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 218, 'page_label': '203'}, page_content=\"In [184]: bag = np.array([5, 7, -1, 6, 4])\\nIn [185]: sampler = np.random.randint(0, len(bag), size=10)\\nIn [186]: sampler\\nOut[186]: array([4, 4, 2, 2, 2, 0, 3, 0, 4, 1])\\nIn [187]: draws = bag.take(sampler)\\nIn [188]: draws\\nOut[188]: array([ 4,  4, -1, -1, -1,  5,  6,  5,  4,  7])\\nComputing Indicator/Dummy Variables\\nAnother type of transformation for statistical modeling or machine learning applica-\\ntions is converting a categorical variable into a “dummy” or “indicator” matrix. If a\\ncolumn in a DataFrame has k distinct values, you would derive a matrix or DataFrame\\ncontaining k columns containing all 1’s and 0’s. pandas has a get_dummies function for\\ndoing this, though devising one yourself is not difficult. Let’s return to an earlier ex-\\nample DataFrame:\\nIn [189]: df = DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'b'],\\n   .....:                 'data1': range(6)})\\nIn [190]: pd.get_dummies(df['key'])\\nOut[190]:\\n   a  b  c\\n0  0  1  0\\n1  0  1  0\\n2  1  0  0\\n3  0  0  1\\n4  1  0  0\\n5  0  1  0\\nIn some cases, you may want to add a prefix to the columns in the indicator DataFrame,\\nwhich can then be merged with the other data. get_dummies has a prefix argument for\\ndoing just this:\\nIn [191]: dummies = pd.get_dummies(df['key'], prefix='key')\\nIn [192]: df_with_dummy = df[['data1']].join(dummies)\\nIn [193]: df_with_dummy\\nOut[193]:\\n   data1  key_a  key_b  key_c\\n0      0      0      1      0\\n1      1      0      1      0\\n2      2      1      0      0\\n3      3      0      0      1\\n4      4      1      0      0\\n5      5      0      1      0\\nData Transformation | 203\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 219, 'page_label': '204'}, page_content=\"If a row in a DataFrame belongs to multiple categories, things are a bit more compli-\\ncated. Let’s return to the MovieLens 1M dataset from earlier in the book:\\nIn [194]: mnames = ['movie_id', 'title', 'genres']\\nIn [195]: movies = pd.read_table('ch07/movies.dat', sep='::', header=None,\\n   .....:                         names=mnames)\\nIn [196]: movies[:10]\\nOut[196]:\\n   movie_id                               title                        genres\\n0         1                    Toy Story (1995)   Animation|Children's|Comedy\\n1         2                      Jumanji (1995)  Adventure|Children's|Fantasy\\n2         3             Grumpier Old Men (1995)                Comedy|Romance\\n3         4            Waiting to Exhale (1995)                  Comedy|Drama\\n4         5  Father of the Bride Part II (1995)                        Comedy\\n5         6                         Heat (1995)         Action|Crime|Thriller\\n6         7                      Sabrina (1995)                Comedy|Romance\\n7         8                 Tom and Huck (1995)          Adventure|Children's\\n8         9                 Sudden Death (1995)                        Action\\n9        10                    GoldenEye (1995)     Action|Adventure|Thriller\\nAdding indicator variables for each genre requires a little bit of wrangling. First, we\\nextract the list of unique genres in the dataset (using a nice set.union trick):\\nIn [197]: genre_iter = (set(x.split('|')) for x in movies.genres)\\nIn [198]: genres = sorted(set.union(*genre_iter))\\nNow, one way to construct the indicator DataFrame is to start with a DataFrame of all\\nzeros:\\nIn [199]: dummies = DataFrame(np.zeros((len(movies), len(genres))), columns=genres)\\nNow, iterate through each movie and set entries in each row of dummies to 1:\\nIn [200]: for i, gen in enumerate(movies.genres):\\n   .....:     dummies.ix[i, gen.split('|')] = 1\\nThen, as above, you can combine this with movies:\\nIn [201]: movies_windic = movies.join(dummies.add_prefix('Genre_'))\\nIn [202]: movies_windic.ix[0]\\nOut[202]:\\nmovie_id                                       1\\ntitle                           Toy Story (1995)\\ngenres               Animation|Children's|Comedy\\nGenre_Action                                   0\\nGenre_Adventure                                0\\nGenre_Animation                                1\\nGenre_Children's                               1\\nGenre_Comedy                                   1\\nGenre_Crime                                    0\\nGenre_Documentary                              0\\nGenre_Drama                                    0\\nGenre_Fantasy                                  0\\n204 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 220, 'page_label': '205'}, page_content='Genre_Film-Noir                                0\\nGenre_Horror                                   0\\nGenre_Musical                                  0\\nGenre_Mystery                                  0\\nGenre_Romance                                  0\\nGenre_Sci-Fi                                   0\\nGenre_Thriller                                 0\\nGenre_War                                      0\\nGenre_Western                                  0\\nName: 0\\nFor much larger data, this method of constructing indicator variables\\nwith multiple membership is not especially speedy. A lower-level func-\\ntion leveraging the internals of the DataFrame could certainly be writ-\\nten.\\nA useful recipe for statistical applications is to combine get_dummies with a discretiza-\\ntion function like cut:\\nIn [204]: values = np.random.rand(10)\\nIn [205]: values\\nOut[205]:\\narray([ 0.9296,  0.3164,  0.1839,  0.2046,  0.5677,  0.5955,  0.9645,\\n        0.6532,  0.7489,  0.6536])\\nIn [206]: bins = [0, 0.2, 0.4, 0.6, 0.8, 1]\\nIn [207]: pd.get_dummies(pd.cut(values, bins))\\nOut[207]:\\n   (0, 0.2]  (0.2, 0.4]  (0.4, 0.6]  (0.6, 0.8]  (0.8, 1]\\n0         0           0           0           0         1\\n1         0           1           0           0         0\\n2         1           0           0           0         0\\n3         0           1           0           0         0\\n4         0           0           1           0         0\\n5         0           0           1           0         0\\n6         0           0           0           0         1\\n7         0           0           0           1         0\\n8         0           0           0           1         0\\n9         0           0           0           1         0\\nString Manipulation\\nPython has long been a popular data munging language in part due to its ease-of-use\\nfor string and text processing. Most text operations are made simple with the string\\nobject’s built-in methods. For more complex pattern matching and text manipulations,\\nregular expressions may be needed. pandas adds to the mix by enabling you to apply\\nstring and regular expressions concisely on whole arrays of data, additionally handling\\nthe annoyance of missing data.\\nString Manipulation | 205\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 221, 'page_label': '206'}, page_content=\"String Object Methods\\nIn many string munging and scripting applications, built-in string methods are suffi-\\ncient. As an example, a comma-separated string can be broken into pieces with split:\\nIn [208]: val = 'a,b,  guido'\\nIn [209]: val.split(',')\\nOut[209]: ['a', 'b', '  guido']\\nsplit is often combined with strip to trim whitespace (including newlines):\\nIn [210]: pieces = [x.strip() for x in val.split(',')]\\nIn [211]: pieces\\nOut[211]: ['a', 'b', 'guido']\\nThese substrings could be concatenated together with a two-colon delimiter using ad-\\ndition:\\nIn [212]: first, second, third = pieces\\nIn [213]: first + '::' + second + '::' + third\\nOut[213]: 'a::b::guido'\\nBut, this isn’t a practical generic method. A faster and more Pythonic way is to pass a\\nlist or tuple to the join method on the string '::':\\nIn [214]: '::'.join(pieces)\\nOut[214]: 'a::b::guido'\\nOther methods are concerned with locating substrings. Using Python’s in keyword is\\nthe best way to detect a substring, though index and find can also be used:\\nIn [215]: 'guido' in val\\nOut[215]: True\\nIn [216]: val.index(',')        In [217]: val.find(':')\\nOut[216]: 1                     Out[217]: -1\\nNote the difference between find and index is that index raises an exception if the string\\nisn’t found (versus returning -1):\\nIn [218]: val.index(':')\\n---------------------------------------------------------------------------\\nValueError                                Traceback (most recent call last)\\n<ipython-input-218-280f8b2856ce> in <module>()\\n----> 1 val.index(':')\\nValueError: substring not found\\nRelatedly, count returns the number of occurrences of a particular substring:\\nIn [219]: val.count(',')\\nOut[219]: 2\\nreplace will substitute occurrences of one pattern for another. This is commonly used\\nto delete patterns, too, by passing an empty string:\\n206 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 222, 'page_label': '207'}, page_content=\"In [220]: val.replace(',', '::')        In [221]: val.replace(',', '')\\nOut[220]: 'a::b::  guido'               Out[221]: 'ab  guido'\\nRegular expressions can also be used with many of these operations as you’ll see below.\\nTable 7-3. Python built-in string methods\\nArgument Description\\ncount Return the number of non-overlapping occurrences of substring in the string.\\nendswith, startswith Returns True if string ends with suffix (starts with prefix).\\njoin Use string as delimiter for concatenating a sequence of other strings.\\nindex Return position of first character in substring if found in the string. Raises ValueEr\\nror if not found.\\nfind Return position of first character of first occurrence of substring in the string. Like\\nindex, but returns -1 if not found.\\nrfind Return position of first character of last occurrence of substring in the string. Returns -1\\nif not found.\\nreplace Replace occurrences of string with another string.\\nstrip, rstrip, lstrip Trim whitespace, including newlines; equivalent to x.strip() (and rstrip,\\nlstrip, respectively) for each element.\\nsplit Break string into list of substrings using passed delimiter.\\nlower, upper Convert alphabet characters to lowercase or uppercase, respectively.\\nljust, rjust Left justify or right justify, respectively. Pad opposite side of string with spaces (or some\\nother fill character) to return a string with a minimum width.\\nRegular expressions\\nRegular expressions provide a flexible way to search or match string patterns in text. A\\nsingle expression, commonly called a regex, is a string formed according to the regular\\nexpression language. Python’s built-in re module is responsible for applying regular\\nexpressions to strings; I’ll give a number of examples of its use here.\\nThe art of writing regular expressions could be a chapter of its own and\\nthus is outside the book’s scope. There are many excellent tutorials and\\nreferences on the internet, such as Zed Shaw’s Learn Regex The Hard\\nWay (http://regex.learncodethehardway.org/book/).\\nThe re module functions fall into three categories: pattern matching, substitution, and\\nsplitting. Naturally these are all related; a regex describes a pattern to locate in the text,\\nwhich can then be used for many purposes. Let’s look at a simple example: suppose I\\nwanted to split a string with a variable number of whitespace characters (tabs, spaces,\\nand newlines). The regex describing one or more whitespace characters is \\\\s+:\\nString Manipulation | 207\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 223, 'page_label': '208'}, page_content='In [222]: import re\\nIn [223]: text = \"foo    bar\\\\t baz  \\\\tqux\"\\nIn [224]: re.split(\\'\\\\s+\\', text)\\nOut[224]: [\\'foo\\', \\'bar\\', \\'baz\\', \\'qux\\']\\nWhen you call re.split(\\'\\\\s+\\', text), the regular expression is first compiled, then its\\nsplit method is called on the passed text. You can compile the regex yourself with \\nre.compile, forming a reusable regex object:\\nIn [225]: regex = re.compile(\\'\\\\s+\\')\\nIn [226]: regex.split(text)\\nOut[226]: [\\'foo\\', \\'bar\\', \\'baz\\', \\'qux\\']\\nIf, instead, you wanted to get a list of all patterns matching the regex, you can use the \\nfindall method:\\nIn [227]: regex.findall(text)\\nOut[227]: [\\'    \\', \\'\\\\t \\', \\'  \\\\t\\']\\nTo avoid unwanted escaping with \\\\ in a regular expression, use raw\\nstring literals like r\\'C:\\\\x\\' instead of the equivalent \\'C:\\\\\\\\x\\'.\\nCreating a regex object with re.compile is highly recommended if you intend to apply\\nthe same expression to many strings; doing so will save CPU cycles.\\nmatch and search are closely related to findall. While findall returns all matches in a\\nstring, search returns only the first match. More rigidly, match only matches at the\\nbeginning of the string. As a less trivial example, let’s consider a block of text and a\\nregular expression capable of identifying most email addresses:\\ntext = \"\"\"Dave dave@google.com\\nSteve steve@gmail.com\\nRob rob@gmail.com\\nRyan ryan@yahoo.com\\n\"\"\"\\npattern = r\\'[A-Z0-9._%+-]+@[A-Z0-9.-]+\\\\.[A-Z]{2,4}\\'\\n# re.IGNORECASE makes the regex case-insensitive\\nregex = re.compile(pattern, flags=re.IGNORECASE)\\nUsing findall on the text produces a list of the e-mail addresses:\\nIn [229]: regex.findall(text)\\nOut[229]: [\\'dave@google.com\\', \\'steve@gmail.com\\', \\'rob@gmail.com\\', \\'ryan@yahoo.com\\']\\nsearch returns a special match object for the first email address in the text. For the\\nabove regex, the match object can only tell us the start and end position of the pattern\\nin the string:\\n208 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 224, 'page_label': '209'}, page_content=\"In [230]: m = regex.search(text)\\nIn [231]: m\\nOut[231]: <_sre.SRE_Match at 0x10a05de00>\\nIn [232]: text[m.start():m.end()]\\nOut[232]: 'dave@google.com'\\nregex.match returns None, as it only will match if the pattern occurs at the start of the\\nstring:\\nIn [233]: print regex.match(text)\\nNone\\nRelatedly, sub will return a new string with occurrences of the pattern replaced by the\\na new string:\\nIn [234]: print regex.sub('REDACTED', text)\\nDave REDACTED\\nSteve REDACTED\\nRob REDACTED\\nRyan REDACTED\\nSuppose you wanted to find email addresses and simultaneously segment each address\\ninto its 3 components: username, domain name, and domain suffix. To do this, put\\nparentheses around the parts of the pattern to segment:\\nIn [235]: pattern = r'([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\\\\.([A-Z]{2,4})'\\nIn [236]: regex = re.compile(pattern, flags=re.IGNORECASE)\\nA match object produced by this modified regex returns a tuple of the pattern compo-\\nnents with its groups method:\\nIn [237]: m = regex.match('wesm@bright.net')\\nIn [238]: m.groups()\\nOut[238]: ('wesm', 'bright', 'net')\\nfindall returns a list of tuples when the pattern has groups:\\nIn [239]: regex.findall(text)\\nOut[239]:\\n[('dave', 'google', 'com'),\\n ('steve', 'gmail', 'com'),\\n ('rob', 'gmail', 'com'),\\n ('ryan', 'yahoo', 'com')]\\nsub also has access to groups in each match using special symbols like \\\\1, \\\\2, etc.:\\nIn [240]: print regex.sub(r'Username: \\\\1, Domain: \\\\2, Suffix: \\\\3', text)\\nDave Username: dave, Domain: google, Suffix: com\\nSteve Username: steve, Domain: gmail, Suffix: com\\nRob Username: rob, Domain: gmail, Suffix: com\\nRyan Username: ryan, Domain: yahoo, Suffix: com\\nString Manipulation | 209\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 225, 'page_label': '210'}, page_content='There is much more to regular expressions in Python, most of which is outside the\\nbook’s scope. To give you a flavor, one variation on the above email regex gives names\\nto the match groups:\\nregex = re.compile(r\"\"\"\\n    (?P<username>[A-Z0-9._%+-]+)\\n    @\\n    (?P<domain>[A-Z0-9.-]+)\\n    \\\\.\\n    (?P<suffix>[A-Z]{2,4})\"\"\", flags=re.IGNORECASE|re.VERBOSE)\\nThe match object produced by such a regex can produce a handy dict with the specified\\ngroup names:\\nIn [242]: m = regex.match(\\'wesm@bright.net\\')\\nIn [243]: m.groupdict()\\nOut[243]: {\\'domain\\': \\'bright\\', \\'suffix\\': \\'net\\', \\'username\\': \\'wesm\\'}\\nTable 7-4. Regular expression methods\\nArgument Description\\nfindall, finditer Return all non-overlapping matching patterns in a string. findall returns a list of all\\npatterns while finditer returns them one by one from an iterator.\\nmatch Match pattern at start of string and optionally segment pattern components into groups.\\nIf the pattern matches, returns a match object, otherwise None.\\nsearch Scan string for match to pattern; returning a match object if so. Unlike match, the match\\ncan be anywhere in the string as opposed to only at the beginning.\\nsplit Break string into pieces at each occurrence of pattern.\\nsub, subn Replace all (sub) or first n occurrences (subn) of pattern in string with replacement\\nexpression. Use symbols \\\\1, \\\\2, ... to refer to match group elements in the re-\\nplacement string.\\nVectorized string functions in pandas\\nCleaning up a messy data set for analysis often requires a lot of string munging and\\nregularization. To complicate matters, a column containing strings will sometimes have\\nmissing data:\\nIn [244]: data = {\\'Dave\\': \\'dave@google.com\\', \\'Steve\\': \\'steve@gmail.com\\',\\n   .....:         \\'Rob\\': \\'rob@gmail.com\\', \\'Wes\\': np.nan}\\nIn [245]: data = Series(data)\\nIn [246]: data                  In [247]: data.isnull()\\nOut[246]:                       Out[247]:\\nDave     dave@google.com        Dave     False\\nRob        rob@gmail.com        Rob      False\\nSteve    steve@gmail.com        Steve    False\\nWes                  NaN        Wes       True\\n210 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 226, 'page_label': '211'}, page_content=\"String and regular expression methods can be applied (passing a lambda or other func-\\ntion) to each value using data.map, but it will fail on the NA. To cope with this, Series\\nhas concise methods for string operations that skip NA values. These are accessed\\nthrough Series’s str attribute; for example, we could check whether each email address\\nhas 'gmail' in it with str.contains:\\nIn [248]: data.str.contains('gmail')\\nOut[248]:\\nDave     False\\nRob       True\\nSteve     True\\nWes        NaN\\nRegular expressions can be used, too, along with any re options like IGNORECASE:\\nIn [249]: pattern\\nOut[249]: '([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\\\\\\\\.([A-Z]{2,4})'\\nIn [250]: data.str.findall(pattern, flags=re.IGNORECASE)\\nOut[250]:\\nDave     [('dave', 'google', 'com')]\\nRob        [('rob', 'gmail', 'com')]\\nSteve    [('steve', 'gmail', 'com')]\\nWes                              NaN\\nThere are a couple of ways to do vectorized element retrieval. Either use str.get or\\nindex into the str attribute:\\nIn [251]: matches = data.str.match(pattern, flags=re.IGNORECASE)\\nIn [252]: matches\\nOut[252]:\\nDave     ('dave', 'google', 'com')\\nRob        ('rob', 'gmail', 'com')\\nSteve    ('steve', 'gmail', 'com')\\nWes                            NaN\\nIn [253]: matches.str.get(1)      In [254]: matches.str[0]\\nOut[253]:                         Out[254]:\\nDave     google                   Dave      dave\\nRob       gmail                   Rob        rob\\nSteve     gmail                   Steve    steve\\nWes         NaN                   Wes        NaN\\nYou can similarly slice strings using this syntax:\\nIn [255]: data.str[:5]\\nOut[255]:\\nDave     dave@\\nRob      rob@g\\nSteve    steve\\nWes        NaN\\nString Manipulation | 211\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 227, 'page_label': '212'}, page_content='Table 7-5. Vectorized string methods\\nMethod Description\\ncat Concatenate strings element-wise with optional delimiter\\ncontains Return boolean array if each string contains pattern/regex\\ncount Count occurrences of pattern\\nendswith, startswith Equivalent to x.endswith(pattern) or x.startswith(pattern) for each el-\\nement.\\nfindall Compute list of all occurrences of pattern/regex for each string\\nget Index into each element (retrieve i-th element)\\njoin Join strings in each element of the Series with passed separator\\nlen Compute length of each string\\nlower, upper Convert cases; equivalent to x.lower() or x.upper() for each element.\\nmatch Use re.match with the passed regular expression on each element, returning matched\\ngroups as list.\\npad Add whitespace to left, right, or both sides of strings\\ncenter Equivalent to pad(side=\\'both\\')\\nrepeat Duplicate values; for example s.str.repeat(3) equivalent to x * 3 for each string.\\nreplace Replace occurrences of pattern/regex with some other string\\nslice Slice each string in the Series.\\nsplit Split strings on delimiter or regular expression\\nstrip, rstrip, lstrip Trim whitespace, including newlines; equivalent to x.strip() (and rstrip,\\nlstrip, respectively) for each element.\\nExample: USDA Food Database\\nThe US Department of Agriculture makes available a database of food nutrient infor-\\nmation. Ashley Williams, an English hacker, has made available a version of this da-\\ntabase in JSON format ( http://ashleyw.co.uk/project/food-nutrient-database). The re-\\ncords look like this:\\n{\\n  \"id\": 21441,\\n  \"description\": \"KENTUCKY FRIED CHICKEN, Fried Chicken, EXTRA CRISPY,\\nWing, meat and skin with breading\",\\n  \"tags\": [\"KFC\"],\\n  \"manufacturer\": \"Kentucky Fried Chicken\",\\n  \"group\": \"Fast Foods\",\\n  \"portions\": [\\n    {\\n      \"amount\": 1,\\n      \"unit\": \"wing, with skin\",\\n      \"grams\": 68.0\\n    },\\n212 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 228, 'page_label': '213'}, page_content='...\\n  ],\\n  \"nutrients\": [\\n    {\\n      \"value\": 20.8,\\n      \"units\": \"g\",\\n      \"description\": \"Protein\",\\n      \"group\": \"Composition\"\\n    },\\n    ...\\n  ]\\n}\\nEach food has a number of identifying attributes along with two lists of nutrients and\\nportion sizes. Having the data in this form is not particularly amenable for analysis, so\\nwe need to do some work to wrangle the data into a better form.\\nAfter downloading and extracting the data from the link above, you can load it into\\nPython with any JSON library of your choosing. I’ll use the built-in Python json mod-\\nule:\\nIn [256]: import json\\nIn [257]: db = json.load(open(\\'ch07/foods-2011-10-03.json\\'))\\nIn [258]: len(db)\\nOut[258]: 6636\\nEach entry in db is a dict containing all the data for a single food. The \\'nutrients\\' field\\nis a list of dicts, one for each nutrient:\\nIn [259]: db[0].keys()        In [260]: db[0][\\'nutrients\\'][0]\\nOut[259]:                     Out[260]:\\n[u\\'portions\\',                 {u\\'description\\': u\\'Protein\\',\\n u\\'description\\',               u\\'group\\': u\\'Composition\\',\\n u\\'tags\\',                      u\\'units\\': u\\'g\\',\\n u\\'nutrients\\',                 u\\'value\\': 25.18}\\n u\\'group\\',\\n u\\'id\\',\\n u\\'manufacturer\\']\\nIn [261]: nutrients = DataFrame(db[0][\\'nutrients\\'])\\nIn [262]: nutrients[:7]\\nOut[262]:\\n                   description        group units    value\\n0                      Protein  Composition     g    25.18\\n1            Total lipid (fat)  Composition     g    29.20\\n2  Carbohydrate, by difference  Composition     g     3.06\\n3                          Ash        Other     g     3.28\\n4                       Energy       Energy  kcal   376.00\\n5                        Water  Composition     g    39.28\\n6                       Energy       Energy    kJ  1573.00\\nExample: USDA Food Database | 213\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 229, 'page_label': '214'}, page_content=\"When converting a list of dicts to a DataFrame, we can specify a list of fields to extract.\\nWe’ll take the food names, group, id, and manufacturer:\\nIn [263]: info_keys = ['description', 'group', 'id', 'manufacturer']\\nIn [264]: info = DataFrame(db, columns=info_keys)\\nIn [265]: info[:5]\\nOut[265]:\\n                          description                   group    id manufacturer\\n0                     Cheese, caraway  Dairy and Egg Products  1008\\n1                     Cheese, cheddar  Dairy and Egg Products  1009\\n2                        Cheese, edam  Dairy and Egg Products  1018\\n3                        Cheese, feta  Dairy and Egg Products  1019\\n4  Cheese, mozzarella, part skim milk  Dairy and Egg Products  1028\\nIn [266]: info\\nOut[266]:\\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 6636 entries, 0 to 6635\\nData columns:\\ndescription     6636  non-null values\\ngroup           6636  non-null values\\nid              6636  non-null values\\nmanufacturer    5195  non-null values\\ndtypes: int64(1), object(3)\\nYou can see the distribution of food groups with value_counts:\\nIn [267]: pd.value_counts(info.group)[:10]\\nOut[267]:\\nVegetables and Vegetable Products    812\\nBeef Products                        618\\nBaked Products                       496\\nBreakfast Cereals                    403\\nLegumes and Legume Products          365\\nFast Foods                           365\\nLamb, Veal, and Game Products        345\\nSweets                               341\\nPork Products                        328\\nFruits and Fruit Juices              328\\nNow, to do some analysis on all of the nutrient data, it’s easiest to assemble the nutrients\\nfor each food into a single large table. To do so, we need to take several steps. First, I’ll\\nconvert each list of food nutrients to a DataFrame, add a column for the food id, and\\nappend the DataFrame to a list. Then, these can be concatenated together with concat:\\nnutrients = []\\nfor rec in db:\\n    fnuts = DataFrame(rec['nutrients'])\\n    fnuts['id'] = rec['id']\\n    nutrients.append(fnuts)\\nnutrients = pd.concat(nutrients, ignore_index=True)\\n214 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 230, 'page_label': '215'}, page_content=\"If all goes well, nutrients should look like this:\\nIn [269]: nutrients\\nOut[269]:\\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 389355 entries, 0 to 389354\\nData columns:\\ndescription    389355  non-null values\\ngroup          389355  non-null values\\nunits          389355  non-null values\\nvalue          389355  non-null values\\nid             389355  non-null values\\ndtypes: float64(1), int64(1), object(3)\\nI noticed that, for whatever reason, there are duplicates in this DataFrame, so it makes\\nthings easier to drop them:\\nIn [270]: nutrients.duplicated().sum()\\nOut[270]: 14179\\nIn [271]: nutrients = nutrients.drop_duplicates()\\nSince 'group' and 'description' is in both DataFrame objects, we can rename them to\\nmake it clear what is what:\\nIn [272]: col_mapping = {'description' : 'food',\\n   .....:                'group'       : 'fgroup'}\\nIn [273]: info = info.rename(columns=col_mapping, copy=False)\\nIn [274]: info\\nOut[274]:\\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 6636 entries, 0 to 6635\\nData columns:\\nfood            6636  non-null values\\nfgroup          6636  non-null values\\nid              6636  non-null values\\nmanufacturer    5195  non-null values\\ndtypes: int64(1), object(3)\\nIn [275]: col_mapping = {'description' : 'nutrient',\\n   .....:                'group' : 'nutgroup'}\\nIn [276]: nutrients = nutrients.rename(columns=col_mapping, copy=False)\\nIn [277]: nutrients\\nOut[277]:\\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 375176 entries, 0 to 389354\\nData columns:\\nnutrient    375176  non-null values\\nnutgroup    375176  non-null values\\nunits       375176  non-null values\\nvalue       375176  non-null values\\nExample: USDA Food Database | 215\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 231, 'page_label': '216'}, page_content=\"id          375176  non-null values\\ndtypes: float64(1), int64(1), object(3)\\nWith all of this done, we’re ready to merge info with nutrients:\\nIn [278]: ndata = pd.merge(nutrients, info, on='id', how='outer')\\nIn [279]: ndata\\nOut[279]:\\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 375176 entries, 0 to 375175\\nData columns:\\nnutrient        375176  non-null values\\nnutgroup        375176  non-null values\\nunits           375176  non-null values\\nvalue           375176  non-null values\\nid              375176  non-null values\\nfood            375176  non-null values\\nfgroup          375176  non-null values\\nmanufacturer    293054  non-null values\\ndtypes: float64(1), int64(1), object(6)\\nIn [280]: ndata.ix[30000]\\nOut[280]:\\nnutrient                       Folic acid\\nnutgroup                         Vitamins\\nunits                                 mcg\\nvalue                                   0\\nid                                   5658\\nfood            Ostrich, top loin, cooked\\nfgroup                   Poultry Products\\nmanufacturer\\nName: 30000\\nThe tools that you need to slice and dice, aggregate, and visualize this dataset will be\\nexplored in detail in the next two chapters, so after you get a handle on those methods\\nyou might return to this dataset. For example, we could a plot of median values by food\\ngroup and nutrient type (see Figure 7-1):\\nIn [281]: result = ndata.groupby(['nutrient', 'fgroup'])['value'].quantile(0.5)\\nIn [282]: result['Zinc, Zn'].order().plot(kind='barh')\\nWith a little cleverness, you can find which food is most dense in each nutrient:\\nby_nutrient = ndata.groupby(['nutgroup', 'nutrient'])\\nget_maximum = lambda x: x.xs(x.value.idxmax())\\nget_minimum = lambda x: x.xs(x.value.idxmin())\\nmax_foods = by_nutrient.apply(get_maximum)[['value', 'food']]\\n# make the food a little smaller\\nmax_foods.food = max_foods.food.str[:50]\\n216 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 232, 'page_label': '217'}, page_content=\"The resulting DataFrame is a bit too large to display in the book; here is just the 'Amino\\nAcids' nutrient group:\\nIn [284]: max_foods.ix['Amino Acids']['food']\\nOut[284]:\\nnutrient\\nAlanine                           Gelatins, dry powder, unsweetened\\nArginine                               Seeds, sesame flour, low-fat\\nAspartic acid                                   Soy protein isolate\\nCystine                Seeds, cottonseed flour, low fat (glandless)\\nGlutamic acid                                   Soy protein isolate\\nGlycine                           Gelatins, dry powder, unsweetened\\nHistidine                Whale, beluga, meat, dried (Alaska Native)\\nHydroxyproline    KENTUCKY FRIED CHICKEN, Fried Chicken, ORIGINAL R\\nIsoleucine        Soy protein isolate, PROTEIN TECHNOLOGIES INTERNA\\nLeucine           Soy protein isolate, PROTEIN TECHNOLOGIES INTERNA\\nLysine            Seal, bearded (Oogruk), meat, dried (Alaska Nativ\\nMethionine                    Fish, cod, Atlantic, dried and salted\\nPhenylalanine     Soy protein isolate, PROTEIN TECHNOLOGIES INTERNA\\nProline                           Gelatins, dry powder, unsweetened\\nSerine            Soy protein isolate, PROTEIN TECHNOLOGIES INTERNA\\nThreonine         Soy protein isolate, PROTEIN TECHNOLOGIES INTERNA\\nTryptophan         Sea lion, Steller, meat with fat (Alaska Native)\\nTyrosine          Soy protein isolate, PROTEIN TECHNOLOGIES INTERNA\\nValine            Soy protein isolate, PROTEIN TECHNOLOGIES INTERNA\\nName: food\\nFigure 7-1. Median Zinc values by nutrient group\\nExample: USDA Food Database | 217\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 233, 'page_label': '218'}, page_content='www.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 234, 'page_label': '219'}, page_content='CHAPTER 8\\nPlotting and Visualization\\nMaking plots and static or interactive visualizations is one of the most important tasks\\nin data analysis. It may be a part of the exploratory process; for example, helping iden-\\ntify outliers, needed data transformations, or coming up with ideas for models. For\\nothers, building an interactive visualization for the web using a toolkit like d3.js (http:\\n//d3js.org/) may be the end goal. Python has many visualization tools (see the end of\\nthis chapter), but I’ll be mainly focused on matplotlib ( http://matplotlib.sourceforge\\n.net).\\nmatplotlib is a (primarily 2D) desktop plotting package designed for creating publica-\\ntion-quality plots. The project was started by John Hunter in 2002 to enable a MAT-\\nLAB-like plotting interface in Python. He, Fernando Pérez (of IPython), and others have\\ncollaborated for many years since then to make IPython combined with matplotlib a\\nvery functional and productive environment for scientific computing. When used in\\ntandem with a GUI toolkit (for example, within IPython), matplotlib has interactive\\nfeatures like zooming and panning. It supports many different GUI backends on all\\noperating systems and additionally can export graphics to all of the common vector\\nand raster graphics formats: PDF, SVG, JPG, PNG, BMP, GIF, etc. I have used it to\\nproduce almost all of the graphics outside of diagrams in this book.\\nmatplotlib has a number of add-on toolkits, such as mplot3d for 3D plots and basemap\\nfor mapping and projections. I will give an example using basemap to plot data on a map\\nand to read shapefiles at the end of the chapter.\\nTo follow along with the code examples in the chapter, make sure you have started\\nIPython in Pylab mode (ipython --pylab) or enabled GUI event loop integration with\\nthe %gui magic.\\nA Brief matplotlib API Primer\\nThere are several ways to interact with matplotlib. The most common is through pylab\\nmode in IPython by running ipython --pylab. This launches IPython configured to be\\nable to support the matplotlib GUI backend of your choice (Tk, wxPython, PyQt, Mac\\n219\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 235, 'page_label': '220'}, page_content='OS X native, GTK). For most users, the default backend will be sufficient. Pylab mode\\nalso imports a large set of modules and functions into IPython to provide a more MAT-\\nLAB-like interface. You can test that everything is working by making a simple plot:\\nplot(np.arange(10))\\nIf everything is set up right, a new window should pop up with a line plot. You can\\nclose it by using the mouse or entering close(). Matplotlib API functions like plot and \\nclose are all in the matplotlib.pyplot module, which is typically imported by conven-\\ntion as:\\nimport matplotlib.pyplot as plt\\nWhile the pandas plotting functions described later deal with many of the mundane\\ndetails of making plots, should you wish to customize them beyond the function op-\\ntions provided you will need to learn a bit about the matplotlib API.\\nThere is not enough room in the book to give a comprehensive treatment\\nto the breadth and depth of functionality in matplotlib. It should be\\nenough to teach you the ropes to get up and running. The matplotlib\\ngallery and documentation are the best resource for becoming a plotting\\nguru and using advanced features.\\nFigures and Subplots\\nPlots in matplotlib reside within a Figure object. You can create a new figure with\\nplt.figure:\\nIn [13]: fig = plt.figure()\\nFigure 8-1. A more complex matplotlib financial plot\\n220 | Chapter 8: \\u2002Plotting and Visualization\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 236, 'page_label': '221'}, page_content=\"If you are in pylab mode in IPython, a new empty window should pop up. plt.fig\\nure has a number of options, notably figsize will guarantee the figure has a certain size\\nand aspect ratio if saved to disk. Figures in matplotlib also support a numbering scheme\\n(for example, plt.figure(2)) that mimics MATLAB. You can get a reference to the\\nactive figure using plt.gcf().\\nYou can’t make a plot with a blank figure. You have to create one or more subplots\\nusing add_subplot:\\nIn [14]: ax1 = fig.add_subplot(2, 2, 1)\\nThis means that the figure should be 2 × 2, and we’re selecting the first of 4 subplots\\n(numbered from 1). If you create the next two subplots, you’ll end up with a figure that\\nlooks like Figure 8-2.\\nIn [15]: ax2 = fig.add_subplot(2, 2, 2)\\nIn [16]: ax3 = fig.add_subplot(2, 2, 3)\\nFigure 8-2. An empty matplotlib Figure with 3 subplots\\nWhen you issue a plotting command like plt.plot([1.5, 3.5, -2, 1.6]), matplotlib\\ndraws on the last figure and subplot used (creating one if necessary), thus hiding the\\nfigure and subplot creation. Thus, if we run the following command, you’ll get some-\\nthing like Figure 8-3:\\nIn [17]: from numpy.random import randn\\nIn [18]: plt.plot(randn(50).cumsum(), 'k--')\\nThe 'k--' is a style option instructing matplotlib to plot a black dashed line. The objects\\nreturned by fig.add_subplot above are AxesSubplot objects, on which you can directly\\nplot on the other empty subplots by calling each one’s instance methods, see Figure 8-4:\\nA Brief matplotlib API Primer | 221\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 237, 'page_label': '222'}, page_content=\"In [19]: _ = ax1.hist(randn(100), bins=20, color='k', alpha=0.3)\\nIn [20]: ax2.scatter(np.arange(30), np.arange(30) + 3 * randn(30))\\nYou can find a comprehensive catalogue of plot types in the matplotlib documentation.\\nSince creating a figure with multiple subplots according to a particular layout is such\\na common task, there is a convenience method, plt.subplots, that creates a new figure\\nand returns a NumPy array containing the created subplot objects:\\nFigure 8-3. Figure after single plot\\nFigure 8-4. Figure after additional plots\\n222 | Chapter 8: \\u2002Plotting and Visualization\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 238, 'page_label': '223'}, page_content=\"In [22]: fig, axes = plt.subplots(2, 3)\\nIn [23]: axes\\nOut[23]: \\narray([[Axes(0.125,0.536364;0.227941x0.363636),\\n        Axes(0.398529,0.536364;0.227941x0.363636),\\n        Axes(0.672059,0.536364;0.227941x0.363636)],\\n       [Axes(0.125,0.1;0.227941x0.363636),\\n        Axes(0.398529,0.1;0.227941x0.363636),\\n        Axes(0.672059,0.1;0.227941x0.363636)]], dtype=object)\\nThis is very useful as the axes array can be easily indexed like a two-dimensional array;\\nfor example, axes[0, 1]. You can also indicate that subplots should have the same X\\nor Y axis using sharex and sharey, respectively. This is especially useful when comparing\\ndata on the same scale; otherwise, matplotlib auto-scales plot limits independently. See\\nTable 8-1 for more on this method.\\nTable 8-1. pyplot.subplots options\\nArgument Description\\nnrows Number of rows of subplots\\nncols Number of columns of subplots\\nsharex All subplots should use the same X-axis ticks (adjusting the xlim will affect all subplots)\\nsharey All subplots should use the same Y-axis ticks (adjusting the ylim will affect all subplots)\\nsubplot_kw Dict of keywords for creating the\\n**fig_kw Additional keywords to subplots are used when creating the figure, such as plt.subplots(2, 2,\\nfigsize=(8, 6))\\nAdjusting the spacing around subplots\\nBy default matplotlib leaves a certain amount of padding around the outside of the\\nsubplots and spacing between subplots. This spacing is all specified relative to the\\nheight and width of the plot, so that if you resize the plot either programmatically or\\nmanually using the GUI window, the plot will dynamically adjust itself. The spacing\\ncan be most easily changed using the subplots_adjust Figure method, also available as\\na top-level function:\\nsubplots_adjust(left=None, bottom=None, right=None, top=None,\\n                wspace=None, hspace=None)\\nwspace and hspace controls the percent of the figure width and figure height, respec-\\ntively, to use as spacing between subplots. Here is a small example where I shrink the\\nspacing all the way to zero (see Figure 8-5):\\nfig, axes = plt.subplots(2, 2, sharex=True, sharey=True)\\nfor i in range(2):\\n    for j in range(2):\\n        axes[i, j].hist(randn(500), bins=50, color='k', alpha=0.5)\\nplt.subplots_adjust(wspace=0, hspace=0)\\nA Brief matplotlib API Primer | 223\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 239, 'page_label': '224'}, page_content=\"Figure 8-5. Figure with no inter-subplot spacing\\nYou may notice that the axis labels overlap. matplotlib doesn’t check whether the labels\\noverlap, so in a case like this you would need to fix the labels yourself by specifying\\nexplicit tick locations and tick labels. More on this in the coming sections.\\nColors, Markers, and Line Styles\\nMatplotlib’s main plot function accepts arrays of X and Y coordinates and optionally\\na string abbreviation indicating color and line style. For example, to plot x versus y with\\ngreen dashes, you would execute:\\nax.plot(x, y, 'g--')\\nThis way of specifying both color and linestyle in a string is provided as a convenience;\\nin practice if you were creating plots programmatically you might prefer not to have to\\nmunge strings together to create plots with the desired style. The same plot could also\\nhave been expressed more explicitly as:\\nax.plot(x, y, linestyle='--', color='g')\\nThere are a number of color abbreviations provided for commonly-used colors, but any\\ncolor on the spectrum can be used by specifying its RGB value (for example, '#CECE\\nCE'). You can see the full set of linestyles by looking at the docstring for plot.\\nLine plots can additionally have markers to highlight the actual data points. Since mat-\\nplotlib creates a continuous line plot, interpolating between points, it can occasionally\\nbe unclear where the points lie. The marker can be part of the style string, which must\\nhave color followed by marker type and line style (see Figure 8-6):\\nIn [28]: plt.plot(randn(30).cumsum(), 'ko--')\\n224 | Chapter 8: \\u2002Plotting and Visualization\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 240, 'page_label': '225'}, page_content=\"This could also have been written more explicitly as:\\nplot(randn(30).cumsum(), color='k', linestyle='dashed', marker='o')\\nFor line plots, you will notice that subsequent points are linearly interpolated by de-\\nfault. This can be altered with the drawstyle option:\\nIn [30]: data = randn(30).cumsum()\\nIn [31]: plt.plot(data, 'k--', label='Default')\\nOut[31]: [<matplotlib.lines.Line2D at 0x461cdd0>]\\nIn [32]: plt.plot(data, 'k-', drawstyle='steps-post', label='steps-post')\\nOut[32]: [<matplotlib.lines.Line2D at 0x461f350>]\\nIn [33]: plt.legend(loc='best')\\nTicks, Labels, and Legends\\nFor most kinds of plot decorations, there are two main ways to do things: using the\\nprocedural pyplot interface (which will be very familiar to MATLAB users) and the\\nmore object-oriented native matplotlib API.\\nThe pyplot interface, designed for interactive use, consists of methods like xlim,\\nxticks, and xticklabels. These control the plot range, tick locations, and tick labels,\\nrespectively. They can be used in two ways:\\n• Called with no arguments returns the current parameter value. For example \\nplt.xlim() returns the current X axis plotting range\\nFigure 8-6. Line plot with markers example\\nA Brief matplotlib API Primer | 225\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 241, 'page_label': '226'}, page_content=\"• Called with parameters sets the parameter value. So plt.xlim([0, 10]), sets the X\\naxis range to 0 to 10\\nAll such methods act on the active or most recently-created AxesSubplot. Each of them\\ncorresponds to two methods on the subplot object itself; in the case of xlim these are \\nax.get_xlim and ax.set_xlim. I prefer to use the subplot instance methods myself in\\nthe interest of being explicit (and especially when working with multiple subplots), but\\nyou can certainly use whichever you find more convenient.\\nSetting the title, axis labels, ticks, and ticklabels\\nTo illustrate customizing the axes, I’ll create a simple figure and plot of a random walk\\n(see Figure 8-8):\\nIn [34]: fig = plt.figure(); ax = fig.add_subplot(1, 1, 1)\\nIn [35]: ax.plot(randn(1000).cumsum())\\nTo change the X axis ticks, it’s easiest to use set_xticks and set_xticklabels. The\\nformer instructs matplotlib where to place the ticks along the data range; by default\\nthese locations will also be the labels. But we can set any other values as the labels using\\nset_xticklabels:\\nIn [36]: ticks = ax.set_xticks([0, 250, 500, 750, 1000])\\nIn [37]: labels = ax.set_xticklabels(['one', 'two', 'three', 'four', 'five'],\\n   ....:                             rotation=30, fontsize='small')\\nLastly, set_xlabel gives a name to the X axis and set_title the subplot title:\\nFigure 8-7. Line plot with different drawstyle options\\n226 | Chapter 8: \\u2002Plotting and Visualization\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 242, 'page_label': '227'}, page_content=\"In [38]: ax.set_title('My first matplotlib plot')\\nOut[38]: <matplotlib.text.Text at 0x7f9190912850>\\nIn [39]: ax.set_xlabel('Stages')\\nSee Figure 8-9 for the resulting figure. Modifying the Y axis consists of the same process,\\nsubstituting y for x in the above.\\nFigure 8-9. Simple plot for illustrating xticks\\nFigure 8-8. Simple plot for illustrating xticks\\nA Brief matplotlib API Primer | 227\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 243, 'page_label': '228'}, page_content=\"Adding legends\\nLegends are another critical element for identifying plot elements. There are a couple\\nof ways to add one. The easiest is to pass the label argument when adding each piece\\nof the plot:\\nIn [40]: fig = plt.figure(); ax = fig.add_subplot(1, 1, 1)\\nIn [41]: ax.plot(randn(1000).cumsum(), 'k', label='one')\\nOut[41]: [<matplotlib.lines.Line2D at 0x4720a90>]\\nIn [42]: ax.plot(randn(1000).cumsum(), 'k--', label='two')\\nOut[42]: [<matplotlib.lines.Line2D at 0x4720f90>]\\nIn [43]: ax.plot(randn(1000).cumsum(), 'k.', label='three')\\nOut[43]: [<matplotlib.lines.Line2D at 0x4723550>]\\nOnce you’ve done this, you can either call ax.legend() or plt.legend() to automatically\\ncreate a legend:\\nIn [44]: ax.legend(loc='best')\\nSee Figure 8-10. The loc tells matplotlib where to place the plot. If you aren’t picky\\n'best' is a good option, as it will choose a location that is most out of the way. To\\nexclude one or more elements from the legend, pass no label or label='_nolegend_'.\\nAnnotations and Drawing on a Subplot\\nIn addition to the standard plot types, you may wish to draw your own plot annotations,\\nwhich could consist of text, arrows, or other shapes.\\nFigure 8-10. Simple plot with 3 lines and legend\\n228 | Chapter 8: \\u2002Plotting and Visualization\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 244, 'page_label': '229'}, page_content=\"Annotations and text can be added using the text, arrow, and annotate functions.\\ntext draws text at given coordinates (x, y) on the plot with optional custom styling:\\nax.text(x, y, 'Hello world!',\\n        family='monospace', fontsize=10)\\nAnnotations can draw both text and arrows arranged appropriately. As an example,\\nlet’s plot the closing S&P 500 index price since 2007 (obtained from Yahoo! Finance)\\nand annotate it with some of the important dates from the 2008-2009 financial crisis.\\nSee Figure 8-11 for the result:\\nfrom datetime import datetime\\nfig = plt.figure()\\nax = fig.add_subplot(1, 1, 1)\\ndata = pd.read_csv('ch08/spx.csv', index_col=0, parse_dates=True)\\nspx = data['SPX']\\nspx.plot(ax=ax, style='k-')\\ncrisis_data = [\\n    (datetime(2007, 10, 11), 'Peak of bull market'),\\n    (datetime(2008, 3, 12), 'Bear Stearns Fails'),\\n    (datetime(2008, 9, 15), 'Lehman Bankruptcy')\\n]\\nfor date, label in crisis_data:\\n    ax.annotate(label, xy=(date, spx.asof(date) + 50),\\n                xytext=(date, spx.asof(date) + 200),\\n                arrowprops=dict(facecolor='black'),\\n                horizontalalignment='left', verticalalignment='top')\\n# Zoom in on 2007-2010\\nax.set_xlim(['1/1/2007', '1/1/2011'])\\nax.set_ylim([600, 1800])\\nax.set_title('Important dates in 2008-2009 financial crisis')\\nSee the online matplotlib gallery for many more annotation examples to learn from.\\nDrawing shapes requires some more care. matplotlib has objects that represent many\\ncommon shapes, referred to as patches. Some of these, like Rectangle and Circle are\\nfound in matplotlib.pyplot, but the full set is located in matplotlib.patches.\\nTo add a shape to a plot, you create the patch object shp and add it to a subplot by\\ncalling ax.add_patch(shp) (see Figure 8-12):\\nfig = plt.figure()\\nax = fig.add_subplot(1, 1, 1)\\nrect = plt.Rectangle((0.2, 0.75), 0.4, 0.15, color='k', alpha=0.3)\\ncirc = plt.Circle((0.7, 0.2), 0.15, color='b', alpha=0.3)\\npgon = plt.Polygon([[0.15, 0.15], [0.35, 0.4], [0.2, 0.6]],\\n                   color='g', alpha=0.5)\\nA Brief matplotlib API Primer | 229\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 245, 'page_label': '230'}, page_content='ax.add_patch(rect)\\nax.add_patch(circ)\\nax.add_patch(pgon)\\nFigure 8-11. Important dates in 2008-2009 financial crisis\\nFigure 8-12. Figure composed from 3 different patches\\nIf you look at the implementation of many familiar plot types, you will see that they\\nare assembled from patches.\\n230 | Chapter 8: \\u2002Plotting and Visualization\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 246, 'page_label': '231'}, page_content=\"Saving Plots to File\\nThe active figure can be saved to file using plt.savefig. This method is equivalent to\\nthe figure object’s savefig instance method. For example, to save an SVG version of a\\nfigure, you need only type:\\nplt.savefig('figpath.svg')\\nThe file type is inferred from the file extension. So if you used .pdf instead you would\\nget a PDF. There are a couple of important options that I use frequently for publishing\\ngraphics: dpi, which controls the dots-per-inch resolution, and bbox_inches, which can\\ntrim the whitespace around the actual figure. To get the same plot as a PNG above with\\nminimal whitespace around the plot and at 400 DPI, you would do:\\nplt.savefig('figpath.png', dpi=400, bbox_inches='tight')\\nsavefig doesn’t have to write to disk; it can also write to any file-like object, such as a\\nStringIO:\\nfrom io import StringIO\\nbuffer = StringIO()\\nplt.savefig(buffer)\\nplot_data = buffer.getvalue()\\nFor example, this is useful for serving dynamically-generated images over the web.\\nTable 8-2. Figure.savefig options\\nArgument Description\\nfname String containing a filepath or a Python file-like object. The figure format is inferred from the file\\nextension, e.g. .pdf for PDF or .png for PNG.\\ndpi The figure resolution in dots per inch; defaults to 100 out of the box but can be configured\\nfacecolor, edge\\ncolor\\nThe color of the figure background outside of the subplots. 'w' (white), by default\\nformat The explicit file format to use ('png', 'pdf', 'svg', 'ps', 'eps', ...)\\nbbox_inches The portion of the figure to save. If 'tight' is passed, will attempt to trim the empty space around\\nthe figure\\nmatplotlib Configuration\\nmatplotlib comes configured with color schemes and defaults that are geared primarily\\ntoward preparing figures for publication. Fortunately, nearly all of the default behavior\\ncan be customized via an extensive set of global parameters governing figure size, sub-\\nplot spacing, colors, font sizes, grid styles, and so on. There are two main ways to\\ninteract with the matplotlib configuration system. The first is programmatically from\\nPython using the rc method. For example, to set the global default figure size to be 10\\nx 10, you could enter:\\nplt.rc('figure', figsize=(10, 10))\\nA Brief matplotlib API Primer | 231\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 247, 'page_label': '232'}, page_content=\"The first argument to rc is the component you wish to customize, such as 'figure',\\n'axes', 'xtick', 'ytick', 'grid', 'legend' or many others. After that can follow a\\nsequence of keyword arguments indicating the new parameters. An easy way to write\\ndown the options in your program is as a dict:\\nfont_options = {'family' : 'monospace',\\n                'weight' : 'bold',\\n                'size'   : 'small'}\\nplt.rc('font', **font_options)\\nFor more extensive customization and to see a list of all the options, matplotlib comes\\nwith a configuration file matplotlibrc in the matplotlib/mpl-data directory. If you cus-\\ntomize this file and place it in your home directory titled .matplotlibrc, it will be loaded\\neach time you use matplotlib.\\nPlotting Functions in pandas\\nAs you’ve seen, matplotlib is actually a fairly low-level tool. You assemble a plot from\\nits base components: the data display (the type of plot: line, bar, box, scatter, contour,\\netc.), legend, title, tick labels, and other annotations. Part of the reason for this is that\\nin many cases the data needed to make a complete plot is spread across many objects.\\nIn pandas we have row labels, column labels, and possibly grouping information. This\\nmeans that many kinds of fully-formed plots that would ordinarily require a lot of\\nmatplotlib code can be expressed in one or two concise statements. Therefore, pandas\\nhas an increasing number of high-level plotting methods for creating standard visual-\\nizations that take advantage of how data is organized in DataFrame objects.\\nAs of this writing, the plotting functionality in pandas is undergoing\\nquite a bit of work. As part of the 2012 Google Summer of Code pro-\\ngram, a student is working full time to add features and to make the\\ninterface more consistent and usable. Thus, it’s possible that this code\\nmay fall out-of-date faster than the other things in this book. The online\\npandas documentation will be the best resource in that event.\\nLine Plots\\nSeries and DataFrame each have a plot method for making many different plot types.\\nBy default, they make line plots (see Figure 8-13):\\nIn [55]: s = Series(np.random.randn(10).cumsum(), index=np.arange(0, 100, 10))\\nIn [56]: s.plot()\\nThe Series object’s index is passed to matplotlib for plotting on the X axis, though this\\ncan be disabled by passing use_index=False. The X axis ticks and limits can be adjusted\\nusing the xticks and xlim options, and Y axis respectively using yticks and ylim. See\\n232 | Chapter 8: \\u2002Plotting and Visualization\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 248, 'page_label': '233'}, page_content=\"Table 8-3 for a full listing of plot options. I’ll comment on a few more of them through-\\nout this section and leave the rest to you to explore.\\nMost of pandas’s plotting methods accept an optional ax parameter, which can be a\\nmatplotlib subplot object. This gives you more flexible placement of subplots in a grid\\nlayout. There will be more on this in the later section on the matplotlib API.\\nDataFrame’s plot method plots each of its columns as a different line on the same\\nsubplot, creating a legend automatically (see Figure 8-14):\\nIn [57]: df = DataFrame(np.random.randn(10, 4).cumsum(0),\\n   ....:                columns=['A', 'B', 'C', 'D'],\\n   ....:                index=np.arange(0, 100, 10))\\nIn [58]: df.plot()\\nAdditional keyword arguments to plot are passed through to the re-\\nspective matplotlib plotting function, so you can further customize\\nthese plots by learning more about the matplotlib API.\\nTable 8-3. Series.plot method arguments\\nArgument Description\\nlabel Label for plot legend\\nax matplotlib subplot object to plot on. If nothing passed, uses active matplotlib subplot\\nstyle Style string, like 'ko--', to be passed to matplotlib.\\nalpha The plot fill opacity (from 0 to 1)\\nFigure 8-13. Simple Series plot example\\nPlotting Functions in pandas | 233\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 249, 'page_label': '234'}, page_content=\"Argument Description\\nkind Can be 'line', 'bar', 'barh', 'kde'\\nlogy Use logarithmic scaling on the Y axis\\nuse_index Use the object index for tick labels\\nrot Rotation of tick labels (0 through 360)\\nxticks Values to use for X axis ticks\\nyticks Values to use for Y axis ticks\\nxlim X axis limits (e.g. [0, 10])\\nylim Y axis limits\\ngrid Display axis grid (on by default)\\nDataFrame has a number of options allowing some flexibility with how the columns\\nare handled; for example, whether to plot them all on the same subplot or to create\\nseparate subplots. See Table 8-4 for more on these.\\nTable 8-4. DataFrame-specific plot arguments\\nArgument Description\\nsubplots Plot each DataFrame column in a separate subplot\\nsharex If subplots=True, share the same X axis, linking ticks and limits\\nsharey If subplots=True, share the same Y axis\\nfigsize Size of figure to create as tuple\\nFigure 8-14. Simple DataFrame plot example\\n234 | Chapter 8: \\u2002Plotting and Visualization\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 250, 'page_label': '235'}, page_content=\"Argument Description\\ntitle Plot title as string\\nlegend Add a subplot legend (True by default)\\nsort_columns Plot columns in alphabetical order; by default uses existing column order\\nFor time series plotting, see Chapter 10.\\nBar Plots\\nMaking bar plots instead of line plots is a simple as passing kind='bar' (for vertical\\nbars) or kind='barh' (for horizontal bars). In this case, the Series or DataFrame index\\nwill be used as the X (bar) or Y (barh) ticks (see Figure 8-15):\\nIn [59]: fig, axes = plt.subplots(2, 1)\\nIn [60]: data = Series(np.random.rand(16), index=list('abcdefghijklmnop'))\\nIn [61]: data.plot(kind='bar', ax=axes[0], color='k', alpha=0.7)\\nOut[61]: <matplotlib.axes.AxesSubplot at 0x4ee7750>\\nIn [62]: data.plot(kind='barh', ax=axes[1], color='k', alpha=0.7)\\nFor more on the plt.subplots function and matplotlib axes and figures,\\nsee the later section in this chapter.\\nWith a DataFrame, bar plots group the values in each row together in a group in bars,\\nside by side, for each value. See Figure 8-16:\\nIn [63]: df = DataFrame(np.random.rand(6, 4),\\n   ....:                index=['one', 'two', 'three', 'four', 'five', 'six'],\\n   ....:                columns=pd.Index(['A', 'B', 'C', 'D'], name='Genus'))\\nIn [64]: df\\nOut[64]: \\nGenus         A         B         C         D\\none    0.301686  0.156333  0.371943  0.270731\\ntwo    0.750589  0.525587  0.689429  0.358974\\nthree  0.381504  0.667707  0.473772  0.632528\\nfour   0.942408  0.180186  0.708284  0.641783\\nfive   0.840278  0.909589  0.010041  0.653207\\nsix    0.062854  0.589813  0.811318  0.060217\\nIn [65]: df.plot(kind='bar')\\nPlotting Functions in pandas | 235\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 251, 'page_label': '236'}, page_content=\"Figure 8-15. Horizonal and vertical bar plot example\\nNote that the name “Genus” on the DataFrame’s columns is used to title the legend.\\nStacked bar plots are created from a DataFrame by passing stacked=True, resulting in\\nthe value in each row being stacked together (see Figure 8-17):\\nIn [67]: df.plot(kind='barh', stacked=True, alpha=0.5)\\nA useful recipe for bar plots (as seen in an earlier chapter) is to visualize\\na Series’s value frequency using value_counts: s.value_counts\\n().plot(kind='bar')\\nReturning to the tipping data set used earlier in the book, suppose we wanted to make\\na stacked bar plot showing the percentage of data points for each party size on each\\nday. I load the data using read_csv and make a cross-tabulation by day and party size:\\nIn [68]: tips = pd.read_csv('ch08/tips.csv')\\nIn [69]: party_counts = pd.crosstab(tips.day, tips.size)\\nIn [70]: party_counts\\nOut[70]: \\nsize  1   2   3   4  5  6\\nday                      \\nFri   1  16   1   1  0  0\\nSat   2  53  18  13  1  0\\nSun   0  39  15  18  3  1\\nThur  1  48   4   5  1  3\\n236 | Chapter 8: \\u2002Plotting and Visualization\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 252, 'page_label': '237'}, page_content='# Not many 1- and 6-person parties\\nIn [71]: party_counts = party_counts.ix[:, 2:5]\\nFigure 8-16. DataFrame bar plot example\\nFigure 8-17. DataFrame stacked bar plot example\\nThen, normalize so that each row sums to 1 (I have to cast to float to avoid integer\\ndivision issues on Python 2.7) and make the plot (see Figure 8-18):\\n# Normalize to sum to 1\\nIn [72]: party_pcts = party_counts.div(party_counts.sum(1).astype(float), axis=0)\\nPlotting Functions in pandas | 237\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 253, 'page_label': '238'}, page_content=\"In [73]: party_pcts\\nOut[73]: \\nsize         2         3         4         5\\nday                                         \\nFri   0.888889  0.055556  0.055556  0.000000\\nSat   0.623529  0.211765  0.152941  0.011765\\nSun   0.520000  0.200000  0.240000  0.040000\\nThur  0.827586  0.068966  0.086207  0.017241\\nIn [74]: party_pcts.plot(kind='bar', stacked=True)\\nFigure 8-18. Fraction of parties by size on each day\\nSo you can see that party sizes appear to increase on the weekend in this data set.\\nHistograms and Density Plots\\nA histogram, with which you may be well-acquainted, is a kind of bar plot that gives a\\ndiscretized display of value frequency. The data points are split into discrete, evenly\\nspaced bins, and the number of data points in each bin is plotted. Using the tipping\\ndata from before, we can make a histogram of tip percentages of the total bill using the \\nhist method on the Series (see Figure 8-19):\\nIn [76]: tips['tip_pct'] = tips['tip'] / tips['total_bill']\\nIn [77]: tips['tip_pct'].hist(bins=50)\\n238 | Chapter 8: \\u2002Plotting and Visualization\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 254, 'page_label': '239'}, page_content=\"Figure 8-19. Histogram of tip percentages\\nA related plot type is a density plot, which is formed by computing an estimate of a\\ncontinuous probability distribution that might have generated the observed data. A\\nusual procedure is to approximate this distribution as a mixture of kernels, that is,\\nsimpler distributions like the normal (Gaussian) distribution. Thus, density plots are\\nalso known as KDE (kernel density estimate) plots. Using plot with kind='kde' makes\\na density plot using the standard mixture-of-normals KDE (see Figure 8-20):\\nIn [79]: tips['tip_pct'].plot(kind='kde')\\nThese two plot types are often plotted together; the histogram in normalized form (to\\ngive a binned density) with a kernel density estimate plotted on top. As an example,\\nconsider a bimodal distribution consisting of draws from two different standard normal\\ndistributions (see Figure 8-21):\\nIn [81]: comp1 = np.random.normal(0, 1, size=200)  # N(0, 1)\\nIn [82]: comp2 = np.random.normal(10, 2, size=200)  # N(10, 4)\\nIn [83]: values = Series(np.concatenate([comp1, comp2]))\\nIn [84]: values.hist(bins=100, alpha=0.3, color='k', normed=True)\\nOut[84]: <matplotlib.axes.AxesSubplot at 0x5cd2350>\\nIn [85]: values.plot(kind='kde', style='k--')\\nScatter Plots\\nScatter plots are a useful way of examining the relationship between two one-dimen-\\nsional data series. matplotlib has a scatter plotting method that is the workhorse of\\nPlotting Functions in pandas | 239\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 255, 'page_label': '240'}, page_content=\"making these kinds of plots. To give an example, I load the macrodata dataset from the\\nstatsmodels project, select a few variables, then compute log differences:\\nIn [86]: macro = pd.read_csv('ch08/macrodata.csv')\\nIn [87]: data = macro[['cpi', 'm1', 'tbilrate', 'unemp']]\\nIn [88]: trans_data = np.log(data).diff().dropna()\\nFigure 8-20. Density plot of tip percentages\\nFigure 8-21. Normalized histogram of normal mixture with density estimate\\n240 | Chapter 8: \\u2002Plotting and Visualization\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 256, 'page_label': '241'}, page_content=\"In [89]: trans_data[-5:]\\nOut[89]: \\n          cpi        m1  tbilrate     unemp\\n198 -0.007904  0.045361 -0.396881  0.105361\\n199 -0.021979  0.066753 -2.277267  0.139762\\n200  0.002340  0.010286  0.606136  0.160343\\n201  0.008419  0.037461 -0.200671  0.127339\\n202  0.008894  0.012202 -0.405465  0.042560\\nIt’s easy to plot a simple scatter plot using plt.scatter (see Figure 8-22):\\nIn [91]: plt.scatter(trans_data['m1'], trans_data['unemp'])\\nOut[91]: <matplotlib.collections.PathCollection at 0x43c31d0>\\nIn [92]: plt.title('Changes in log %s vs. log %s' % ('m1', 'unemp'))\\nFigure 8-22. A simple scatter plot\\nIn exploratory data analysis it’s helpful to be able to look at all the scatter plots among\\na group of variables; this is known as a pairs plot or scatter plot matrix. Making such a\\nplot from scratch is a bit of work, so pandas has a scatter_matrix function for creating\\none from a DataFrame. It also supports placing histograms or density plots of each\\nvariable along the diagonal. See Figure 8-23 for the resulting plot:\\nIn [93]: scatter_matrix(trans_data, diagonal='kde', color='k', alpha=0.3)\\nPlotting Maps: Visualizing Haiti Earthquake Crisis Data\\nUshahidi is a non-profit software company that enables crowdsourcing of information\\nrelated to natural disasters and geopolitical events via text message. Many of these data\\nsets are then published on their website for analysis and visualization. I downloaded\\nPlotting Maps: Visualizing Haiti Earthquake Crisis Data | 241\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 257, 'page_label': '242'}, page_content=\"the data collected during the 2010 Haiti earthquake crisis and aftermath, and I’ll show\\nyou how I prepared the data for analysis and visualization using pandas and other tools\\nwe have looked at thus far. After downloading the CSV file from the above link, we can\\nload it into a DataFrame using read_csv:\\nIn [94]: data = pd.read_csv('ch08/Haiti.csv')\\nIn [95]: data\\nOut[95]: \\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 3593 entries, 0 to 3592\\nData columns:\\nSerial            3593  non-null values\\nINCIDENT TITLE    3593  non-null values\\nINCIDENT DATE     3593  non-null values\\nLOCATION          3593  non-null values\\nDESCRIPTION       3593  non-null values\\nCATEGORY          3587  non-null values\\nLATITUDE          3593  non-null values\\nLONGITUDE         3593  non-null values\\nAPPROVED          3593  non-null values\\nVERIFIED          3593  non-null values\\ndtypes: float64(2), int64(1), object(7)\\nIt’s easy now to tinker with this data set to see what kinds of things we might want to\\ndo with it. Each row represents a report sent from someone’s mobile phone indicating\\nan emergency or some other problem. Each has an associated timestamp and a location\\nas latitude and longitude:\\nIn [96]: data[['INCIDENT DATE', 'LATITUDE', 'LONGITUDE']][:10]\\nOut[96]: \\n      INCIDENT DATE   LATITUDE   LONGITUDE\\nFigure 8-23. Scatter plot matrix of statsmodels macro data\\n242 | Chapter 8: \\u2002Plotting and Visualization\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 258, 'page_label': '243'}, page_content=\"0  05/07/2010 17:26  18.233333  -72.533333\\n1  28/06/2010 23:06  50.226029    5.729886\\n2  24/06/2010 16:21  22.278381  114.174287\\n3  20/06/2010 21:59  44.407062    8.933989\\n4  18/05/2010 16:26  18.571084  -72.334671\\n5  26/04/2010 13:14  18.593707  -72.310079\\n6  26/04/2010 14:19  18.482800  -73.638800\\n7  26/04/2010 14:27  18.415000  -73.195000\\n8  15/03/2010 10:58  18.517443  -72.236841\\n9  15/03/2010 11:00  18.547790  -72.410010\\nThe CATEGORY field contains a comma-separated list of codes indicating the type of\\nmessage:\\nIn [97]: data['CATEGORY'][:6]\\nOut[97]: \\n0          1. Urgences | Emergency, 3. Public Health, \\n1    1. Urgences | Emergency, 2. Urgences logistiques \\n2    2. Urgences logistiques | Vital Lines, 8. Autre |\\n3                            1. Urgences | Emergency, \\n4                            1. Urgences | Emergency, \\n5                       5e. Communication lines down, \\nName: CATEGORY\\nIf you notice above in the data summary, some of the categories are missing, so we\\nmight want to drop these data points. Additionally, calling describe shows that there\\nare some aberrant locations:\\nIn [98]: data.describe()\\nOut[98]: \\n            Serial     LATITUDE    LONGITUDE\\ncount  3593.000000  3593.000000  3593.000000\\nmean   2080.277484    18.611495   -72.322680\\nstd    1171.100360     0.738572     3.650776\\nmin       4.000000    18.041313   -74.452757\\n25%    1074.000000    18.524070   -72.417500\\n50%    2163.000000    18.539269   -72.335000\\n75%    3088.000000    18.561820   -72.293570\\nmax    4052.000000    50.226029   114.174287\\nCleaning the bad locations and removing the missing categories is now fairly simple:\\nIn [99]: data = data[(data.LATITUDE > 18) & (data.LATITUDE < 20) &\\n   ....:             (data.LONGITUDE > -75) & (data.LONGITUDE < -70)\\n   ....:             & data.CATEGORY.notnull()]\\nNow we might want to do some analysis or visualization of this data by category, but\\neach category field may have multiple categories. Additionally, each category is given\\nas a code plus an English and possibly also a French code name. Thus, a little bit of\\nwrangling is required to get the data into a more agreeable form. First, I wrote these\\ntwo functions to get a list of all the categories and to split each category into a code and\\nan English name:\\ndef to_cat_list(catstr):\\n    stripped = (x.strip() for x in catstr.split(','))\\nPlotting Maps: Visualizing Haiti Earthquake Crisis Data | 243\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 259, 'page_label': '244'}, page_content=\"return [x for x in stripped if x]\\ndef get_all_categories(cat_series):\\n    cat_sets = (set(to_cat_list(x)) for x in cat_series)\\n    return sorted(set.union(*cat_sets))\\ndef get_english(cat):\\n    code, names = cat.split('.')\\n    if '|' in names:\\n        names = names.split(' | ')[1]\\n    return code, names.strip()\\nYou can test out that the get_english function does what you expect:\\nIn [101]: get_english('2. Urgences logistiques | Vital Lines')\\nOut[101]: ('2', 'Vital Lines')\\nNow, I make a dict mapping code to name because we’ll use the codes for analysis.\\nWe’ll use this later when adorning plots (note the use of a generator expression in lieu\\nof a list comprehension):\\nIn [102]: all_cats = get_all_categories(data.CATEGORY)\\n# Generator expression\\nIn [103]: english_mapping = dict(get_english(x) for x in all_cats)\\nIn [104]: english_mapping['2a']\\nOut[104]: 'Food Shortage'\\nIn [105]: english_mapping['6c']\\nOut[105]: 'Earthquake and aftershocks'\\nThere are many ways to go about augmenting the data set to be able to easily select\\nrecords by category. One way is to add indicator (or dummy) columns, one for each\\ncategory. To do that, first extract the unique category codes and construct a DataFrame\\nof zeros having those as its columns and the same index as data:\\ndef get_code(seq):\\n    return [x.split('.')[0] for x in seq if x]\\nall_codes = get_code(all_cats)\\ncode_index = pd.Index(np.unique(all_codes))\\ndummy_frame = DataFrame(np.zeros((len(data), len(code_index))),\\n                        index=data.index, columns=code_index)\\nIf all goes well, dummy_frame should look something like this:\\nIn [107]: dummy_frame.ix[:, :6]\\nOut[107]: \\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 3569 entries, 0 to 3592\\nData columns:\\n1     3569  non-null values\\n1a    3569  non-null values\\n1b    3569  non-null values\\n1c    3569  non-null values\\n244 | Chapter 8: \\u2002Plotting and Visualization\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 260, 'page_label': '245'}, page_content=\"1d    3569  non-null values\\n2     3569  non-null values\\ndtypes: float64(6)\\nAs you recall, the trick is then to set the appropriate entries of each row to 1, lastly\\njoining this with data:\\nfor row, cat in zip(data.index, data.CATEGORY):\\n    codes = get_code(to_cat_list(cat))\\n    dummy_frame.ix[row, codes] = 1\\ndata = data.join(dummy_frame.add_prefix('category_'))\\ndata finally now has new columns like:\\nIn [109]: data.ix[:, 10:15]\\nOut[109]: \\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 3569 entries, 0 to 3592\\nData columns:\\ncategory_1     3569  non-null values\\ncategory_1a    3569  non-null values\\ncategory_1b    3569  non-null values\\ncategory_1c    3569  non-null values\\ncategory_1d    3569  non-null values\\ndtypes: float64(5)\\nLet’s make some plots! As this is spatial data, we’d like to plot the data by category on\\na map of Haiti. The basemap toolkit (http://matplotlib.github.com/basemap), an add-on\\nto matplotlib, enables plotting 2D data on maps in Python. basemap provides many\\ndifferent globe projections and a means for transforming projecting latitude and lon-\\ngitude coordinates on the globe onto a two-dimensional matplotlib plot. After some\\ntrial and error and using the above data as a guideline, I wrote this function which draws\\na simple black and white map of Haiti:\\nfrom mpl_toolkits.basemap import Basemap\\nimport matplotlib.pyplot as plt\\ndef basic_haiti_map(ax=None, lllat=17.25, urlat=20.25,\\n                    lllon=-75, urlon=-71):\\n    # create polar stereographic Basemap instance.\\n    m = Basemap(ax=ax, projection='stere',\\n                lon_0=(urlon + lllon) / 2,\\n                lat_0=(urlat + lllat) / 2,\\n                llcrnrlat=lllat, urcrnrlat=urlat,\\n                llcrnrlon=lllon, urcrnrlon=urlon,\\n                resolution='f')\\n    # draw coastlines, state and country boundaries, edge of map.\\n    m.drawcoastlines()\\n    m.drawstates()\\n    m.drawcountries()\\n    return m\\nThe idea, now, is that the returned Basemap object, knows how to transform coordinates\\nonto the canvas. I wrote the following code to plot the data observations for a number\\nPlotting Maps: Visualizing Haiti Earthquake Crisis Data | 245\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 261, 'page_label': '246'}, page_content=\"of report categories. For each category, I filter down the data set to the coordinates\\nlabeled by that category, plot a Basemap on the appropriate subplot, transform the co-\\nordinates, then plot the points using the Basemap’s plot method:\\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 10))\\nfig.subplots_adjust(hspace=0.05, wspace=0.05)\\nto_plot = ['2a', '1', '3c', '7a']\\nlllat=17.25; urlat=20.25; lllon=-75; urlon=-71\\nfor code, ax in zip(to_plot, axes.flat):\\n    m = basic_haiti_map(ax, lllat=lllat, urlat=urlat,\\n                        lllon=lllon, urlon=urlon)\\n    cat_data = data[data['category_%s' % code] == 1]\\n    # compute map proj coordinates.\\n    x, y = m(cat_data.LONGITUDE, cat_data.LATITUDE)\\n    m.plot(x, y, 'k.', alpha=0.5)\\n    ax.set_title('%s: %s' % (code, english_mapping[code]))\\nThe resulting figure can be seen in Figure 8-24.\\nIt seems from the plot that most of the data is concentrated around the most populous\\ncity, Port-au-Prince. basemap allows you to overlap additional map data which comes\\nfrom what are called shapefiles. I first downloaded a shapefile with roads in Port-au-\\nPrince (see http://cegrp.cga.harvard.edu/haiti/?q=resources_data). The Basemap object\\nconveniently has a readshapefile method so that, after extracting the road data archive,\\nI added just the following lines to my code:\\nFigure 8-24. Haiti crisis data for 4 categories\\n246 | Chapter 8: \\u2002Plotting and Visualization\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 262, 'page_label': '247'}, page_content=\"shapefile_path = 'ch08/PortAuPrince_Roads/PortAuPrince_Roads'\\nm.readshapefile(shapefile_path, 'roads')\\nAfter a little more trial and error with the latitude and longitude boundaries, I was able\\nto make Figure 8-25 for the “Food shortage” category.\\nPython Visualization Tool Ecosystem\\nAs is common with open source, there are a plethora of options for creating graphics\\nin Python (too many to list). In addition to open source, there are numerous commercial\\nlibraries with Python bindings.\\nIn this chapter and throughout the book, I have been primarily concerned with mat-\\nplotlib as it is the most widely used plotting tool in Python. While it’s an important\\npart of the scientific Python ecosystem, matplotlib has plenty of shortcomings when it\\ncomes to the creation and display of statistical graphics. MATLAB users will likely find\\nmatplotlib familiar, while R users (especially users of the excellent ggplot2 and trel\\nlis packages) may be somewhat disappointed (at least as of this writing). It is possible\\nto make beautiful plots for display on the web in matplotlib, but doing so often requires\\nsignificant effort as the library is designed for the printed page. Aesthetics aside, it is\\nsufficient for most needs. In pandas, I, along with the other developers, have sought to\\nbuild a convenient user interface that makes it easier to make most kinds of plots com-\\nmonplace in data analysis.\\nThere are a number of other visualization tools in wide use. I list a few of them here\\nand encourage you to explore the ecosystem.\\nFigure 8-25. Food shortage reports in Port-au-Prince during the Haiti earthquake crisis\\nPython Visualization Tool Ecosystem | 247\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 263, 'page_label': '248'}, page_content='Chaco\\nChaco (http://code.enthought.com/chaco/), developed by Enthought, is a plotting tool-\\nkit suitable both for static plotting and interactive visualizations. It is especially well-\\nsuited for expressing complex visualizations with data interrelationships. Compared\\nwith matplotlib, Chaco has much better support for interacting with plot elements and\\nrendering is very fast, making it a good choice for building interactive GUI applications.\\nFigure 8-26. A Chaco example plot\\nmayavi\\nThe mayavi project, developed by Prabhu Ramachandran, Gaël Varoquaux, and others,\\nis a 3D graphics toolkit built on the open source C++ graphics library VTK. mayavi,\\nlike matplotlib, integrates with IPython so that it is easy to use interactively. The plots\\ncan be panned, rotated, and zoomed using the mouse and keyboard. I used mayavi to\\nmake one of the illustrations of broadcasting in Chapter 12. While I don’t show any\\nmayavi-using code here, there is plenty of documentation and examples available on-\\nline. In many cases, I believe it is a good alternative to a technology like WebGL, though\\nthe graphics are harder to share in interactive form.\\nOther Packages\\nOf course, there are numerous other visualization libraries and applications available\\nin Python: PyQwt, Veusz, gnuplot-py, biggles, and others. I have seen PyQwt put to\\ngood use in GUI applications built using the Qt application framework using PyQt.\\nWhile many of these libraries continue to be under active development (some of them\\n248 | Chapter 8: \\u2002Plotting and Visualization\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 264, 'page_label': '249'}, page_content='are part of much larger applications), I have noted in the last few years a general trend\\ntoward web-based technologies and away from desktop graphics. I’ll say a few more\\nwords about this in the next section.\\nThe Future of Visualization Tools?\\nVisualizations built on web technologies (that is, JavaScript-based) appear to be the\\ninevitable future. Doubtlessly you have used many different kinds of static or interactive\\nvisualizations built in Flash or JavaScript over the years. New toolkits (such as d3.js\\nand its numerous off-shoot projects) for building such displays are appearing all the\\ntime. In contrast, development in non web-based visualization has slowed significantly\\nin recent years. This holds true of Python as well as other data analysis and statistical\\ncomputing environments like R.\\nThe development challenge, then, will be in building tighter integration between data\\nanalysis and preparation tools, such as pandas, and the web browser. I am hopeful that\\nthis will become a fruitful point of collaboration between Python and non-Python users\\nas well.\\nPython Visualization Tool Ecosystem | 249\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 265, 'page_label': '250'}, page_content='www.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 266, 'page_label': '251'}, page_content='CHAPTER 9\\nData Aggregation and Group\\nOperations\\nCategorizing a data set and applying a function to each group, whether an aggregation\\nor transformation, is often a critical component of a data analysis workflow. After\\nloading, merging, and preparing a data set, a familiar task is to compute group statistics\\nor possibly pivot tables for reporting or visualization purposes. pandas provides a flex-\\nible and high-performance groupby facility, enabling you to slice and dice, and sum-\\nmarize data sets in a natural way.\\nOne reason for the popularity of relational databases and SQL (which stands for\\n“structured query language”) is the ease with which data can be joined, filtered, trans-\\nformed, and aggregated. However, query languages like SQL are rather limited in the\\nkinds of group operations that can be performed. As you will see, with the expressive-\\nness and power of Python and pandas, we can perform much more complex grouped\\noperations by utilizing any function that accepts a pandas object or NumPy array. In\\nthis chapter, you will learn how to:\\n• Split a pandas object into pieces using one or more keys (in the form of functions,\\narrays, or DataFrame column names)\\n• Computing group summary statistics, like count, mean, or standard deviation, or\\na user-defined function\\n• Apply a varying set of functions to each column of a DataFrame\\n• Apply within-group transformations or other manipulations, like normalization,\\nlinear regression, rank, or subset selection\\n• Compute pivot tables and cross-tabulations\\n• Perform quantile analysis and other data-derived group analyses\\n251\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 267, 'page_label': '252'}, page_content='Aggregation of time series data, a special use case of groupby, is referred\\nto as resampling in this book and will receive separate treatment in\\nChapter 10.\\nGroupBy Mechanics\\nHadley Wickham, an author of many popular packages for the R programming lan-\\nguage, coined the term split-apply-combine for talking about group operations, and I\\nthink that’s a good description of the process. In the first stage of the process, data\\ncontained in a pandas object, whether a Series, DataFrame, or otherwise, is split into\\ngroups based on one or more keys that you provide. The splitting is performed on a\\nparticular axis of an object. For example, a DataFrame can be grouped on its rows\\n(axis=0) or its columns (axis=1). Once this is done, a function is applied to each group,\\nproducing a new value. Finally, the results of all those function applications are com-\\nbined into a result object. The form of the resulting object will usually depend on what’s\\nbeing done to the data. See Figure 9-1 for a mockup of a simple group aggregation.\\nFigure 9-1. Illustration of a group aggregation\\nEach grouping key can take many forms, and the keys do not have to be all of the same\\ntype:\\n• A list or array of values that is the same length as the axis being grouped\\n• A value indicating a column name in a DataFrame\\n252 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 268, 'page_label': '253'}, page_content=\"• A dict or Series giving a correspondence between the values on the axis being\\ngrouped and the group names\\n• A function to be invoked on the axis index or the individual labels in the index\\nNote that the latter three methods are all just shortcuts for producing an array of values\\nto be used to split up the object. Don’t worry if this all seems very abstract. Throughout\\nthis chapter, I will give many examples of all of these methods. To get started, here is\\na very simple small tabular dataset as a DataFrame:\\nIn [13]: df = DataFrame({'key1' : ['a', 'a', 'b', 'b', 'a'],\\n   ....:                 'key2' : ['one', 'two', 'one', 'two', 'one'],\\n   ....:                 'data1' : np.random.randn(5),\\n   ....:                 'data2' : np.random.randn(5)})\\nIn [14]: df\\nOut[14]: \\n      data1     data2 key1 key2\\n0 -0.204708  1.393406    a  one\\n1  0.478943  0.092908    a  two\\n2 -0.519439  0.281746    b  one\\n3 -0.555730  0.769023    b  two\\n4  1.965781  1.246435    a  one\\nSuppose you wanted to compute the mean of the data1 column using the groups labels\\nfrom key1. There are a number of ways to do this. One is to access data1 and call\\ngroupby with the column (a Series) at key1:\\nIn [15]: grouped = df['data1'].groupby(df['key1'])\\nIn [16]: grouped\\nOut[16]: <pandas.core.groupby.SeriesGroupBy at 0x2d78b10>\\nThis grouped variable is now a GroupBy object. It has not actually computed anything\\nyet except for some intermediate data about the group key df['key1']. The idea is that\\nthis object has all of the information needed to then apply some operation to each of\\nthe groups. For example, to compute group means we can call the GroupBy’s mean\\nmethod:\\nIn [17]: grouped.mean()\\nOut[17]: \\nkey1\\na       0.746672\\nb      -0.537585\\nLater, I'll explain more about what’s going on when you call .mean(). The important\\nthing here is that the data (a Series) has been aggregated according to the group key,\\nproducing a new Series that is now indexed by the unique values in the key1 column.\\nThe result index has the name 'key1' because the DataFrame column df['key1'] did.\\nIf instead we had passed multiple arrays as a list, we get something different:\\nIn [18]: means = df['data1'].groupby([df['key1'], df['key2']]).mean()\\nGroupBy Mechanics | 253\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 269, 'page_label': '254'}, page_content=\"In [19]: means\\nOut[19]: \\nkey1  key2\\na     one     0.880536\\n      two     0.478943\\nb     one    -0.519439\\n      two    -0.555730\\nIn this case, we grouped the data using two keys, and the resulting Series now has a\\nhierarchical index consisting of the unique pairs of keys observed:\\nIn [20]: means.unstack()\\nOut[20]: \\nkey2       one       two\\nkey1                    \\na     0.880536  0.478943\\nb    -0.519439 -0.555730\\nIn these examples, the group keys are all Series, though they could be any arrays of the\\nright length:\\nIn [21]: states = np.array(['Ohio', 'California', 'California', 'Ohio', 'Ohio'])\\nIn [22]: years = np.array([2005, 2005, 2006, 2005, 2006])\\nIn [23]: df['data1'].groupby([states, years]).mean()\\nOut[23]: \\nCalifornia  2005    0.478943\\n            2006   -0.519439\\nOhio        2005   -0.380219\\n            2006    1.965781\\nFrequently the grouping information to be found in the same DataFrame as the data\\nyou want to work on. In that case, you can pass column names (whether those are\\nstrings, numbers, or other Python objects) as the group keys:\\nIn [24]: df.groupby('key1').mean()\\nOut[24]: \\n         data1     data2\\nkey1                    \\na     0.746672  0.910916\\nb    -0.537585  0.525384\\nIn [25]: df.groupby(['key1', 'key2']).mean()\\nOut[25]: \\n              data1     data2\\nkey1 key2                    \\na    one   0.880536  1.319920\\n     two   0.478943  0.092908\\nb    one  -0.519439  0.281746\\n     two  -0.555730  0.769023\\nYou may have noticed in the first case df.groupby('key1').mean() that there is no\\nkey2 column in the result. Because df['key2'] is not numeric data, it is said to be a\\nnuisance column, which is therefore excluded from the result. By default, all of the\\n254 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 270, 'page_label': '255'}, page_content=\"numeric columns are aggregated, though it is possible to filter down to a subset as you’ll\\nsee soon.\\nRegardless of the objective in using groupby, a generally useful GroupBy method is \\nsize which return a Series containing group sizes:\\nIn [26]: df.groupby(['key1', 'key2']).size()\\nOut[26]: \\nkey1  key2\\na     one     2\\n      two     1\\nb     one     1\\n      two     1\\nAs of this writing, any missing values in a group key will be excluded\\nfrom the result. It’s possible (and, in fact, quite likely), that by the time\\nyou are reading this there will be an option to include the NA group in\\nthe result.\\nIterating Over Groups\\nThe GroupBy object supports iteration, generating a sequence of 2-tuples containing\\nthe group name along with the chunk of data. Consider the following small example\\ndata set:\\nIn [27]: for name, group in df.groupby('key1'):\\n   ....:     print name\\n   ....:     print group\\n   ....:\\na\\n      data1     data2 key1 key2\\n0 -0.204708  1.393406    a  one\\n1  0.478943  0.092908    a  two\\n4  1.965781  1.246435    a  one\\nb\\n      data1     data2 key1 key2\\n2 -0.519439  0.281746    b  one\\n3 -0.555730  0.769023    b  two\\nIn the case of multiple keys, the first element in the tuple will be a tuple of key values:\\nIn [28]: for (k1, k2), group in df.groupby(['key1', 'key2']):\\n   ....:     print k1, k2\\n   ....:     print group\\n   ....:\\na one\\n      data1     data2 key1 key2\\n0 -0.204708  1.393406    a  one\\n4  1.965781  1.246435    a  one\\na two\\n      data1     data2 key1 key2\\n1  0.478943  0.092908    a  two\\nb one\\n      data1     data2 key1 key2\\nGroupBy Mechanics | 255\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 271, 'page_label': '256'}, page_content=\"2 -0.519439  0.281746    b  one\\nb two\\n     data1     data2 key1 key2\\n3 -0.55573  0.769023    b  two\\nOf course, you can choose to do whatever you want with the pieces of data. A recipe\\nyou may find useful is computing a dict of the data pieces as a one-liner:\\nIn [29]: pieces = dict(list(df.groupby('key1')))\\nIn [30]: pieces['b']\\nOut[30]: \\n      data1     data2 key1 key2\\n2 -0.519439  0.281746    b  one\\n3 -0.555730  0.769023    b  two\\nBy default groupby groups on axis=0, but you can group on any of the other axes. For\\nexample, we could group the columns of our example df here by dtype like so:\\nIn [31]: df.dtypes\\nOut[31]: \\ndata1    float64\\ndata2    float64\\nkey1      object\\nkey2      object\\nIn [32]: grouped = df.groupby(df.dtypes, axis=1)\\nIn [33]: dict(list(grouped))\\nOut[33]: \\n{dtype('float64'):       data1     data2\\n0 -0.204708  1.393406\\n1  0.478943  0.092908\\n2 -0.519439  0.281746\\n3 -0.555730  0.769023\\n4  1.965781  1.246435,\\n dtype('object'):   key1 key2\\n0    a  one\\n1    a  two\\n2    b  one\\n3    b  two\\n4    a  one}\\nSelecting a Column or Subset of Columns\\nIndexing a GroupBy object created from a DataFrame with a column name or array of\\ncolumn names has the effect of selecting those columns for aggregation. This means that:\\ndf.groupby('key1')['data1']\\ndf.groupby('key1')[['data2']]\\nare syntactic sugar for:\\ndf['data1'].groupby(df['key1'])\\ndf[['data2']].groupby(df['key1'])\\n256 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 272, 'page_label': '257'}, page_content=\"Especially for large data sets, it may be desirable to aggregate only a few columns. For\\nexample, in the above data set, to compute means for just the data2 column and get\\nthe result as a DataFrame, we could write:\\nIn [34]: df.groupby(['key1', 'key2'])[['data2']].mean()\\nOut[34]: \\n              data2\\nkey1 key2          \\na    one   1.319920\\n     two   0.092908\\nb    one   0.281746\\n     two   0.769023\\nThe object returned by this indexing operation is a grouped DataFrame if a list or array\\nis passed and a grouped Series is just a single column name that is passed as a scalar:\\nIn [35]: s_grouped = df.groupby(['key1', 'key2'])['data2']\\nIn [36]: s_grouped\\nOut[36]: <pandas.core.groupby.SeriesGroupBy at 0x2e215d0>\\nIn [37]: s_grouped.mean()\\nOut[37]: \\nkey1  key2\\na     one     1.319920\\n      two     0.092908\\nb     one     0.281746\\n      two     0.769023\\nName: data2\\nGrouping with Dicts and Series\\nGrouping information may exist in a form other than an array. Let’s consider another\\nexample DataFrame:\\nIn [38]: people = DataFrame(np.random.randn(5, 5),\\n   ....:                    columns=['a', 'b', 'c', 'd', 'e'],\\n   ....:                    index=['Joe', 'Steve', 'Wes', 'Jim', 'Travis'])\\nIn [39]: people.ix[2:3, ['b', 'c']] = np.nan # Add a few NA values\\nIn [40]: people\\nOut[40]: \\n               a         b         c         d         e\\nJoe     1.007189 -1.296221  0.274992  0.228913  1.352917\\nSteve   0.886429 -2.001637 -0.371843  1.669025 -0.438570\\nWes    -0.539741       NaN       NaN -1.021228 -0.577087\\nJim     0.124121  0.302614  0.523772  0.000940  1.343810\\nTravis -0.713544 -0.831154 -2.370232 -1.860761 -0.860757\\nNow, suppose I have a group correspondence for the columns and want to sum together\\nthe columns by group:\\nIn [41]: mapping = {'a': 'red', 'b': 'red', 'c': 'blue',\\n   ....:            'd': 'blue', 'e': 'red', 'f' : 'orange'}\\nGroupBy Mechanics | 257\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 273, 'page_label': '258'}, page_content='Now, you could easily construct an array from this dict to pass to groupby, but instead\\nwe can just pass the dict:\\nIn [42]: by_column = people.groupby(mapping, axis=1)\\nIn [43]: by_column.sum()\\nOut[43]: \\n            blue       red\\nJoe     0.503905  1.063885\\nSteve   1.297183 -1.553778\\nWes    -1.021228 -1.116829\\nJim     0.524712  1.770545\\nTravis -4.230992 -2.405455\\nThe same functionality holds for Series, which can be viewed as a fixed size mapping.\\nWhen I used Series as group keys in the above examples, pandas does, in fact, inspect\\neach Series to ensure that its index is aligned with the axis it’s grouping:\\nIn [44]: map_series = Series(mapping)\\nIn [45]: map_series\\nOut[45]: \\na       red\\nb       red\\nc      blue\\nd      blue\\ne       red\\nf    orange\\nIn [46]: people.groupby(map_series, axis=1).count()\\nOut[46]: \\n        blue  red\\nJoe        2    3\\nSteve      2    3\\nWes        1    2\\nJim        2    3\\nTravis     2    3\\nGrouping with Functions\\nUsing Python functions in what can be fairly creative ways is a more abstract way of\\ndefining a group mapping compared with a dict or Series. Any function passed as a\\ngroup key will be called once per index value, with the return values being used as the\\ngroup names. More concretely, consider the example DataFrame from the previous\\nsection, which has people’s first names as index values. Suppose you wanted to group\\nby the length of the names; you could compute an array of string lengths, but instead\\nyou can just pass the len function:\\nIn [47]: people.groupby(len).sum()\\nOut[47]: \\n          a         b         c         d         e\\n3  0.591569 -0.993608  0.798764 -0.791374  2.119639\\n258 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 274, 'page_label': '259'}, page_content=\"5  0.886429 -2.001637 -0.371843  1.669025 -0.438570\\n6 -0.713544 -0.831154 -2.370232 -1.860761 -0.860757\\nMixing functions with arrays, dicts, or Series is not a problem as everything gets con-\\nverted to arrays internally:\\nIn [48]: key_list = ['one', 'one', 'one', 'two', 'two']\\nIn [49]: people.groupby([len, key_list]).min()\\nOut[49]: \\n              a         b         c         d         e\\n3 one -0.539741 -1.296221  0.274992 -1.021228 -0.577087\\n  two  0.124121  0.302614  0.523772  0.000940  1.343810\\n5 one  0.886429 -2.001637 -0.371843  1.669025 -0.438570\\n6 two -0.713544 -0.831154 -2.370232 -1.860761 -0.860757\\nGrouping by Index Levels\\nA final convenience for hierarchically-indexed data sets is the ability to aggregate using\\none of the levels of an axis index. To do this, pass the level number or name using the \\nlevel keyword:\\nIn [50]: columns = pd.MultiIndex.from_arrays([['US', 'US', 'US', 'JP', 'JP'],\\n   ....:                                     [1, 3, 5, 1, 3]], names=['cty', 'tenor'])\\nIn [51]: hier_df = DataFrame(np.random.randn(4, 5), columns=columns)\\nIn [52]: hier_df\\nOut[52]: \\ncty          US                            JP          \\ntenor         1         3         5         1         3\\n0      0.560145 -1.265934  0.119827 -1.063512  0.332883\\n1     -2.359419 -0.199543 -1.541996 -0.970736 -1.307030\\n2      0.286350  0.377984 -0.753887  0.331286  1.349742\\n3      0.069877  0.246674 -0.011862  1.004812  1.327195\\nIn [53]: hier_df.groupby(level='cty', axis=1).count()\\nOut[53]: \\ncty  JP  US\\n0     2   3\\n1     2   3\\n2     2   3\\n3     2   3\\nData Aggregation\\nBy aggregation, I am generally referring to any data transformation that produces scalar\\nvalues from arrays. In the examples above I have used several of them, such as mean,\\ncount, min and sum. You may wonder what is going on when you invoke mean() on a\\nGroupBy object. Many common aggregations, such as those found in Table 9-1, have\\noptimized implementations that compute the statistics on the dataset in place. How-\\never, you are not limited to only this set of methods. You can use aggregations of your\\nData Aggregation | 259\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 275, 'page_label': '260'}, page_content=\"own devising and additionally call any method that is also defined on the grouped\\nobject. For example, as you recall quantile computes sample quantiles of a Series or a\\nDataFrame’s columns 1:\\nIn [54]: df\\nOut[54]: \\n      data1     data2 key1 key2\\n0 -0.204708  1.393406    a  one\\n1  0.478943  0.092908    a  two\\n2 -0.519439  0.281746    b  one\\n3 -0.555730  0.769023    b  two\\n4  1.965781  1.246435    a  one\\nIn [55]: grouped = df.groupby('key1')\\nIn [56]: grouped['data1'].quantile(0.9)\\nOut[56]: \\nkey1\\na       1.668413\\nb      -0.523068\\nWhile quantile is not explicitly implemented for GroupBy, it is a Series method and\\nthus available for use. Internally, GroupBy efficiently slices up the Series, calls\\npiece.quantile(0.9) for each piece, then assembles those results together into the result\\nobject.\\nTo use your own aggregation functions, pass any function that aggregates an array to\\nthe aggregate or agg method:\\nIn [57]: def peak_to_peak(arr):\\n   ....:     return arr.max() - arr.min()\\nIn [58]: grouped.agg(peak_to_peak)\\nOut[58]: \\n         data1     data2\\nkey1                    \\na     2.170488  1.300498\\nb     0.036292  0.487276\\nYou’ll notice that some methods like describe also work, even though they are not\\naggregations, strictly speaking:\\nIn [59]: grouped.describe()\\nOut[59]: \\n               data1     data2\\nkey1                          \\na    count  3.000000  3.000000\\n     mean   0.746672  0.910916\\n     std    1.109736  0.712217\\n     min   -0.204708  0.092908\\n     25%    0.137118  0.669671\\n     50%    0.478943  1.246435\\n1. Note that quantile performs linear interpolation if there is no value at exactly the passed percentile.\\n260 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 276, 'page_label': '261'}, page_content=\"75%    1.222362  1.319920\\n     max    1.965781  1.393406\\nb    count  2.000000  2.000000\\n     mean  -0.537585  0.525384\\n     std    0.025662  0.344556\\n     min   -0.555730  0.281746\\n     25%   -0.546657  0.403565\\n     50%   -0.537585  0.525384\\n     75%   -0.528512  0.647203\\n     max   -0.519439  0.769023\\nI will explain in more detail what has happened here in the next major section on group-\\nwise operations and transformations.\\nYou may notice that custom aggregation functions are much slower than\\nthe optimized functions found in Table 9-1. This is because there is\\nsignificant overhead (function calls, data rearrangement) in construct-\\ning the intermediate group data chunks.\\nTable 9-1. Optimized groupby methods\\nFunction name Description\\ncount Number of non-NA values in the group\\nsum Sum of non-NA values\\nmean Mean of non-NA values\\nmedian Arithmetic median of non-NA values\\nstd, var Unbiased (n - 1 denominator) standard deviation and variance\\nmin, max Minimum and maximum of non-NA values\\nprod Product of non-NA values\\nfirst, last First and last non-NA values\\nTo illustrate some more advanced aggregation features, I’ll use a less trivial dataset, a\\ndataset on restaurant tipping. I obtained it from the R reshape2 package; it was origi-\\nnally found in Bryant & Smith’s 1995 text on business statistics (and found in the book’s\\nGitHub repository). After loading it with read_csv, I add a tipping percentage column\\ntip_pct.\\nIn [60]: tips = pd.read_csv('ch08/tips.csv')\\n# Add tip percentage of total bill\\nIn [61]: tips['tip_pct'] = tips['tip'] / tips['total_bill']\\nIn [62]: tips[:6]\\nOut[62]: \\n   total_bill   tip     sex smoker  day    time  size   tip_pct\\n0       16.99  1.01  Female     No  Sun  Dinner     2  0.059447\\n1       10.34  1.66    Male     No  Sun  Dinner     3  0.160542\\nData Aggregation | 261\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 277, 'page_label': '262'}, page_content=\"2       21.01  3.50    Male     No  Sun  Dinner     3  0.166587\\n3       23.68  3.31    Male     No  Sun  Dinner     2  0.139780\\n4       24.59  3.61  Female     No  Sun  Dinner     4  0.146808\\n5       25.29  4.71    Male     No  Sun  Dinner     4  0.186240\\nColumn-wise and Multiple Function Application\\nAs you’ve seen above, aggregating a Series or all of the columns of a DataFrame is a\\nmatter of using aggregate with the desired function or calling a method like mean or\\nstd. However, you may want to aggregate using a different function depending on the\\ncolumn or multiple functions at once. Fortunately, this is straightforward to do, which\\nI’ll illustrate through a number of examples. First, I’ll group the tips by sex and smoker:\\nIn [63]: grouped = tips.groupby(['sex', 'smoker'])\\nNote that for descriptive statistics like those in Table 9-1, you can pass the name of the\\nfunction as a string:\\nIn [64]: grouped_pct = grouped['tip_pct']\\nIn [65]: grouped_pct.agg('mean')\\nOut[65]: \\nsex     smoker\\nFemale  No        0.156921\\n        Yes       0.182150\\nMale    No        0.160669\\n        Yes       0.152771\\nName: tip_pct\\nIf you pass a list of functions or function names instead, you get back a DataFrame with\\ncolumn names taken from the functions:\\nIn [66]: grouped_pct.agg(['mean', 'std', peak_to_peak])\\nOut[66]: \\n                   mean       std  peak_to_peak\\nsex    smoker                                  \\nFemale No      0.156921  0.036421      0.195876\\n       Yes     0.182150  0.071595      0.360233\\nMale   No      0.160669  0.041849      0.220186\\n       Yes     0.152771  0.090588      0.674707\\nYou don’t need to accept the names that GroupBy gives to the columns; notably \\nlambda functions have the name '<lambda>' which make them hard to identify (you can\\nsee for yourself by looking at a function’s __name__ attribute). As such, if you pass a list\\nof (name, function) tuples, the first element of each tuple will be used as the DataFrame\\ncolumn names (you can think of a list of 2-tuples as an ordered mapping):\\nIn [67]: grouped_pct.agg([('foo', 'mean'), ('bar', np.std)])\\nOut[67]: \\n                    foo       bar\\nsex    smoker                    \\nFemale No      0.156921  0.036421\\n       Yes     0.182150  0.071595\\n262 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 278, 'page_label': '263'}, page_content=\"Male   No      0.160669  0.041849\\n       Yes     0.152771  0.090588\\nWith a DataFrame, you have more options as you can specify a list of functions to apply\\nto all of the columns or different functions per column. To start, suppose we wanted\\nto compute the same three statistics for the tip_pct and total_bill columns:\\nIn [68]: functions = ['count', 'mean', 'max']\\nIn [69]: result = grouped['tip_pct', 'total_bill'].agg(functions)\\nIn [70]: result\\nOut[70]: \\n               tip_pct                      total_bill                  \\n                 count      mean       max       count       mean    max\\nsex    smoker                                                           \\nFemale No           54  0.156921  0.252672          54  18.105185  35.83\\n       Yes          33  0.182150  0.416667          33  17.977879  44.30\\nMale   No           97  0.160669  0.291990          97  19.791237  48.33\\n       Yes          60  0.152771  0.710345          60  22.284500  50.81\\nAs you can see, the resulting DataFrame has hierarchical columns, the same as you\\nwould get aggregating each column separately and using concat to glue the results\\ntogether using the column names as the keys argument:\\nIn [71]: result['tip_pct']\\nOut[71]: \\n               count      mean       max\\nsex    smoker                           \\nFemale No         54  0.156921  0.252672\\n       Yes        33  0.182150  0.416667\\nMale   No         97  0.160669  0.291990\\n       Yes        60  0.152771  0.710345\\nAs above, a list of tuples with custom names can be passed:\\nIn [72]: ftuples = [('Durchschnitt', 'mean'), ('Abweichung', np.var)]\\nIn [73]: grouped['tip_pct', 'total_bill'].agg(ftuples)\\nOut[73]: \\n                    tip_pct                total_bill            \\n               Durchschnitt  Abweichung  Durchschnitt  Abweichung\\nsex    smoker                                                    \\nFemale No          0.156921    0.001327     18.105185   53.092422\\n       Yes         0.182150    0.005126     17.977879   84.451517\\nMale   No          0.160669    0.001751     19.791237   76.152961\\n       Yes         0.152771    0.008206     22.284500   98.244673\\nNow, suppose you wanted to apply potentially different functions to one or more of\\nthe columns. The trick is to pass a dict to agg that contains a mapping of column names\\nto any of the function specifications listed so far:\\nIn [74]: grouped.agg({'tip' : np.max, 'size' : 'sum'})\\nOut[74]: \\n               size   tip\\nsex    smoker            \\nData Aggregation | 263\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 279, 'page_label': '264'}, page_content=\"Female No       140   5.2\\n       Yes       74   6.5\\nMale   No       263   9.0\\n       Yes      150  10.0\\nIn [75]: grouped.agg({'tip_pct' : ['min', 'max', 'mean', 'std'],\\n   ....:              'size' : 'sum'})\\nOut[75]: \\n                tip_pct                                size\\n                    min       max      mean       std   sum\\nsex    smoker                                              \\nFemale No      0.056797  0.252672  0.156921  0.036421   140\\n       Yes     0.056433  0.416667  0.182150  0.071595    74\\nMale   No      0.071804  0.291990  0.160669  0.041849   263\\n       Yes     0.035638  0.710345  0.152771  0.090588   150\\nA DataFrame will have hierarchical columns only if multiple functions are applied to\\nat least one column.\\nReturning Aggregated Data in “unindexed” Form\\nIn all of the examples up until now, the aggregated data comes back with an index,\\npotentially hierarchical, composed from the unique group key combinations observed.\\nSince this isn’t always desirable, you can disable this behavior in most cases by passing\\nas_index=False to groupby:\\nIn [76]: tips.groupby(['sex', 'smoker'], as_index=False).mean()\\nOut[76]: \\n      sex smoker  total_bill       tip      size   tip_pct\\n0  Female     No   18.105185  2.773519  2.592593  0.156921\\n1  Female    Yes   17.977879  2.931515  2.242424  0.182150\\n2    Male     No   19.791237  3.113402  2.711340  0.160669\\n3    Male    Yes   22.284500  3.051167  2.500000  0.152771\\nOf course, it’s always possible to obtain the result in this format by calling\\nreset_index on the result.\\nUsing groupby in this way is generally less flexible; results with hier-\\narchical columns, for example, are not currently implemented as the\\nform of the result would have to be somewhat arbitrary.\\nGroup-wise Operations and Transformations\\nAggregation is only one kind of group operation. It is a special case in the more general\\nclass of data transformations; that is, it accepts functions that reduce a one-dimensional\\narray to a scalar value. In this section, I will introduce you to the transform and apply\\nmethods, which will enable you to do many other kinds of group operations.\\nSuppose, instead, we wanted to add a column to a DataFrame containing group means\\nfor each index. One way to do this is to aggregate, then merge:\\n264 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 280, 'page_label': '265'}, page_content=\"In [77]: df\\nOut[77]: \\n      data1     data2 key1 key2\\n0 -0.204708  1.393406    a  one\\n1  0.478943  0.092908    a  two\\n2 -0.519439  0.281746    b  one\\n3 -0.555730  0.769023    b  two\\n4  1.965781  1.246435    a  one\\nIn [78]: k1_means = df.groupby('key1').mean().add_prefix('mean_')\\nIn [79]: k1_means\\nOut[79]: \\n      mean_data1  mean_data2\\nkey1                        \\na       0.746672    0.910916\\nb      -0.537585    0.525384\\nIn [80]: pd.merge(df, k1_means, left_on='key1', right_index=True)\\nOut[80]: \\n      data1     data2 key1 key2  mean_data1  mean_data2\\n0 -0.204708  1.393406    a  one    0.746672    0.910916\\n1  0.478943  0.092908    a  two    0.746672    0.910916\\n4  1.965781  1.246435    a  one    0.746672    0.910916\\n2 -0.519439  0.281746    b  one   -0.537585    0.525384\\n3 -0.555730  0.769023    b  two   -0.537585    0.525384\\nThis works, but is somewhat inflexible. You can think of the operation as transforming\\nthe two data columns using the np.mean function. Let’s look back at the people Data-\\nFrame from earlier in the chapter and use the transform method on GroupBy:\\nIn [81]: key = ['one', 'two', 'one', 'two', 'one']\\nIn [82]: people.groupby(key).mean()\\nOut[82]: \\n            a         b         c         d         e\\none -0.082032 -1.063687 -1.047620 -0.884358 -0.028309\\ntwo  0.505275 -0.849512  0.075965  0.834983  0.452620\\nIn [83]: people.groupby(key).transform(np.mean)\\nOut[83]: \\n               a         b         c         d         e\\nJoe    -0.082032 -1.063687 -1.047620 -0.884358 -0.028309\\nSteve   0.505275 -0.849512  0.075965  0.834983  0.452620\\nWes    -0.082032 -1.063687 -1.047620 -0.884358 -0.028309\\nJim     0.505275 -0.849512  0.075965  0.834983  0.452620\\nTravis -0.082032 -1.063687 -1.047620 -0.884358 -0.028309\\nAs you may guess, transform applies a function to each group, then places the results\\nin the appropriate locations. If each group produces a scalar value, it will be propagated\\n(broadcasted). Suppose instead you wanted to subtract the mean value from each\\ngroup. To do this, create a demeaning function and pass it to transform:\\nIn [84]: def demean(arr):\\n   ....:     return arr - arr.mean()\\nGroup-wise Operations and Transformations | 265\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 281, 'page_label': '266'}, page_content=\"In [85]: demeaned = people.groupby(key).transform(demean)\\nIn [86]: demeaned\\nOut[86]: \\n               a         b         c         d         e\\nJoe     1.089221 -0.232534  1.322612  1.113271  1.381226\\nSteve   0.381154 -1.152125 -0.447807  0.834043 -0.891190\\nWes    -0.457709       NaN       NaN -0.136869 -0.548778\\nJim    -0.381154  1.152125  0.447807 -0.834043  0.891190\\nTravis -0.631512  0.232534 -1.322612 -0.976402 -0.832448\\nYou can check that demeaned now has zero group means:\\nIn [87]: demeaned.groupby(key).mean()\\nOut[87]: \\n     a  b  c  d  e\\none  0 -0  0  0  0\\ntwo -0  0  0  0  0\\nAs you’ll see in the next section, group demeaning can be achieved using apply also.\\nApply: General split-apply-combine\\nLike aggregate, transform is a more specialized function having rigid requirements: the\\npassed function must either produce a scalar value to be broadcasted (like np.mean) or\\na transformed array of the same size. The most general purpose GroupBy method is\\napply, which is the subject of the rest of this section. As in Figure 9-1, apply splits the\\nobject being manipulated into pieces, invokes the passed function on each piece, then\\nattempts to concatenate the pieces together.\\nReturning to the tipping data set above, suppose you wanted to select the top five\\ntip_pct values by group. First, it’s straightforward to write a function that selects the\\nrows with the largest values in a particular column:\\nIn [88]: def top(df, n=5, column='tip_pct'):\\n   ....:     return df.sort_index(by=column)[-n:]\\nIn [89]: top(tips, n=6)\\nOut[89]: \\n     total_bill   tip     sex smoker  day    time  size   tip_pct\\n109       14.31  4.00  Female    Yes  Sat  Dinner     2  0.279525\\n183       23.17  6.50    Male    Yes  Sun  Dinner     4  0.280535\\n232       11.61  3.39    Male     No  Sat  Dinner     2  0.291990\\n67         3.07  1.00  Female    Yes  Sat  Dinner     1  0.325733\\n178        9.60  4.00  Female    Yes  Sun  Dinner     2  0.416667\\n172        7.25  5.15    Male    Yes  Sun  Dinner     2  0.710345\\nNow, if we group by smoker, say, and call apply with this function, we get the following:\\nIn [90]: tips.groupby('smoker').apply(top)\\nOut[90]: \\n            total_bill   tip     sex smoker   day    time  size   tip_pct\\nsmoker                                                                   \\n266 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 282, 'page_label': '267'}, page_content=\"No     88        24.71  5.85    Male     No  Thur   Lunch     2  0.236746\\n       185       20.69  5.00    Male     No   Sun  Dinner     5  0.241663\\n       51        10.29  2.60  Female     No   Sun  Dinner     2  0.252672\\n       149        7.51  2.00    Male     No  Thur   Lunch     2  0.266312\\n       232       11.61  3.39    Male     No   Sat  Dinner     2  0.291990\\nYes    109       14.31  4.00  Female    Yes   Sat  Dinner     2  0.279525\\n       183       23.17  6.50    Male    Yes   Sun  Dinner     4  0.280535\\n       67         3.07  1.00  Female    Yes   Sat  Dinner     1  0.325733\\n       178        9.60  4.00  Female    Yes   Sun  Dinner     2  0.416667\\n       172        7.25  5.15    Male    Yes   Sun  Dinner     2  0.710345\\nWhat has happened here? The top function is called on each piece of the DataFrame,\\nthen the results are glued together using pandas.concat, labeling the pieces with the\\ngroup names. The result therefore has a hierarchical index whose inner level contains\\nindex values from the original DataFrame.\\nIf you pass a function to apply that takes other arguments or keywords, you can pass\\nthese after the function:\\nIn [91]: tips.groupby(['smoker', 'day']).apply(top, n=1, column='total_bill')\\nOut[91]: \\n                 total_bill    tip     sex smoker   day    time  size   tip_pct\\nsmoker day                                                                     \\nNo     Fri  94        22.75   3.25  Female     No   Fri  Dinner     2  0.142857\\n       Sat  212       48.33   9.00    Male     No   Sat  Dinner     4  0.186220\\n       Sun  156       48.17   5.00    Male     No   Sun  Dinner     6  0.103799\\n       Thur 142       41.19   5.00    Male     No  Thur   Lunch     5  0.121389\\nYes    Fri  95        40.17   4.73    Male    Yes   Fri  Dinner     4  0.117750\\n       Sat  170       50.81  10.00    Male    Yes   Sat  Dinner     3  0.196812\\n       Sun  182       45.35   3.50    Male    Yes   Sun  Dinner     3  0.077178\\n       Thur 197       43.11   5.00  Female    Yes  Thur   Lunch     4  0.115982\\nBeyond these basic usage mechanics, getting the most out of apply is\\nlargely a matter of creativity. What occurs inside the function passed is\\nup to you; it only needs to return a pandas object or a scalar value. The\\nrest of this chapter will mainly consist of examples showing you how to\\nsolve various problems using groupby.\\nYou may recall above I called describe on a GroupBy object:\\nIn [92]: result = tips.groupby('smoker')['tip_pct'].describe()\\nIn [93]: result\\nOut[93]: \\nsmoker       \\nNo      count    151.000000\\n        mean       0.159328\\n        std        0.039910\\n        min        0.056797\\n        25%        0.136906\\n        50%        0.155625\\n        75%        0.185014\\n        max        0.291990\\nGroup-wise Operations and Transformations | 267\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 283, 'page_label': '268'}, page_content=\"Yes     count     93.000000\\n        mean       0.163196\\n        std        0.085119\\n        min        0.035638\\n        25%        0.106771\\n        50%        0.153846\\n        75%        0.195059\\n        max        0.710345\\nIn [94]: result.unstack('smoker')\\nOut[94]: \\nsmoker          No        Yes\\ncount   151.000000  93.000000\\nmean      0.159328   0.163196\\nstd       0.039910   0.085119\\nmin       0.056797   0.035638\\n25%       0.136906   0.106771\\n50%       0.155625   0.153846\\n75%       0.185014   0.195059\\nmax       0.291990   0.710345\\nInside GroupBy, when you invoke a method like describe, it is actually just a shortcut\\nfor:\\nf = lambda x: x.describe()\\ngrouped.apply(f)\\nSuppressing the group keys\\nIn the examples above, you see that the resulting object has a hierarchical index formed\\nfrom the group keys along with the indexes of each piece of the original object. This\\ncan be disabled by passing group_keys=False to groupby:\\nIn [95]: tips.groupby('smoker', group_keys=False).apply(top)\\nOut[95]: \\n     total_bill   tip     sex smoker   day    time  size   tip_pct\\n88        24.71  5.85    Male     No  Thur   Lunch     2  0.236746\\n185       20.69  5.00    Male     No   Sun  Dinner     5  0.241663\\n51        10.29  2.60  Female     No   Sun  Dinner     2  0.252672\\n149        7.51  2.00    Male     No  Thur   Lunch     2  0.266312\\n232       11.61  3.39    Male     No   Sat  Dinner     2  0.291990\\n109       14.31  4.00  Female    Yes   Sat  Dinner     2  0.279525\\n183       23.17  6.50    Male    Yes   Sun  Dinner     4  0.280535\\n67         3.07  1.00  Female    Yes   Sat  Dinner     1  0.325733\\n178        9.60  4.00  Female    Yes   Sun  Dinner     2  0.416667\\n172        7.25  5.15    Male    Yes   Sun  Dinner     2  0.710345\\nQuantile and Bucket Analysis\\nAs you may recall from Chapter 7, pandas has some tools, in particular cut and qcut,\\nfor slicing data up into buckets with bins of your choosing or by sample quantiles.\\nCombining these functions with groupby, it becomes very simple to perform bucket or\\n268 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 284, 'page_label': '269'}, page_content=\"quantile analysis on a data set. Consider a simple random data set and an equal-length\\nbucket categorization using cut:\\nIn [96]: frame = DataFrame({'data1': np.random.randn(1000),\\n   ....:                    'data2': np.random.randn(1000)})\\nIn [97]: factor = pd.cut(frame.data1, 4)\\nIn [98]: factor[:10]\\nOut[98]: \\nCategorical: \\narray([(-1.23, 0.489], (-2.956, -1.23], (-1.23, 0.489], (0.489, 2.208],\\n       (-1.23, 0.489], (0.489, 2.208], (-1.23, 0.489], (-1.23, 0.489],\\n       (0.489, 2.208], (0.489, 2.208]], dtype=object)\\nLevels (4): Index([(-2.956, -1.23], (-1.23, 0.489], (0.489, 2.208],\\n                   (2.208, 3.928]], dtype=object)\\nThe Factor object returned by cut can be passed directly to groupby. So we could com-\\npute a set of statistics for the data2 column like so:\\nIn [99]: def get_stats(group):\\n   ....:     return {'min': group.min(), 'max': group.max(),\\n   ....:             'count': group.count(), 'mean': group.mean()}\\nIn [100]: grouped = frame.data2.groupby(factor)\\nIn [101]: grouped.apply(get_stats).unstack()\\nOut[101]: \\n                 count       max      mean       min\\ndata1                                               \\n(-1.23, 0.489]     598  3.260383 -0.002051 -2.989741\\n(-2.956, -1.23]     95  1.670835 -0.039521 -3.399312\\n(0.489, 2.208]     297  2.954439  0.081822 -3.745356\\n(2.208, 3.928]      10  1.765640  0.024750 -1.929776\\nThese were equal-length buckets; to compute equal-size buckets based on sample\\nquantiles, use qcut. I’ll pass labels=False to just get quantile numbers.\\n# Return quantile numbers\\nIn [102]: grouping = pd.qcut(frame.data1, 10, labels=False)\\nIn [103]: grouped = frame.data2.groupby(grouping)\\nIn [104]: grouped.apply(get_stats).unstack()\\nOut[104]: \\n   count       max      mean       min\\n0    100  1.670835 -0.049902 -3.399312\\n1    100  2.628441  0.030989 -1.950098\\n2    100  2.527939 -0.067179 -2.925113\\n3    100  3.260383  0.065713 -2.315555\\n4    100  2.074345 -0.111653 -2.047939\\n5    100  2.184810  0.052130 -2.989741\\n6    100  2.458842 -0.021489 -2.223506\\n7    100  2.954439 -0.026459 -3.056990\\n8    100  2.735527  0.103406 -3.745356\\n9    100  2.377020  0.220122 -2.064111\\nGroup-wise Operations and Transformations | 269\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 285, 'page_label': '270'}, page_content=\"Example: Filling Missing Values with Group-specific Values\\nWhen cleaning up missing data, in some cases you will filter out data observations\\nusing dropna, but in others you may want to impute (fill in) the NA values using a fixed\\nvalue or some value derived from the data. fillna is the right tool to use; for example\\nhere I fill in NA values with the mean:\\nIn [105]: s = Series(np.random.randn(6))\\nIn [106]: s[::2] = np.nan\\nIn [107]: s\\nOut[107]: \\n0         NaN\\n1   -0.125921\\n2         NaN\\n3   -0.884475\\n4         NaN\\n5    0.227290\\nIn [108]: s.fillna(s.mean())\\nOut[108]: \\n0   -0.261035\\n1   -0.125921\\n2   -0.261035\\n3   -0.884475\\n4   -0.261035\\n5    0.227290\\nSuppose you need the fill value to vary by group. As you may guess, you need only\\ngroup the data and use apply with a function that calls fillna on each data chunk. Here\\nis some sample data on some US states divided into eastern and western states:\\nIn [109]: states = ['Ohio', 'New York', 'Vermont', 'Florida',\\n   .....:           'Oregon', 'Nevada', 'California', 'Idaho']\\nIn [110]: group_key = ['East'] * 4 + ['West'] * 4\\nIn [111]: data = Series(np.random.randn(8), index=states)\\nIn [112]: data[['Vermont', 'Nevada', 'Idaho']] = np.nan\\nIn [113]: data\\nOut[113]: \\nOhio          0.922264\\nNew York     -2.153545\\nVermont            NaN\\nFlorida      -0.375842\\nOregon        0.329939\\nNevada             NaN\\nCalifornia    1.105913\\nIdaho              NaN\\nIn [114]: data.groupby(group_key).mean()\\nOut[114]: \\n270 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 286, 'page_label': '271'}, page_content=\"East   -0.535707\\nWest    0.717926\\nWe can fill the NA values using the group means like so:\\nIn [115]: fill_mean = lambda g: g.fillna(g.mean())\\nIn [116]: data.groupby(group_key).apply(fill_mean)\\nOut[116]: \\nOhio          0.922264\\nNew York     -2.153545\\nVermont      -0.535707\\nFlorida      -0.375842\\nOregon        0.329939\\nNevada        0.717926\\nCalifornia    1.105913\\nIdaho         0.717926\\nIn another case, you might have pre-defined fill values in your code that vary by group.\\nSince the groups have a name attribute set internally, we can use that:\\nIn [117]: fill_values = {'East': 0.5, 'West': -1}\\nIn [118]: fill_func = lambda g: g.fillna(fill_values[g.name])\\nIn [119]: data.groupby(group_key).apply(fill_func)\\nOut[119]: \\nOhio          0.922264\\nNew York     -2.153545\\nVermont       0.500000\\nFlorida      -0.375842\\nOregon        0.329939\\nNevada       -1.000000\\nCalifornia    1.105913\\nIdaho        -1.000000\\nExample: Random Sampling and Permutation\\nSuppose you wanted to draw a random sample (with or without replacement) from a\\nlarge dataset for Monte Carlo simulation purposes or some other application. There\\nare a number of ways to perform the “draws”; some are much more efficient than others.\\nOne way is to select the first K elements of np.random.permutation(N), where N is the\\nsize of your complete dataset and K the desired sample size. As a more fun example,\\nhere’s a way to construct a deck of English-style playing cards:\\n# Hearts, Spades, Clubs, Diamonds\\nsuits = ['H', 'S', 'C', 'D']\\ncard_val = (range(1, 11) + [10] * 3) * 4\\nbase_names = ['A'] + range(2, 11) + ['J', 'K', 'Q']\\ncards = []\\nfor suit in ['H', 'S', 'C', 'D']:\\n    cards.extend(str(num) + suit for num in base_names)\\ndeck = Series(card_val, index=cards)\\nGroup-wise Operations and Transformations | 271\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 287, 'page_label': '272'}, page_content='So now we have a Series of length 52 whose index contains card names and values are\\nthe ones used in blackjack and other games (to keep things simple, I just let the ace be\\n1):\\nIn [121]: deck[:13]\\nOut[121]: \\nAH      1\\n2H      2\\n3H      3\\n4H      4\\n5H      5\\n6H      6\\n7H      7\\n8H      8\\n9H      9\\n10H    10\\nJH     10\\nKH     10\\nQH     10\\nNow, based on what I said above, drawing a hand of 5 cards from the desk could be\\nwritten as:\\nIn [122]: def draw(deck, n=5):\\n   .....:     return deck.take(np.random.permutation(len(deck))[:n])\\nIn [123]: draw(deck)\\nOut[123]: \\nAD     1\\n8C     8\\n5H     5\\nKC    10\\n2C     2\\nSuppose you wanted two random cards from each suit. Because the suit is the last\\ncharacter of each card name, we can group based on this and use apply:\\nIn [124]: get_suit = lambda card: card[-1] # last letter is suit\\nIn [125]: deck.groupby(get_suit).apply(draw, n=2)\\nOut[125]: \\nC  2C     2\\n   3C     3\\nD  KD    10\\n   8D     8\\nH  KH    10\\n   3H     3\\nS  2S     2\\n   4S     4\\n# alternatively\\nIn [126]: deck.groupby(get_suit, group_keys=False).apply(draw, n=2)\\nOut[126]: \\nKC    10\\nJC    10\\nAD     1\\n272 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 288, 'page_label': '273'}, page_content=\"5D     5\\n5H     5\\n6H     6\\n7S     7\\nKS    10\\nExample: Group Weighted Average and Correlation\\nUnder the split-apply-combine paradigm of groupby, operations between columns in a\\nDataFrame or two Series, such a group weighted average, become a routine affair. As\\nan example, take this dataset containing group keys, values, and some weights:\\nIn [127]: df = DataFrame({'category': ['a', 'a', 'a', 'a', 'b', 'b', 'b', 'b'],\\n   .....:                 'data': np.random.randn(8),\\n   .....:                 'weights': np.random.rand(8)})\\nIn [128]: df\\nOut[128]: \\n  category      data   weights\\n0        a  1.561587  0.957515\\n1        a  1.219984  0.347267\\n2        a -0.482239  0.581362\\n3        a  0.315667  0.217091\\n4        b -0.047852  0.894406\\n5        b -0.454145  0.918564\\n6        b -0.556774  0.277825\\n7        b  0.253321  0.955905\\nThe group weighted average by category would then be:\\nIn [129]: grouped = df.groupby('category')\\nIn [130]: get_wavg = lambda g: np.average(g['data'], weights=g['weights'])\\nIn [131]: grouped.apply(get_wavg)\\nOut[131]: \\ncategory\\na           0.811643\\nb          -0.122262\\nAs a less trivial example, consider a data set from Yahoo! Finance containing end of\\nday prices for a few stocks and the S&P 500 index (the SPX ticker):\\nIn [132]: close_px = pd.read_csv('ch09/stock_px.csv', parse_dates=True, index_col=0)\\nIn [133]: close_px\\nOut[133]: \\n<class 'pandas.core.frame.DataFrame'>\\nDatetimeIndex: 2214 entries, 2003-01-02 00:00:00 to 2011-10-14 00:00:00\\nData columns:\\nAAPL    2214  non-null values\\nMSFT    2214  non-null values\\nXOM     2214  non-null values\\nSPX     2214  non-null values\\ndtypes: float64(4)\\nGroup-wise Operations and Transformations | 273\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 289, 'page_label': '274'}, page_content=\"In [134]: close_px[-4:]\\nOut[134]: \\n              AAPL   MSFT    XOM      SPX\\n2011-10-11  400.29  27.00  76.27  1195.54\\n2011-10-12  402.19  26.96  77.16  1207.25\\n2011-10-13  408.43  27.18  76.37  1203.66\\n2011-10-14  422.00  27.27  78.11  1224.58\\nOne task of interest might be to compute a DataFrame consisting of the yearly corre-\\nlations of daily returns (computed from percent changes) with SPX. Here is one way to\\ndo it:\\nIn [135]: rets = close_px.pct_change().dropna()\\nIn [136]: spx_corr = lambda x: x.corrwith(x['SPX'])\\nIn [137]: by_year = rets.groupby(lambda x: x.year)\\nIn [138]: by_year.apply(spx_corr)\\nOut[138]: \\n          AAPL      MSFT       XOM  SPX\\n2003  0.541124  0.745174  0.661265    1\\n2004  0.374283  0.588531  0.557742    1\\n2005  0.467540  0.562374  0.631010    1\\n2006  0.428267  0.406126  0.518514    1\\n2007  0.508118  0.658770  0.786264    1\\n2008  0.681434  0.804626  0.828303    1\\n2009  0.707103  0.654902  0.797921    1\\n2010  0.710105  0.730118  0.839057    1\\n2011  0.691931  0.800996  0.859975    1\\nThere is, of course, nothing to stop you from computing inter-column correlations:\\n# Annual correlation of Apple with Microsoft\\nIn [139]: by_year.apply(lambda g: g['AAPL'].corr(g['MSFT']))\\nOut[139]: \\n2003    0.480868\\n2004    0.259024\\n2005    0.300093\\n2006    0.161735\\n2007    0.417738\\n2008    0.611901\\n2009    0.432738\\n2010    0.571946\\n2011    0.581987\\nExample: Group-wise Linear Regression\\nIn the same vein as the previous example, you can use groupby to perform more complex\\ngroup-wise statistical analysis, as long as the function returns a pandas object or scalar\\nvalue. For example, I can define the following regress function (using the statsmo\\ndels econometrics library) which executes an ordinary least squares (OLS) regression\\non each chunk of data:\\n274 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 290, 'page_label': '275'}, page_content=\"import statsmodels.api as sm\\ndef regress(data, yvar, xvars):\\n    Y = data[yvar]\\n    X = data[xvars]\\n    X['intercept'] = 1.\\n    result = sm.OLS(Y, X).fit()\\n    return result.params\\nNow, to run a yearly linear regression of AAPL on SPX returns, I execute:\\nIn [141]: by_year.apply(regress, 'AAPL', ['SPX'])\\nOut[141]: \\n           SPX  intercept\\n2003  1.195406   0.000710\\n2004  1.363463   0.004201\\n2005  1.766415   0.003246\\n2006  1.645496   0.000080\\n2007  1.198761   0.003438\\n2008  0.968016  -0.001110\\n2009  0.879103   0.002954\\n2010  1.052608   0.001261\\n2011  0.806605   0.001514\\nPivot Tables and Cross-Tabulation\\nA pivot table is a data summarization tool frequently found in spreadsheet programs\\nand other data analysis software. It aggregates a table of data by one or more keys,\\narranging the data in a rectangle with some of the group keys along the rows and some\\nalong the columns. Pivot tables in Python with pandas are made possible using the\\ngroupby facility described in this chapter combined with reshape operations utilizing\\nhierarchical indexing. DataFrame has a pivot_table method, and additionally there is\\na top-level pandas.pivot_table function. In addition to providing a convenience inter-\\nface to groupby, pivot_table also can add partial totals, also known as margins.\\nReturning to the tipping data set, suppose I wanted to compute a table of group means\\n(the default pivot_table aggregation type) arranged by sex and smoker on the rows:\\nIn [142]: tips.pivot_table(rows=['sex', 'smoker'])\\nOut[142]: \\n                   size       tip   tip_pct  total_bill\\nsex    smoker                                          \\nFemale No      2.592593  2.773519  0.156921   18.105185\\n       Yes     2.242424  2.931515  0.182150   17.977879\\nMale   No      2.711340  3.113402  0.160669   19.791237\\n       Yes     2.500000  3.051167  0.152771   22.284500\\nThis could have been easily produced using groupby. Now, suppose we want to aggre-\\ngate only tip_pct and size, and additionally group by day. I’ll put smoker in the table\\ncolumns and day in the rows:\\nIn [143]: tips.pivot_table(['tip_pct', 'size'], rows=['sex', 'day'],\\n   .....:                  cols='smoker')\\nOut[143]: \\nPivot Tables and Cross-Tabulation | 275\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 291, 'page_label': '276'}, page_content=\"tip_pct                size          \\nsmoker             No       Yes        No       Yes\\nsex    day                                         \\nFemale Fri   0.165296  0.209129  2.500000  2.000000\\n       Sat   0.147993  0.163817  2.307692  2.200000\\n       Sun   0.165710  0.237075  3.071429  2.500000\\n       Thur  0.155971  0.163073  2.480000  2.428571\\nMale   Fri   0.138005  0.144730  2.000000  2.125000\\n       Sat   0.162132  0.139067  2.656250  2.629630\\n       Sun   0.158291  0.173964  2.883721  2.600000\\n       Thur  0.165706  0.164417  2.500000  2.300000\\nThis table could be augmented to include partial totals by passing margins=True. This\\nhas the effect of adding All row and column labels, with corresponding values being\\nthe group statistics for all the data within a single tier. In this below example, the All\\nvalues are means without taking into account smoker vs. non-smoker (the All columns)\\nor any of the two levels of grouping on the rows (the All row):\\nIn [144]: tips.pivot_table(['tip_pct', 'size'], rows=['sex', 'day'],\\n   .....:                  cols='smoker', margins=True)\\nOut[144]: \\n                 size                       tip_pct                    \\nsmoker             No       Yes       All        No       Yes       All\\nsex    day                                                             \\nFemale Fri   2.500000  2.000000  2.111111  0.165296  0.209129  0.199388\\n       Sat   2.307692  2.200000  2.250000  0.147993  0.163817  0.156470\\n       Sun   3.071429  2.500000  2.944444  0.165710  0.237075  0.181569\\n       Thur  2.480000  2.428571  2.468750  0.155971  0.163073  0.157525\\nMale   Fri   2.000000  2.125000  2.100000  0.138005  0.144730  0.143385\\n       Sat   2.656250  2.629630  2.644068  0.162132  0.139067  0.151577\\n       Sun   2.883721  2.600000  2.810345  0.158291  0.173964  0.162344\\n       Thur  2.500000  2.300000  2.433333  0.165706  0.164417  0.165276\\nAll          2.668874  2.408602  2.569672  0.159328  0.163196  0.160803\\nTo use a different aggregation function, pass it to aggfunc. For example, 'count' or\\nlen will give you a cross-tabulation (count or frequency) of group sizes:\\nIn [145]: tips.pivot_table('tip_pct', rows=['sex', 'smoker'], cols='day',\\n   .....:                  aggfunc=len, margins=True)\\nOut[145]: \\nday            Fri  Sat  Sun  Thur  All\\nsex    smoker                          \\nFemale No        2   13   14    25   54\\n       Yes       7   15    4     7   33\\nMale   No        2   32   43    20   97\\n       Yes       8   27   15    10   60\\nAll             19   87   76    62  244\\nIf some combinations are empty (or otherwise NA), you may wish to pass a fill_value:\\nIn [146]: tips.pivot_table('size', rows=['time', 'sex', 'smoker'],\\n   .....:                  cols='day', aggfunc='sum', fill_value=0)\\nOut[146]: \\nday                   Fri  Sat  Sun  Thur\\ntime   sex    smoker                     \\nDinner Female No        2   30   43     2\\n276 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 292, 'page_label': '277'}, page_content=\"Yes       8   33   10     0\\n       Male   No        4   85  124     0\\n              Yes      12   71   39     0\\nLunch  Female No        3    0    0    60\\n              Yes       6    0    0    17\\n       Male   No        0    0    0    50\\n              Yes       5    0    0    23\\nSee Table 9-2 for a summary of pivot_table methods.\\nTable 9-2. pivot_table options\\nFunction name Description\\nvalues Column name or names to aggregate. By default aggregates all numeric columns\\nrows Column names or other group keys to group on the rows of the resulting pivot table\\ncols Column names or other group keys to group on the columns of the resulting pivot table\\naggfunc Aggregation function or list of functions; 'mean' by default. Can be any function valid in a groupby context\\nfill_value Replace missing values in result table\\nmargins Add row/column subtotals and grand total, False by default\\nCross-Tabulations: Crosstab\\nA cross-tabulation (or crosstab for short) is a special case of a pivot table that computes\\ngroup frequencies. Here is a canonical example taken from the Wikipedia page on cross-\\ntabulation:\\nIn [150]: data\\nOut[150]: \\n   Sample  Gender    Handedness\\n0       1  Female  Right-handed\\n1       2    Male   Left-handed\\n2       3  Female  Right-handed\\n3       4    Male  Right-handed\\n4       5    Male   Left-handed\\n5       6    Male  Right-handed\\n6       7  Female  Right-handed\\n7       8  Female   Left-handed\\n8       9    Male  Right-handed\\n9      10  Female  Right-handed\\nAs part of some survey analysis, we might want to summarize this data by gender and\\nhandedness. You could use pivot_table to do this, but the pandas.crosstab function\\nis very convenient:\\nIn [151]: pd.crosstab(data.Gender, data.Handedness, margins=True)\\nOut[151]: \\nHandedness  Left-handed  Right-handed  All\\nGender                                    \\nFemale                1             4    5\\nMale                  2             3    5\\nAll                   3             7   10\\nPivot Tables and Cross-Tabulation | 277\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 293, 'page_label': '278'}, page_content=\"The first two arguments to crosstab can each either be an array or Series or a list of\\narrays. As in the tips data:\\nIn [152]: pd.crosstab([tips.time, tips.day], tips.smoker, margins=True)\\nOut[152]: \\nsmoker        No  Yes  All\\ntime   day                \\nDinner Fri     3    9   12\\n       Sat    45   42   87\\n       Sun    57   19   76\\n       Thur    1    0    1\\nLunch  Fri     1    6    7\\n       Thur   44   17   61\\nAll          151   93  244\\nExample: 2012 Federal Election Commission Database\\nThe US Federal Election Commission publishes data on contributions to political cam-\\npaigns. This includes contributor names, occupation and employer, address, and con-\\ntribution amount. An interesting dataset is from the 2012 US presidential election\\n(http://www.fec.gov/disclosurep/PDownload.do). As of this writing (June 2012), the full\\ndataset for all states is a 150 megabyte CSV file P00000001-ALL.csv, which can be loaded\\nwith pandas.read_csv:\\nIn [13]: fec = pd.read_csv('ch09/P00000001-ALL.csv')\\nIn [14]: fec\\nOut[14]:\\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 1001731 entries, 0 to 1001730\\nData columns:\\ncmte_id              1001731  non-null values\\ncand_id              1001731  non-null values\\ncand_nm              1001731  non-null values\\ncontbr_nm            1001731  non-null values\\ncontbr_city          1001716  non-null values\\ncontbr_st            1001727  non-null values\\ncontbr_zip           1001620  non-null values\\ncontbr_employer      994314   non-null values\\ncontbr_occupation    994433   non-null values\\ncontb_receipt_amt    1001731  non-null values\\ncontb_receipt_dt     1001731  non-null values\\nreceipt_desc         14166    non-null values\\nmemo_cd              92482    non-null values\\nmemo_text            97770    non-null values\\nform_tp              1001731  non-null values\\nfile_num             1001731  non-null values\\ndtypes: float64(1), int64(1), object(14)\\nA sample record in the DataFrame looks like this:\\nIn [15]: fec.ix[123456]\\nOut[15]:\\ncmte_id                             C00431445\\n278 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 294, 'page_label': '279'}, page_content='cand_id                             P80003338\\ncand_nm                         Obama, Barack\\ncontbr_nm                         ELLMAN, IRA\\ncontbr_city                             TEMPE\\ncontbr_st                                  AZ\\ncontbr_zip                          852816719\\ncontbr_employer      ARIZONA STATE UNIVERSITY\\ncontbr_occupation                   PROFESSOR\\ncontb_receipt_amt                          50\\ncontb_receipt_dt                    01-DEC-11\\nreceipt_desc                              NaN\\nmemo_cd                                   NaN\\nmemo_text                                 NaN\\nform_tp                                 SA17A\\nfile_num                               772372\\nName: 123456\\nYou can probably think of many ways to start slicing and dicing this data to extract\\ninformative statistics about donors and patterns in the campaign contributions. I’ll\\nspend the next several pages showing you a number of different analyses that apply\\ntechniques you have learned about so far.\\nYou can see that there are no political party affiliations in the data, so this would be\\nuseful to add. You can get a list of all the unique political candidates using unique (note\\nthat NumPy suppresses the quotes around the strings in the output):\\nIn [16]: unique_cands = fec.cand_nm.unique()\\nIn [17]: unique_cands\\nOut[17]:\\narray([Bachmann, Michelle, Romney, Mitt, Obama, Barack,\\n       Roemer, Charles E. \\'Buddy\\' III, Pawlenty, Timothy,\\n       Johnson, Gary Earl, Paul, Ron, Santorum, Rick, Cain, Herman,\\n       Gingrich, Newt, McCotter, Thaddeus G, Huntsman, Jon, Perry, Rick], dtype=object)\\nIn [18]: unique_cands[2]\\nOut[18]: \\'Obama, Barack\\'\\nAn easy way to indicate party affiliation is using a dict:2\\nparties = {\\'Bachmann, Michelle\\': \\'Republican\\',\\n           \\'Cain, Herman\\': \\'Republican\\',\\n           \\'Gingrich, Newt\\': \\'Republican\\',\\n           \\'Huntsman, Jon\\': \\'Republican\\',\\n           \\'Johnson, Gary Earl\\': \\'Republican\\',\\n           \\'McCotter, Thaddeus G\\': \\'Republican\\',\\n           \\'Obama, Barack\\': \\'Democrat\\',\\n           \\'Paul, Ron\\': \\'Republican\\',\\n           \\'Pawlenty, Timothy\\': \\'Republican\\',\\n           \\'Perry, Rick\\': \\'Republican\\',\\n           \"Roemer, Charles E. \\'Buddy\\' III\": \\'Republican\\',\\n2. This makes the simplifying assumption that Gary Johnson is a Republican even though he later became\\nthe Libertarian party candidate.\\nExample: 2012 Federal Election Commission Database | 279\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 295, 'page_label': '280'}, page_content=\"'Romney, Mitt': 'Republican',\\n           'Santorum, Rick': 'Republican'}\\nNow, using this mapping and the map method on Series objects, you can compute an\\narray of political parties from the candidate names:\\nIn [20]: fec.cand_nm[123456:123461]\\nOut[20]:\\n123456    Obama, Barack\\n123457    Obama, Barack\\n123458    Obama, Barack\\n123459    Obama, Barack\\n123460    Obama, Barack\\nName: cand_nm\\nIn [21]: fec.cand_nm[123456:123461].map(parties)\\nOut[21]:\\n123456    Democrat\\n123457    Democrat\\n123458    Democrat\\n123459    Democrat\\n123460    Democrat\\nName: cand_nm\\n# Add it as a column\\nIn [22]: fec['party'] = fec.cand_nm.map(parties)\\nIn [23]: fec['party'].value_counts()\\nOut[23]:\\nDemocrat      593746\\nRepublican    407985\\nA couple of data preparation points. First, this data includes both contributions and\\nrefunds (negative contribution amount):\\nIn [24]: (fec.contb_receipt_amt > 0).value_counts()\\nOut[24]:\\nTrue     991475\\nFalse     10256\\nTo simplify the analysis, I’ll restrict the data set to positive contributions:\\nIn [25]: fec = fec[fec.contb_receipt_amt > 0]\\nSince Barack Obama and Mitt Romney are the main two candidates, I’ll also prepare\\na subset that just has contributions to their campaigns:\\nIn [26]: fec_mrbo = fec[fec.cand_nm.isin(['Obama, Barack', 'Romney, Mitt'])]\\nDonation Statistics by Occupation and Employer\\nDonations by occupation is another oft-studied statistic. For example, lawyers (attor-\\nneys) tend to donate more money to Democrats, while business executives tend to\\ndonate more to Republicans. You have no reason to believe me; you can see for yourself\\nin the data. First, the total number of donations by occupation is easy:\\n280 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 296, 'page_label': '281'}, page_content=\"In [27]: fec.contbr_occupation.value_counts()[:10]\\nOut[27]:\\nRETIRED                                   233990\\nINFORMATION REQUESTED                      35107\\nATTORNEY                                   34286\\nHOMEMAKER                                  29931\\nPHYSICIAN                                  23432\\nINFORMATION REQUESTED PER BEST EFFORTS     21138\\nENGINEER                                   14334\\nTEACHER                                    13990\\nCONSULTANT                                 13273\\nPROFESSOR                                  12555\\nYou will notice by looking at the occupations that many refer to the same basic job\\ntype, or there are several variants of the same thing. Here is a code snippet illustrates a\\ntechnique for cleaning up a few of them by mapping from one occupation to another;\\nnote the “trick” of using dict.get to allow occupations with no mapping to “pass\\nthrough”:\\nocc_mapping = {\\n   'INFORMATION REQUESTED PER BEST EFFORTS' : 'NOT PROVIDED',\\n   'INFORMATION REQUESTED' : 'NOT PROVIDED',\\n   'INFORMATION REQUESTED (BEST EFFORTS)' : 'NOT PROVIDED',\\n   'C.E.O.': 'CEO'\\n}\\n# If no mapping provided, return x\\nf = lambda x: occ_mapping.get(x, x)\\nfec.contbr_occupation = fec.contbr_occupation.map(f)\\nI’ll also do the same thing for employers:\\nemp_mapping = {\\n   'INFORMATION REQUESTED PER BEST EFFORTS' : 'NOT PROVIDED',\\n   'INFORMATION REQUESTED' : 'NOT PROVIDED',\\n   'SELF' : 'SELF-EMPLOYED',\\n   'SELF EMPLOYED' : 'SELF-EMPLOYED',\\n}\\n# If no mapping provided, return x\\nf = lambda x: emp_mapping.get(x, x)\\nfec.contbr_employer = fec.contbr_employer.map(f)\\nNow, you can use pivot_table to aggregate the data by party and occupation, then\\nfilter down to the subset that donated at least $2 million overall:\\nIn [34]: by_occupation = fec.pivot_table('contb_receipt_amt',\\n   ....:                                 rows='contbr_occupation',\\n   ....:                                 cols='party', aggfunc='sum')\\nIn [35]: over_2mm = by_occupation[by_occupation.sum(1) > 2000000]\\nIn [36]: over_2mm\\nOut[36]:\\nparty                 Democrat       Republican\\ncontbr_occupation\\nExample: 2012 Federal Election Commission Database | 281\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 297, 'page_label': '282'}, page_content=\"ATTORNEY           11141982.97   7477194.430000\\nCEO                 2074974.79   4211040.520000\\nCONSULTANT          2459912.71   2544725.450000\\nENGINEER             951525.55   1818373.700000\\nEXECUTIVE           1355161.05   4138850.090000\\nHOMEMAKER           4248875.80  13634275.780000\\nINVESTOR             884133.00   2431768.920000\\nLAWYER              3160478.87    391224.320000\\nMANAGER              762883.22   1444532.370000\\nNOT PROVIDED        4866973.96  20565473.010000\\nOWNER               1001567.36   2408286.920000\\nPHYSICIAN           3735124.94   3594320.240000\\nPRESIDENT           1878509.95   4720923.760000\\nPROFESSOR           2165071.08    296702.730000\\nREAL ESTATE          528902.09   1625902.250000\\nRETIRED            25305116.38  23561244.489999\\nSELF-EMPLOYED        672393.40   1640252.540000\\nIt can be easier to look at this data graphically as a bar plot ( 'barh' means horizontal\\nbar plot, see Figure 9-2):\\nIn [38]: over_2mm.plot(kind='barh')\\nFigure 9-2. Total donations by party for top occupations\\nYou might be interested in the top donor occupations or top companies donating to\\nObama and Romney. To do this, you can group by candidate name and use a variant\\nof the top method from earlier in the chapter:\\ndef get_top_amounts(group, key, n=5):\\n    totals = group.groupby(key)['contb_receipt_amt'].sum()\\n    # Order totals by key in descending order\\n    return totals.order(ascending=False)[-n:]\\n282 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 298, 'page_label': '283'}, page_content=\"Then aggregated by occupation and employer:\\nIn [40]: grouped = fec_mrbo.groupby('cand_nm')\\nIn [41]: grouped.apply(get_top_amounts, 'contbr_occupation', n=7)\\nOut[41]:\\ncand_nm        contbr_occupation\\nObama, Barack  RETIRED              25305116.38\\n               ATTORNEY             11141982.97\\n               NOT PROVIDED          4866973.96\\n               HOMEMAKER             4248875.80\\n               PHYSICIAN             3735124.94\\n               LAWYER                3160478.87\\n               CONSULTANT            2459912.71\\nRomney, Mitt   RETIRED              11508473.59\\n               NOT PROVIDED         11396894.84\\n               HOMEMAKER             8147446.22\\n               ATTORNEY              5364718.82\\n               PRESIDENT             2491244.89\\n               EXECUTIVE             2300947.03\\n               C.E.O.                1968386.11\\nName: contb_receipt_amt\\nIn [42]: grouped.apply(get_top_amounts, 'contbr_employer', n=10)\\nOut[42]:\\ncand_nm        contbr_employer\\nObama, Barack  RETIRED               22694358.85\\n               SELF-EMPLOYED         18626807.16\\n               NOT EMPLOYED           8586308.70\\n               NOT PROVIDED           5053480.37\\n               HOMEMAKER              2605408.54\\n               STUDENT                 318831.45\\n               VOLUNTEER               257104.00\\n               MICROSOFT               215585.36\\n               SIDLEY AUSTIN LLP       168254.00\\n               REFUSED                 149516.07\\nRomney, Mitt   NOT PROVIDED          12059527.24\\n               RETIRED               11506225.71\\n               HOMEMAKER              8147196.22\\n               SELF-EMPLOYED          7414115.22\\n               STUDENT                 496490.94\\n               CREDIT SUISSE           281150.00\\n               MORGAN STANLEY          267266.00\\n               GOLDMAN SACH & CO.      238250.00\\n               BARCLAYS CAPITAL        162750.00\\n               H.I.G. CAPITAL          139500.00\\nName: contb_receipt_amt\\nBucketing Donation Amounts\\nA useful way to analyze this data is to use the cut function to discretize the contributor\\namounts into buckets by contribution size:\\nIn [43]: bins = np.array([0, 1, 10, 100, 1000, 10000, 100000, 1000000, 10000000])\\nExample: 2012 Federal Election Commission Database | 283\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 299, 'page_label': '284'}, page_content=\"In [44]: labels = pd.cut(fec_mrbo.contb_receipt_amt, bins)\\nIn [45]: labels\\nOut[45]:\\nCategorical:contb_receipt_amt\\narray([(10, 100], (100, 1000], (100, 1000], ..., (1, 10], (10, 100],\\n       (100, 1000]], dtype=object)\\nLevels (8): array([(0, 1], (1, 10], (10, 100], (100, 1000], (1000, 10000],\\n       (10000, 100000], (100000, 1000000], (1000000, 10000000]], dtype=object)\\nWe can then group the data for Obama and Romney by name and bin label to get a\\nhistogram by donation size:\\nIn [46]: grouped = fec_mrbo.groupby(['cand_nm', labels])\\nIn [47]: grouped.size().unstack(0)\\nOut[47]:\\ncand_nm              Obama, Barack  Romney, Mitt\\ncontb_receipt_amt\\n(0, 1]                         493            77\\n(1, 10]                      40070          3681\\n(10, 100]                   372280         31853\\n(100, 1000]                 153991         43357\\n(1000, 10000]                22284         26186\\n(10000, 100000]                  2             1\\n(100000, 1000000]                3           NaN\\n(1000000, 10000000]              4           NaN\\nThis data shows that Obama has received a significantly larger number of small don-\\nations than Romney. You can also sum the contribution amounts and normalize within\\nbuckets to visualize percentage of total donations of each size by candidate:\\nIn [48]: bucket_sums = grouped.contb_receipt_amt.sum().unstack(0)\\nIn [49]: bucket_sums\\nOut[49]:\\ncand_nm              Obama, Barack  Romney, Mitt\\ncontb_receipt_amt\\n(0, 1]                      318.24         77.00\\n(1, 10]                  337267.62      29819.66\\n(10, 100]              20288981.41    1987783.76\\n(100, 1000]            54798531.46   22363381.69\\n(1000, 10000]          51753705.67   63942145.42\\n(10000, 100000]           59100.00      12700.00\\n(100000, 1000000]       1490683.08           NaN\\n(1000000, 10000000]     7148839.76           NaN\\nIn [50]: normed_sums = bucket_sums.div(bucket_sums.sum(axis=1), axis=0)\\nIn [51]: normed_sums\\nOut[51]:\\ncand_nm              Obama, Barack  Romney, Mitt\\ncontb_receipt_amt\\n(0, 1]                    0.805182      0.194818\\n(1, 10]                   0.918767      0.081233\\n(10, 100]                 0.910769      0.089231\\n284 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 300, 'page_label': '285'}, page_content=\"(100, 1000]               0.710176      0.289824\\n(1000, 10000]             0.447326      0.552674\\n(10000, 100000]           0.823120      0.176880\\n(100000, 1000000]         1.000000           NaN\\n(1000000, 10000000]       1.000000           NaN\\nIn [52]: normed_sums[:-2].plot(kind='barh', stacked=True)\\nI excluded the two largest bins as these are not donations by individuals. See Fig-\\nure 9-3 for the resulting figure.\\nFigure 9-3. Percentage of total donations received by candidates for each donation size\\nThere are of course many refinements and improvements of this analysis. For example,\\nyou could aggregate donations by donor name and zip code to adjust for donors who\\ngave many small amounts versus one or more large donations. I encourage you to\\ndownload it and explore it yourself.\\nDonation Statistics by State\\nAggregating the data by candidate and state is a routine affair:\\nIn [53]: grouped = fec_mrbo.groupby(['cand_nm', 'contbr_st'])\\nIn [54]: totals = grouped.contb_receipt_amt.sum().unstack(0).fillna(0)\\nIn [55]: totals = totals[totals.sum(1) > 100000]\\nIn [56]: totals[:10]\\nOut[56]:\\ncand_nm    Obama, Barack  Romney, Mitt\\ncontbr_st\\nExample: 2012 Federal Election Commission Database | 285\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 301, 'page_label': '286'}, page_content=\"AK             281840.15      86204.24\\nAL             543123.48     527303.51\\nAR             359247.28     105556.00\\nAZ            1506476.98    1888436.23\\nCA           23824984.24   11237636.60\\nCO            2132429.49    1506714.12\\nCT            2068291.26    3499475.45\\nDC            4373538.80    1025137.50\\nDE             336669.14      82712.00\\nFL            7318178.58    8338458.81\\nIf you divide each row by the total contribution amount, you get the relative percentage\\nof total donations by state for each candidate:\\nIn [57]: percent = totals.div(totals.sum(1), axis=0)\\nIn [58]: percent[:10]\\nOut[58]:\\ncand_nm    Obama, Barack  Romney, Mitt\\ncontbr_st\\nAK              0.765778      0.234222\\nAL              0.507390      0.492610\\nAR              0.772902      0.227098\\nAZ              0.443745      0.556255\\nCA              0.679498      0.320502\\nCO              0.585970      0.414030\\nCT              0.371476      0.628524\\nDC              0.810113      0.189887\\nDE              0.802776      0.197224\\nFL              0.467417      0.532583\\nI thought it would be interesting to look at this data plotted on a map, using ideas from\\nChapter 8. After locating a shape file for the state boundaries (http://nationalatlas.gov/\\natlasftp.html?openChapters=chpbound) and learning a bit more about matplotlib and\\nits basemap toolkit (I was aided by a blog posting from Thomas Lecocq) 3, I ended up\\nwith the following code for plotting these relative percentages:\\nfrom mpl_toolkits.basemap import Basemap, cm\\nimport numpy as np\\nfrom matplotlib import rcParams\\nfrom matplotlib.collections import LineCollection\\nimport matplotlib.pyplot as plt\\nfrom shapelib import ShapeFile\\nimport dbflib\\nobama = percent['Obama, Barack']\\nfig = plt.figure(figsize=(12, 12))\\nax = fig.add_axes([0.1,0.1,0.8,0.8])\\nlllat = 21; urlat = 53; lllon = -118; urlon = -62\\n3. http://www.geophysique.be/2011/01/27/matplotlib-basemap-tutorial-07-shapefiles-unleached/\\n286 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 302, 'page_label': '287'}, page_content=\"m = Basemap(ax=ax, projection='stere',\\n            lon_0=(urlon + lllon) / 2, lat_0=(urlat + lllat) / 2,\\n            llcrnrlat=lllat, urcrnrlat=urlat, llcrnrlon=lllon,\\n            urcrnrlon=urlon, resolution='l')\\nm.drawcoastlines()\\nm.drawcountries()\\nshp = ShapeFile('../states/statesp020')\\ndbf = dbflib.open('../states/statesp020')\\nfor npoly in range(shp.info()[0]):\\n    # Draw colored polygons on the map\\n    shpsegs = []\\n    shp_object = shp.read_object(npoly)\\n    verts = shp_object.vertices()\\n    rings = len(verts)\\n    for ring in range(rings):\\n        lons, lats = zip(*verts[ring])\\n        x, y = m(lons, lats)\\n        shpsegs.append(zip(x,y))\\n        if ring == 0:\\n            shapedict = dbf.read_record(npoly)\\n        name = shapedict['STATE']\\n    lines = LineCollection(shpsegs,antialiaseds=(1,))\\n    # state_to_code dict, e.g. 'ALASKA' -> 'AK', omitted\\n    try:\\n        per = obama[state_to_code[name.upper()]]\\n    except KeyError:\\n        continue\\n    lines.set_facecolors('k')\\n    lines.set_alpha(0.75 * per) # Shrink the percentage a bit\\n    lines.set_edgecolors('k')\\n    lines.set_linewidth(0.3)\\n    ax.add_collection(lines)\\nplt.show()\\nSee Figure 9-4 for the result.\\nExample: 2012 Federal Election Commission Database | 287\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 303, 'page_label': '288'}, page_content='Figure 9-4. US map aggregated donation statistics overlay (darker means more Democratic)\\n288 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 304, 'page_label': '289'}, page_content='CHAPTER 10\\nTime Series\\nTime series data is an important form of structured data in many different fields, such\\nas finance, economics, ecology, neuroscience, or physics. Anything that is observed or\\nmeasured at many points in time forms a time series. Many time series are fixed fre-\\nquency, which is to say that data points occur at regular intervals according to some\\nrule, such as every 15 seconds, every 5 minutes, or once per month. Time series can\\nalso be irregular without a fixed unit or time or offset between units. How you mark\\nand refer to time series data depends on the application and you may have one of the\\nfollowing:\\n• Timestamps, specific instants in time\\n• Fixed periods, such as the month January 2007 or the full year 2010\\n• Intervals of time, indicated by a start and end timestamp. Periods can be thought\\nof as special cases of intervals\\n• Experiment or elapsed time; each timestamp is a measure of time relative to a\\nparticular start time. For example, the diameter of a cookie baking each second\\nsince being placed in the oven\\nIn this chapter, I am mainly concerned with time series in the first 3 categories, though\\nmany of the techniques can be applied to experimental time series where the index may\\nbe an integer or floating point number indicating elapsed time from the start of the\\nexperiment. The simplest and most widely used kind of time series are those indexed\\nby timestamp.\\npandas provides a standard set of time series tools and data algorithms. With this, you\\ncan efficiently work with very large time series and easily slice and dice, aggregate, and\\nresample irregular and fixed frequency time series. As you might guess, many of these\\ntools are especially useful for financial and economics applications, but you could cer-\\ntainly use them to analyze server log data, too.\\n289\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 305, 'page_label': '290'}, page_content='Some of the features and code, in particular period logic, presented in\\nthis chapter were derived from the now defunct scikits.timeseries li-\\nbrary.\\nDate and Time Data Types and Tools\\nThe Python standard library includes data types for date and time data, as well as\\ncalendar-related functionality. The datetime, time, and calendar modules are the main\\nplaces to start. The datetime.datetime type, or simply datetime, is widely used:\\nIn [317]: from datetime import datetime\\nIn [318]: now = datetime.now()\\nIn [319]: now\\nOut[319]: datetime.datetime(2012, 8, 4, 17, 9, 21, 832092)\\nIn [320]: now.year, now.month, now.day\\nOut[320]: (2012, 8, 4)\\ndatetime stores both the date and time down to the microsecond. datetime.time\\ndelta represents the temporal difference between two datetime objects:\\nIn [321]: delta = datetime(2011, 1, 7) - datetime(2008, 6, 24, 8, 15)\\nIn [322]: delta\\nOut[322]: datetime.timedelta(926, 56700)\\nIn [323]: delta.days        In [324]: delta.seconds\\nOut[323]: 926               Out[324]: 56700\\nYou can add (or subtract) a timedelta or multiple thereof to a datetime object to yield\\na new shifted object:\\nIn [325]: from datetime import timedelta\\nIn [326]: start = datetime(2011, 1, 7)\\nIn [327]: start + timedelta(12)\\nOut[327]: datetime.datetime(2011, 1, 19, 0, 0)\\nIn [328]: start - 2 * timedelta(12)\\nOut[328]: datetime.datetime(2010, 12, 14, 0, 0)\\nThe data types in the datetime module are summarized in Table 10-1. While this chap-\\nter is mainly concerned with the data types in pandas and higher level time series ma-\\nnipulation, you will undoubtedly encounter the datetime-based types in many other\\nplaces in Python the wild.\\n290 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 306, 'page_label': '291'}, page_content=\"Table 10-1. Types in datetime module\\nType Description\\ndate Store calendar date (year, month, day) using the Gregorian calendar.\\ntime Store time of day as hours, minutes, seconds, and microseconds\\ndatetime Stores both date and time\\ntimedelta Represents the difference between two datetime values (as days, seconds, and micro-\\nseconds)\\nConverting between string and datetime\\ndatetime objects and pandas Timestamp objects, which I’ll introduce later, can be for-\\nmatted as strings using str or the strftime method, passing a format specification:\\nIn [329]: stamp = datetime(2011, 1, 3)\\nIn [330]: str(stamp)                   In [331]: stamp.strftime('%Y-%m-%d')\\nOut[330]: '2011-01-03 00:00:00'        Out[331]: '2011-01-03'\\nSee Table 10-2 for a complete list of the format codes. These same format codes can be\\nused to convert strings to dates using datetime.strptime:\\nIn [332]: value = '2011-01-03'\\nIn [333]: datetime.strptime(value, '%Y-%m-%d')\\nOut[333]: datetime.datetime(2011, 1, 3, 0, 0)\\nIn [334]: datestrs = ['7/6/2011', '8/6/2011']\\nIn [335]: [datetime.strptime(x, '%m/%d/%Y') for x in datestrs]\\nOut[335]: [datetime.datetime(2011, 7, 6, 0, 0), datetime.datetime(2011, 8, 6, 0, 0)]\\ndatetime.strptime is the best way to parse a date with a known format. However, it\\ncan be a bit annoying to have to write a format spec each time, especially for common\\ndate formats. In this case, you can use the parser.parse method in the third party \\ndateutil package:\\nIn [336]: from dateutil.parser import parse\\nIn [337]: parse('2011-01-03')\\nOut[337]: datetime.datetime(2011, 1, 3, 0, 0)\\ndateutil is capable of parsing almost any human-intelligible date representation:\\nIn [338]: parse('Jan 31, 1997 10:45 PM')\\nOut[338]: datetime.datetime(1997, 1, 31, 22, 45)\\nIn international locales, day appearing before month is very common, so you can pass\\ndayfirst=True to indicate this:\\nIn [339]: parse('6/12/2011', dayfirst=True)\\nOut[339]: datetime.datetime(2011, 12, 6, 0, 0)\\nDate and Time Data Types and Tools | 291\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 307, 'page_label': '292'}, page_content=\"pandas is generally oriented toward working with arrays of dates, whether used as an\\naxis index or a column in a DataFrame. The to_datetime method parses many different\\nkinds of date representations. Standard date formats like ISO8601 can be parsed very\\nquickly.\\nIn [340]: datestrs\\nOut[340]: ['7/6/2011', '8/6/2011']\\nIn [341]: pd.to_datetime(datestrs)\\nOut[341]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n[2011-07-06 00:00:00, 2011-08-06 00:00:00]\\nLength: 2, Freq: None, Timezone: None\\nIt also handles values that should be considered missing (None, empty string, etc.):\\nIn [342]: idx = pd.to_datetime(datestrs + [None])\\nIn [343]: idx\\nOut[343]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n[2011-07-06 00:00:00, ..., NaT]\\nLength: 3, Freq: None, Timezone: None\\nIn [344]: idx[2]\\nOut[344]: NaT\\nIn [345]: pd.isnull(idx)\\nOut[345]: array([False, False, True], dtype=bool)\\nNaT (Not a Time) is pandas’s NA value for timestamp data.\\ndateutil.parser is a useful, but not perfect tool. Notably, it will recog-\\nnize some strings as dates that you might prefer that it didn’t, like\\n'42' will be parsed as the year 2042 with today’s calendar date.\\nTable 10-2. Datetime format specification (ISO C89 compatible)\\nType Description\\n%Y 4-digit year\\n%y 2-digit year\\n%m 2-digit month [01, 12]\\n%d 2-digit day [01, 31]\\n%H Hour (24-hour clock) [00, 23]\\n%I Hour (12-hour clock) [01, 12]\\n%M 2-digit minute [00, 59]\\n%S Second [00, 61] (seconds 60, 61 account for leap seconds)\\n%w Weekday as integer [0 (Sunday), 6]\\n292 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 308, 'page_label': '293'}, page_content='Type Description\\n%U Week number of the year [00, 53]. Sunday is considered the first day of the week, and days before the first\\nSunday of the year are “week 0”.\\n%W Week number of the year [00, 53]. Monday is considered the first day of the week, and days before the first\\nMonday of the year are “week 0”.\\n%z UTC time zone offset as +HHMM or -HHMM, empty if time zone naive\\n%F Shortcut for %Y-%m-%d, for example 2012-4-18\\n%D Shortcut for %m/%d/%y, for example 04/18/12\\ndatetime objects also have a number of locale-specific formatting options for systems\\nin other countries or languages. For example, the abbreviated month names will be\\ndifferent on German or French systems compared with English systems. \\nTable 10-3. Locale-specific date formatting\\nType Description\\n%a Abbreviated weekday name\\n%A Full weekday name\\n%b Abbreviated month name\\n%B Full month name\\n%c Full date and time, for example ‘Tue 01 May 2012 04:20:57 PM’\\n%p Locale equivalent of AM or PM\\n%x Locale-appropriate formatted date; e.g. in US May 1, 2012 yields ’05/01/2012’\\n%X Locale-appropriate time, e.g. ’04:24:12 PM’\\nTime Series Basics\\nThe most basic kind of time series object in pandas is a Series indexed by timestamps,\\nwhich is often represented external to pandas as Python strings or datetime objects:\\nIn [346]: from datetime import datetime\\nIn [347]: dates = [datetime(2011, 1, 2), datetime(2011, 1, 5), datetime(2011, 1, 7),\\n   .....:          datetime(2011, 1, 8), datetime(2011, 1, 10), datetime(2011, 1, 12)]\\nIn [348]: ts = Series(np.random.randn(6), index=dates)\\nIn [349]: ts\\nOut[349]: \\n2011-01-02    0.690002\\n2011-01-05    1.001543\\n2011-01-07   -0.503087\\n2011-01-08   -0.622274\\nTime Series Basics | 293\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 309, 'page_label': '294'}, page_content=\"2011-01-10   -0.921169\\n2011-01-12   -0.726213\\nUnder the hood, these datetime objects have been put in a DatetimeIndex, and the\\nvariable ts is now of type TimeSeries:\\nIn [350]: type(ts)\\nOut[350]: pandas.core.series.TimeSeries\\nIn [351]: ts.index\\nOut[351]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n[2011-01-02 00:00:00, ..., 2011-01-12 00:00:00]\\nLength: 6, Freq: None, Timezone: None\\nIt’s not necessary to use the TimeSeries constructor explicitly; when\\ncreating a Series with a DatetimeIndex, pandas knows that the object is\\na time series.\\nLike other Series, arithmetic operations between differently-indexed time series auto-\\nmatically align on the dates:\\nIn [352]: ts + ts[::2]\\nOut[352]: \\n2011-01-02    1.380004\\n2011-01-05         NaN\\n2011-01-07   -1.006175\\n2011-01-08         NaN\\n2011-01-10   -1.842337\\n2011-01-12         NaN\\npandas stores timestamps using NumPy’s datetime64 data type at the nanosecond res-\\nolution:\\nIn [353]: ts.index.dtype\\nOut[353]: dtype('datetime64[ns]')\\nScalar values from a DatetimeIndex are pandas Timestamp objects\\nIn [354]: stamp = ts.index[0]\\nIn [355]: stamp\\nOut[355]: <Timestamp: 2011-01-02 00:00:00>\\nA Timestamp can be substituted anywhere you would use a datetime object. Addition-\\nally, it can store frequency information (if any) and understands how to do time zone\\nconversions and other kinds of manipulations. More on both of these things later.\\nIndexing, Selection, Subsetting\\nTimeSeries is a subclass of Series and thus behaves in the same way with regard to\\nindexing and selecting data based on label:\\n294 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 310, 'page_label': '295'}, page_content=\"In [356]: stamp = ts.index[2]\\nIn [357]: ts[stamp]\\nOut[357]: -0.50308739136034464\\nAs a convenience, you can also pass a string that is interpretable as a date:\\nIn [358]: ts['1/10/2011']             In [359]: ts['20110110']      \\nOut[358]: -0.92116860801301081        Out[359]: -0.92116860801301081\\nFor longer time series, a year or only a year and month can be passed to easily select\\nslices of data:\\nIn [360]: longer_ts = Series(np.random.randn(1000),\\n   .....:                    index=pd.date_range('1/1/2000', periods=1000))\\nIn [361]: longer_ts\\nOut[361]: \\n2000-01-01    0.222896\\n2000-01-02    0.051316\\n2000-01-03   -1.157719\\n2000-01-04    0.816707\\n...\\n2002-09-23   -0.395813\\n2002-09-24   -0.180737\\n2002-09-25    1.337508\\n2002-09-26   -0.416584\\nFreq: D, Length: 1000\\nIn [362]: longer_ts['2001']        In [363]: longer_ts['2001-05']\\nOut[362]:                          Out[363]:                     \\n2001-01-01   -1.499503             2001-05-01    1.662014        \\n2001-01-02    0.545154             2001-05-02   -1.189203        \\n2001-01-03    0.400823             2001-05-03    0.093597        \\n2001-01-04   -1.946230             2001-05-04   -0.539164        \\n...                                ...                           \\n2001-12-28   -1.568139             2001-05-28   -0.683066        \\n2001-12-29   -0.900887             2001-05-29   -0.950313        \\n2001-12-30    0.652346             2001-05-30    0.400710        \\n2001-12-31    0.871600             2001-05-31   -0.126072        \\nFreq: D, Length: 365               Freq: D, Length: 31\\nSlicing with dates works just like with a regular Series:\\nIn [364]: ts[datetime(2011, 1, 7):]\\nOut[364]: \\n2011-01-07   -0.503087\\n2011-01-08   -0.622274\\n2011-01-10   -0.921169\\n2011-01-12   -0.726213\\nBecause most time series data is ordered chronologically, you can slice with timestamps\\nnot contained in a time series to perform a range query:\\nIn [365]: ts                  In [366]: ts['1/6/2011':'1/11/2011']\\nOut[365]:                     Out[366]:                           \\n2011-01-02    0.690002        2011-01-07   -0.503087              \\nTime Series Basics | 295\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 311, 'page_label': '296'}, page_content=\"2011-01-05    1.001543        2011-01-08   -0.622274              \\n2011-01-07   -0.503087        2011-01-10   -0.921169              \\n2011-01-08   -0.622274                                            \\n2011-01-10   -0.921169                                            \\n2011-01-12   -0.726213\\nAs before you can pass either a string date, datetime, or Timestamp. Remember that\\nslicing in this manner produces views on the source time series just like slicing NumPy\\narrays. There is an equivalent instance method truncate which slices a TimeSeries be-\\ntween two dates:\\nIn [367]: ts.truncate(after='1/9/2011')\\nOut[367]: \\n2011-01-02    0.690002\\n2011-01-05    1.001543\\n2011-01-07   -0.503087\\n2011-01-08   -0.622274\\nAll of the above holds true for DataFrame as well, indexing on its rows:\\nIn [368]: dates = pd.date_range('1/1/2000', periods=100, freq='W-WED')\\nIn [369]: long_df = DataFrame(np.random.randn(100, 4),\\n   .....:                     index=dates,\\n   .....:                     columns=['Colorado', 'Texas', 'New York', 'Ohio'])\\nIn [370]: long_df.ix['5-2001']\\nOut[370]: \\n            Colorado     Texas  New York      Ohio\\n2001-05-02  0.943479 -0.349366  0.530412 -0.508724\\n2001-05-09  0.230643 -0.065569 -0.248717 -0.587136\\n2001-05-16 -1.022324  1.060661  0.954768 -0.511824\\n2001-05-23 -1.387680  0.767902 -1.164490  1.527070\\n2001-05-30  0.287542  0.715359 -0.345805  0.470886\\nTime Series with Duplicate Indices\\nIn some applications, there may be multiple data observations falling on a particular\\ntimestamp. Here is an example:\\nIn [371]: dates = pd.DatetimeIndex(['1/1/2000', '1/2/2000', '1/2/2000', '1/2/2000',\\n   .....:                           '1/3/2000'])\\nIn [372]: dup_ts = Series(np.arange(5), index=dates)\\nIn [373]: dup_ts\\nOut[373]: \\n2000-01-01    0\\n2000-01-02    1\\n2000-01-02    2\\n2000-01-02    3\\n2000-01-03    4\\nWe can tell that the index is not unique by checking its is_unique property:\\n296 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 312, 'page_label': '297'}, page_content=\"In [374]: dup_ts.index.is_unique\\nOut[374]: False\\nIndexing into this time series will now either produce scalar values or slices depending\\non whether a timestamp is duplicated:\\nIn [375]: dup_ts['1/3/2000']  # not duplicated\\nOut[375]: 4\\nIn [376]: dup_ts['1/2/2000']  # duplicated\\nOut[376]: \\n2000-01-02    1\\n2000-01-02    2\\n2000-01-02    3\\nSuppose you wanted to aggregate the data having non-unique timestamps. One way\\nto do this is to use groupby and pass level=0 (the only level of indexing!):\\nIn [377]: grouped = dup_ts.groupby(level=0)\\nIn [378]: grouped.mean()      In [379]: grouped.count()\\nOut[378]:                     Out[379]:                \\n2000-01-01    0               2000-01-01    1          \\n2000-01-02    2               2000-01-02    3          \\n2000-01-03    4               2000-01-03    1\\nDate Ranges, Frequencies, and Shifting\\nGeneric time series in pandas are assumed to be irregular; that is, they have no fixed\\nfrequency. For many applications this is sufficient. However, it’s often desirable to work\\nrelative to a fixed frequency, such as daily, monthly, or every 15 minutes, even if that\\nmeans introducing missing values into a time series. Fortunately pandas has a full suite\\nof standard time series frequencies and tools for resampling, inferring frequencies, and\\ngenerating fixed frequency date ranges. For example, in the example time series, con-\\nverting it to be fixed daily frequency can be accomplished by calling resample:\\nIn [380]: ts                  In [381]: ts.resample('D')\\nOut[380]:                     Out[381]:                 \\n2011-01-02    0.690002        2011-01-02    0.690002    \\n2011-01-05    1.001543        2011-01-03         NaN    \\n2011-01-07   -0.503087        2011-01-04         NaN    \\n2011-01-08   -0.622274        2011-01-05    1.001543    \\n2011-01-10   -0.921169        2011-01-06         NaN    \\n2011-01-12   -0.726213        2011-01-07   -0.503087    \\n                              2011-01-08   -0.622274    \\n                              2011-01-09         NaN    \\n                              2011-01-10   -0.921169    \\n                              2011-01-11         NaN    \\n                              2011-01-12   -0.726213    \\n                              Freq: D\\nConversion between frequencies or resampling is a big enough topic to have its own\\nsection later. Here I’ll show you how to use the base frequencies and multiples thereof.\\nDate Ranges, Frequencies, and Shifting | 297\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 313, 'page_label': '298'}, page_content=\"Generating Date Ranges\\nWhile I used it previously without explanation, you may have guessed that pan\\ndas.date_range is responsible for generating a DatetimeIndex with an indicated length\\naccording to a particular frequency:\\nIn [382]: index = pd.date_range('4/1/2012', '6/1/2012')\\nIn [383]: index\\nOut[383]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n[2012-04-01 00:00:00, ..., 2012-06-01 00:00:00]\\nLength: 62, Freq: D, Timezone: None\\nBy default, date_range generates daily timestamps. If you pass only a start or end date,\\nyou must pass a number of periods to generate:\\nIn [384]: pd.date_range(start='4/1/2012', periods=20)\\nOut[384]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n[2012-04-01 00:00:00, ..., 2012-04-20 00:00:00]\\nLength: 20, Freq: D, Timezone: None\\nIn [385]: pd.date_range(end='6/1/2012', periods=20)\\nOut[385]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n[2012-05-13 00:00:00, ..., 2012-06-01 00:00:00]\\nLength: 20, Freq: D, Timezone: None\\nThe start and end dates define strict boundaries for the generated date index. For ex-\\nample, if you wanted a date index containing the last business day of each month, you\\nwould pass the 'BM' frequency (business end of month) and only dates falling on or\\ninside the date interval will be included:\\nIn [386]: pd.date_range('1/1/2000', '12/1/2000', freq='BM')\\nOut[386]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n[2000-01-31 00:00:00, ..., 2000-11-30 00:00:00]\\nLength: 11, Freq: BM, Timezone: None\\ndate_range by default preserves the time (if any) of the start or end timestamp:\\nIn [387]: pd.date_range('5/2/2012 12:56:31', periods=5)\\nOut[387]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n[2012-05-02 12:56:31, ..., 2012-05-06 12:56:31]\\nLength: 5, Freq: D, Timezone: None\\nSometimes you will have start or end dates with time information but want to generate\\na set of timestamps normalized to midnight as a convention. To do this, there is a\\nnormalize option:\\nIn [388]: pd.date_range('5/2/2012 12:56:31', periods=5, normalize=True)\\nOut[388]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n298 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 314, 'page_label': '299'}, page_content=\"[2012-05-02 00:00:00, ..., 2012-05-06 00:00:00]\\nLength: 5, Freq: D, Timezone: None\\nFrequencies and Date Offsets\\nFrequencies in pandas are composed of a base frequency and a multiplier. Base fre-\\nquencies are typically referred to by a string alias, like 'M' for monthly or 'H' for hourly.\\nFor each base frequency, there is an object defined generally referred to as a date off-\\nset. For example, hourly frequency can be represented with the Hour class:\\nIn [389]: from pandas.tseries.offsets import Hour, Minute\\nIn [390]: hour = Hour()\\nIn [391]: hour\\nOut[391]: <1 Hour>\\nYou can define a multiple of an offset by passing an integer:\\nIn [392]: four_hours = Hour(4)\\nIn [393]: four_hours\\nOut[393]: <4 Hours>\\nIn most applications, you would never need to explicitly create one of these objects,\\ninstead using a string alias like 'H' or '4H'. Putting an integer before the base frequency\\ncreates a multiple:\\nIn [394]: pd.date_range('1/1/2000', '1/3/2000 23:59', freq='4h')\\nOut[394]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n[2000-01-01 00:00:00, ..., 2000-01-03 20:00:00]\\nLength: 18, Freq: 4H, Timezone: None\\nMany offsets can be combined together by addition:\\nIn [395]: Hour(2) + Minute(30)\\nOut[395]: <150 Minutes>\\nSimilarly, you can pass frequency strings like '2h30min' which will effectively be parsed\\nto the same expression:\\nIn [396]: pd.date_range('1/1/2000', periods=10, freq='1h30min')\\nOut[396]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n[2000-01-01 00:00:00, ..., 2000-01-01 13:30:00]\\nLength: 10, Freq: 90T, Timezone: None\\nSome frequencies describe points in time that are not evenly spaced. For example,\\n'M' (calendar month end) and 'BM' (last business/weekday of month) depend on the\\nnumber of days in a month and, in the latter case, whether the month ends on a weekend\\nor not. For lack of a better term, I call these anchored offsets.\\nSee Table 10-4 for a listing of frequency codes and date offset classes available in pandas.\\nDate Ranges, Frequencies, and Shifting | 299\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 315, 'page_label': '300'}, page_content='Users can define their own custom frequency classes to provide date\\nlogic not available in pandas, though the full details of that are outside\\nthe scope of this book.\\nTable 10-4. Base Time Series Frequencies\\nAlias Offset Type Description\\nD Day Calendar daily\\nB BusinessDay Business daily\\nH Hour Hourly\\nT or min Minute Minutely\\nS Second Secondly\\nL or ms Milli Millisecond (1/1000th of 1 second)\\nU Micro Microsecond (1/1000000th of 1 second)\\nM MonthEnd Last calendar day of month\\nBM BusinessMonthEnd Last business day (weekday) of month\\nMS MonthBegin First calendar day of month\\nBMS BusinessMonthBegin First weekday of month\\nW-MON, W-TUE, ... Week Weekly on given day of week: MON, TUE, WED, THU, FRI, SAT,\\nor SUN.\\nWOM-1MON, WOM-2MON, ... WeekOfMonth Generate weekly dates in the first, second, third, or fourth week\\nof the month. For example, WOM-3FRI for the 3rd Friday of\\neach month.\\nQ-JAN, Q-FEB, ... QuarterEnd Quarterly dates anchored on last calendar day of each month,\\nfor year ending in indicated month: JAN, FEB, MAR, APR, MAY,\\nJUN, JUL, AUG, SEP, OCT, NOV, or DEC.\\nBQ-JAN, BQ-FEB, ... BusinessQuarterEnd Quarterly dates anchored on last weekday day of each month,\\nfor year ending in indicated month\\nQS-JAN, QS-FEB, ... QuarterBegin Quarterly dates anchored on first calendar day of each month,\\nfor year ending in indicated month\\nBQS-JAN, BQS-FEB, ... BusinessQuarterBegin Quarterly dates anchored on first weekday day of each month,\\nfor year ending in indicated month\\nA-JAN, A-FEB, ... YearEnd Annual dates anchored on last calendar day of given month:\\nJAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, NOV, or DEC.\\nBA-JAN, BA-FEB, ... BusinessYearEnd Annual dates anchored on last weekday of given month\\nAS-JAN, AS-FEB, ... YearBegin Annual dates anchored on first day of given month\\nBAS-JAN, BAS-FEB, ... BusinessYearBegin Annual dates anchored on first weekday of given month\\n300 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 316, 'page_label': '301'}, page_content=\"Week of month dates\\nOne useful frequency class is “week of month”, starting with WOM. This enables you to\\nget dates like the third Friday of each month:\\nIn [397]: rng = pd.date_range('1/1/2012', '9/1/2012', freq='WOM-3FRI')\\nIn [398]: list(rng)\\nOut[398]: \\n[<Timestamp: 2012-01-20 00:00:00>,\\n <Timestamp: 2012-02-17 00:00:00>,\\n <Timestamp: 2012-03-16 00:00:00>,\\n <Timestamp: 2012-04-20 00:00:00>,\\n <Timestamp: 2012-05-18 00:00:00>,\\n <Timestamp: 2012-06-15 00:00:00>,\\n <Timestamp: 2012-07-20 00:00:00>,\\n <Timestamp: 2012-08-17 00:00:00>]\\nTraders of US equity options will recognize these dates as the standard dates of monthly\\nexpiry.\\nShifting (Leading and Lagging) Data\\n“Shifting” refers to moving data backward and forward through time. Both Series and\\nDataFrame have a shift method for doing naive shifts forward or backward, leaving\\nthe index unmodified:\\nIn [399]: ts = Series(np.random.randn(4),\\n   .....:             index=pd.date_range('1/1/2000', periods=4, freq='M'))\\nIn [400]: ts                In [401]: ts.shift(2)       In [402]: ts.shift(-2)\\nOut[400]:                   Out[401]:                   Out[402]:             \\n2000-01-31    0.575283      2000-01-31         NaN      2000-01-31    1.814582\\n2000-02-29    0.304205      2000-02-29         NaN      2000-02-29    1.634858\\n2000-03-31    1.814582      2000-03-31    0.575283      2000-03-31         NaN\\n2000-04-30    1.634858      2000-04-30    0.304205      2000-04-30         NaN\\nFreq: M                     Freq: M                     Freq: M\\nA common use of shift is computing percent changes in a time series or multiple time\\nseries as DataFrame columns. This is expressed as\\nts / ts.shift(1) - 1\\nBecause naive shifts leave the index unmodified, some data is discarded. Thus if the\\nfrequency is known, it can be passed to shift to advance the timestamps instead of\\nsimply the data:\\nIn [403]: ts.shift(2, freq='M')\\nOut[403]: \\n2000-03-31    0.575283\\n2000-04-30    0.304205\\n2000-05-31    1.814582\\n2000-06-30    1.634858\\nFreq: M\\nDate Ranges, Frequencies, and Shifting | 301\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 317, 'page_label': '302'}, page_content=\"Other frequencies can be passed, too, giving you a lot of flexibility in how to lead and\\nlag the data:\\nIn [404]: ts.shift(3, freq='D')        In [405]: ts.shift(1, freq='3D')\\nOut[404]:                              Out[405]:                       \\n2000-02-03    0.575283                 2000-02-03    0.575283          \\n2000-03-03    0.304205                 2000-03-03    0.304205          \\n2000-04-03    1.814582                 2000-04-03    1.814582          \\n2000-05-03    1.634858                 2000-05-03    1.634858          \\n                                                                       \\nIn [406]: ts.shift(1, freq='90T')\\nOut[406]: \\n2000-01-31 01:30:00    0.575283\\n2000-02-29 01:30:00    0.304205\\n2000-03-31 01:30:00    1.814582\\n2000-04-30 01:30:00    1.634858\\nShifting dates with offsets\\nThe pandas date offsets can also be used with datetime or Timestamp objects:\\nIn [407]: from pandas.tseries.offsets import Day, MonthEnd\\nIn [408]: now = datetime(2011, 11, 17)\\nIn [409]: now + 3 * Day()\\nOut[409]: datetime.datetime(2011, 11, 20, 0, 0)\\nIf you add an anchored offset like MonthEnd, the first increment will roll forward a date\\nto the next date according to the frequency rule:\\nIn [410]: now + MonthEnd()\\nOut[410]: datetime.datetime(2011, 11, 30, 0, 0)\\nIn [411]: now + MonthEnd(2)\\nOut[411]: datetime.datetime(2011, 12, 31, 0, 0)\\nAnchored offsets can explicitly “roll” dates forward or backward using their rollfor\\nward and rollback methods, respectively:\\nIn [412]: offset = MonthEnd()\\nIn [413]: offset.rollforward(now)\\nOut[413]: datetime.datetime(2011, 11, 30, 0, 0)\\nIn [414]: offset.rollback(now)\\nOut[414]: datetime.datetime(2011, 10, 31, 0, 0)\\nA clever use of date offsets is to use these methods with groupby:\\nIn [415]: ts = Series(np.random.randn(20),\\n   .....:             index=pd.date_range('1/15/2000', periods=20, freq='4d'))\\nIn [416]: ts.groupby(offset.rollforward).mean()\\nOut[416]: \\n2000-01-31   -0.448874\\n302 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 318, 'page_label': '303'}, page_content=\"2000-02-29   -0.683663\\n2000-03-31    0.251920\\nOf course, an easier and faster way to do this is using resample (much more on this later):\\nIn [417]: ts.resample('M', how='mean')\\nOut[417]: \\n2000-01-31   -0.448874\\n2000-02-29   -0.683663\\n2000-03-31    0.251920\\nFreq: M\\nTime Zone Handling\\nWorking with time zones is generally considered one of the most unpleasant parts of\\ntime series manipulation. In particular, daylight savings time (DST) transitions are a\\ncommon source of complication. As such, many time series users choose to work with\\ntime series in coordinated universal time or UTC, which is the successor to Greenwich\\nMean Time and is the current international standard. Time zones are expressed as\\noffsets from UTC; for example, New York is four hours behind UTC during daylight\\nsavings time and 5 hours the rest of the year.\\nIn Python, time zone information comes from the 3rd party pytz library, which exposes\\nthe Olson database, a compilation of world time zone information. This is especially\\nimportant for historical data because the DST transition dates (and even UTC offsets)\\nhave been changed numerous times depending on the whims of local governments. In\\nthe United States,the DST transition times have been changed many times since 1900!\\nFor detailed information about pytz library, you’ll need to look at that library’s docu-\\nmentation. As far as this book is concerned, pandas wraps pytz’s functionality so you\\ncan ignore its API outside of the time zone names. Time zone names can be found\\ninteractively and in the docs:\\nIn [418]: import pytz\\nIn [419]: pytz.common_timezones[-5:]\\nOut[419]: ['US/Eastern', 'US/Hawaii', 'US/Mountain', 'US/Pacific', 'UTC']\\nTo get a time zone object from pytz, use pytz.timezone:\\nIn [420]: tz = pytz.timezone('US/Eastern')\\nIn [421]: tz\\nOut[421]: <DstTzInfo 'US/Eastern' EST-1 day, 19:00:00 STD>\\nMethods in pandas will accept either time zone names or these objects. I recommend\\njust using the names.\\nTime Zone Handling | 303\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 319, 'page_label': '304'}, page_content=\"Localization and Conversion\\nBy default, time series in pandas are time zone naive. Consider the following time series:\\nrng = pd.date_range('3/9/2012 9:30', periods=6, freq='D')\\nts = Series(np.random.randn(len(rng)), index=rng)\\nThe index’s tz field is None:\\nIn [423]: print(ts.index.tz)\\nNone\\nDate ranges can be generated with a time zone set:\\nIn [424]: pd.date_range('3/9/2012 9:30', periods=10, freq='D', tz='UTC')\\nOut[424]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n[2012-03-09 09:30:00, ..., 2012-03-18 09:30:00]\\nLength: 10, Freq: D, Timezone: UTC\\nConversion from naive to localized is handled by the tz_localize method:\\nIn [425]: ts_utc = ts.tz_localize('UTC')\\nIn [426]: ts_utc\\nOut[426]: \\n2012-03-09 09:30:00+00:00    0.414615\\n2012-03-10 09:30:00+00:00    0.427185\\n2012-03-11 09:30:00+00:00    1.172557\\n2012-03-12 09:30:00+00:00   -0.351572\\n2012-03-13 09:30:00+00:00    1.454593\\n2012-03-14 09:30:00+00:00    2.043319\\nFreq: D\\nIn [427]: ts_utc.index\\nOut[427]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n[2012-03-09 09:30:00, ..., 2012-03-14 09:30:00]\\nLength: 6, Freq: D, Timezone: UTC\\nOnce a time series has been localized to a particular time zone, it can be converted to\\nanother time zone using tz_convert:\\nIn [428]: ts_utc.tz_convert('US/Eastern')\\nOut[428]: \\n2012-03-09 04:30:00-05:00    0.414615\\n2012-03-10 04:30:00-05:00    0.427185\\n2012-03-11 05:30:00-04:00    1.172557\\n2012-03-12 05:30:00-04:00   -0.351572\\n2012-03-13 05:30:00-04:00    1.454593\\n2012-03-14 05:30:00-04:00    2.043319\\nFreq: D\\nIn the case of the above time series, which straddles a DST transition in the US/Eastern\\ntime zone, we could localize to EST and convert to, say, UTC or Berlin time:\\nIn [429]: ts_eastern = ts.tz_localize('US/Eastern')\\n304 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 320, 'page_label': '305'}, page_content=\"In [430]: ts_eastern.tz_convert('UTC')\\nOut[430]: \\n2012-03-09 14:30:00+00:00    0.414615\\n2012-03-10 14:30:00+00:00    0.427185\\n2012-03-11 13:30:00+00:00    1.172557\\n2012-03-12 13:30:00+00:00   -0.351572\\n2012-03-13 13:30:00+00:00    1.454593\\n2012-03-14 13:30:00+00:00    2.043319\\nFreq: D\\nIn [431]: ts_eastern.tz_convert('Europe/Berlin')\\nOut[431]: \\n2012-03-09 15:30:00+01:00    0.414615\\n2012-03-10 15:30:00+01:00    0.427185\\n2012-03-11 14:30:00+01:00    1.172557\\n2012-03-12 14:30:00+01:00   -0.351572\\n2012-03-13 14:30:00+01:00    1.454593\\n2012-03-14 14:30:00+01:00    2.043319\\nFreq: D\\ntz_localize and tz_convert are also instance methods on DatetimeIndex:\\nIn [432]: ts.index.tz_localize('Asia/Shanghai')\\nOut[432]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n[2012-03-09 09:30:00, ..., 2012-03-14 09:30:00]\\nLength: 6, Freq: D, Timezone: Asia/Shanghai\\nLocalizing naive timestamps also checks for ambiguous or non-existent\\ntimes around daylight savings time transitions.\\nOperations with Time Zone−aware Timestamp Objects\\nSimilar to time series and date ranges, individual Timestamp objects similarly can be\\nlocalized from naive to time zone-aware and converted from one time zone to another:\\nIn [433]: stamp = pd.Timestamp('2011-03-12 04:00')\\nIn [434]: stamp_utc = stamp.tz_localize('utc')\\nIn [435]: stamp_utc.tz_convert('US/Eastern')\\nOut[435]: <Timestamp: 2011-03-11 23:00:00-0500 EST, tz=US/Eastern>\\nYou can also pass a time zone when creating the Timestamp:\\nIn [436]: stamp_moscow = pd.Timestamp('2011-03-12 04:00', tz='Europe/Moscow')\\nIn [437]: stamp_moscow\\nOut[437]: <Timestamp: 2011-03-12 04:00:00+0300 MSK, tz=Europe/Moscow>\\nTime zone-aware Timestamp objects internally store a UTC timestamp value as nano-\\nseconds since the UNIX epoch (January 1, 1970); this UTC value is invariant between\\ntime zone conversions:\\nTime Zone Handling | 305\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 321, 'page_label': '306'}, page_content=\"In [438]: stamp_utc.value\\nOut[438]: 1299902400000000000\\nIn [439]: stamp_utc.tz_convert('US/Eastern').value\\nOut[439]: 1299902400000000000\\nWhen performing time arithmetic using pandas’s DateOffset objects, daylight savings\\ntime transitions are respected where possible:\\n# 30 minutes before DST transition\\nIn [440]: from pandas.tseries.offsets import Hour\\nIn [441]: stamp = pd.Timestamp('2012-03-12 01:30', tz='US/Eastern')\\nIn [442]: stamp\\nOut[442]: <Timestamp: 2012-03-12 01:30:00-0400 EDT, tz=US/Eastern>\\nIn [443]: stamp + Hour()\\nOut[443]: <Timestamp: 2012-03-12 02:30:00-0400 EDT, tz=US/Eastern>\\n# 90 minutes before DST transition\\nIn [444]: stamp = pd.Timestamp('2012-11-04 00:30', tz='US/Eastern')\\nIn [445]: stamp\\nOut[445]: <Timestamp: 2012-11-04 00:30:00-0400 EDT, tz=US/Eastern>\\nIn [446]: stamp + 2 * Hour()\\nOut[446]: <Timestamp: 2012-11-04 01:30:00-0500 EST, tz=US/Eastern>\\nOperations between Different Time Zones\\nIf two time series with different time zones are combined, the result will be UTC. Since\\nthe timestamps are stored under the hood in UTC, this is a straightforward operation\\nand requires no conversion to happen:\\nIn [447]: rng = pd.date_range('3/7/2012 9:30', periods=10, freq='B')\\nIn [448]: ts = Series(np.random.randn(len(rng)), index=rng)\\nIn [449]: ts\\nOut[449]: \\n2012-03-07 09:30:00   -1.749309\\n2012-03-08 09:30:00   -0.387235\\n2012-03-09 09:30:00   -0.208074\\n2012-03-12 09:30:00   -1.221957\\n2012-03-13 09:30:00   -0.067460\\n2012-03-14 09:30:00    0.229005\\n2012-03-15 09:30:00   -0.576234\\n2012-03-16 09:30:00    0.816895\\n2012-03-19 09:30:00   -0.772192\\n2012-03-20 09:30:00   -1.333576\\nFreq: B\\nIn [450]: ts1 = ts[:7].tz_localize('Europe/London')\\n306 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 322, 'page_label': '307'}, page_content=\"In [451]: ts2 = ts1[2:].tz_convert('Europe/Moscow')\\nIn [452]: result = ts1 + ts2\\nIn [453]: result.index\\nOut[453]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n[2012-03-07 09:30:00, ..., 2012-03-15 09:30:00]\\nLength: 7, Freq: B, Timezone: UTC\\nPeriods and Period Arithmetic\\nPeriods represent time spans, like days, months, quarters, or years. The Period class\\nrepresents this data type, requiring a string or integer and a frequency from the above\\ntable:\\nIn [454]: p = pd.Period(2007, freq='A-DEC')\\nIn [455]: p\\nOut[455]: Period('2007', 'A-DEC')\\nIn this case, the Period object represents the full timespan from January 1, 2007 to\\nDecember 31, 2007, inclusive. Conveniently, adding and subtracting integers from pe-\\nriods has the effect of shifting by their frequency:\\nIn [456]: p + 5                          In [457]: p - 2                  \\nOut[456]: Period('2012', 'A-DEC')        Out[457]: Period('2005', 'A-DEC')\\nIf two periods have the same frequency, their difference is the number of units between\\nthem:\\nIn [458]: pd.Period('2014', freq='A-DEC') - p\\nOut[458]: 7\\nRegular ranges of periods can be constructed using the period_range function:\\nIn [459]: rng = pd.period_range('1/1/2000', '6/30/2000', freq='M')\\nIn [460]: rng\\nOut[460]: \\n<class 'pandas.tseries.period.PeriodIndex'>\\nfreq: M\\n[2000-01, ..., 2000-06]\\nlength: 6\\nThe PeriodIndex class stores a sequence of periods and can serve as an axis index in\\nany pandas data structure:\\nIn [461]: Series(np.random.randn(6), index=rng)\\nOut[461]: \\n2000-01   -0.309119\\n2000-02    0.028558\\n2000-03    1.129605\\n2000-04   -0.374173\\n2000-05   -0.011401\\nPeriods and Period Arithmetic | 307\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 323, 'page_label': '308'}, page_content=\"2000-06    0.272924\\nFreq: M\\nIf you have an array of strings, you can also appeal to the PeriodIndex class itself:\\nIn [462]: values = ['2001Q3', '2002Q2', '2003Q1']\\nIn [463]: index = pd.PeriodIndex(values, freq='Q-DEC')\\nIn [464]: index\\nOut[464]: \\n<class 'pandas.tseries.period.PeriodIndex'>\\nfreq: Q-DEC\\n[2001Q3, ..., 2003Q1]\\nlength: 3\\nPeriod Frequency Conversion\\nPeriods and PeriodIndex objects can be converted to another frequency using their \\nasfreq method. As an example, suppose we had an annual period and wanted to convert\\nit into a monthly period either at the start or end of the year. This is fairly straightfor-\\nward:\\nIn [465]: p = pd.Period('2007', freq='A-DEC')\\nIn [466]: p.asfreq('M', how='start')      In [467]: p.asfreq('M', how='end')\\nOut[466]: Period('2007-01', 'M')         Out[467]: Period('2007-12', 'M')\\nYou can think of Period('2007', 'A-DEC') as being a cursor pointing to a span of time,\\nsubdivided by monthly periods. See Figure 10-1 for an illustration of this. For a fiscal\\nyear ending on a month other than December, the monthly subperiods belonging are\\ndifferent:\\nIn [468]: p = pd.Period('2007', freq='A-JUN')\\nIn [469]: p.asfreq('M', 'start')       In [470]: p.asfreq('M', 'end')   \\nOut[469]: Period('2006-07', 'M')      Out[470]: Period('2007-07', 'M')\\nWhen converting from high to low frequency, the superperiod will be determined de-\\npending on where the subperiod “belongs”. For example, in A-JUN frequency, the month\\nAug-2007 is actually part of the 2008 period:\\nIn [471]: p = pd.Period('2007-08', 'M')\\nIn [472]: p.asfreq('A-JUN')\\nOut[472]: Period('2008', 'A-JUN')\\nWhole PeriodIndex objects or TimeSeries can be similarly converted with the same\\nsemantics:\\nIn [473]: rng = pd.period_range('2006', '2009', freq='A-DEC')\\nIn [474]: ts = Series(np.random.randn(len(rng)), index=rng)\\nIn [475]: ts\\n308 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 324, 'page_label': '309'}, page_content=\"Out[475]: \\n2006   -0.601544\\n2007    0.574265\\n2008   -0.194115\\n2009    0.202225\\nFreq: A-DEC\\nIn [476]: ts.asfreq('M', how='start')      In [477]: ts.asfreq('B', how='end')\\nOut[476]:                                  Out[477]:                          \\n2006-01   -0.601544                        2006-12-29   -0.601544            \\n2007-01    0.574265                        2007-12-31    0.574265            \\n2008-01   -0.194115                        2008-12-31   -0.194115            \\n2009-01    0.202225                        2009-12-31    0.202225            \\nFreq: M                                    Freq: B\\nFigure 10-1. Period frequency conversion illustration\\nQuarterly Period Frequencies\\nQuarterly data is standard in accounting, finance, and other fields. Much quarterly data\\nis reported relative to a fiscal year end, typically the last calendar or business day of one\\nof the 12 months of the year. As such, the period 2012Q4 has a different meaning de-\\npending on fiscal year end. pandas supports all 12 possible quarterly frequencies as Q-\\nJAN through Q-DEC:\\nIn [478]: p = pd.Period('2012Q4', freq='Q-JAN')\\nIn [479]: p\\nOut[479]: Period('2012Q4', 'Q-JAN')\\nIn the case of fiscal year ending in January, 2012Q4 runs from November through Jan-\\nuary, which you can check by converting to daily frequency. See Figure 10-2 for an\\nillustration:\\nIn [480]: p.asfreq('D', 'start')          In [481]: p.asfreq('D', 'end')      \\nOut[480]: Period('2011-11-01', 'D')      Out[481]: Period('2012-01-31', 'D')\\nPeriods and Period Arithmetic | 309\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 325, 'page_label': '310'}, page_content=\"Thus, it’s possible to do period arithmetic very easily; for example, to get the timestamp\\nat 4PM on the 2nd to last business day of the quarter, you could do:\\nIn [482]: p4pm = (p.asfreq('B', 'e') - 1).asfreq('T', 's') + 16 * 60\\nIn [483]: p4pm\\nOut[483]: Period('2012-01-30 16:00', 'T')\\nIn [484]: p4pm.to_timestamp()\\nOut[484]: <Timestamp: 2012-01-30 16:00:00>\\nFigure 10-2. Different quarterly frequency conventions\\nGenerating quarterly ranges works as you would expect using period_range. Arithmetic\\nis identical, too:\\nIn [485]: rng = pd.period_range('2011Q3', '2012Q4', freq='Q-JAN')\\nIn [486]: ts = Series(np.arange(len(rng)), index=rng)\\nIn [487]: ts\\nOut[487]: \\n2011Q3    0\\n2011Q4    1\\n2012Q1    2\\n2012Q2    3\\n2012Q3    4\\n2012Q4    5\\nFreq: Q-JAN\\nIn [488]: new_rng = (rng.asfreq('B', 'e') - 1).asfreq('T', 's') + 16 * 60\\nIn [489]: ts.index = new_rng.to_timestamp()\\nIn [490]: ts\\nOut[490]: \\n2010-10-28 16:00:00    0\\n2011-01-28 16:00:00    1\\n2011-04-28 16:00:00    2\\n2011-07-28 16:00:00    3\\n2011-10-28 16:00:00    4\\n2012-01-30 16:00:00    5\\n310 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 326, 'page_label': '311'}, page_content=\"Converting Timestamps to Periods (and Back)\\nSeries and DataFrame objects indexed by timestamps can be converted to periods using\\nthe to_period method:\\nIn [491]: rng = pd.date_range('1/1/2000', periods=3, freq='M')\\nIn [492]: ts = Series(randn(3), index=rng)\\nIn [493]: pts = ts.to_period()\\nIn [494]: ts                  In [495]: pts       \\nOut[494]:                     Out[495]:           \\n2000-01-31   -0.505124        2000-01   -0.505124\\n2000-02-29    2.954439        2000-02    2.954439\\n2000-03-31   -2.630247        2000-03   -2.630247\\nFreq: M                       Freq: M\\nSince periods always refer to non-overlapping timespans, a timestamp can only belong\\nto a single period for a given frequency. While the frequency of the new PeriodIndex is\\ninferred from the timestamps by default, you can specify any frequency you want. There\\nis also no problem with having duplicate periods in the result:\\nIn [496]: rng = pd.date_range('1/29/2000', periods=6, freq='D')\\nIn [497]: ts2 = Series(randn(6), index=rng)\\nIn [498]: ts2.to_period('M')\\nOut[498]: \\n2000-01   -0.352453\\n2000-01   -0.477808\\n2000-01    0.161594\\n2000-02    1.686833\\n2000-02    0.821965\\n2000-02   -0.667406\\nFreq: M\\nTo convert back to timestamps, use to_timestamp:\\nIn [499]: pts = ts.to_period()\\nIn [500]: pts\\nOut[500]: \\n2000-01   -0.505124\\n2000-02    2.954439\\n2000-03   -2.630247\\nFreq: M\\nIn [501]: pts.to_timestamp(how='end')\\nOut[501]: \\n2000-01-31   -0.505124\\n2000-02-29    2.954439\\n2000-03-31   -2.630247\\nFreq: M\\nPeriods and Period Arithmetic | 311\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 327, 'page_label': '312'}, page_content=\"Creating a PeriodIndex from Arrays\\nFixed frequency data sets are sometimes stored with timespan information spread\\nacross multiple columns. For example, in this macroeconomic data set, the year and\\nquarter are in different columns:\\nIn [502]: data = pd.read_csv('ch08/macrodata.csv')\\nIn [503]: data.year            In [504]: data.quarter    \\nOut[503]:                      Out[504]:                 \\n0    1959                      0    1                    \\n1    1959                      1    2                    \\n2    1959                      2    3                    \\n3    1959                      3    4                    \\n...                            ...                       \\n199    2008                    199    4                  \\n200    2009                    200    1                  \\n201    2009                    201    2                  \\n202    2009                    202    3                  \\nName: year, Length: 203        Name: quarter, Length: 203\\nBy passing these arrays to PeriodIndex with a frequency, they can be combined to form\\nan index for the DataFrame:\\nIn [505]: index = pd.PeriodIndex(year=data.year, quarter=data.quarter, freq='Q-DEC')\\nIn [506]: index\\nOut[506]: \\n<class 'pandas.tseries.period.PeriodIndex'>\\nfreq: Q-DEC\\n[1959Q1, ..., 2009Q3]\\nlength: 203\\nIn [507]: data.index = index\\nIn [508]: data.infl\\nOut[508]: \\n1959Q1    0.00\\n1959Q2    2.34\\n1959Q3    2.74\\n1959Q4    0.27\\n...\\n2008Q4   -8.79\\n2009Q1    0.94\\n2009Q2    3.37\\n2009Q3    3.56\\nFreq: Q-DEC, Name: infl, Length: 203\\nResampling and Frequency Conversion\\nResampling refers to the process of converting a time series from one frequency to\\nanother. Aggregating higher frequency data to lower frequency is called downsam-\\npling, while converting lower frequency to higher frequency is called upsampling. Not\\n312 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 328, 'page_label': '313'}, page_content=\"all resampling falls into either of these categories; for example, converting W-WED (weekly\\non Wednesday) to W-FRI is neither upsampling nor downstampling.\\npandas objects are equipped with a resample method, which is the workhorse function\\nfor all frequency conversion:\\nIn [509]: rng = pd.date_range('1/1/2000', periods=100, freq='D')\\nIn [510]: ts = Series(randn(len(rng)), index=rng)\\nIn [511]: ts.resample('M', how='mean')\\nOut[511]: \\n2000-01-31    0.170876\\n2000-02-29    0.165020\\n2000-03-31    0.095451\\n2000-04-30    0.363566\\nFreq: M\\nIn [512]: ts.resample('M', how='mean', kind='period')\\nOut[512]: \\n2000-01    0.170876\\n2000-02    0.165020\\n2000-03    0.095451\\n2000-04    0.363566\\nFreq: M\\nresample is a flexible and high-performance method that can be used to process very\\nlarge time series. I’ll illustrate its semantics and use through a series of examples.\\nTable 10-5. Resample method arguments\\nArgument Description\\nfreq String or DateOffset indicating desired resampled frequency, e.g. ‘M', ’5min', or Sec\\nond(15)\\nhow='mean' Function name or array function producing aggregated value, for example 'mean',\\n'ohlc', np.max. Defaults to 'mean'. Other common values: 'first', 'last',\\n'median', 'ohlc', 'max', 'min'.\\naxis=0 Axis to resample on, default axis=0\\nfill_method=None How to interpolate when upsampling, as in 'ffill' or 'bfill'. By default does no\\ninterpolation.\\nclosed='right' In downsampling, which end of each interval is closed (inclusive), 'right' or\\n'left'. Defaults to 'right'\\nlabel='right' In downsampling, how to label the aggregated result, with the 'right' or 'left'\\nbin edge. For example, the 9:30 to 9:35 5-minute interval could be labeled 9:30 or\\n9:35. Defaults to 'right' (or 9:35, in this example).\\nloffset=None Time adjustment to the bin labels, such as '-1s' / Second(-1) to shift the aggregate\\nlabels one second earlier\\nlimit=None When forward or backward filling, the maximum number of periods to fill\\nResampling and Frequency Conversion | 313\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 329, 'page_label': '314'}, page_content=\"Argument Description\\nkind=None Aggregate to periods ('period') or timestamps ('timestamp'); defaults to kind of\\nindex the time series has\\nconvention=None When resampling periods, the convention ('start' or 'end') for converting the low\\nfrequency period to high frequency. Defaults to 'end'\\nDownsampling\\nAggregating data to a regular, lower frequency is a pretty normal time series task. The\\ndata you’re aggregating doesn’t need to be fixed frequently; the desired frequency de-\\nfines bin edges that are used to slice the time series into pieces to aggregate. For example,\\nto convert to monthly, 'M' or 'BM', the data need to be chopped up into one month\\nintervals. Each interval is said to be half-open; a data point can only belong to one\\ninterval, and the union of the intervals must make up the whole time frame. There are\\na couple things to think about when using resample to downsample data:\\n• Which side of each interval is closed\\n• How to label each aggregated bin, either with the start of the interval or the end\\nTo illustrate, let’s look at some one-minute data:\\nIn [513]: rng = pd.date_range('1/1/2000', periods=12, freq='T')\\nIn [514]: ts = Series(np.arange(12), index=rng)\\nIn [515]: ts\\nOut[515]: \\n2000-01-01 00:00:00     0\\n2000-01-01 00:01:00     1\\n2000-01-01 00:02:00     2\\n2000-01-01 00:03:00     3\\n2000-01-01 00:04:00     4\\n2000-01-01 00:05:00     5\\n2000-01-01 00:06:00     6\\n2000-01-01 00:07:00     7\\n2000-01-01 00:08:00     8\\n2000-01-01 00:09:00     9\\n2000-01-01 00:10:00    10\\n2000-01-01 00:11:00    11\\nFreq: T\\nSuppose you wanted to aggregate this data into five-minute chunks or bars by taking\\nthe sum of each group:\\nIn [516]: ts.resample('5min', how='sum')\\nOut[516]: \\n2000-01-01 00:00:00     0\\n2000-01-01 00:05:00    15\\n2000-01-01 00:10:00    40\\n2000-01-01 00:15:00    11\\nFreq: 5T\\n314 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 330, 'page_label': '315'}, page_content=\"The frequency you pass defines bin edges in five-minute increments. By default, the\\nright bin edge is inclusive, so the 00:05 value is included in the 00:00 to 00:05 inter-\\nval.1 Passing closed='left' changes the interval to be closed on the left:\\nIn [517]: ts.resample('5min', how='sum', closed='left')\\nOut[517]: \\n2000-01-01 00:05:00    10\\n2000-01-01 00:10:00    35\\n2000-01-01 00:15:00    21\\nFreq: 5T\\nAs you can see, the resulting time series is labeled by the timestamps from the right side\\nof each bin. By passing label='left' you can label them with the left bin edge:\\nIn [518]: ts.resample('5min', how='sum', closed='left', label='left')\\nOut[518]: \\n2000-01-01 00:00:00    10\\n2000-01-01 00:05:00    35\\n2000-01-01 00:10:00    21\\nFreq: 5T\\nSee Figure 10-3 for an illustration of minutely data being resampled to five-minute.\\nFigure 10-3. 5-minute resampling illustration of closed, label conventions\\nLastly, you might want to shift the result index by some amount, say subtracting one\\nsecond from the right edge to make it more clear which interval the timestamp refers\\nto. To do this, pass a string or date offset to loffset:\\nIn [519]: ts.resample('5min', how='sum', loffset='-1s')\\nOut[519]: \\n1999-12-31 23:59:59     0\\n2000-01-01 00:04:59    15\\n2000-01-01 00:09:59    40\\n2000-01-01 00:14:59    11\\nFreq: 5T\\n1. The choice of closed='right', label='right' as the default might seem a bit odd to some users. In\\npractice the choice is somewhat arbitrary; for some target frequencies, closed='left' is preferable, while\\nfor others closed='right' makes more sense. The important thing is that you keep in mind exactly how\\nyou are segmenting the data.\\nResampling and Frequency Conversion | 315\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 331, 'page_label': '316'}, page_content=\"This also could have been accomplished by calling the shift method on the result\\nwithout the loffset.\\nOpen-High-Low-Close (OHLC) resampling\\nIn finance, an ubiquitous way to aggregate a time series is to compute four values for\\neach bucket: the first (open), last (close), maximum (high), and minimal (low) values.\\nBy passing how='ohlc' you will obtain a DataFrame having columns containing these\\nfour aggregates, which are efficiently computed in a single sweep of the data:\\nIn [520]: ts.resample('5min', how='ohlc')\\nOut[520]: \\n                     open  high  low  close\\n2000-01-01 00:00:00     0     0    0      0\\n2000-01-01 00:05:00     1     5    1      5\\n2000-01-01 00:10:00     6    10    6     10\\n2000-01-01 00:15:00    11    11   11     11\\nResampling with GroupBy\\nAn alternate way to downsample is to use pandas’s groupby functionality. For example,\\nyou can group by month or weekday by passing a function that accesses those fields\\non the time series’s index:\\nIn [521]: rng = pd.date_range('1/1/2000', periods=100, freq='D')\\nIn [522]: ts = Series(np.arange(100), index=rng)\\nIn [523]: ts.groupby(lambda x: x.month).mean()\\nOut[523]: \\n1    15\\n2    45\\n3    75\\n4    95\\nIn [524]: ts.groupby(lambda x: x.weekday).mean()\\nOut[524]: \\n0    47.5\\n1    48.5\\n2    49.5\\n3    50.5\\n4    51.5\\n5    49.0\\n6    50.0\\nUpsampling and Interpolation\\nWhen converting from a low frequency to a higher frequency, no aggregation is needed.\\nLet’s consider a DataFrame with some weekly data:\\nIn [525]: frame = DataFrame(np.random.randn(2, 4),\\n   .....:                   index=pd.date_range('1/1/2000', periods=2, freq='W-WED'),\\n   .....:                   columns=['Colorado', 'Texas', 'New York', 'Ohio'])\\n316 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 332, 'page_label': '317'}, page_content=\"In [526]: frame[:5]\\nOut[526]: \\n            Colorado     Texas  New York     Ohio\\n2000-01-05 -0.609657 -0.268837  0.195592  0.85979\\n2000-01-12 -0.263206  1.141350 -0.101937 -0.07666\\nWhen resampling this to daily frequency, by default missing values are introduced:\\nIn [527]: df_daily = frame.resample('D')\\nIn [528]: df_daily\\nOut[528]: \\n            Colorado     Texas  New York     Ohio\\n2000-01-05 -0.609657 -0.268837  0.195592  0.85979\\n2000-01-06       NaN       NaN       NaN      NaN\\n2000-01-07       NaN       NaN       NaN      NaN\\n2000-01-08       NaN       NaN       NaN      NaN\\n2000-01-09       NaN       NaN       NaN      NaN\\n2000-01-10       NaN       NaN       NaN      NaN\\n2000-01-11       NaN       NaN       NaN      NaN\\n2000-01-12 -0.263206  1.141350 -0.101937 -0.07666\\nSuppose you wanted to fill forward each weekly value on the non-Wednesdays. The\\nsame filling or interpolation methods available in the fillna and reindex methods are\\navailable for resampling:\\nIn [529]: frame.resample('D', fill_method='ffill')\\nOut[529]: \\n            Colorado     Texas  New York     Ohio\\n2000-01-05 -0.609657 -0.268837  0.195592  0.85979\\n2000-01-06 -0.609657 -0.268837  0.195592  0.85979\\n2000-01-07 -0.609657 -0.268837  0.195592  0.85979\\n2000-01-08 -0.609657 -0.268837  0.195592  0.85979\\n2000-01-09 -0.609657 -0.268837  0.195592  0.85979\\n2000-01-10 -0.609657 -0.268837  0.195592  0.85979\\n2000-01-11 -0.609657 -0.268837  0.195592  0.85979\\n2000-01-12 -0.263206  1.141350 -0.101937 -0.07666\\nYou can similarly choose to only fill a certain number of periods forward to limit how\\nfar to continue using an observed value:\\nIn [530]: frame.resample('D', fill_method='ffill', limit=2)\\nOut[530]: \\n            Colorado     Texas  New York     Ohio\\n2000-01-05 -0.609657 -0.268837  0.195592  0.85979\\n2000-01-06 -0.609657 -0.268837  0.195592  0.85979\\n2000-01-07 -0.609657 -0.268837  0.195592  0.85979\\n2000-01-08       NaN       NaN       NaN      NaN\\n2000-01-09       NaN       NaN       NaN      NaN\\n2000-01-10       NaN       NaN       NaN      NaN\\n2000-01-11       NaN       NaN       NaN      NaN\\n2000-01-12 -0.263206  1.141350 -0.101937 -0.07666\\nNotably, the new date index need not overlap with the old one at all:\\nResampling and Frequency Conversion | 317\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 333, 'page_label': '318'}, page_content=\"In [531]: frame.resample('W-THU', fill_method='ffill')\\nOut[531]: \\n            Colorado     Texas  New York     Ohio\\n2000-01-06 -0.609657 -0.268837  0.195592  0.85979\\n2000-01-13 -0.263206  1.141350 -0.101937 -0.07666\\nResampling with Periods\\nResampling data indexed by periods is reasonably straightforward and works as you\\nwould hope:\\nIn [532]: frame = DataFrame(np.random.randn(24, 4),\\n   .....:                   index=pd.period_range('1-2000', '12-2001', freq='M'),\\n   .....:                   columns=['Colorado', 'Texas', 'New York', 'Ohio'])\\nIn [533]: frame[:5]\\nOut[533]: \\n         Colorado     Texas  New York      Ohio\\n2000-01  0.120837  1.076607  0.434200  0.056432\\n2000-02 -0.378890  0.047831  0.341626  1.567920\\n2000-03 -0.047619 -0.821825 -0.179330 -0.166675\\n2000-04  0.333219 -0.544615 -0.653635 -2.311026\\n2000-05  1.612270 -0.806614  0.557884  0.580201\\nIn [534]: annual_frame = frame.resample('A-DEC', how='mean')\\nIn [535]: annual_frame\\nOut[535]: \\n      Colorado     Texas  New York      Ohio\\n2000  0.352070 -0.553642  0.196642 -0.094099\\n2001  0.158207  0.042967 -0.360755  0.184687\\nUpsampling is more nuanced as you must make a decision about which end of the\\ntimespan in the new frequency to place the values before resampling, just like the \\nasfreq method. The convention argument defaults to 'end' but can also be 'start':\\n# Q-DEC: Quarterly, year ending in December\\nIn [536]: annual_frame.resample('Q-DEC', fill_method='ffill')\\nOut[536]: \\n        Colorado     Texas  New York      Ohio\\n2000Q4  0.352070 -0.553642  0.196642 -0.094099\\n2001Q1  0.352070 -0.553642  0.196642 -0.094099\\n2001Q2  0.352070 -0.553642  0.196642 -0.094099\\n2001Q3  0.352070 -0.553642  0.196642 -0.094099\\n2001Q4  0.158207  0.042967 -0.360755  0.184687\\nIn [537]: annual_frame.resample('Q-DEC', fill_method='ffill', convention='start')\\nOut[537]: \\n        Colorado     Texas  New York      Ohio\\n2000Q1  0.352070 -0.553642  0.196642 -0.094099\\n2000Q2  0.352070 -0.553642  0.196642 -0.094099\\n2000Q3  0.352070 -0.553642  0.196642 -0.094099\\n2000Q4  0.352070 -0.553642  0.196642 -0.094099\\n2001Q1  0.158207  0.042967 -0.360755  0.184687\\n318 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 334, 'page_label': '319'}, page_content=\"Since periods refer to timespans, the rules about upsampling and downsampling are\\nmore rigid:\\n• In downsampling, the target frequency must be a subperiod of the source frequency.\\n• In upsampling, the target frequency must be a superperiod of the source frequency.\\nIf these rules are not satisfied, an exception will be raised. This mainly affects the quar-\\nterly, annual, and weekly frequencies; for example, the timespans defined by Q-MAR only\\nline up with A-MAR, A-JUN, A-SEP, and A-DEC:\\nIn [538]: annual_frame.resample('Q-MAR', fill_method='ffill')\\nOut[538]: \\n        Colorado     Texas  New York      Ohio\\n2001Q3  0.352070 -0.553642  0.196642 -0.094099\\n2001Q4  0.352070 -0.553642  0.196642 -0.094099\\n2002Q1  0.352070 -0.553642  0.196642 -0.094099\\n2002Q2  0.352070 -0.553642  0.196642 -0.094099\\n2002Q3  0.158207  0.042967 -0.360755  0.184687\\nTime Series Plotting\\nPlots with pandas time series have improved date formatting compared with matplotlib\\nout of the box. As an example, I downloaded some stock price data on a few common\\nUS stock from Yahoo! Finance:\\nIn [539]: close_px_all = pd.read_csv('ch09/stock_px.csv', parse_dates=True, index_col=0)\\nIn [540]: close_px = close_px_all[['AAPL', 'MSFT', 'XOM']]\\nIn [541]: close_px = close_px.resample('B', fill_method='ffill')\\nIn [542]: close_px\\nOut[542]: \\n<class 'pandas.core.frame.DataFrame'>\\nDatetimeIndex: 2292 entries, 2003-01-02 00:00:00 to 2011-10-14 00:00:00\\nFreq: B\\nData columns:\\nAAPL    2292  non-null values\\nMSFT    2292  non-null values\\nXOM     2292  non-null values\\ndtypes: float64(3)\\nCalling plot on one of the columns grenerates a simple plot, seen in Figure 10-4.\\nIn [544]: close_px['AAPL'].plot()\\nWhen called on a DataFrame, as you would expect, all of the time series are drawn on\\na single subplot with a legend indicating which is which. I’ll plot only the year 2009\\ndata so you can see how both months and years are formatted on the X axis; see\\nFigure 10-5.\\nIn [546]: close_px.ix['2009'].plot()\\nTime Series Plotting | 319\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 335, 'page_label': '320'}, page_content=\"In [548]: close_px['AAPL'].ix['01-2011':'03-2011'].plot()\\nQuarterly frequency data is also more nicely formatted with quarterly markers, some-\\nthing that would be quite a bit more work to do by hand. See Figure 10-7.\\nIn [550]: appl_q = close_px['AAPL'].resample('Q-DEC', fill_method='ffill')\\nIn [551]: appl_q.ix['2009':].plot()\\nA last feature of time series plotting in pandas is that by right-clicking and dragging to\\nzoom in and out, the dates will be dynamically expanded or contracted and reformat-\\nting depending on the timespan contained in the plot view. This is of course only true\\nwhen using matplotlib in interactive mode.\\nMoving Window Functions\\nA common class of array transformations intended for time series operations are sta-\\ntistics and other functions evaluated over a sliding window or with exponentially de-\\nFigure 10-4. AAPL Daily Price\\nFigure 10-5. Stock Prices in 2009\\n320 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 336, 'page_label': '321'}, page_content='caying weights. I call these moving window functions, even though it includes functions\\nwithout a fixed-length window like exponentially-weighted moving average. Like other\\nstatistical functions, these also automatically exclude missing data.\\nrolling_mean is one of the simplest such functions. It takes a TimeSeries or DataFrame\\nalong with a window (expressed as a number of periods):\\nIn [555]: close_px.AAPL.plot()\\nOut[555]: <matplotlib.axes.AxesSubplot at 0x1099b3990>\\nIn [556]: pd.rolling_mean(close_px.AAPL, 250).plot()\\nSee Figure 10-8 for the plot. By default functions like rolling_mean require the indicated\\nnumber of non-NA observations. This behavior can be changed to account for missing\\ndata and, in particular, the fact that you will have fewer than window periods of data at\\nthe beginning of the time series (see Figure 10-9):\\nFigure 10-6. Apple Daily Price in 1/2011-3/2011\\nFigure 10-7. Apple Quarterly Price 2009-2011\\nMoving Window Functions | 321\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 337, 'page_label': '322'}, page_content='In [558]: appl_std250 = pd.rolling_std(close_px.AAPL, 250, min_periods=10)\\nIn [559]: appl_std250[5:12]\\nOut[559]: \\n2003-01-09         NaN\\n2003-01-10         NaN\\n2003-01-13         NaN\\n2003-01-14         NaN\\n2003-01-15    0.077496\\n2003-01-16    0.074760\\n2003-01-17    0.112368\\nFreq: B\\nIn [560]: appl_std250.plot()\\nFigure 10-8. Apple Price with 250-day MA\\nFigure 10-9. Apple 250-day daily return standard deviation\\nTo compute an expanding window mean, you can see that an expanding window is just\\na special case where the window is the length of the time series, but only one or more\\nperiods is required to compute a value:\\n322 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 338, 'page_label': '323'}, page_content='# Define expanding mean in terms of rolling_mean\\nIn [561]: expanding_mean = lambda x: rolling_mean(x, len(x), min_periods=1)\\nCalling rolling_mean and friends on a DataFrame applies the transformation to each\\ncolumn (see Figure 10-10):\\nIn [563]: pd.rolling_mean(close_px, 60).plot(logy=True)\\nFigure 10-10. Stocks Prices 60-day MA (log Y-axis)\\nSee Table 10-6 for a listing of related functions in pandas.\\nTable 10-6. Moving window and exponentially-weighted functions\\nFunction Description\\nrolling_count Returns number of non-NA observations in each trailing window.\\nrolling_sum Moving window sum.\\nrolling_mean Moving window mean.\\nrolling_median Moving window median.\\nrolling_var, rolling_std Moving window variance and standard deviation, respectively. Uses n - 1 denom-\\ninator.\\nrolling_skew, rolling_kurt Moving window skewness (3rd moment) and kurtosis (4th moment), respectively.\\nrolling_min, rolling_max Moving window minimum and maximum.\\nrolling_quantile Moving window score at percentile/sample quantile.\\nrolling_corr, rolling_cov Moving window correlation and covariance.\\nrolling_apply Apply generic array function over a moving window.\\newma Exponentially-weighted moving average.\\newmvar, ewmstd Exponentially-weighted moving variance and standard deviation.\\newmcorr, ewmcov Exponentially-weighted moving correlation and covariance.\\nMoving Window Functions | 323\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 339, 'page_label': '324'}, page_content=\"bottleneck, a Python library by Keith Goodman, provides an alternate\\nimplementation of NaN-friendly moving window functions and may be\\nworth looking at depending on your application.\\nExponentially-weighted functions\\nAn alternative to using a static window size with equally-weighted observations is to\\nspecify a constant decay factor to give more weight to more recent observations. In\\nmathematical terms, if mat is the moving average result at time t and x is the time series\\nin question, each value in the result is computed as mat = a * mat - 1 + (a - 1) * x_t, where\\na is the decay factor. There are a couple of ways to specify the decay factor, a popular\\none is using a span, which makes the result comparable to a simple moving window\\nfunction with window size equal to the span.\\nSince an exponentially-weighted statistic places more weight on more recent observa-\\ntions, it “adapts” faster to changes compared with the equal-weighted version. Here’s\\nan example comparing a 60-day moving average of Apple’s stock price with an EW\\nmoving average with span=60 (see Figure 10-11):\\nfig, axes = plt.subplots(nrows=2, ncols=1, sharex=True, sharey=True,\\n                         figsize=(12, 7))\\naapl_px = close_px.AAPL['2005':'2009']\\nma60 = pd.rolling_mean(aapl_px, 60, min_periods=50)\\newma60 = pd.ewma(aapl_px, span=60)\\naapl_px.plot(style='k-', ax=axes[0])\\nma60.plot(style='k--', ax=axes[0])\\naapl_px.plot(style='k-', ax=axes[1])\\newma60.plot(style='k--', ax=axes[1])\\naxes[0].set_title('Simple MA')\\naxes[1].set_title('Exponentially-weighted MA')\\nBinary Moving Window Functions\\nSome statistical operators, like correlation and covariance, need to operate on two time\\nseries. As an example, financial analysts are often interested in a stock’s correlation to\\na benchmark index like the S&P 500. We can compute that by computing the percent\\nchanges and using rolling_corr (see Figure 10-12):\\nIn [570]: spx_rets = spx_px / spx_px.shift(1) - 1\\nIn [571]: returns = close_px.pct_change()\\nIn [572]: corr = pd.rolling_corr(returns.AAPL, spx_rets, 125, min_periods=100)\\nIn [573]: corr.plot()\\n324 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 340, 'page_label': '325'}, page_content='Suppose you wanted to compute the correlation of the S&P 500 index with many stocks\\nat once. Writing a loop and creating a new DataFrame would be easy but maybe get\\nrepetitive, so if you pass a TimeSeries and a DataFrame, a function like rolling_corr\\nwill compute the correlation of the TimeSeries (spx_rets in this case) with each column\\nin the DataFrame. See Figure 10-13 for the plot of the result:\\nIn [575]: corr = pd.rolling_corr(returns, spx_rets, 125, min_periods=100)\\nIn [576]: corr.plot()\\nFigure 10-11. Simple moving average versus exponentially-weighted\\nFigure 10-12. Six-month AAPL return correlation to S&P 500\\nMoving Window Functions | 325\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 341, 'page_label': '326'}, page_content='User-Defined Moving Window Functions\\nThe rolling_apply function provides a means to apply an array function of your own\\ndevising over a moving window. The only requirement is that the function produce a\\nsingle value (a reduction) from each piece of the array. For example, while we can\\ncompute sample quantiles using rolling_quantile, we might be interested in the per-\\ncentile rank of a particular value over the sample. The scipy.stats.percentileof\\nscore function does just this:\\nIn [578]: from scipy.stats import percentileofscore\\nIn [579]: score_at_2percent = lambda x: percentileofscore(x, 0.02)\\nIn [580]: result = pd.rolling_apply(returns.AAPL, 250, score_at_2percent)\\nIn [581]: result.plot()\\nFigure 10-13. Six-month return correlations to S&P 500\\nFigure 10-14. Percentile rank of 2% AAPL return over 1 year window\\n326 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 342, 'page_label': '327'}, page_content=\"Performance and Memory Usage Notes\\nTimestamps and periods are represented as 64-bit integers using NumPy’s date\\ntime64 dtype. This means that for each data point, there is an associated 8 bytes of\\nmemory per timestamp. Thus, a time series with 1 million float64 data points has a\\nmemory footprint of approximately 16 megabytes. Since pandas makes every effort to\\nshare indexes among time series, creating views on existing time series do not cause\\nany more memory to be used. Additionally, indexes for lower frequencies (daily and\\nup) are stored in a central cache, so that any fixed-frequency index is a view on the date\\ncache. Thus, if you have a large collection of low-frequency time series, the memory\\nfootprint of the indexes will not be as significant.\\nPerformance-wise, pandas has been highly optimized for data alignment operations\\n(the behind-the-scenes work of differently indexed ts1 + ts2) and resampling. Here is\\nan example of aggregating 10MM data points to OHLC:\\nIn [582]: rng = pd.date_range('1/1/2000', periods=10000000, freq='10ms')\\nIn [583]: ts = Series(np.random.randn(len(rng)), index=rng)\\nIn [584]: ts\\nOut[584]: \\n2000-01-01 00:00:00          -1.402235\\n2000-01-01 00:00:00.010000    2.424667\\n2000-01-01 00:00:00.020000   -1.956042\\n2000-01-01 00:00:00.030000   -0.897339\\n...\\n2000-01-02 03:46:39.960000    0.495530\\n2000-01-02 03:46:39.970000    0.574766\\n2000-01-02 03:46:39.980000    1.348374\\n2000-01-02 03:46:39.990000    0.665034\\nFreq: 10L, Length: 10000000\\nIn [585]: ts.resample('15min', how='ohlc')\\nOut[585]: \\n<class 'pandas.core.frame.DataFrame'>\\nDatetimeIndex: 113 entries, 2000-01-01 00:00:00 to 2000-01-02 04:00:00\\nFreq: 15T\\nData columns:\\nopen     113  non-null values\\nhigh     113  non-null values\\nlow      113  non-null values\\nclose    113  non-null values\\ndtypes: float64(4)\\nIn [586]: %timeit ts.resample('15min', how='ohlc')\\n10 loops, best of 3: 61.1 ms per loop\\nThe runtime may depend slightly on the relative size of the aggregated result; higher\\nfrequency aggregates unsurprisingly take longer to compute:\\nIn [587]: rng = pd.date_range('1/1/2000', periods=10000000, freq='1s')\\nPerformance and Memory Usage Notes | 327\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 343, 'page_label': '328'}, page_content=\"In [588]: ts = Series(np.random.randn(len(rng)), index=rng)\\nIn [589]: %timeit ts.resample('15s', how='ohlc')\\n1 loops, best of 3: 88.2 ms per loop\\nIt’s possible that by the time you read this, the performance of these algorithms may\\nbe even further improved. As an example, there are currently no optimizations for\\nconversions between regular frequencies, but that would be fairly straightforward to do.\\n328 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 344, 'page_label': '329'}, page_content='CHAPTER 11\\nFinancial and Economic Data\\nApplications\\nThe use of Python in the financial industry has been increasing rapidly since 2005, led\\nlargely by the maturation of libraries (like NumPy and pandas) and the availability of\\nskilled Python programmers. Institutions have found that Python is well-suited both\\nas an interactive analysis environment as well as enabling robust systems to be devel-\\noped often in a fraction of the time it would have taken in Java or C++. Python is also\\nan ideal glue layer; it is easy to build Python interfaces to legacy libraries built in C or\\nC++.\\nWhile the field of financial analysis is broad enough to fill an entire book, I hope to\\nshow you how the tools in this book can be applied to a number of specific problems\\nin finance. As with other research and analysis domains, too much programming effort\\nis often spent wrangling data rather than solving the core modeling and research prob-\\nlems. I personally got started building pandas in 2008 while grappling with inadequate\\ndata tools.\\nIn these examples, I’ll use the term cross-section to refer to data at a fixed point in time.\\nFor example, the closing prices of all the stocks in the S&P 500 index on a particular\\ndate form a cross-section. Cross-sectional data at multiple points in time over multiple\\ndata items (for example, prices together with volume) form a panel. Panel data can\\neither be represented as a hierarchically-indexed DataFrame or using the three-dimen-\\nsional Panel pandas object.\\nData Munging Topics\\nMany helpful data munging tools for financial applications are spread across the earlier\\nchapters. Here I’ll highlight a number of topics as they relate to this problem domain.\\n329\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 345, 'page_label': '330'}, page_content='Time Series and Cross-Section Alignment\\nOne of the most time-consuming issues in working with financial data is the so-called\\ndata alignment problem. Two related time series may have indexes that don’t line up\\nperfectly, or two DataFrame objects might have columns or row labels that don’t match.\\nUsers of MATLAB, R, and other matrix-programming languages often invest significant\\neffort in wrangling data into perfectly aligned forms. In my experience, having to align\\ndata by hand (and worse, having to verify that data is aligned) is a far too rigid and\\ntedious way to work. It is also rife with potential for bugs due to combining misaligned\\ndata.\\npandas take an alternate approach by automatically aligning data in arithmetic opera-\\ntions. In practice, this grants immense freedom and enhances your productivity. As an\\nexample, let’s consider a couple of DataFrames containing time series of stock prices\\nand volume:\\nIn [16]: prices\\nOut[16]: \\n              AAPL    JNJ      SPX    XOM\\n2011-09-06  379.74  64.64  1165.24  71.15\\n2011-09-07  383.93  65.43  1198.62  73.65\\n2011-09-08  384.14  64.95  1185.90  72.82\\n2011-09-09  377.48  63.64  1154.23  71.01\\n2011-09-12  379.94  63.59  1162.27  71.84\\n2011-09-13  384.62  63.61  1172.87  71.65\\n2011-09-14  389.30  63.73  1188.68  72.64\\nIn [17]: volume\\nOut[17]: \\n                AAPL       JNJ       XOM\\n2011-09-06  18173500  15848300  25416300\\n2011-09-07  12492000  10759700  23108400\\n2011-09-08  14839800  15551500  22434800\\n2011-09-09  20171900  17008200  27969100\\n2011-09-12  16697300  13448200  26205800\\nSuppose you wanted to compute a volume-weighted average price using all available\\ndata (and making the simplifying assumption that the volume data is a subset of the\\nprice data). Since pandas aligns the data automatically in arithmetic and excludes\\nmissing data in functions like sum, we can express this concisely as:\\nIn [18]: prices * volume\\nOut[18]: \\n                  AAPL         JNJ  SPX         XOM\\n2011-09-06  6901204890  1024434112  NaN  1808369745\\n2011-09-07  4796053560   704007171  NaN  1701933660\\n2011-09-08  5700560772  1010069925  NaN  1633702136\\n2011-09-09  7614488812  1082401848  NaN  1986085791\\n2011-09-12  6343972162   855171038  NaN  1882624672\\n2011-09-13         NaN         NaN  NaN         NaN\\n2011-09-14         NaN         NaN  NaN         NaN\\nIn [19]: vwap = (prices * volume).sum() / volume.sum()\\n330 | Chapter 11: \\u2002Financial and Economic Data Applications\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 346, 'page_label': '331'}, page_content=\"In [20]: vwap             In [21]: vwap.dropna()\\nOut[20]:                  Out[21]:              \\nAAPL    380.655181        AAPL    380.655181    \\nJNJ      64.394769        JNJ      64.394769    \\nSPX            NaN        XOM      72.024288    \\nXOM      72.024288\\nSince SPX wasn’t found in volume, you can choose to explicitly discard that at any point.\\nShould you wish to align by hand, you can use DataFrame’s align method, which\\nreturns a tuple of reindexed versions of the two objects:\\nIn [22]: prices.align(volume, join='inner')\\nOut[22]: \\n(              AAPL    JNJ    XOM\\n2011-09-06  379.74  64.64  71.15\\n2011-09-07  383.93  65.43  73.65\\n2011-09-08  384.14  64.95  72.82\\n2011-09-09  377.48  63.64  71.01\\n2011-09-12  379.94  63.59  71.84,\\n                 AAPL       JNJ       XOM\\n2011-09-06  18173500  15848300  25416300\\n2011-09-07  12492000  10759700  23108400\\n2011-09-08  14839800  15551500  22434800\\n2011-09-09  20171900  17008200  27969100\\n2011-09-12  16697300  13448200  26205800)\\nAnother indispensable feature is constructing a DataFrame from a collection of poten-\\ntially differently indexed Series:\\nIn [23]: s1 = Series(range(3), index=['a', 'b', 'c'])\\nIn [24]: s2 = Series(range(4), index=['d', 'b', 'c', 'e'])\\nIn [25]: s3 = Series(range(3), index=['f', 'a', 'c'])\\nIn [26]: DataFrame({'one': s1, 'two': s2, 'three': s3})\\nOut[26]: \\n   one  three  two\\na    0      1  NaN\\nb    1    NaN    1\\nc    2      2    2\\nd  NaN    NaN    0\\ne  NaN    NaN    3\\nf  NaN      0  NaN\\nAs you have seen earlier, you can of course specify explicitly the index of the result,\\ndiscarding the rest of the data:\\nIn [27]: DataFrame({'one': s1, 'two': s2, 'three': s3}, index=list('face'))\\nOut[27]: \\n   one  three  two\\nf  NaN      0  NaN\\na    0      1  NaN\\nc    2      2    2\\ne  NaN    NaN    3\\nData Munging Topics | 331\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 347, 'page_label': '332'}, page_content=\"Operations with Time Series of Different Frequencies\\nEconomic time series are often of annual, quarterly, monthly, daily, or some other more\\nspecialized frequency. Some are completely irregular; for example, earnings revisions\\nfor a stock may arrive at any time. The two main tools for frequency conversion and\\nrealignment are the resample and reindex methods. resample converts data to a fixed\\nfrequency while reindex conforms data to a new index. Both support optional inter-\\npolation (such as forward filling) logic.\\nLet’s consider a small weekly time series:\\nIn [28]: ts1 = Series(np.random.randn(3),\\n   ....:              index=pd.date_range('2012-6-13', periods=3, freq='W-WED'))\\nIn [29]: ts1\\nOut[29]: \\n2012-06-13   -1.124801\\n2012-06-20    0.469004\\n2012-06-27   -0.117439\\nFreq: W-WED\\nIf you resample this to business daily (Monday-Friday) frequency, you get holes on the\\ndays where there is no data:\\nIn [30]: ts1.resample('B')\\nOut[30]: \\n2012-06-13   -1.124801\\n2012-06-14         NaN\\n2012-06-15         NaN\\n2012-06-18         NaN\\n2012-06-19         NaN\\n2012-06-20    0.469004\\n2012-06-21         NaN\\n2012-06-22         NaN\\n2012-06-25         NaN\\n2012-06-26         NaN\\n2012-06-27   -0.117439\\nFreq: B\\nOf course, using 'ffill' as the fill_method forward fills values in those gaps. This is\\na common practice with lower frequency data as you compute a time series of values\\non each timestamp having the latest valid or “as of” value:\\nIn [31]: ts1.resample('B', fill_method='ffill')\\nOut[31]: \\n2012-06-13   -1.124801\\n2012-06-14   -1.124801\\n2012-06-15   -1.124801\\n2012-06-18   -1.124801\\n2012-06-19   -1.124801\\n2012-06-20    0.469004\\n2012-06-21    0.469004\\n2012-06-22    0.469004\\n2012-06-25    0.469004\\n2012-06-26    0.469004\\n332 | Chapter 11: \\u2002Financial and Economic Data Applications\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 348, 'page_label': '333'}, page_content=\"2012-06-27   -0.117439\\nFreq: B\\nIn practice, upsampling lower frequency data to a higher, regular frequency is a fine\\nsolution, but in the more general irregular time series case it may be a poor fit. Consider\\nan irregularly sampled time series from the same general time period:\\nIn [32]: dates = pd.DatetimeIndex(['2012-6-12', '2012-6-17', '2012-6-18',\\n   ....:                           '2012-6-21', '2012-6-22', '2012-6-29'])\\nIn [33]: ts2 = Series(np.random.randn(6), index=dates)\\nIn [34]: ts2\\nOut[34]: \\n2012-06-12   -0.449429\\n2012-06-17    0.459648\\n2012-06-18   -0.172531\\n2012-06-21    0.835938\\n2012-06-22   -0.594779\\n2012-06-29    0.027197\\nIf you wanted to add the “as of” values in ts1 (forward filling) to ts2. One option would\\nbe to resample both to a regular frequency then add, but if you want to maintain the\\ndate index in ts2, using reindex is a more precise solution:\\nIn [35]: ts1.reindex(ts2.index, method='ffill')\\nOut[35]: \\n2012-06-12         NaN\\n2012-06-17   -1.124801\\n2012-06-18   -1.124801\\n2012-06-21    0.469004\\n2012-06-22    0.469004\\n2012-06-29   -0.117439\\nIn [36]: ts2 + ts1.reindex(ts2.index, method='ffill')\\nOut[36]: \\n2012-06-12         NaN\\n2012-06-17   -0.665153\\n2012-06-18   -1.297332\\n2012-06-21    1.304942\\n2012-06-22   -0.125775\\n2012-06-29   -0.090242\\nUsing periods instead of timestamps\\nPeriods (representing time spans) provide an alternate means of working with different\\nfrequency time series, especially financial or economic series with annual or quarterly\\nfrequency having a particular reporting convention. For example, a company might\\nannounce its quarterly earnings with fiscal year ending in June, thus having Q-JUN fre-\\nquency. Consider a pair of macroeconomic time series related to GDP and inflation:\\nIn [37]: gdp = Series([1.78, 1.94, 2.08, 2.01, 2.15, 2.31, 2.46],\\n   ....:              index=pd.period_range('1984Q2', periods=7, freq='Q-SEP'))\\nData Munging Topics | 333\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 349, 'page_label': '334'}, page_content=\"In [38]: infl = Series([0.025, 0.045, 0.037, 0.04],\\n   ....:               index=pd.period_range('1982', periods=4, freq='A-DEC'))\\nIn [39]: gdp          In [40]: infl\\nOut[39]:              Out[40]:     \\n1984Q2    1.78        1982    0.025\\n1984Q3    1.94        1983    0.045\\n1984Q4    2.08        1984    0.037\\n1985Q1    2.01        1985    0.040\\n1985Q2    2.15        Freq: A-DEC  \\n1985Q3    2.31                     \\n1985Q4    2.46                     \\nFreq: Q-SEP\\nUnlike time series with timestamps, operations between different-frequency time series\\nindexed by periods are not possible without explicit conversions. In this case, if we\\nknow that infl values were observed at the end of each year, we can then convert to\\nQ-SEP to get the right periods in that frequency:\\nIn [41]: infl_q = infl.asfreq('Q-SEP', how='end')\\nIn [42]: infl_q\\nOut[42]: \\n1983Q1    0.025\\n1984Q1    0.045\\n1985Q1    0.037\\n1986Q1    0.040\\nFreq: Q-SEP\\nThat time series can then be reindexed with forward-filling to match gdp:\\nIn [43]: infl_q.reindex(gdp.index, method='ffill')\\nOut[43]: \\n1984Q2    0.045\\n1984Q3    0.045\\n1984Q4    0.045\\n1985Q1    0.037\\n1985Q2    0.037\\n1985Q3    0.037\\n1985Q4    0.037\\nFreq: Q-SEP\\nTime of Day and “as of” Data Selection\\nSuppose you have a long time series containing intraday market data and you want to\\nextract the prices at a particular time of day on each day of the data. What if the data\\nare irregular such that observations do not fall exactly on the desired time? In practice\\nthis task can make for error-prone data munging if you are not careful. Here is an\\nexample for illustration purposes:\\n# Make an intraday date range and time series\\nIn [44]: rng = pd.date_range('2012-06-01 09:30', '2012-06-01 15:59', freq='T')\\n# Make a 5-day series of 9:30-15:59 values\\n334 | Chapter 11: \\u2002Financial and Economic Data Applications\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 350, 'page_label': '335'}, page_content='In [45]: rng = rng.append([rng + pd.offsets.BDay(i) for i in range(1, 4)])\\nIn [46]: ts = Series(np.arange(len(rng), dtype=float), index=rng)\\nIn [47]: ts\\nOut[47]: \\n2012-06-01 09:30:00    0\\n2012-06-01 09:31:00    1\\n2012-06-01 09:32:00    2\\n2012-06-01 09:33:00    3\\n...\\n2012-06-06 15:56:00    1556\\n2012-06-06 15:57:00    1557\\n2012-06-06 15:58:00    1558\\n2012-06-06 15:59:00    1559\\nLength: 1560\\nIndexing with a Python datetime.time object will extract values at those times:\\nIn [48]: from datetime import time\\nIn [49]: ts[time(10, 0)]\\nOut[49]: \\n2012-06-01 10:00:00      30\\n2012-06-04 10:00:00     420\\n2012-06-05 10:00:00     810\\n2012-06-06 10:00:00    1200\\nUnder the hood, this uses an instance method at_time (available on individual time\\nseries and DataFrame objects alike):\\nIn [50]: ts.at_time(time(10, 0))\\nOut[50]: \\n2012-06-01 10:00:00      30\\n2012-06-04 10:00:00     420\\n2012-06-05 10:00:00     810\\n2012-06-06 10:00:00    1200\\nYou can select values between two times using the related between_time method:\\nIn [51]: ts.between_time(time(10, 0), time(10, 1))\\nOut[51]: \\n2012-06-01 10:00:00      30\\n2012-06-01 10:01:00      31\\n2012-06-04 10:00:00     420\\n2012-06-04 10:01:00     421\\n2012-06-05 10:00:00     810\\n2012-06-05 10:01:00     811\\n2012-06-06 10:00:00    1200\\n2012-06-06 10:01:00    1201\\nAs mentioned above, it might be the case that no data actually fall exactly at a time like\\n10 AM, but you might want to know the last known value at 10 AM:\\n# Set most of the time series randomly to NA\\nIn [53]: indexer = np.sort(np.random.permutation(len(ts))[700:])\\nData Munging Topics | 335\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 351, 'page_label': '336'}, page_content=\"In [54]: irr_ts = ts.copy()\\nIn [55]: irr_ts[indexer] = np.nan\\nIn [56]: irr_ts['2012-06-01 09:50':'2012-06-01 10:00']\\nOut[56]: \\n2012-06-01 09:50:00    20\\n2012-06-01 09:51:00   NaN\\n2012-06-01 09:52:00    22\\n2012-06-01 09:53:00    23\\n2012-06-01 09:54:00   NaN\\n2012-06-01 09:55:00    25\\n2012-06-01 09:56:00   NaN\\n2012-06-01 09:57:00   NaN\\n2012-06-01 09:58:00   NaN\\n2012-06-01 09:59:00   NaN\\n2012-06-01 10:00:00   NaN\\nBy passing an array of timestamps to the asof method, you will obtain an array of the\\nlast valid (non-NA) values at or before each timestamp. So we construct a date range\\nat 10 AM for each day and pass that to asof:\\nIn [57]: selection = pd.date_range('2012-06-01 10:00', periods=4, freq='B')\\nIn [58]: irr_ts.asof(selection)\\nOut[58]: \\n2012-06-01 10:00:00      25\\n2012-06-04 10:00:00     420\\n2012-06-05 10:00:00     810\\n2012-06-06 10:00:00    1197\\nFreq: B\\nSplicing Together Data Sources\\nIn Chapter 7, I described a number of strategies for merging together two related data\\nsets. In a financial or economic context, there are a few widely occurring use cases:\\n• Switching from one data source (a time series or collection of time series) to another\\nat a specific point in time\\n• “Patching” missing values in a time series at the beginning, middle, or end using\\nanother time series\\n• Completely replacing the data for a subset of symbols (countries, asset tickers, and\\nso on)\\nIn the first case, switching from one set of time series to another at a specific instant, it\\nis a matter of splicing together two TimeSeries or DataFrame objects using pandas.con\\ncat:\\nIn [59]: data1 = DataFrame(np.ones((6, 3), dtype=float),\\n   ....:                   columns=['a', 'b', 'c'],\\n   ....:                   index=pd.date_range('6/12/2012', periods=6))\\n336 | Chapter 11: \\u2002Financial and Economic Data Applications\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 352, 'page_label': '337'}, page_content=\"In [60]: data2 = DataFrame(np.ones((6, 3), dtype=float) * 2,\\n   ....:                   columns=['a', 'b', 'c'],\\n   ....:                   index=pd.date_range('6/13/2012', periods=6))\\nIn [61]: spliced = pd.concat([data1.ix[:'2012-06-14'], data2.ix['2012-06-15':]])\\nIn [62]: spliced\\nOut[62]: \\n            a  b  c\\n2012-06-12  1  1  1\\n2012-06-13  1  1  1\\n2012-06-14  1  1  1\\n2012-06-15  2  2  2\\n2012-06-16  2  2  2\\n2012-06-17  2  2  2\\n2012-06-18  2  2  2\\nSuppose in a similar example that data1 was missing a time series present in data2:\\nIn [63]: data2 = DataFrame(np.ones((6, 4), dtype=float) * 2,\\n   ....:                   columns=['a', 'b', 'c', 'd'],\\n   ....:                   index=pd.date_range('6/13/2012', periods=6))\\nIn [64]: spliced = pd.concat([data1.ix[:'2012-06-14'], data2.ix['2012-06-15':]])\\nIn [65]: spliced\\nOut[65]: \\n            a  b  c   d\\n2012-06-12  1  1  1 NaN\\n2012-06-13  1  1  1 NaN\\n2012-06-14  1  1  1 NaN\\n2012-06-15  2  2  2   2\\n2012-06-16  2  2  2   2\\n2012-06-17  2  2  2   2\\n2012-06-18  2  2  2   2\\nUsing combine_first, you can bring in data from before the splice point to extend the\\nhistory for 'd' item:\\nIn [66]: spliced_filled = spliced.combine_first(data2)\\nIn [67]: spliced_filled\\nOut[67]: \\n            a  b  c   d\\n2012-06-12  1  1  1 NaN\\n2012-06-13  1  1  1   2\\n2012-06-14  1  1  1   2\\n2012-06-15  2  2  2   2\\n2012-06-16  2  2  2   2\\n2012-06-17  2  2  2   2\\n2012-06-18  2  2  2   2\\nSince data2 does not have any values for 2012-06-12, no values are filled on that day.\\nDataFrame has a related method update for performing in-place updates. You have to\\npass overwrite=False to make it only fill the holes:\\nData Munging Topics | 337\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 353, 'page_label': '338'}, page_content=\"In [68]: spliced.update(data2, overwrite=False)\\nIn [69]: spliced\\nOut[69]: \\n            a  b  c   d\\n2012-06-12  1  1  1 NaN\\n2012-06-13  1  1  1   2\\n2012-06-14  1  1  1   2\\n2012-06-15  2  2  2   2\\n2012-06-16  2  2  2   2\\n2012-06-17  2  2  2   2\\n2012-06-18  2  2  2   2\\nTo replace the data for a subset of symbols, you can use any of the above techniques,\\nbut sometimes it’s simpler to just set the columns directly with DataFrame indexing:\\nIn [70]: cp_spliced = spliced.copy()\\nIn [71]: cp_spliced[['a', 'c']] = data1[['a', 'c']]\\nIn [72]: cp_spliced\\nOut[72]: \\n             a  b   c   d\\n2012-06-12   1  1   1 NaN\\n2012-06-13   1  1   1   2\\n2012-06-14   1  1   1   2\\n2012-06-15   1  2   1   2\\n2012-06-16   1  2   1   2\\n2012-06-17   1  2   1   2\\n2012-06-18 NaN  2 NaN   2\\nReturn Indexes and Cumulative Returns\\nIn a financial context, returns usually refer to percent changes in the price of an asset.\\nLet’s consider price data for Apple in 2011 and 2012:\\nIn [73]: import pandas.io.data as web\\nIn [74]: price = web.get_data_yahoo('AAPL', '2011-01-01')['Adj Close']\\nIn [75]: price[-5:]\\nOut[75]: \\nDate\\n2012-07-23    603.83\\n2012-07-24    600.92\\n2012-07-25    574.97\\n2012-07-26    574.88\\n2012-07-27    585.16\\nName: Adj Close\\nFor Apple, which has no dividends, computing the cumulative percent return between\\ntwo points in time requires computing only the percent change in the price:\\nIn [76]: price['2011-10-03'] / price['2011-3-01'] - 1\\nOut[76]: 0.072399874037388123\\n338 | Chapter 11: \\u2002Financial and Economic Data Applications\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 354, 'page_label': '339'}, page_content=\"For other stocks with dividend payouts, computing how much money you make from\\nholding a stock can be more complicated. The adjusted close values used here have\\nbeen adjusted for splits and dividends, however. In all cases, it’s quite common to derive\\na return index, which is a time series indicating the value of a unit investment (one\\ndollar, say). Many assumptions can underlie the return index; for example, some will\\nchoose to reinvest profit and others not. In the case of Apple, we can compute a simple\\nreturn index using cumprod:\\nIn [77]: returns = price.pct_change()\\nIn [78]: ret_index = (1 + returns).cumprod()\\nIn [79]: ret_index[0] = 1  # Set first value to 1\\nIn [80]: ret_index\\nOut[80]: \\nDate\\n2011-01-03    1.000000\\n2011-01-04    1.005219\\n2011-01-05    1.013442\\n2011-01-06    1.012623\\n...\\n2012-07-24    1.823346\\n2012-07-25    1.744607\\n2012-07-26    1.744334\\n2012-07-27    1.775526\\nLength: 396\\nWith a return index in hand, computing cumulative returns at a particular resolution\\nis simple:\\nIn [81]: m_returns = ret_index.resample('BM', how='last').pct_change()\\nIn [82]: m_returns['2012']\\nOut[82]: \\nDate\\n2012-01-31    0.127111\\n2012-02-29    0.188311\\n2012-03-30    0.105284\\n2012-04-30   -0.025969\\n2012-05-31   -0.010702\\n2012-06-29    0.010853\\n2012-07-31    0.001986\\nFreq: BM\\nOf course, in this simple case (no dividends or other adjustments to take into account)\\nthese could have been computed from the daily percent changed by resampling with\\naggregation (here, to periods):\\nIn [83]: m_rets = (1 + returns).resample('M', how='prod', kind='period') - 1\\nIn [84]: m_rets['2012']\\nOut[84]: \\nDate\\nData Munging Topics | 339\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 355, 'page_label': '340'}, page_content=\"2012-01    0.127111\\n2012-02    0.188311\\n2012-03    0.105284\\n2012-04   -0.025969\\n2012-05   -0.010702\\n2012-06    0.010853\\n2012-07    0.001986\\nFreq: M\\nIf you had dividend dates and percentages, including them in the total return per day\\nwould look like:\\nreturns[dividend_dates] += dividend_pcts\\nGroup Transforms and Analysis\\nIn Chapter 9, you learned the basics of computing group statistics and applying your\\nown transformations to groups in a dataset.\\nLet’s consider a collection of hypothetical stock portfolios. I first randomly generate a\\nbroad universe of 2000 tickers:\\nimport random; random.seed(0)\\nimport string\\nN = 1000\\ndef rands(n):\\n    choices = string.ascii_uppercase\\n    return ''.join([random.choice(choices) for _ in xrange(n)])\\ntickers = np.array([rands(5) for _ in xrange(N)])\\nI then create a DataFrame containing 3 columns representing hypothetical, but random\\nportfolios for a subset of tickers:\\nM = 500\\ndf = DataFrame({'Momentum' : np.random.randn(M) / 200 + 0.03,\\n                'Value' : np.random.randn(M) / 200 + 0.08,\\n                'ShortInterest' : np.random.randn(M) / 200 - 0.02},\\n                index=tickers[:M])\\nNext, let’s create a random industry classification for the tickers. To keep things simple,\\nI’ll just keep it to 2 industries, storing the mapping in a Series:\\nind_names = np.array(['FINANCIAL', 'TECH'])\\nsampler = np.random.randint(0, len(ind_names), N)\\nindustries = Series(ind_names[sampler], index=tickers,\\n                    name='industry')\\nNow we can group by industries and carry out group aggregation and transformations:\\nIn [90]: by_industry = df.groupby(industries)\\nIn [91]: by_industry.mean()\\nOut[91]: \\n           Momentum  ShortInterest     Value\\n340 | Chapter 11: \\u2002Financial and Economic Data Applications\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 356, 'page_label': '341'}, page_content=\"industry                                    \\nFINANCIAL  0.029485      -0.020739  0.079929\\nTECH       0.030407      -0.019609  0.080113\\nIn [92]: by_industry.describe()\\nOut[92]: \\n                   Momentum  ShortInterest       Value\\nindustry                                              \\nFINANCIAL count  246.000000     246.000000  246.000000\\n          mean     0.029485      -0.020739    0.079929\\n          std      0.004802       0.004986    0.004548\\n          min      0.017210      -0.036997    0.067025\\n          25%      0.026263      -0.024138    0.076638\\n          50%      0.029261      -0.020833    0.079804\\n          75%      0.032806      -0.017345    0.082718\\n          max      0.045884      -0.006322    0.093334\\nTECH      count  254.000000     254.000000  254.000000\\n          mean     0.030407      -0.019609    0.080113\\n          std      0.005303       0.005074    0.004886\\n          min      0.016778      -0.032682    0.065253\\n          25%      0.026456      -0.022779    0.076737\\n          50%      0.030650      -0.019829    0.080296\\n          75%      0.033602      -0.016923    0.083353\\n          max      0.049638      -0.003698    0.093081\\nBy defining transformation functions, it’s easy to transform these portfolios by industry.\\nFor example, standardizing within industry is widely used in equity portfolio construc-\\ntion:\\n# Within-Industry Standardize\\ndef zscore(group):\\n    return (group - group.mean()) / group.std()\\ndf_stand = by_industry.apply(zscore)\\nYou can verify that each industry has mean 0 and standard deviation 1:\\nIn [94]: df_stand.groupby(industries).agg(['mean', 'std'])\\nOut[94]: \\n           Momentum       ShortInterest       Value     \\n               mean  std           mean  std   mean  std\\nindustry                                                \\nFINANCIAL         0    1              0    1      0    1\\nTECH             -0    1             -0    1     -0    1\\nOther, built-in kinds of transformations, like rank, can be used more concisely:\\n# Within-industry rank descending\\nIn [95]: ind_rank = by_industry.rank(ascending=False)\\nIn [96]: ind_rank.groupby(industries).agg(['min', 'max'])\\nOut[96]: \\n           Momentum       ShortInterest       Value     \\n                min  max            min  max    min  max\\nindustry                                                \\nFINANCIAL         1  246              1  246      1  246\\nTECH              1  254              1  254      1  254\\nGroup Transforms and Analysis | 341\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 357, 'page_label': '342'}, page_content=\"In quantitative equity, “rank and standardize” is a common sequence of transforms.\\nYou could do this by chaining together rank and zscore like so:\\n# Industry rank and standardize\\nIn [97]: by_industry.apply(lambda x: zscore(x.rank()))\\nOut[97]: \\n<class 'pandas.core.frame.DataFrame'>\\nIndex: 500 entries, VTKGN to PTDQE\\nData columns:\\nMomentum         500  non-null values\\nShortInterest    500  non-null values\\nValue            500  non-null values\\ndtypes: float64(3)\\nGroup Factor Exposures\\nFactor analysis is a technique in quantitative portfolio management. Portfolio holdings\\nand performance (profit and less) are decomposed using one or more factors (risk fac-\\ntors are one example) represented as a portfolio of weights. For example, a stock price’s\\nco-movement with a benchmark (like S&P 500 index) is known as its beta, a common\\nrisk factor. Let’s consider a contrived example of a portfolio constructed from 3 ran-\\ndomly-generated factors (usually called the factor loadings) and some weights:\\nfrom numpy.random import rand\\nfac1, fac2, fac3 = np.random.rand(3, 1000)\\nticker_subset = tickers.take(np.random.permutation(N)[:1000])\\n# Weighted sum of factors plus noise\\nport = Series(0.7 * fac1 - 1.2 * fac2 + 0.3 * fac3 + rand(1000),\\n              index=ticker_subset)\\nfactors = DataFrame({'f1': fac1, 'f2': fac2, 'f3': fac3},\\n                    index=ticker_subset)\\nVector correlations between each factor and the portfolio may not indicate too much:\\nIn [99]: factors.corrwith(port)\\nOut[99]: \\nf1    0.402377\\nf2   -0.680980\\nf3    0.168083\\nThe standard way to compute the factor exposures is by least squares regression; using\\npandas.ols with factors as the explanatory variables we can compute exposures over\\nthe entire set of tickers:\\nIn [100]: pd.ols(y=port, x=factors).beta\\nOut[100]: \\nf1           0.761789\\nf2          -1.208760\\nf3           0.289865\\nintercept    0.484477\\n342 | Chapter 11: \\u2002Financial and Economic Data Applications\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 358, 'page_label': '343'}, page_content=\"As you can see, the original factor weights can nearly be recovered since there was not\\ntoo much additional random noise added to the portfolio. Using groupby you can com-\\npute exposures industry by industry. To do so, write a function like so:\\ndef beta_exposure(chunk, factors=None):\\n    return pd.ols(y=chunk, x=factors).beta\\nThen, group by industries and apply that function, passing the DataFrame of factor\\nloadings:\\nIn [102]: by_ind = port.groupby(industries)\\nIn [103]: exposures = by_ind.apply(beta_exposure, factors=factors)\\nIn [104]: exposures.unstack()\\nOut[104]: \\n                 f1        f2        f3  intercept\\nindustry                                          \\nFINANCIAL  0.790329 -1.182970  0.275624   0.455569\\nTECH       0.740857 -1.232882  0.303811   0.508188\\nDecile and Quartile Analysis\\nAnalyzing data based on sample quantiles is another important tool for financial ana-\\nlysts. For example, the performance of a stock portfolio could be broken down into\\nquartiles (four equal-sized chunks) based on each stock’s price-to-earnings. Using pan\\ndas.qcut combined with groupby makes quantile analysis reasonably straightforward.\\nAs an example, let’s consider a simple trend following or momentum strategy trading\\nthe S&P 500 index via the SPY exchange-traded fund. You can download the price\\nhistory from Yahoo! Finance:\\nIn [105]: import pandas.io.data as web\\nIn [106]: data = web.get_data_yahoo('SPY', '2006-01-01')\\nIn [107]: data\\nOut[107]: \\n<class 'pandas.core.frame.DataFrame'>\\nDatetimeIndex: 1655 entries, 2006-01-03 00:00:00 to 2012-07-27 00:00:00\\nData columns:\\nOpen         1655  non-null values\\nHigh         1655  non-null values\\nLow          1655  non-null values\\nClose        1655  non-null values\\nVolume       1655  non-null values\\nAdj Close    1655  non-null values\\ndtypes: float64(5), int64(1)\\nNow, we’ll compute daily returns and a function for transforming the returns into a\\ntrend signal formed from a lagged moving sum:\\npx = data['Adj Close']\\nreturns = px.pct_change()\\nGroup Transforms and Analysis | 343\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 359, 'page_label': '344'}, page_content=\"def to_index(rets):\\n    index = (1 + rets).cumprod()\\n    first_loc = max(index.notnull().argmax() - 1, 0)\\n    index.values[first_loc] = 1\\n    return index\\ndef trend_signal(rets, lookback, lag):\\n    signal = pd.rolling_sum(rets, lookback, min_periods=lookback - 5)\\n    return signal.shift(lag)\\nUsing this function, we can (naively) create and test a trading strategy that trades this\\nmomentum signal every Friday:\\nIn [109]: signal = trend_signal(returns, 100, 3)\\nIn [110]: trade_friday = signal.resample('W-FRI').resample('B', fill_method='ffill')\\nIn [111]: trade_rets = trade_friday.shift(1) * returns\\nWe can then convert the strategy returns to a return index and plot them (see Fig-\\nure 11-1):\\nIn [112]: to_index(trade_rets).plot()\\nFigure 11-1. SPY momentum strategy return index\\nSuppose you wanted to decompose the strategy performance into more and less volatile\\nperiods of trading. Trailing one-year annualized standard deviation is a simple measure\\nof volatility, and we can compute Sharpe ratios to assess the reward-to-risk ratio in\\nvarious volatility regimes:\\nvol = pd.rolling_std(returns, 250, min_periods=200) * np.sqrt(250)\\n344 | Chapter 11: \\u2002Financial and Economic Data Applications\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 360, 'page_label': '345'}, page_content=\"def sharpe(rets, ann=250):\\n    return rets.mean() / rets.std()  * np.sqrt(ann)\\nNow, dividing vol into quartiles with qcut and aggregating with sharpe we obtain:\\nIn [114]: trade_rets.groupby(pd.qcut(vol, 4)).agg(sharpe)\\nOut[114]: \\n[0.0955, 0.16]    0.490051\\n(0.16, 0.188]     0.482788\\n(0.188, 0.231]   -0.731199\\n(0.231, 0.457]    0.570500\\nThese results show that the strategy performed the best during the period when the\\nvolatility was the highest.\\nMore Example Applications\\nHere is a small set of additional examples.\\nSignal Frontier Analysis\\nIn this section, I’ll describe a simplified cross-sectional momentum portfolio and show\\nhow you might explore a grid of model parameterizations. First, I’ll load historical\\nprices for a portfolio of financial and technology stocks:\\nnames = ['AAPL', 'GOOG', 'MSFT', 'DELL', 'GS', 'MS', 'BAC', 'C']\\ndef get_px(stock, start, end):\\n    return web.get_data_yahoo(stock, start, end)['Adj Close']\\npx = DataFrame({n: get_px(n, '1/1/2009', '6/1/2012') for n in names})\\nWe can easily plot the cumulative returns of each stock (see Figure 11-2):\\nIn [117]: px = px.asfreq('B').fillna(method='pad')\\nIn [118]: rets = px.pct_change()\\nIn [119]: ((1 + rets).cumprod() - 1).plot()\\nFor the portfolio construction, we’ll compute momentum over a certain lookback, then\\nrank in descending order and standardize:\\ndef calc_mom(price, lookback, lag):\\n    mom_ret = price.shift(lag).pct_change(lookback)\\n    ranks = mom_ret.rank(axis=1, ascending=False)\\n    demeaned = ranks - ranks.mean(axis=1)\\n    return demeaned / demeaned.std(axis=1)\\nWith this transform function in hand, we can set up a strategy backtesting function\\nthat computes a portfolio for a particular lookback and holding period (days between\\ntrading), returning the overall Sharpe ratio:\\ncompound = lambda x : (1 + x).prod() - 1\\ndaily_sr = lambda x: x.mean() / x.std()\\nMore Example Applications | 345\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 361, 'page_label': '346'}, page_content=\"def strat_sr(prices, lb, hold):\\n    # Compute portfolio weights\\n    freq = '%dB' % hold\\n    port = calc_mom(prices, lb, lag=1)\\n    daily_rets = prices.pct_change()\\n    # Compute portfolio returns\\n    port = port.shift(1).resample(freq, how='first')\\n    returns = daily_rets.resample(freq, how=compound)\\n    port_rets = (port * returns).sum(axis=1)\\n    return daily_sr(port_rets) * np.sqrt(252 / hold)\\nFigure 11-2. Cumulative returns for each of the stocks\\nWhen called with the prices and a parameter combination, this function returns a scalar\\nvalue:\\nIn [122]: strat_sr(px, 70, 30)\\nOut[122]: 0.27421582756800583\\nFrom there, you can evaluate the strat_sr function over a grid of parameters, storing\\nthem as you go in a defaultdict and finally putting the results in a DataFrame:\\nfrom collections import defaultdict\\nlookbacks = range(20, 90, 5)\\nholdings = range(20, 90, 5)\\ndd = defaultdict(dict)\\nfor lb in lookbacks:\\n    for hold in holdings:\\n        dd[lb][hold] = strat_sr(px, lb, hold)\\n346 | Chapter 11: \\u2002Financial and Economic Data Applications\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 362, 'page_label': '347'}, page_content=\"ddf = DataFrame(dd)\\nddf.index.name = 'Holding Period'\\nddf.columns.name = 'Lookback Period'\\nTo visualize the results and get an idea of what’s going on, here is a function that uses\\nmatplotlib to produce a heatmap with some adornments:\\nimport matplotlib.pyplot as plt\\ndef heatmap(df, cmap=plt.cm.gray_r):\\n    fig = plt.figure()\\n    ax = fig.add_subplot(111)\\n    axim = ax.imshow(df.values, cmap=cmap, interpolation='nearest')\\n    ax.set_xlabel(df.columns.name)\\n    ax.set_xticks(np.arange(len(df.columns)))\\n    ax.set_xticklabels(list(df.columns))\\n    ax.set_ylabel(df.index.name)\\n    ax.set_yticks(np.arange(len(df.index)))\\n    ax.set_yticklabels(list(df.index))\\n    plt.colorbar(axim)\\nCalling this function on the backtest results, we get Figure 11-3:\\nIn [125]: heatmap(ddf)\\nFigure 11-3. Heatmap of momentum strategy Sharpe ratio (higher is better) over various lookbacks\\nand holding periods\\nFuture Contract Rolling\\nA future is an ubiquitous form of derivative contract; it is an agreement to take delivery\\nof a certain asset (such as oil, gold, or shares of the FTSE 100 index) on a particular\\ndate. In practice, modeling and trading futures contracts on equities, currencies,\\nMore Example Applications | 347\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 363, 'page_label': '348'}, page_content=\"commodities, bonds, and other asset classes is complicated by the time-limited nature\\nof each contract. For example, at any given time for a type of future (say silver or copper\\nfutures) multiple contracts with different expiration dates may be traded. In many cases,\\nthe future contract expiring next (the near contract) will be the most liquid (highest\\nvolume and lowest bid-ask spread).\\nFor the purposes of modeling and forecasting, it can be much easier to work with a \\ncontinuous return index indicating the profit and loss associated with always holding\\nthe near contract. Transitioning from an expiring contract to the next (or far) contract\\nis referred to as rolling. Computing a continuous future series from the individual con-\\ntract data is not necessarily a straightforward exercise and typically requires a deeper\\nunderstanding of the market and how the instruments are traded. For example, in\\npractice when and how quickly would you trade out of an expiring contract and into\\nthe next contract? Here I describe one such process.\\nFirst, I’ll use scaled prices for the SPY exchange-traded fund as a proxy for the S&P 500\\nindex:\\nIn [127]: import pandas.io.data as web\\n# Approximate price of S&P 500 index\\nIn [128]: px = web.get_data_yahoo('SPY')['Adj Close'] * 10\\nIn [129]: px\\nOut[129]: \\nDate\\n2011-08-01    1261.0\\n2011-08-02    1228.8\\n2011-08-03    1235.5\\n...\\n2012-07-25    1339.6\\n2012-07-26    1361.7\\n2012-07-27    1386.8\\nName: Adj Close, Length: 251\\nNow, a little bit of setup. I put a couple of S&P 500 future contracts and expiry dates\\nin a Series:\\nfrom datetime import datetime\\nexpiry = {'ESU2': datetime(2012, 9, 21),\\n          'ESZ2': datetime(2012, 12, 21)}\\nexpiry = Series(expiry).order()\\nexpiry then looks like:\\nIn [131]: expiry\\nOut[131]: \\nESU2    2012-09-21 00:00:00\\nESZ2    2012-12-21 00:00:00\\n348 | Chapter 11: \\u2002Financial and Economic Data Applications\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 364, 'page_label': '349'}, page_content=\"Then, I use the Yahoo! Finance prices along with a random walk and some noise to\\nsimulate the two contracts into the future:\\nnp.random.seed(12347)\\nN = 200\\nwalk = (np.random.randint(0, 200, size=N) - 100) * 0.25\\nperturb = (np.random.randint(0, 20, size=N) - 10) * 0.25\\nwalk = walk.cumsum()\\nrng = pd.date_range(px.index[0], periods=len(px) + N, freq='B')\\nnear = np.concatenate([px.values, px.values[-1] + walk])\\nfar = np.concatenate([px.values, px.values[-1] + walk + perturb])\\nprices = DataFrame({'ESU2': near, 'ESZ2': far}, index=rng)\\nprices then has two time series for the contracts that differ from each other by a random\\namount:\\nIn [133]: prices.tail()\\nOut[133]: \\n               ESU2     ESZ2\\n2013-04-16  1416.05  1417.80\\n2013-04-17  1402.30  1404.55\\n2013-04-18  1410.30  1412.05\\n2013-04-19  1426.80  1426.05\\n2013-04-22  1406.80  1404.55\\nOne way to splice time series together into a single continuous series is to construct a\\nweighting matrix. Active contracts would have a weight of 1 until the expiry date ap-\\nproaches. At that point you have to decide on a roll convention. Here is a function that\\ncomputes a weighting matrix with linear decay over a number of periods leading up to\\nexpiry:\\ndef get_roll_weights(start, expiry, items, roll_periods=5):\\n    # start : first date to compute weighting DataFrame\\n    # expiry : Series of ticker -> expiration dates\\n    # items : sequence of contract names\\n    dates = pd.date_range(start, expiry[-1], freq='B')\\n    weights = DataFrame(np.zeros((len(dates), len(items))),\\n                        index=dates, columns=items)\\n    prev_date = weights.index[0]\\n    for i, (item, ex_date) in enumerate(expiry.iteritems()):\\n        if i < len(expiry) - 1:\\n            weights.ix[prev_date:ex_date - pd.offsets.BDay(), item] = 1\\n            roll_rng = pd.date_range(end=ex_date - pd.offsets.BDay(),\\n                                     periods=roll_periods + 1, freq='B')\\n            decay_weights = np.linspace(0, 1, roll_periods + 1)\\n            weights.ix[roll_rng, item] = 1 - decay_weights\\n            weights.ix[roll_rng, expiry.index[i + 1]] = decay_weights\\n        else:\\n            weights.ix[prev_date:, item] = 1\\n        prev_date = ex_date\\nMore Example Applications | 349\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 365, 'page_label': '350'}, page_content=\"return weights\\nThe weights look like this around the ESU2 expiry:\\nIn [135]: weights = get_roll_weights('6/1/2012', expiry, prices.columns)\\nIn [136]: weights.ix['2012-09-12':'2012-09-21']\\nOut[136]: \\n            ESU2  ESZ2\\n2012-09-12   1.0   0.0\\n2012-09-13   1.0   0.0\\n2012-09-14   0.8   0.2\\n2012-09-17   0.6   0.4\\n2012-09-18   0.4   0.6\\n2012-09-19   0.2   0.8\\n2012-09-20   0.0   1.0\\n2012-09-21   0.0   1.0\\nFinally, the rolled future returns are just a weighted sum of the contract returns:\\nIn [137]: rolled_returns = (prices.pct_change() * weights).sum(1)\\nRolling Correlation and Linear Regression\\nDynamic models play an important role in financial modeling as they can be used to\\nsimulate trading decisions over a historical period. Moving window and exponentially-\\nweighted time series functions are an example of tools that are used for dynamic models.\\nCorrelation is one way to look at the co-movement between the changes in two asset\\ntime series. pandas’s rolling_corr function can be called with two return series to\\ncompute the moving window correlation. First, I load some price series from Yahoo!\\nFinance and compute daily returns:\\naapl = web.get_data_yahoo('AAPL', '2000-01-01')['Adj Close']\\nmsft = web.get_data_yahoo('MSFT', '2000-01-01')['Adj Close']\\naapl_rets = aapl.pct_change()\\nmsft_rets = msft.pct_change()\\nThen, I compute and plot the one-year moving correlation (see Figure 11-4):\\nIn [140]: pd.rolling_corr(aapl_rets, msft_rets, 250).plot()\\nOne issue with correlation between two assets is that it does not capture differences in\\nvolatility. Least-squares regression provides another means for modeling the dynamic\\nrelationship between a variable and one or more other predictor variables.\\nIn [142]: model = pd.ols(y=aapl_rets, x={'MSFT': msft_rets}, window=250)\\nIn [143]: model.beta\\nOut[143]: \\n<class 'pandas.core.frame.DataFrame'>\\nDatetimeIndex: 2913 entries, 2000-12-28 00:00:00 to 2012-07-27 00:00:00\\nData columns:\\n350 | Chapter 11: \\u2002Financial and Economic Data Applications\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 366, 'page_label': '351'}, page_content=\"MSFT         2913  non-null values\\nintercept    2913  non-null values\\ndtypes: float64(2)\\nIn [144]: model.beta['MSFT'].plot()\\nFigure 11-4. One-year correlation of Apple with Microsoft\\nFigure 11-5. One-year beta (OLS regression coefficient) of Apple to Microsoft\\npandas’s ols function implements static and dynamic (expanding or rolling window)\\nleast squares regressions. For more sophisticated statistical and econometrics models,\\nsee the statsmodels project (http://statsmodels.sourceforge.net).\\nMore Example Applications | 351\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 367, 'page_label': '352'}, page_content='www.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 368, 'page_label': '353'}, page_content='CHAPTER 12\\nAdvanced NumPy\\nndarray Object Internals\\nThe NumPy ndarray provides a means to interpret a block of homogeneous data (either\\ncontiguous or strided, more on this later) as a multidimensional array object. As you’ve\\nseen, the data type, or dtype, determines how the data is interpreted as being floating\\npoint, integer, boolean, or any of the other types we’ve been looking at.\\nPart of what makes ndarray powerful is that every array object is a strided view on a\\nblock of data. You might wonder, for example, how the array view arr[::2, ::-1] does\\nnot copy any data. Simply put, the ndarray is more than just a chunk of memory and\\na dtype; it also has striding information which enables the array to move through\\nmemory with varying step sizes. More precisely, the ndarray internally consists of the\\nfollowing:\\n• A pointer to data, that is a block of system memory\\n• The data type or dtype\\n• A tuple indicating the array’s shape; For example, a 10 by 5 array would have shape\\n(10, 5)\\nIn [8]: np.ones((10, 5)).shape\\nOut[8]: (10, 5)\\n• A tuple of strides, integers indicating the number of bytes to “step” in order to\\nadvance one element along a dimension; For example, a typical (C order, more on\\nthis later) 3 x 4 x 5 array of float64 (8-byte) values has strides (160, 40, 8)\\nIn [9]: np.ones((3, 4, 5), dtype=np.float64).strides\\nOut[9]: (160, 40, 8)\\nWhile it is rare that a typical NumPy user would be interested in the array strides,\\nthey are the critical ingredient in constructing copyless array views. Strides can\\neven be negative which enables an array to move backward through memory, which\\nwould be the case in a slice like obj[::-1] or obj[:, ::-1].\\n353\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 369, 'page_label': '354'}, page_content='See Figure 12-1 for a simple mockup the ndarray innards.\\nFigure 12-1. The NumPy ndarray object\\nNumPy dtype Hierarchy\\nYou may occasionally have code which needs to check whether an array contains in-\\ntegers, floating point numbers, strings, or Python objects. Because there are many types\\nof floating point numbers (float16 through float128), checking that the dtype is among\\na list of types would be very verbose. Fortunately, the dtypes have superclasses such as\\nnp.integer and np.floating which can be used in conjunction with the np.issubd\\ntype function:\\nIn [10]: ints = np.ones(10, dtype=np.uint16)\\nIn [11]: floats = np.ones(10, dtype=np.float32)\\nIn [12]: np.issubdtype(ints.dtype, np.integer)\\nOut[12]: True\\nIn [13]: np.issubdtype(floats.dtype, np.floating)\\nOut[13]: True\\nYou can see all of the parent classes of a specific dtype by calling the type’s mro method:\\nIn [14]: np.float64.mro()\\nOut[14]: \\n[numpy.float64,\\n numpy.floating,\\n numpy.inexact,\\n numpy.number,\\n numpy.generic,\\n float,\\n object]\\nMost NumPy users will never have to know about this, but it occasionally comes in\\nhandy. See Figure 12-2  for a graph of the dtype hierarchy and parent-subclass\\nrelationships 1.\\n1. Some of the dtypes have trailing underscores in their names. These are there to avoid variable name\\nconflicts between the NumPy-specific types and the Python built-in ones.\\n354 | Chapter 12: \\u2002Advanced NumPy\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 370, 'page_label': '355'}, page_content='Advanced Array Manipulation\\nThere are many ways to work with arrays beyond fancy indexing, slicing, and boolean\\nsubsetting. While much of the heavy lifting for data analysis applications is handled by\\nhigher level functions in pandas, you may at some point need to write a data algorithm\\nthat is not found in one of the existing libraries.\\nReshaping Arrays\\nGiven what we know about NumPy arrays, it should come as little surprise that you\\ncan convert an array from one shape to another without copying any data. To do this,\\npass a tuple indicating the new shape to the reshape array instance method. For exam-\\nple, suppose we had a one-dimensional array of values that we wished to rearrange into\\na matrix:\\nIn [15]: arr = np.arange(8)\\nIn [16]: arr\\nOut[16]: array([0, 1, 2, 3, 4, 5, 6, 7])\\nIn [17]: arr.reshape((4, 2))\\nOut[17]: \\narray([[0, 1],\\n       [2, 3],\\n       [4, 5],\\n       [6, 7]])\\nA multidimensional array can also be reshaped:\\nIn [18]: arr.reshape((4, 2)).reshape((2, 4))\\nOut[18]: \\nFigure 12-2. The NumPy dtype class hierarchy\\nAdvanced Array Manipulation | 355\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 371, 'page_label': '356'}, page_content='array([[0, 1, 2, 3],\\n       [4, 5, 6, 7]])\\nOne of the passed shape dimensions can be -1, in which case the value used for that\\ndimension will be inferred from the data:\\nIn [19]: arr = np.arange(15)      In [20]: arr.reshape((5, -1))\\n                                  Out[20]:                     \\n                                  array([[ 0,  1,  2],         \\n                                         [ 3,  4,  5],         \\n                                         [ 6,  7,  8],         \\n                                         [ 9, 10, 11],         \\n                                         [12, 13, 14]])\\nSince an array’s shape attribute is a tuple, it can be passed to reshape, too:\\nIn [21]: other_arr = np.ones((3, 5))\\nIn [22]: other_arr.shape\\nOut[22]: (3, 5)\\nIn [23]: arr.reshape(other_arr.shape)\\nOut[23]: \\narray([[ 0,  1,  2,  3,  4],\\n       [ 5,  6,  7,  8,  9],\\n       [10, 11, 12, 13, 14]])\\nThe opposite operation of reshape from one-dimensional to a higher dimension is typ-\\nically known as flattening or raveling:\\nIn [24]: arr = np.arange(15).reshape((5, 3))      In [25]: arr         \\n                                                  Out[25]:             \\n                                                  array([[ 0,  1,  2], \\n                                                         [ 3,  4,  5], \\n                                                         [ 6,  7,  8], \\n                                                         [ 9, 10, 11], \\n                                                         [12, 13, 14]])\\n                                                                       \\nIn [26]: arr.ravel()\\nOut[26]: array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])\\nravel does not produce a copy of the underlying data if it does not have to (more on\\nthis below). The flatten method behaves like ravel except it always returns a copy of\\nthe data:\\nIn [27]: arr.flatten()\\nOut[27]: array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])\\nThe data can be reshaped or raveled in different orders. This is a slightly nuanced topic\\nfor new NumPy users and is therefore the next subtopic.\\nC versus Fortran Order\\nContrary to some other scientific computing environments like R and MATLAB,\\nNumPy gives you much more control and flexibility over the layout of your data in\\n356 | Chapter 12: \\u2002Advanced NumPy\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 372, 'page_label': '357'}, page_content=\"memory. By default, NumPy arrays are created in row major order. Spatially this means\\nthat if you have a two-dimensional array of data, the items in each row of the array are\\nstored in adjacent memory locations. The alternative to row major ordering is column\\nmajor order, which means that (you guessed it) values within each column of data are\\nstored in adjacent memory locations.\\nFor historical reasons, row and column major order are also know as C and Fortran\\norder, respectively. In FORTRAN 77, the language of our forebears, matrices were all\\ncolumn major.\\nFunctions like reshape and ravel, accept an order argument indicating the order to use\\nthe data in the array. This can be 'C' or 'F' in most cases (there are also less commonly-\\nused options 'A' and 'K'; see the NumPy documentation). These are illustrated in\\nFigure 12-3.\\nIn [28]: arr = np.arange(12).reshape((3, 4))\\nIn [29]: arr\\nOut[29]: \\narray([[ 0,  1,  2,  3],\\n       [ 4,  5,  6,  7],\\n       [ 8,  9, 10, 11]])\\nIn [30]: arr.ravel()\\nOut[30]: array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\\nIn [31]: arr.ravel('F')\\nOut[31]: array([ 0,  4,  8,  1,  5,  9,  2,  6, 10,  3,  7, 11])\\nReshaping arrays with more than two dimensions can be a bit mind-bending. The key\\ndifference between C and Fortran order is the order in which the dimensions are\\nwalked:\\n• C / row major order: traverse higher dimensions first (e.g. axis 1 before advancing\\non axis 0).\\n• Fortran / column major order:  traverse higher dimensions last (e.g. axis 0 before\\nadvancing on axis 1).\\nConcatenating and Splitting Arrays\\nnumpy.concatenate takes a sequence (tuple, list, etc.) of arrays and joins them together\\nin order along the input axis.\\nIn [32]: arr1 = np.array([[1, 2, 3], [4, 5, 6]])\\nIn [33]: arr2 = np.array([[7, 8, 9], [10, 11, 12]])\\nIn [34]: np.concatenate([arr1, arr2], axis=0)\\nOut[34]: \\narray([[ 1,  2,  3],\\n       [ 4,  5,  6],\\nAdvanced Array Manipulation | 357\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 373, 'page_label': '358'}, page_content='[ 7,  8,  9],\\n       [10, 11, 12]])\\nIn [35]: np.concatenate([arr1, arr2], axis=1)\\nOut[35]: \\narray([[ 1,  2,  3,  7,  8,  9],\\n       [ 4,  5,  6, 10, 11, 12]])\\nFigure 12-3. Reshaping in C (row major) or Fortran (column major) order\\nThere are some convenience functions, like vstack and hstack, for common kinds of\\nconcatenation. The above operations could have been expressed as:\\nIn [36]: np.vstack((arr1, arr2))      In [37]: np.hstack((arr1, arr2)) \\nOut[36]:                              Out[37]:                         \\narray([[ 1,  2,  3],                  array([[ 1,  2,  3,  7,  8,  9], \\n       [ 4,  5,  6],                         [ 4,  5,  6, 10, 11, 12]])\\n       [ 7,  8,  9],                                                   \\n       [10, 11, 12]])\\nsplit, on the other hand, slices apart an array into multiple arrays along an axis:\\nIn [38]: from numpy.random import randn\\nIn [39]: arr = randn(5, 2)      In [40]: arr               \\n                                Out[40]:                   \\n                                array([[ 0.1689,  0.3287], \\n                                       [ 0.4703,  0.8989], \\n                                       [ 0.1535,  0.0243], \\n                                       [-0.2832,  1.1536], \\n                                       [ 0.2707,  0.8075]])\\n                                                           \\nIn [41]: first, second, third = np.split(arr, [1, 3])\\nIn [42]: first\\n358 | Chapter 12: \\u2002Advanced NumPy\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 374, 'page_label': '359'}, page_content='Out[42]: array([[ 0.1689,  0.3287]])\\nIn [43]: second                  In [44]: third             \\nOut[43]:                         Out[44]:                   \\narray([[ 0.4703,  0.8989],       array([[-0.2832,  1.1536], \\n       [ 0.1535,  0.0243]])             [ 0.2707,  0.8075]])\\nSee Table 12-1 for a list of all relevant concatenation and splitting functions, some of\\nwhich are provided only as a convenience of the very general purpose concatenate.\\nTable 12-1. Array concatenation functions\\nFunction Description\\nconcatenate Most general function, concatenates collection of arrays along one axis\\nvstack, row_stack Stack arrays row-wise (along axis 0)\\nhstack Stack arrays column-wise (along axis 1)\\ncolumn_stack Like hstack, but converts 1D arrays to 2D column vectors first\\ndstack Stack arrays “depth\"-wise (along axis 2)\\nsplit Split array at passed locations along a particular axis\\nhsplit / vsplit / dsplit Convenience functions for splitting on axis 0, 1, and 2, respectively.\\nStacking helpers: r_ and c_\\nThere are two special objects in the NumPy namespace, r_ and c_, that make stacking\\narrays more concise:\\nIn [45]: arr = np.arange(6)\\nIn [46]: arr1 = arr.reshape((3, 2))\\nIn [47]: arr2 = randn(3, 2)\\nIn [48]: np.r_[arr1, arr2]       In [49]: np.c_[np.r_[arr1, arr2], arr]\\nOut[48]:                         Out[49]:                              \\narray([[ 0.    ,  1.    ],       array([[ 0.    ,  1.    ,  0.    ],   \\n       [ 2.    ,  3.    ],              [ 2.    ,  3.    ,  1.    ],   \\n       [ 4.    ,  5.    ],              [ 4.    ,  5.    ,  2.    ],   \\n       [ 0.7258, -1.5325],              [ 0.7258, -1.5325,  3.    ],   \\n       [-0.4696, -0.2127],              [-0.4696, -0.2127,  4.    ],   \\n       [-0.1072,  1.2871]])             [-0.1072,  1.2871,  5.    ]])\\nThese additionally can translate slices to arrays:\\nIn [50]: np.c_[1:6, -10:-5]\\nOut[50]: \\narray([[  1, -10],\\n       [  2,  -9],\\n       [  3,  -8],\\n       [  4,  -7],\\n       [  5,  -6]])\\nSee the docstring for more on what you can do with c_ and r_.\\nAdvanced Array Manipulation | 359\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 375, 'page_label': '360'}, page_content='Repeating Elements: Tile and Repeat\\nThe need to replicate or repeat arrays is less common with NumPy than\\nit is with other popular array programming languages like MATLAB.\\nThe main reason for this is that broadcasting fulfills this need better,\\nwhich is the subject of the next section.\\nThe two main tools for repeating or replicating arrays to produce larger arrays are the \\nrepeat and tile functions. repeat replicates each element in an array some number of\\ntimes, producing a larger array:\\nIn [51]: arr = np.arange(3)\\nIn [52]: arr.repeat(3)\\nOut[52]: array([0, 0, 0, 1, 1, 1, 2, 2, 2])\\nBy default, if you pass an integer, each element will be repeated that number of times.\\nIf you pass an array of integers, each element can be repeated a different number of\\ntimes:\\nIn [53]: arr.repeat([2, 3, 4])\\nOut[53]: array([0, 0, 1, 1, 1, 2, 2, 2, 2])\\nMultidimensional arrays can have their elements repeated along a particular axis.\\nIn [54]: arr = randn(2, 2)\\nIn [55]: arr                       In [56]: arr.repeat(2, axis=0)\\nOut[55]:                           Out[56]:                      \\narray([[ 0.7157, -0.6387],         array([[ 0.7157, -0.6387],    \\n       [ 0.3626,  0.849 ]])               [ 0.7157, -0.6387],    \\n                                          [ 0.3626,  0.849 ],    \\n                                          [ 0.3626,  0.849 ]])\\nNote that if no axis is passed, the array will be flattened first, which is likely not what\\nyou want. Similarly you can pass an array of integers when repeating a multidimen-\\nsional array to repeat a given slice a different number of times:\\nIn [57]: arr.repeat([2, 3], axis=0)\\nOut[57]: \\narray([[ 0.7157, -0.6387],\\n       [ 0.7157, -0.6387],\\n       [ 0.3626,  0.849 ],\\n       [ 0.3626,  0.849 ],\\n       [ 0.3626,  0.849 ]])\\nIn [58]: arr.repeat([2, 3], axis=1)\\nOut[58]: \\narray([[ 0.7157,  0.7157, -0.6387, -0.6387, -0.6387],\\n       [ 0.3626,  0.3626,  0.849 ,  0.849 ,  0.849 ]])\\n360 | Chapter 12: \\u2002Advanced NumPy\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 376, 'page_label': '361'}, page_content='tile, on the other hand, is a shortcut for stacking copies of an array along an axis. You\\ncan visually think about it as like “laying down tiles”:\\nIn [59]: arr\\nOut[59]: \\narray([[ 0.7157, -0.6387],\\n       [ 0.3626,  0.849 ]])\\nIn [60]: np.tile(arr, 2)\\nOut[60]: \\narray([[ 0.7157, -0.6387,  0.7157, -0.6387],\\n       [ 0.3626,  0.849 ,  0.3626,  0.849 ]])\\nThe second argument is the number of tiles; with a scalar, the tiling is made row-by-\\nrow, rather than column by column: The second argument to tile can be a tuple in-\\ndicating the layout of the “tiling”:\\nIn [61]: arr\\nOut[61]: \\narray([[ 0.7157, -0.6387],\\n       [ 0.3626,  0.849 ]])\\nIn [62]: np.tile(arr, (2, 1))      In [63]: np.tile(arr, (3, 2))                \\nOut[62]:                           Out[63]:                                     \\narray([[ 0.7157, -0.6387],         array([[ 0.7157, -0.6387,  0.7157, -0.6387], \\n       [ 0.3626,  0.849 ],                [ 0.3626,  0.849 ,  0.3626,  0.849 ], \\n       [ 0.7157, -0.6387],                [ 0.7157, -0.6387,  0.7157, -0.6387], \\n       [ 0.3626,  0.849 ]])               [ 0.3626,  0.849 ,  0.3626,  0.849 ], \\n                                          [ 0.7157, -0.6387,  0.7157, -0.6387], \\n                                          [ 0.3626,  0.849 ,  0.3626,  0.849 ]])\\nFancy Indexing Equivalents: Take and Put\\nAs you may recall from Chapter 4, one way to get and set subsets of arrays is by \\nfancy indexing using integer arrays:\\nIn [64]: arr = np.arange(10) * 100\\nIn [65]: inds = [7, 1, 2, 6]        In [66]: arr[inds]                  \\n                                    Out[66]: array([700, 100, 200, 600])\\nThere are alternate ndarray methods that are useful in the special case of only making\\na selection on a single axis:\\nIn [67]: arr.take(inds)\\nOut[67]: array([700, 100, 200, 600])\\nIn [68]: arr.put(inds, 42)\\nIn [69]: arr\\nOut[69]: array([  0,  42,  42, 300, 400, 500,  42,  42, 800, 900])\\nIn [70]: arr.put(inds, [40, 41, 42, 43])\\nAdvanced Array Manipulation | 361\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 377, 'page_label': '362'}, page_content='In [71]: arr\\nOut[71]: array([  0,  41,  42, 300, 400, 500,  43,  40, 800, 900])\\nTo use take along other axes, you can pass the axis keyword:\\nIn [72]: inds = [2, 0, 2, 1]\\nIn [73]: arr = randn(2, 4)\\nIn [74]: arr\\nOut[74]: \\narray([[-0.8237,  2.6047, -0.4578, -1.    ],\\n       [ 2.3198, -1.0792,  0.518 ,  0.2527]])\\nIn [75]: arr.take(inds, axis=1)\\nOut[75]: \\narray([[-0.4578, -0.8237, -0.4578,  2.6047],\\n       [ 0.518 ,  2.3198,  0.518 , -1.0792]])\\nput does not accept an axis argument but rather indexes into the flattened (one-di-\\nmensional, C order) version of the array (this could be changed in principle). Thus,\\nwhen you need to set elements using an index array on other axes, you will want to use\\nfancy indexing.\\nAs of this writing, the take and put functions in general have better\\nperformance than their fancy indexing equivalents by a significant mar-\\ngin. I regard this as a “bug” and something to be fixed in NumPy, but\\nit’s something worth keeping in mind if you’re selecting subsets of large\\narrays using integer arrays:\\nIn [76]: arr = randn(1000, 50)\\n# Random sample of 500 rows\\nIn [77]: inds = np.random.permutation(1000)[:500]\\nIn [78]: %timeit arr[inds]\\n1000 loops, best of 3: 356 us per loop\\nIn [79]: %timeit arr.take(inds, axis=0)\\n10000 loops, best of 3: 34 us per loop\\nBroadcasting\\nBroadcasting describes how arithmetic works between arrays of different shapes. It is\\na very powerful feature, but one that can be easily misunderstood, even by experienced\\nusers. The simplest example of broadcasting occurs when combining a scalar value\\nwith an array:\\nIn [80]: arr = np.arange(5)\\nIn [81]: arr                           In [82]: arr * 4                    \\nOut[81]: array([0, 1, 2, 3, 4])        Out[82]: array([ 0,  4,  8, 12, 16])\\n362 | Chapter 12: \\u2002Advanced NumPy\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 378, 'page_label': '363'}, page_content='Here we say that the scalar value 4 has been broadcast to all of the other elements in\\nthe multiplication operation.\\nFor example, we can demean each column of an array by subtracting the column means.\\nIn this case, it is very simple:\\nIn [83]: arr = randn(4, 3)\\nIn [84]: arr.mean(0)\\nOut[84]: array([ 0.1321,  0.552 ,  0.8571])\\nIn [85]: demeaned = arr - arr.mean(0)\\nIn [86]: demeaned                           In [87]: demeaned.mean(0)      \\nOut[86]:                                    Out[87]: array([ 0., -0., -0.])\\narray([[ 0.1718, -0.1972, -1.3669],                                        \\n       [-0.1292,  1.6529, -0.3429],                                        \\n       [-0.2891, -0.0435,  1.2322],                                        \\n       [ 0.2465, -1.4122,  0.4776]])\\nSee Figure 12-4 for an illustration of this operation. Demeaning the rows as a broadcast\\noperation requires a bit more care. Fortunately, broadcasting potentially lower dimen-\\nsional values across any dimension of an array (like subtracting the row means from\\neach column of a two-dimensional array) is possible as long as you follow the rules.\\nThis brings us to:\\nFigure 12-4. Broadcasting over axis 0 with a 1D array\\nThe Broadcasting Ru\\nTwo arrays are compatible for broadcasting if for each trailing dimension (that is, start-\\ning from the end), the axis lengths match or if either of the lengths is 1. Broadcasting\\nis then performed over the missing and / or length 1 dimensions.\\nEven as an experienced NumPy user, I often must stop to draw pictures and think about\\nthe broadcasting rule. Consider the last example and suppose we wished instead to\\nsubtract the mean value from each row. Since arr.mean(0) has length 3, it is compatible\\nBroadcasting | 363\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 379, 'page_label': '364'}, page_content='for broadcasting across axis 0 because the trailing dimension in arr is 3 and therefore\\nmatches. According to the rules, to subtract over axis 1 (that is, subtract the row mean\\nfrom each row), the smaller array must have shape (4, 1):\\nIn [88]: arr\\nOut[88]: \\narray([[ 0.3039,  0.3548, -0.5097],\\n       [ 0.0029,  2.2049,  0.5142],\\n       [-0.1571,  0.5085,  2.0893],\\n       [ 0.3786, -0.8602,  1.3347]])\\nIn [89]: row_means = arr.mean(1)        In [90]: row_means.reshape((4, 1))\\n                                        Out[90]:                          \\n                                        array([[ 0.0496],                 \\n                                               [ 0.9073],                 \\n                                               [ 0.8136],                 \\n                                               [ 0.2844]])                \\n                                                                          \\nIn [91]: demeaned = arr - row_means.reshape((4, 1))\\nIn [92]: demeaned.mean(1)\\nOut[92]: array([ 0.,  0.,  0.,  0.])\\nHas your head exploded yet? See Figure 12-5 for an illustration of this operation.\\nFigure 12-5. Broadcasting over axis 1 of a 2D array\\nSee Figure 12-6 for another illustration, this time subtracting a two-dimensional array\\nfrom a three-dimensional one across axis 0.\\nBroadcasting Over Other Axes\\nBroadcasting with higher dimensional arrays can seem even more mind-bending, but\\nit is really a matter of following the rules. If you don’t, you’ll get an error like this:\\nIn [93]: arr - arr.mean(1)\\n---------------------------------------------------------------------------\\nValueError                                Traceback (most recent call last)\\n<ipython-input-93-7b87b85a20b2> in <module>()\\n364 | Chapter 12: \\u2002Advanced NumPy\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 380, 'page_label': '365'}, page_content='----> 1 arr - arr.mean(1)\\nValueError: operands could not be broadcast together with shapes (4,3) (4)\\nFigure 12-6. Broadcasting over axis 0 of a 3D array\\nIt’s quite common to want to perform an arithmetic operation with a lower dimensional\\narray across axes other than axis 0. According to the broadcasting rule, the “broadcast\\ndimensions” must be 1 in the smaller array. In the example of row demeaning above\\nthis meant reshaping the row means to be shape (4, 1) instead of (4,):\\nIn [94]: arr - arr.mean(1).reshape((4, 1))\\nOut[94]: \\narray([[ 0.2542,  0.3051, -0.5594],\\n       [-0.9044,  1.2976, -0.3931],\\n       [-0.9707, -0.3051,  1.2757],\\n       [ 0.0942, -1.1446,  1.0503]])\\nIn the three-dimensional case, broadcasting over any of the three dimensions is only a\\nmatter of reshaping the data to be shape-compatible. See Figure 12-7 for a nice visual-\\nization of the shapes required to broadcast over each axis of a three-dimensional array.\\nA very common problem, therefore, is needing to add a new axis with length 1 specif-\\nically for broadcasting purposes, especially in generic algorithms. Using reshape is one\\noption, but inserting an axis requires constructing a tuple indicating the new shape.\\nThis can often be a tedious exercise. Thus, NumPy arrays offer a special syntax for\\ninserting new axes by indexing. We use the special np.newaxis attribute along with\\n“full” slices to insert the new axis:\\nIn [95]: arr = np.zeros((4, 4))\\nIn [96]: arr_3d = arr[:, np.newaxis, :]      In [97]: arr_3d.shape\\n                                             Out[97]: (4, 1, 4)\\nIn [98]: arr_1d = np.random.normal(size=3)\\nIn [99]: arr_1d[:, np.newaxis]      In [100]: arr_1d[np.newaxis, :]               \\nOut[99]:                            Out[100]: array([[-0.3899,  0.396 , -0.1852]])\\nBroadcasting | 365\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 381, 'page_label': '366'}, page_content='array([[-0.3899],                                                                 \\n       [ 0.396 ],                                                                 \\n       [-0.1852]])\\nFigure 12-7. Compatible 2D array shapes for broadcasting over a 3D array\\nThus, if we had a three-dimensional array and wanted to demean axis 2, say, we would\\nonly need to write:\\nIn [101]: arr = randn(3, 4, 5)\\nIn [102]: depth_means = arr.mean(2)\\nIn [103]: depth_means\\nOut[103]: \\narray([[ 0.1097,  0.3118, -0.5473,  0.2663],\\n       [ 0.1747,  0.1379,  0.1146, -0.4224],\\n       [ 0.0217,  0.3686, -0.0468,  1.3026]])\\nIn [104]: demeaned = arr - depth_means[:, :, np.newaxis]\\nIn [105]: demeaned.mean(2)\\nOut[105]: \\narray([[ 0.,  0., -0.,  0.],\\n       [ 0., -0., -0.,  0.],\\n       [-0., -0.,  0.,  0.]])\\nIf you’re completely confused by this, don’t worry. With practice you will get the hang\\nof it!\\n366 | Chapter 12: \\u2002Advanced NumPy\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 382, 'page_label': '367'}, page_content='Some readers might wonder if there’s a way to generalize demeaning over an axis\\nwithout sacrificing performance. There is, in fact, but it requires some indexing\\ngymnastics:\\ndef demean_axis(arr, axis=0):\\n    means = arr.mean(axis)\\n    # This generalized things like [:, :, np.newaxis] to N dimensions\\n    indexer = [slice(None)] * arr.ndim\\n    indexer[axis] = np.newaxis\\n    return arr - means[indexer]\\nSetting Array Values by Broadcasting\\nThe same broadcasting rule governing arithmetic operations also applies to setting\\nvalues via array indexing. In the simplest case, we can do things like:\\nIn [106]: arr = np.zeros((4, 3))\\nIn [107]: arr[:] = 5        In [108]: arr           \\n                            Out[108]:               \\n                            array([[ 5.,  5.,  5.], \\n                                   [ 5.,  5.,  5.], \\n                                   [ 5.,  5.,  5.], \\n                                   [ 5.,  5.,  5.]])\\nHowever, if we had a one-dimensional array of values we wanted to set into the columns\\nof the array, we can do that as long as the shape is compatible:\\nIn [109]: col = np.array([1.28, -0.42, 0.44, 1.6])\\nIn [110]: arr[:] = col[:, np.newaxis]       In [111]: arr                 \\n                                            Out[111]:                     \\n                                            array([[ 1.28,  1.28,  1.28], \\n                                                   [-0.42, -0.42, -0.42], \\n                                                   [ 0.44,  0.44,  0.44], \\n                                                   [ 1.6 ,  1.6 ,  1.6 ]])\\n                                                                          \\nIn [112]: arr[:2] = [[-1.37], [0.509]]      In [113]: arr                    \\n                                            Out[113]:                        \\n                                            array([[-1.37 , -1.37 , -1.37 ], \\n                                                   [ 0.509,  0.509,  0.509], \\n                                                   [ 0.44 ,  0.44 ,  0.44 ], \\n                                                   [ 1.6  ,  1.6  ,  1.6  ]])\\nAdvanced ufunc Usage\\nWhile many NumPy users will only make use of the fast element-wise operations pro-\\nvided by the universal functions, there are a number of additional features that occa-\\nsionally can help you write more concise code without loops.\\nAdvanced ufunc Usage | 367\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 383, 'page_label': '368'}, page_content='ufunc Instance Methods\\nEach of NumPy’s binary ufuncs has special methods for performing certain kinds of\\nspecial vectorized operations. These are summarized in Table 12-2, but I’ll give a few\\nconcrete examples to illustrate how they work.\\nreduce takes a single array and aggregates its values, optionally along an axis, by per-\\nforming a sequence of binary operations. For example, an alternate way to sum ele-\\nments in an array is to use np.add.reduce:\\nIn [114]: arr = np.arange(10)\\nIn [115]: np.add.reduce(arr)\\nOut[115]: 45\\nIn [116]: arr.sum()\\nOut[116]: 45\\nThe starting value (0 for add) depends on the ufunc. If an axis is passed, the reduction\\nis performed along that axis. This allows you to answer certain kinds of questions in a\\nconcise way. As a less trivial example, we can use np.logical_and to check whether the\\nvalues in each row of an array are sorted:\\nIn [118]: arr = randn(5, 5)\\nIn [119]: arr[::2].sort(1) # sort a few rows\\nIn [120]: arr[:, :-1] < arr[:, 1:]\\nOut[120]: \\narray([[ True,  True,  True,  True],\\n       [False,  True, False, False],\\n       [ True,  True,  True,  True],\\n       [ True, False,  True,  True],\\n       [ True,  True,  True,  True]], dtype=bool)\\nIn [121]: np.logical_and.reduce(arr[:, :-1] < arr[:, 1:], axis=1)\\nOut[121]: array([ True, False,  True, False,  True], dtype=bool)\\nOf course, logical_and.reduce is equivalent to the all method.\\naccumulate is related to reduce like cumsum is related to sum. It produces an array of the\\nsame size with the intermediate “accumulated” values:\\nIn [122]: arr = np.arange(15).reshape((3, 5))\\nIn [123]: np.add.accumulate(arr, axis=1)\\nOut[123]: \\narray([[ 0,  1,  3,  6, 10],\\n       [ 5, 11, 18, 26, 35],\\n       [10, 21, 33, 46, 60]])\\nouter performs a pairwise cross-product between two arrays:\\nIn [124]: arr = np.arange(3).repeat([1, 2, 2])\\n368 | Chapter 12: \\u2002Advanced NumPy\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 384, 'page_label': '369'}, page_content='In [125]: arr\\nOut[125]: array([0, 1, 1, 2, 2])\\nIn [126]: np.multiply.outer(arr, np.arange(5))\\nOut[126]: \\narray([[0, 0, 0, 0, 0],\\n       [0, 1, 2, 3, 4],\\n       [0, 1, 2, 3, 4],\\n       [0, 2, 4, 6, 8],\\n       [0, 2, 4, 6, 8]])\\nThe output of outer will have a dimension that is the sum of the dimensions of the\\ninputs:\\nIn [127]: result = np.subtract.outer(randn(3, 4), randn(5))\\nIn [128]: result.shape\\nOut[128]: (3, 4, 5)\\nThe last method, reduceat, performs a “local reduce”, in essence an array groupby op-\\neration in which slices of the array are aggregated together. While it’s less flexible than\\nthe GroupBy capabilities in pandas, it can be very fast and powerful in the right cir-\\ncumstances. It accepts a sequence of “bin edges” which indicate how to split and ag-\\ngregate the values:\\nIn [129]: arr = np.arange(10)\\nIn [130]: np.add.reduceat(arr, [0, 5, 8])\\nOut[130]: array([10, 18, 17])\\nThe results are the reductions (here, sums) performed over arr[0:5], arr[5:8], and\\narr[8:]. Like the other methods, you can pass an axis argument:\\nIn [131]: arr = np.multiply.outer(np.arange(4), np.arange(5))\\nIn [132]: arr                      In [133]: np.add.reduceat(arr, [0, 2, 4], axis=1)\\nOut[132]:                          Out[133]:                                        \\narray([[ 0,  0,  0,  0,  0],       array([[ 0,  0,  0],                             \\n       [ 0,  1,  2,  3,  4],              [ 1,  5,  4],                             \\n       [ 0,  2,  4,  6,  8],              [ 2, 10,  8],                             \\n       [ 0,  3,  6,  9, 12]])             [ 3, 15, 12]])\\nTable 12-2. ufunc methods\\nMethod Description\\nreduce(x) Aggregate values by successive applications of the operation\\naccumulate(x) Aggregate values, preserving all partial aggregates\\nreduceat(x, bins) “Local” reduce or “group by”. Reduce contiguous slices of data to produce aggregated\\narray.\\nouter(x, y) Apply operation to all pairs of elements in x and y. Result array has shape x.shape +\\ny.shape\\nAdvanced ufunc Usage | 369\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 385, 'page_label': '370'}, page_content=\"Custom ufuncs\\nThere are a couple facilities for creating your own functions with ufunc-like semantics. \\nnumpy.frompyfunc accepts a Python function along with a specification for the number\\nof inputs and outputs. For example, a simple function that adds element-wise would\\nbe specified as:\\nIn [134]: def add_elements(x, y):\\n   .....:     return x + y\\nIn [135]: add_them = np.frompyfunc(add_elements, 2, 1)\\nIn [136]: add_them(np.arange(8), np.arange(8))\\nOut[136]: array([0, 2, 4, 6, 8, 10, 12, 14], dtype=object)\\nFunctions created using frompyfunc always return arrays of Python objects which isn’t\\nvery convenient. Fortunately, there is an alternate, but slightly less featureful function \\nnumpy.vectorize that is a bit more intelligent about type inference:\\nIn [137]: add_them = np.vectorize(add_elements, otypes=[np.float64])\\nIn [138]: add_them(np.arange(8), np.arange(8))\\nOut[138]: array([  0.,   2.,   4.,   6.,   8.,  10.,  12.,  14.])\\nThese functions provide a way to create ufunc-like functions, but they are very slow\\nbecause they require a Python function call to compute each element, which is a lot\\nslower than NumPy’s C-based ufunc loops:\\nIn [139]: arr = randn(10000)\\nIn [140]: %timeit add_them(arr, arr)\\n100 loops, best of 3: 2.12 ms per loop\\nIn [141]: %timeit np.add(arr, arr)\\n100000 loops, best of 3: 11.6 us per loop\\nThere are a number of projects under way in the scientific Python community to make\\nit easier to define new ufuncs whose performance is closer to that of the built-in ones.\\nStructured and Record Arrays\\nYou may have noticed up until now that ndarray is a homogeneous data container; that\\nis, it represents a block of memory in which each element takes up the same number\\nof bytes, determined by the dtype. On the surface, this would appear to not allow you\\nto represent heterogeneous or tabular-like data. A structured array is an ndarray in\\nwhich each element can be thought of as representing a struct in C (hence the “struc-\\ntured” name) or a row in a SQL table with multiple named fields:\\nIn [142]: dtype = [('x', np.float64), ('y', np.int32)]\\nIn [143]: sarr = np.array([(1.5, 6), (np.pi, -2)], dtype=dtype)\\n370 | Chapter 12: \\u2002Advanced NumPy\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 386, 'page_label': '371'}, page_content=\"In [144]: sarr\\nOut[144]: \\narray([(1.5, 6), (3.141592653589793, -2)], \\n      dtype=[('x', '<f8'), ('y', '<i4')])\\nThere are several ways to specify a structured dtype (see the online NumPy documen-\\ntation). One typical way is as a list of tuples with (field_name, field_data_type). Now,\\nthe elements of the array are tuple-like objects whose elements can be accessed like a\\ndictionary:\\nIn [145]: sarr[0]\\nOut[145]: (1.5, 6)\\nIn [146]: sarr[0]['y']\\nOut[146]: 6\\nThe field names are stored in the dtype.names attribute. On accessing a field on the\\nstructured array, a strided view on the data is returned thus copying nothing:\\nIn [147]: sarr['x']\\nOut[147]: array([ 1.5   ,  3.1416])\\nNested dtypes and Multidimensional Fields\\nWhen specifying a structured dtype, you can additionally pass a shape (as an int or\\ntuple):\\nIn [148]: dtype = [('x', np.int64, 3), ('y', np.int32)]\\nIn [149]: arr = np.zeros(4, dtype=dtype)\\nIn [150]: arr\\nOut[150]: \\narray([([0, 0, 0], 0), ([0, 0, 0], 0), ([0, 0, 0], 0), ([0, 0, 0], 0)], \\n      dtype=[('x', '<i8', (3,)), ('y', '<i4')])\\nIn this case, the x field now refers to an array of length three for each record:\\nIn [151]: arr[0]['x']\\nOut[151]: array([0, 0, 0])\\nConveniently, accessing arr['x'] then returns a two-dimensional array instead of a\\none-dimensional array as in prior examples:\\nIn [152]: arr['x']\\nOut[152]: \\narray([[0, 0, 0],\\n       [0, 0, 0],\\n       [0, 0, 0],\\n       [0, 0, 0]])\\nThis enables you to express more complicated, nested structures as a single block of\\nmemory in an array. Though, since dtypes can be arbitrarily complex, why not nested\\ndtypes? Here is a simple example:\\nStructured and Record Arrays | 371\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 387, 'page_label': '372'}, page_content=\"In [153]: dtype = [('x', [('a', 'f8'), ('b', 'f4')]), ('y', np.int32)]\\nIn [154]: data = np.array([((1, 2), 5), ((3, 4), 6)], dtype=dtype)\\nIn [155]: data['x']\\nOut[155]: \\narray([(1.0, 2.0), (3.0, 4.0)], \\n      dtype=[('a', '<f8'), ('b', '<f4')])\\nIn [156]: data['y']\\nOut[156]: array([5, 6], dtype=int32)\\nIn [157]: data['x']['a']\\nOut[157]: array([ 1.,  3.])\\nAs you can see, variable-shape fields and nested records is a very rich feature that can\\nbe the right tool in certain circumstances. A DataFrame from pandas, by contrast, does\\nnot support this feature directly, though it is similar to hierarchical indexing.\\nWhy Use Structured Arrays?\\nCompared with, say, a DataFrame from pandas, NumPy structured arrays are a com-\\nparatively low-level tool. They provide a means to interpreting a block of memory as a\\ntabular structure with arbitrarily complex nested columns. Since each element in the\\narray is represented in memory as a fixed number of bytes, structured arrays provide a\\nvery fast and efficient way of writing data to and from disk (including memory maps,\\nmore on this later), transporting it over the network, and other such use.\\nAs another common use for structured arrays, writing data files as fixed length record\\nbyte streams is a common way to serialize data in C and C++ code, which is commonly\\nfound in legacy systems in industry. As long as the format of the file is known (the size\\nof each record and the order, byte size, and data type of each element), the data can be\\nread into memory using np.fromfile. Specialized uses like this are beyond the scope of\\nthis book, but it’s worth knowing that such things are possible.\\nStructured Array Manipulations: numpy.lib.recfunctions\\nWhile there is not as much functionality available for structured arrays as for Data-\\nFrames, the NumPy module numpy.lib.recfunctions has some helpful tools for adding\\nand dropping fields or doing basic join-like operations. The thing to remember with\\nthese tools is that it is typically necessary to create a new array to make any modifica-\\ntions to the dtype (like adding or dropping a column). These functions are left to the\\ninterested reader to explore as I do not use them anywhere in this book.\\n372 | Chapter 12: \\u2002Advanced NumPy\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 388, 'page_label': '373'}, page_content='More About Sorting\\nLike Python’s built-in list, the ndarray sort instance method is an in-place sort, meaning\\nthat the array contents are rearranged without producing a new array:\\nIn [158]: arr = randn(6)\\nIn [159]: arr.sort()\\nIn [160]: arr\\nOut[160]: array([-1.082 ,  0.3759,  0.8014,  1.1397,  1.2888,  1.8413])\\nWhen sorting arrays in-place, remember that if the array is a view on a different ndarray,\\nthe original array will be modified:\\nIn [161]: arr = randn(3, 5)\\nIn [162]: arr\\nOut[162]: \\narray([[-0.3318, -1.4711,  0.8705, -0.0847, -1.1329],\\n       [-1.0111, -0.3436,  2.1714,  0.1234, -0.0189],\\n       [ 0.1773,  0.7424,  0.8548,  1.038 , -0.329 ]])\\nIn [163]: arr[:, 0].sort()  # Sort first column values in-place\\nIn [164]: arr\\nOut[164]: \\narray([[-1.0111, -1.4711,  0.8705, -0.0847, -1.1329],\\n       [-0.3318, -0.3436,  2.1714,  0.1234, -0.0189],\\n       [ 0.1773,  0.7424,  0.8548,  1.038 , -0.329 ]])\\nOn the other hand, numpy.sort creates a new, sorted copy of an array. Otherwise it\\naccepts the same arguments (such as kind, more on this below) as ndarray.sort:\\nIn [165]: arr = randn(5)\\nIn [166]: arr\\nOut[166]: array([-1.1181, -0.2415, -2.0051,  0.7379, -1.0614])\\nIn [167]: np.sort(arr)\\nOut[167]: array([-2.0051, -1.1181, -1.0614, -0.2415,  0.7379])\\nIn [168]: arr\\nOut[168]: array([-1.1181, -0.2415, -2.0051,  0.7379, -1.0614])\\nAll of these sort methods take an axis argument for sorting the sections of data along\\nthe passed axis independently:\\nIn [169]: arr = randn(3, 5)\\nIn [170]: arr\\nOut[170]: \\narray([[ 0.5955, -0.2682,  1.3389, -0.1872,  0.9111],\\n       [-0.3215,  1.0054, -0.5168,  1.1925, -0.1989],\\n       [ 0.3969, -1.7638,  0.6071, -0.2222, -0.2171]])\\nMore About Sorting | 373\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 389, 'page_label': '374'}, page_content='In [171]: arr.sort(axis=1)\\nIn [172]: arr\\nOut[172]: \\narray([[-0.2682, -0.1872,  0.5955,  0.9111,  1.3389],\\n       [-0.5168, -0.3215, -0.1989,  1.0054,  1.1925],\\n       [-1.7638, -0.2222, -0.2171,  0.3969,  0.6071]])\\nYou may notice that none of the sort methods have an option to sort in descending\\norder. This is not actually a big deal because array slicing produces views, thus not\\nproducing a copy or requiring any computational work. Many Python users are familiar\\nwith the “trick” that for a list values, values[::-1] returns a list in reverse order. The\\nsame is true for ndarrays:\\nIn [173]: arr[:, ::-1]\\nOut[173]: \\narray([[ 1.3389,  0.9111,  0.5955, -0.1872, -0.2682],\\n       [ 1.1925,  1.0054, -0.1989, -0.3215, -0.5168],\\n       [ 0.6071,  0.3969, -0.2171, -0.2222, -1.7638]])\\nIndirect Sorts: argsort and lexsort\\nIn data analysis it’s very common to need to reorder data sets by one or more keys. For\\nexample, a table of data about some students might need to be sorted by last name then\\nby first name. This is an example of an indirect sort, and if you’ve read the pandas-\\nrelated chapters you have already seen many higher-level examples. Given a key or keys\\n(an array or values or multiple arrays of values), you wish to obtain an array of integer\\nindices (I refer to them colloquially as indexers) that tells you how to reorder the data\\nto be in sorted order. The two main methods for this are argsort and numpy.lexsort.\\nAs a trivial example:\\nIn [174]: values = np.array([5, 0, 1, 3, 2])\\nIn [175]: indexer = values.argsort()\\nIn [176]: indexer\\nOut[176]: array([1, 2, 4, 3, 0])\\nIn [177]: values[indexer]\\nOut[177]: array([0, 1, 2, 3, 5])\\nAs a less trivial example, this code reorders a 2D array by its first row:\\nIn [178]: arr = randn(3, 5)\\nIn [179]: arr[0] = values\\nIn [180]: arr\\nOut[180]: \\narray([[ 5.    ,  0.    ,  1.    ,  3.    ,  2.    ],\\n       [-0.3636, -0.1378,  2.1777, -0.4728,  0.8356],\\n       [-0.2089,  0.2316,  0.728 , -1.3918,  1.9956]])\\n374 | Chapter 12: \\u2002Advanced NumPy\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 390, 'page_label': '375'}, page_content=\"In [181]: arr[:, arr[0].argsort()]\\nOut[181]: \\narray([[ 0.    ,  1.    ,  2.    ,  3.    ,  5.    ],\\n       [-0.1378,  2.1777,  0.8356, -0.4728, -0.3636],\\n       [ 0.2316,  0.728 ,  1.9956, -1.3918, -0.2089]])\\nlexsort is similar to argsort, but it performs an indirect lexicographical sort on multiple\\nkey arrays. Suppose we wanted to sort some data identified by first and last names:\\nIn [182]: first_name = np.array(['Bob', 'Jane', 'Steve', 'Bill', 'Barbara'])\\nIn [183]: last_name = np.array(['Jones', 'Arnold', 'Arnold', 'Jones', 'Walters'])\\nIn [184]: sorter = np.lexsort((first_name, last_name))\\nIn [185]: zip(last_name[sorter], first_name[sorter])\\nOut[185]: \\n[('Arnold', 'Jane'),\\n ('Arnold', 'Steve'),\\n ('Jones', 'Bill'),\\n ('Jones', 'Bob'),\\n ('Walters', 'Barbara')]\\nlexsort can be a bit confusing the first time you use it because the order in which the\\nkeys are used to order the data starts with the last array passed. As you can see,\\nlast_name was used before first_name.\\npandas methods like Series’s and DataFrame’s sort_index methods and\\nthe Series order method are implemented with variants of these func-\\ntions (which also must take into account missing values)\\nAlternate Sort Algorithms\\nA stable sorting algorithm preserves the relative position of equal elements. This can\\nbe especially important in indirect sorts where the relative ordering is meaningful:\\nIn [186]: values = np.array(['2:first', '2:second', '1:first', '1:second', '1:third'])\\nIn [187]: key = np.array([2, 2, 1, 1, 1])\\nIn [188]: indexer = key.argsort(kind='mergesort')\\nIn [189]: indexer\\nOut[189]: array([2, 3, 4, 0, 1])\\nIn [190]: values.take(indexer)\\nOut[190]: \\narray(['1:first', '1:second', '1:third', '2:first', '2:second'], \\n      dtype='|S8')\\nThe only stable sort available is mergesort which has guaranteed O(n log n) performance\\n(for complexity buffs), but its performance is on average worse than the default\\nMore About Sorting | 375\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 391, 'page_label': '376'}, page_content=\"quicksort method. See Table 12-3 for a summary of available methods and their relative\\nperformance (and performance guarantees). This is not something that most users will\\never have to think about but useful to know that it’s there.\\nTable 12-3. Array sorting methods\\nKind Speed Stable Work space Worst-case\\n'quicksort' 1 No 0 O(n2)\\n'mergesort' 2 Yes n / 2 O(n log n)\\n'heapsort' 3 No 0 O(n log n)\\nAt the time of this writing, sort algorithms other than quicksort are not\\navailable on arrays of Python objects (dtype=object). This means occa-\\nsionally that algorithms requiring stable sorting will require work-\\narounds when dealing with Python objects.\\nnumpy.searchsorted: Finding elements in a Sorted Array\\nsearchsorted is an array method that performs a binary search on a sorted array, re-\\nturning the location in the array where the value would need to be inserted to maintain\\nsortedness:\\nIn [191]: arr = np.array([0, 1, 7, 12, 15])\\nIn [192]: arr.searchsorted(9)\\nOut[192]: 3\\nAs you might expect, you can also pass an array of values to get an array of indices back:\\nIn [193]: arr.searchsorted([0, 8, 11, 16])\\nOut[193]: array([0, 3, 3, 5])\\nYou might have noticed that searchsorted returned 0 for the 0 element. This is because\\nthe default behavior is to return the index at the left side of a group of equal values:\\nIn [194]: arr = np.array([0, 0, 0, 1, 1, 1, 1])\\nIn [195]: arr.searchsorted([0, 1])\\nOut[195]: array([0, 3])\\nIn [196]: arr.searchsorted([0, 1], side='right')\\nOut[196]: array([3, 7])\\nAs another application of searchsorted, suppose we had an array of values between 0\\nand 10,000) and a separate array of “bucket edges” that we wanted to use to bin the\\ndata:\\nIn [197]: data = np.floor(np.random.uniform(0, 10000, size=50))\\nIn [198]: bins = np.array([0, 100, 1000, 5000, 10000])\\nIn [199]: data\\n376 | Chapter 12: \\u2002Advanced NumPy\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 392, 'page_label': '377'}, page_content='Out[199]: \\narray([ 8304.,  4181.,  9352.,  4907.,  3250.,  8546.,  2673.,  6152.,\\n        2774.,  5130.,  9553.,  4997.,  1794.,  9688.,   426.,  1612.,\\n         651.,  8653.,  1695.,  4764.,  1052.,  4836.,  8020.,  3479.,\\n        1513.,  5872.,  8992.,  7656.,  4764.,  5383.,  2319.,  4280.,\\n        4150.,  8601.,  3946.,  9904.,  7286.,  9969.,  6032.,  4574.,\\n        8480.,  4298.,  2708.,  7358.,  6439.,  7916.,  3899.,  9182.,\\n         871.,  7973.])\\nTo then get a labeling of which interval each data point belongs to (where 1 would\\nmean the bucket [0, 100)), we can simply use searchsorted:\\nIn [200]: labels = bins.searchsorted(data)\\nIn [201]: labels\\nOut[201]: \\narray([4, 3, 4, 3, 3, 4, 3, 4, 3, 4, 4, 3, 3, 4, 2, 3, 2, 4, 3, 3, 3, 3, 4,\\n       3, 3, 4, 4, 4, 3, 4, 3, 3, 3, 4, 3, 4, 4, 4, 4, 3, 4, 3, 3, 4, 4, 4,\\n       3, 4, 2, 4])\\nThis, combined with pandas’s groupby, can be used to easily bin data:\\nIn [202]: Series(data).groupby(labels).mean()\\nOut[202]: \\n2     649.333333\\n3    3411.521739\\n4    7935.041667\\nNote that NumPy actually has a function digitize that computes this bin labeling:\\nIn [203]: np.digitize(data, bins)\\nOut[203]: \\narray([4, 3, 4, 3, 3, 4, 3, 4, 3, 4, 4, 3, 3, 4, 2, 3, 2, 4, 3, 3, 3, 3, 4,\\n       3, 3, 4, 4, 4, 3, 4, 3, 3, 3, 4, 3, 4, 4, 4, 4, 3, 4, 3, 3, 4, 4, 4,\\n       3, 4, 2, 4])\\nNumPy Matrix Class\\nCompared with other languages for matrix operations and linear algebra, like MAT-\\nLAB, Julia, and GAUSS, NumPy’s linear algebra syntax can often be quite verbose. One\\nreason is that matrix multiplication requires using numpy.dot. Also NumPy’s indexing\\nsemantics are different, which makes porting code to Python less straightforward at\\ntimes. Selecting a single row (e.g. X[1, :]) or column (e.g. X[:, 1]) from a 2D array\\nyields a 1D array compared with a 2D array as in, say, MATLAB.\\nIn [204]: X =  np.array([[ 8.82768214,  3.82222409, -1.14276475,  2.04411587],\\n   .....:                [ 3.82222409,  6.75272284,  0.83909108,  2.08293758],\\n   .....:                [-1.14276475,  0.83909108,  5.01690521,  0.79573241],\\n   .....:                [ 2.04411587,  2.08293758,  0.79573241,  6.24095859]])\\nIn [205]: X[:, 0]  # one-dimensional\\nOut[205]: array([ 8.8277,  3.8222, -1.1428,  2.0441])\\nIn [206]: y = X[:, :1]  # two-dimensional by slicing\\nNumPy Matrix Class | 377\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 393, 'page_label': '378'}, page_content='In [207]: X\\nOut[207]: \\narray([[ 8.8277,  3.8222, -1.1428,  2.0441],\\n       [ 3.8222,  6.7527,  0.8391,  2.0829],\\n       [-1.1428,  0.8391,  5.0169,  0.7957],\\n       [ 2.0441,  2.0829,  0.7957,  6.241 ]])\\nIn [208]: y\\nOut[208]: \\narray([[ 8.8277],\\n       [ 3.8222],\\n       [-1.1428],\\n       [ 2.0441]])\\nIn this case, the product yT X y would be expressed like so:\\nIn [209]: np.dot(y.T, np.dot(X, y))\\nOut[209]: array([[ 1195.468]])\\nTo aid in writing code with a lot of matrix operations, NumPy has a matrix class which\\nhas modified indexing behavior to make it more MATLAB-like: single rows and col-\\numns come back two-dimensional and multiplication with * is matrix multiplication.\\nThe above operation with numpy.matrix would look like:\\nIn [210]: Xm = np.matrix(X)\\nIn [211]: ym = Xm[:, 0]\\nIn [212]: Xm\\nOut[212]: \\nmatrix([[ 8.8277,  3.8222, -1.1428,  2.0441],\\n        [ 3.8222,  6.7527,  0.8391,  2.0829],\\n        [-1.1428,  0.8391,  5.0169,  0.7957],\\n        [ 2.0441,  2.0829,  0.7957,  6.241 ]])\\nIn [213]: ym\\nOut[213]: \\nmatrix([[ 8.8277],\\n        [ 3.8222],\\n        [-1.1428],\\n        [ 2.0441]])\\nIn [214]: ym.T * Xm * ym\\nOut[214]: matrix([[ 1195.468]])\\nmatrix also has a special attribute I which returns the matrix inverse:\\nIn [215]: Xm.I * X\\nOut[215]: \\nmatrix([[ 1., -0., -0., -0.],\\n        [ 0.,  1.,  0.,  0.],\\n        [ 0.,  0.,  1.,  0.],\\n        [ 0.,  0.,  0.,  1.]])\\n378 | Chapter 12: \\u2002Advanced NumPy\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 394, 'page_label': '379'}, page_content=\"I do not recommend using numpy.matrix as a replacement for regular ndarrays because\\nthey are generally more seldom used. In individual functions with lots of linear algebra,\\nit may be helpful to convert the function argument to matrix type, then cast back to\\nregular arrays with np.asarray (which does not copy any data) before returning them.\\nAdvanced Array Input and Output\\nIn Chapter 4, I introduced you to np.save and np.load for storing arrays in binary format\\non disk. There are a number of additional options to consider for more sophisticated\\nuse. In particular, memory maps have the additional benefit of enabling you to work\\nwith data sets that do not fit into RAM.\\nMemory-mapped Files\\nA memory-mapped file is a method for treating potentially very large binary data on\\ndisk as an in-memory array. NumPy implements a memmap object that is ndarray-like,\\nenabling small segments of a large file to be read and written without reading the whole\\narray into memory. Additionally, a memmap has the same methods as an in-memory array\\nand thus can be substituted into many algorithms where an ndarray would be expected.\\nTo create a new memmap, use the function np.memmap and pass a file path, dtype, shape,\\nand file mode:\\nIn [216]: mmap = np.memmap('mymmap', dtype='float64', mode='w+', shape=(10000, 10000))\\nIn [217]: mmap\\nOut[217]: \\nmemmap([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\\n       ..., \\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])\\nSlicing a memmap returns views on the data on disk:\\nIn [218]: section = mmap[:5]\\nIf you assign data to these, it will be buffered in memory (like a Python file object), but\\ncan be written to disk by calling flush:\\nIn [219]: section[:] = np.random.randn(5, 10000)\\nIn [220]: mmap.flush()\\nIn [221]: mmap\\nOut[221]: \\nmemmap([[-0.1614, -0.1768,  0.422 , ..., -0.2195, -0.1256, -0.4012],\\n       [ 0.4898, -2.2219, -0.7684, ..., -2.3517, -1.0782,  1.3208],\\n       [-0.6875,  1.6901, -0.7444, ..., -1.4218, -0.0509,  1.2224],\\nAdvanced Array Input and Output | 379\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 395, 'page_label': '380'}, page_content=\"..., \\n       [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\\n       [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\\n       [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ]])\\nIn [222]: del mmap\\nWhenever a memory map falls out of scope and is garbage-collected, any changes will\\nbe flushed to disk also. When opening an existing memory map, you still have to specify\\nthe dtype and shape as the file is just a block of binary data with no metadata on disk:\\nIn [223]: mmap = np.memmap('mymmap', dtype='float64', shape=(10000, 10000))\\nIn [224]: mmap\\nOut[224]: \\nmemmap([[-0.1614, -0.1768,  0.422 , ..., -0.2195, -0.1256, -0.4012],\\n       [ 0.4898, -2.2219, -0.7684, ..., -2.3517, -1.0782,  1.3208],\\n       [-0.6875,  1.6901, -0.7444, ..., -1.4218, -0.0509,  1.2224],\\n       ..., \\n       [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\\n       [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\\n       [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ]])\\nSince a memory map is just an on-disk ndarray, there are no issues using a structured\\ndtype as described above.\\nHDF5 and Other Array Storage Options\\nPyTables and h5py are two Python projects providing NumPy-friendly interfaces for\\nstoring array data in the efficient and compressible HDF5 format (HDF stands for\\nhierarchical data format). You can safely store hundreds of gigabytes or even terabytes\\nof data in HDF5 format. The use of these libraries is unfortunately outside the scope\\nof the book.\\nPyTables provides a rich facility for working with structured arrays with advanced\\nquerying features and the ability to add column indexes to accelerate queries. This is\\nvery similar to the table indexing capabilities provided by relational databases.\\nPerformance Tips\\nGetting good performance out of code utilizing NumPy is often straightforward, as\\narray operations typically replace otherwise comparatively extremely slow pure Python\\nloops. Here is a brief list of some of the things to keep in mind:\\n• Convert Python loops and conditional logic to array operations and boolean array\\noperations\\n• Use broadcasting whenever possible\\n• Avoid copying data using array views (slicing)\\n• Utilize ufuncs and ufunc methods\\n380 | Chapter 12: \\u2002Advanced NumPy\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 396, 'page_label': '381'}, page_content=\"If you can’t get the performance you require after exhausting the capabilities provided\\nby NumPy alone, writing code in C, Fortran, or especially Cython (see a bit more on\\nthis below) may be in order. I personally use Cython (http://cython.org) heavily in my\\nown work as an easy way to get C-like performance with minimal development.\\nThe Importance of Contiguous Memory\\nWhile the full extent of this topic is a bit outside the scope of this book, in some ap-\\nplications the memory layout of an array can significantly affect the speed of compu-\\ntations. This is based partly on performance differences having to do with the cache\\nhierarchy of the CPU; operations accessing contiguous blocks of memory (for example,\\nsumming the rows of a C order array) will generally be the fastest because the memory\\nsubsystem will buffer the appropriate blocks of memory into the ultrafast L1 or L2 CPU\\ncache. Also, certain code paths inside NumPy’s C codebase have been optimized for\\nthe contiguous case in which generic strided memory access can be avoided.\\nTo say that an array’s memory layout is contiguous means that the elements are stored\\nin memory in the order that they appear in the array with respect to Fortran (column\\nmajor) or C (row major) ordering. By default, NumPy arrays are created as C-contigu-\\nous or just simply contiguous. A column major array, such as the transpose of a C-\\ncontiguous array, is thus said to be Fortran-contiguous. These properties can be ex-\\nplicitly checked via the flags attribute on the ndarray:\\nIn [227]: arr_c = np.ones((1000, 1000), order='C')\\nIn [228]: arr_f = np.ones((1000, 1000), order='F')\\nIn [229]: arr_c.flags         In [230]: arr_f.flags \\nOut[229]:                     Out[230]:             \\n  C_CONTIGUOUS : True           C_CONTIGUOUS : False\\n  F_CONTIGUOUS : False          F_CONTIGUOUS : True \\n  OWNDATA : True                OWNDATA : True      \\n  WRITEABLE : True              WRITEABLE : True    \\n  ALIGNED : True                ALIGNED : True      \\n  UPDATEIFCOPY : False          UPDATEIFCOPY : False\\n                                                    \\nIn [231]: arr_f.flags.f_contiguous\\nOut[231]: True\\nIn this example, summing the rows of these arrays should, in theory, be faster for\\narr_c than arr_f since the rows are contiguous in memory. Here I check for sure using\\n%timeit in IPython:\\nIn [232]: %timeit arr_c.sum(1)\\n1000 loops, best of 3: 1.33 ms per loop\\nIn [233]: %timeit arr_f.sum(1)\\n100 loops, best of 3: 8.75 ms per loop\\nPerformance Tips | 381\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 397, 'page_label': '382'}, page_content=\"When looking to squeeze more performance out of NumPy, this is often a place to\\ninvest some effort. If you have an array that does not have the desired memory order,\\nyou can use copy and pass either 'C' or 'F':\\nIn [234]: arr_f.copy('C').flags\\nOut[234]: \\n  C_CONTIGUOUS : True\\n  F_CONTIGUOUS : False\\n  OWNDATA : True\\n  WRITEABLE : True\\n  ALIGNED : True\\n  UPDATEIFCOPY : False\\nWhen constructing a view on an array, keep in mind that the result is not guaranteed\\nto be contiguous:\\nIn [235]: arr_c[:50].flags.contiguous      In [236]: arr_c[:, :50].flags\\nOut[235]: True                             Out[236]:                    \\n                                             C_CONTIGUOUS : False       \\n                                             F_CONTIGUOUS : False       \\n                                             OWNDATA : False            \\n                                             WRITEABLE : True           \\n                                             ALIGNED : True             \\n                                             UPDATEIFCOPY : False\\nOther Speed Options: Cython, f2py, C\\nIn recent years, the Cython project (( http://cython.org) has become the tool of choice\\nfor many scientific Python programmers for implementing fast code that may need to\\ninteract with C or C++ libraries, but without having to write pure C code. You can\\nthink of Cython as Python with static types and the ability to interleave functions im-\\nplemented in C into Python-like code. For example, a simple Cython function to sum\\nthe elements of a one-dimensional array might look like:\\nfrom numpy cimport ndarray, float64_t\\ndef sum_elements(ndarray[float64_t] arr):\\n    cdef Py_ssize_t i, n = len(arr)\\n    cdef float64_t result = 0\\n    for i in range(n):\\n        result += arr[i]\\n    return result\\nCython takes this code, translates it to C, then compiles the generated C code to create\\na Python extension. Cython is an attractive option for performance computing because\\nthe code is only slightly more time-consuming to write than pure Python code and it\\nintegrates closely with NumPy. A common workflow is to get an algorithm working in\\nPython, then translate it to Cython by adding type declarations and a handful of other\\ntweaks. For more, see the project documentation.\\n382 | Chapter 12: \\u2002Advanced NumPy\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 398, 'page_label': '383'}, page_content='Some other options for writing high performance code with NumPy include f2py, a\\nwrapper generator for Fortran 77 and 90 code, and writing pure C extensions.\\nPerformance Tips | 383\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 399, 'page_label': '384'}, page_content='www.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 400, 'page_label': '385'}, page_content='APPENDIX\\nPython Language Essentials\\nKnowledge is a treasure, but practice is the key to it.\\n—Thomas Fuller\\nPeople often ask me about good resources for learning Python for data-centric appli-\\ncations. While there are many excellent Python language books, I am usually hesitant\\nto recommend some of them as they are intended for a general audience rather than\\ntailored for someone who wants to load in some data sets, do some computations, and\\nplot some of the results. There are actually a couple of books on “scientific program-\\nming in Python”, but they are geared toward numerical computing and engineering\\napplications: solving differential equations, computing integrals, doing Monte Carlo\\nsimulations, and various topics that are more mathematically-oriented rather than be-\\ning about data analysis and statistics. As this is a book about becoming proficient at\\nworking with data in Python, I think it is valuable to spend some time highlighting the\\nmost important features of Python’s built-in data structures and libraries from the per-\\nspective of processing and manipulating structured and unstructured data. As such, I\\nwill only present roughly enough information to enable you to follow along with the\\nrest of the book.\\nThis chapter is not intended to be an exhaustive introduction to the Python language\\nbut rather a biased, no-frills overview of features which are used repeatedly throughout\\nthis book. For new Python programmers, I recommend that you supplement this chap-\\nter with the official Python tutorial ( http://docs.python.org) and potentially one of the\\nmany excellent (and much longer) books on general purpose Python programming. In\\nmy opinion, it is not necessary to become proficient at building good software in Python\\nto be able to productively do data analysis. I encourage you to use IPython to experi-\\nment with the code examples and to explore the documentation for the various types,\\nfunctions, and methods. Note that some of the code used in the examples may not\\nnecessarily be fully-introduced at this point.\\nMuch of this book focuses on high performance array-based computing tools for work-\\ning with large data sets. In order to use those tools you must often first do some munging\\nto corral messy data into a more nicely structured form. Fortunately, Python is one of\\n385\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 401, 'page_label': '386'}, page_content='the easiest-to-use languages for rapidly whipping your data into shape. The greater your\\nfacility with Python, the language, the easier it will be for you to prepare new data sets\\nfor analysis.\\nThe Python Interpreter\\nPython is an interpreted language. The Python interpreter runs a program by executing\\none statement at a time. The standard interactive Python interpreter can be invoked on\\nthe command line with the python command:\\n$ python\\nPython 2.7.2 (default, Oct  4 2011, 20:06:09)\\n[GCC 4.6.1] on linux2\\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\\n>>> a = 5\\n>>> print a\\n5\\nThe >>> you see is the prompt where you’ll type expressions. To exit the Python inter-\\npreter and return to the command prompt, you can either type exit() or press Ctrl-D.\\nRunning Python programs is as simple as calling python with a .py file as its first argu-\\nment. Suppose we had created hello_world.py with these contents:\\nprint \\'Hello world\\'\\nThis can be run from the terminal simply as:\\n$ python hello_world.py\\nHello world\\nWhile many Python programmers execute all of their Python code in this way, many\\nscientific Python programmers make use of IPython, an enhanced interactive Python\\ninterpreter. Chapter 3 is dedicated to the IPython system. By using the %run command,\\nIPython executes the code in the specified file in the same process, enabling you to\\nexplore the results interactively when it’s done.\\n$ ipython\\nPython 2.7.2 |EPD 7.1-2 (64-bit)| (default, Jul  3 2011, 15:17:51)\\nType \"copyright\", \"credits\" or \"license\" for more information.\\nIPython 0.12 -- An enhanced Interactive Python.\\n?         -> Introduction and overview of IPython\\'s features.\\n%quickref -> Quick reference.\\nhelp      -> Python\\'s own help system.\\nobject?   -> Details about \\'object\\', use \\'object??\\' for extra details.\\nIn [1]: %run hello_world.py\\nHello world\\nIn [2]:\\n386 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 402, 'page_label': '387'}, page_content='The default IPython prompt adopts the numbered In [2]: style compared with the\\nstandard >>> prompt.\\nThe Basics\\nLanguage Semantics\\nThe Python language design is distinguished by its emphasis on readability, simplicity,\\nand explicitness. Some people go so far as to liken it to “executable pseudocode”.\\nIndentation, not braces\\nPython uses whitespace (tabs or spaces) to structure code instead of using braces as in\\nmany other languages like R, C++, Java, and Perl. Take the for loop in the above\\nquicksort algorithm:\\nfor x in array:\\n    if x < pivot:\\n        less.append(x)\\n    else:\\n        greater.append(x)\\nA colon denotes the start of an indented code block after which all of the code must be\\nindented by the same amount until the end of the block. In another language, you might\\ninstead have something like:\\nfor x in array {\\n        if x < pivot {\\n            less.append(x)\\n        } else {\\n            greater.append(x)\\n        }\\n    }\\nOne major reason that whitespace matters is that it results in most Python code looking\\ncosmetically similar, which means less cognitive dissonance when you read a piece of\\ncode that you didn’t write yourself (or wrote in a hurry a year ago!). In a language\\nwithout significant whitespace, you might stumble on some differently formatted code\\nlike:\\nfor x in array\\n    {\\n      if x < pivot\\n      {\\n        less.append(x)\\n      }\\n      else\\n      {\\n        greater.append(x)\\nThe Basics | 387\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 403, 'page_label': '388'}, page_content=\"}\\n    }\\nLove it or hate it, significant whitespace is a fact of life for Python programmers, and\\nin my experience it helps make Python code a lot more readable than other languages\\nI’ve used. While it may seem foreign at first, I suspect that it will grow on you after a\\nwhile.\\nI strongly recommend that you use 4 spaces to as your default indenta-\\ntion and that your editor replace tabs with 4 spaces. Many text editors\\nhave a setting that will replace tab stops with spaces automatically (do\\nthis!). Some people use tabs or a different number of spaces, with 2\\nspaces not being terribly uncommon. 4 spaces is by and large the stan-\\ndard adopted by the vast majority of Python programmers, so I recom-\\nmend doing that in the absence of a compelling reason otherwise.\\nAs you can see by now, Python statements also do not need to be terminated by sem-\\nicolons. Semicolons can be used, however, to separate multiple statements on a single\\nline:\\na = 5; b = 6; c = 7\\nPutting multiple statements on one line is generally discouraged in Python as it often\\nmakes code less readable.\\nEverything is an object\\nAn important characteristic of the Python language is the consistency of its object\\nmodel. Every number, string, data structure, function, class, module, and so on exists\\nin the Python interpreter in its own “box” which is referred to as a Python object. Each\\nobject has an associated type (for example, string or function) and internal data. In\\npractice this makes the language very flexible, as even functions can be treated just like\\nany other object.\\nComments\\nAny text preceded by the hash mark (pound sign) # is ignored by the Python interpreter.\\nThis is often used to add comments to code. At times you may also want to exclude\\ncertain blocks of code without deleting them. An easy solution is to comment out the\\ncode:\\nresults = []\\nfor line in file_handle:\\n    # keep the empty lines for now\\n    # if len(line) == 0:\\n    #   continue\\n    results.append(line.replace('foo', 'bar'))\\n388 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 404, 'page_label': '389'}, page_content=\"Function and object method calls\\nFunctions are called using parentheses and passing zero or more arguments, optionally\\nassigning the returned value to a variable:\\nresult = f(x, y, z)\\ng()\\nAlmost every object in Python has attached functions, known as methods, that have\\naccess to the object’s internal contents. They can be called using the syntax:\\nobj.some_method(x, y, z)\\nFunctions can take both positional and keyword arguments:\\nresult = f(a, b, c, d=5, e='foo')\\nMore on this later.\\nVariables and pass-by-reference\\nWhen assigning a variable (or name) in Python, you are creating a reference to the object\\non the right hand side of the equals sign. In practical terms, consider a list of integers:\\nIn [241]: a = [1, 2, 3]\\nSuppose we assign a to a new variable b:\\nIn [242]: b = a\\nIn some languages, this assignment would cause the data [1, 2, 3] to be copied. In\\nPython, a and b actually now refer to the same object, the original list [1, 2, 3] (see\\nFigure A-1 for a mockup). You can prove this to yourself by appending an element to\\na and then examining b:\\nIn [243]: a.append(4)\\nIn [244]: b\\nOut[244]: [1, 2, 3, 4]\\nFigure A-1. Two references for the same object\\nUnderstanding the semantics of references in Python and when, how, and why data is\\ncopied is especially critical when working with larger data sets in Python.\\nThe Basics | 389\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 405, 'page_label': '390'}, page_content=\"Assignment is also referred to as binding, as we are binding a name to\\nan object. Variables names that have been assigned may occasionally be\\nreferred to as bound variables.\\nWhen you pass objects as arguments to a function, you are only passing references; no\\ncopying occurs. Thus, Python is said to pass by reference, whereas some other languages\\nsupport both pass by value (creating copies) and pass by reference. This means that a\\nfunction can mutate the internals of its arguments. Suppose we had the following func-\\ntion:\\ndef append_element(some_list, element):\\n    some_list.append(element)\\nThen given what’s been said, this should not come as a surprise:\\nIn [2]: data = [1, 2, 3]\\nIn [3]: append_element(data, 4)\\nIn [4]: data\\nOut[4]: [1, 2, 3, 4]\\nDynamic references, strong types\\nIn contrast with many compiled languages, such as Java and C++, object references in\\nPython have no type associated with them. There is no problem with the following:\\nIn [245]: a = 5        In [246]: type(a)\\n                       Out[246]: int    \\n                                        \\nIn [247]: a = 'foo'    In [248]: type(a)\\n                       Out[248]: str\\nVariables are names for objects within a particular namespace; the type information is\\nstored in the object itself. Some observers might hastily conclude that Python is not a\\n“typed language”. This is not true; consider this example:\\nIn [249]: '5' + 5\\n---------------------------------------------------------------------------\\nTypeError                                 Traceback (most recent call last)\\n<ipython-input-249-f9dbf5f0b234> in <module>()\\n----> 1 '5' + 5\\nTypeError: cannot concatenate 'str' and 'int' objects\\nIn some languages, such as Visual Basic, the string '5' might get implicitly converted\\n(or casted) to an integer, thus yielding 10. Yet in other languages, such as JavaScript,\\nthe integer 5 might be casted to a string, yielding the concatenated string '55'. In this\\nregard Python is considered a strongly-typed language, which means that every object\\nhas a specific type (or class), and implicit conversions will occur only in certain obvious\\ncircumstances, such as the following:\\n390 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 406, 'page_label': '391'}, page_content=\"In [250]: a = 4.5\\nIn [251]: b = 2\\n# String formatting, to be visited later\\nIn [252]: print 'a is %s, b is %s' % (type(a), type(b))\\na is <type 'float'>, b is <type 'int'>\\nIn [253]: a / b\\nOut[253]: 2.25\\nKnowing the type of an object is important, and it’s useful to be able to write functions\\nthat can handle many different kinds of input. You can check that an object is an\\ninstance of a particular type using the isinstance function:\\nIn [254]: a = 5        In [255]: isinstance(a, int)\\n                       Out[255]: True\\nisinstance can accept a tuple of types if you want to check that an object’s type is\\namong those present in the tuple:\\nIn [256]: a = 5; b = 4.5\\nIn [257]: isinstance(a, (int, float))      In [258]: isinstance(b, (int, float))\\nOut[257]: True                             Out[258]: True\\nAttributes and methods\\nObjects in Python typically have both attributes, other Python objects stored “inside”\\nthe object, and methods, functions associated with an object which can have access to\\nthe object’s internal data. Both of them are accessed via the syntax obj.attribute_name:\\nIn [1]: a = 'foo'\\nIn [2]: a.<Tab>\\na.capitalize  a.format      a.isupper     a.rindex      a.strip\\na.center      a.index       a.join        a.rjust       a.swapcase\\na.count       a.isalnum     a.ljust       a.rpartition  a.title\\na.decode      a.isalpha     a.lower       a.rsplit      a.translate\\na.encode      a.isdigit     a.lstrip      a.rstrip      a.upper\\na.endswith    a.islower     a.partition   a.split       a.zfill\\na.expandtabs  a.isspace     a.replace     a.splitlines\\na.find        a.istitle     a.rfind       a.startswith\\nAttributes and methods can also be accessed by name using the getattr function:\\n>>> getattr(a, 'split')\\n<function split>\\nWhile we will not extensively use the functions getattr and related functions hasattr\\nand setattr in this book, they can be used very effectively to write generic, reusable\\ncode.\\nThe Basics | 391\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 407, 'page_label': '392'}, page_content=\"“Duck” typing\\nOften you may not care about the type of an object but rather only whether it has certain\\nmethods or behavior. For example, you can verify that an object is iterable if it imple-\\nmented the iterator protocol. For many objects, this means it has a __iter__ “magic\\nmethod”, though an alternative and better way to check is to try using the iter function:\\ndef isiterable(obj):\\n    try:\\n        iter(obj)\\n        return True\\n    except TypeError: # not iterable\\n        return False\\nThis function would return True for strings as well as most Python collection types:\\nIn [260]: isiterable('a string')        In [261]: isiterable([1, 2, 3])\\nOut[260]: True                          Out[261]: True                 \\n                                                                       \\nIn [262]: isiterable(5)\\nOut[262]: False\\nA place where I use this functionality all the time is to write functions that can accept\\nmultiple kinds of input. A common case is writing a function that can accept any kind\\nof sequence (list, tuple, ndarray) or even an iterator. You can first check if the object is\\na list (or a NumPy array) and, if it is not, convert it to be one:\\nif not isinstance(x, list) and isiterable(x):\\n    x = list(x)\\nImports\\nIn Python a module is simply a .py file containing function and variable definitions\\nalong with such things imported from other .py files. Suppose that we had the following\\nmodule:\\n# some_module.py\\nPI = 3.14159\\ndef f(x):\\n    return x + 2\\ndef g(a, b):\\n    return a + b\\nIf we wanted to access the variables and functions defined in some_module.py, from\\nanother file in the same directory we could do:\\nimport some_module\\nresult = some_module.f(5)\\npi = some_module.PI\\nOr equivalently:\\nfrom some_module import f, g, PI\\nresult = g(5, PI)\\n392 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 408, 'page_label': '393'}, page_content='By using the as keyword you can give imports different variable names:\\nimport some_module as sm\\nfrom some_module import PI as pi, g as gf\\nr1 = sm.f(pi)\\nr2 = gf(6, pi)\\nBinary operators and comparisons\\nMost of the binary math operations and comparisons are as you might expect:\\nIn [263]: 5 - 7        In [264]: 12 + 21.5\\nOut[263]: -2           Out[264]: 33.5     \\n                                          \\nIn [265]: 5 <= 2\\nOut[265]: False\\nSee Table A-1 for all of the available binary operators.\\nTo check if two references refer to the same object, use the is keyword. is not is also\\nperfectly valid if you want to check that two objects are not the same:\\nIn [266]: a = [1, 2, 3]\\nIn [267]: b = a\\n# Note, the list function always creates a new list\\nIn [268]: c = list(a)\\nIn [269]: a is b        In [270]: a is not c\\nOut[269]: True          Out[270]: True\\nNote this is not the same thing is comparing with ==, because in this case we have:\\nIn [271]: a == c\\nOut[271]: True\\nA very common use of is and is not is to check if a variable is None, since there is only\\none instance of None:\\nIn [272]: a = None\\nIn [273]: a is None\\nOut[273]: True\\nTable A-1. Binary operators\\nOperation Description\\na + b Add a and b\\na - b Subtract b from a\\na * b Multiply a by b\\na / b Divide a by b\\na // b Floor-divide a by b, dropping any fractional remainder\\nThe Basics | 393\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 409, 'page_label': '394'}, page_content=\"Operation Description\\na ** b Raise a to the b power\\na & b True if both a and b are True. For integers, take the bitwise AND.\\na | b True if either a or b is True. For integers, take the bitwise OR.\\na ^ b For booleans, True if a or b is True, but not both. For integers, take the bitwise EXCLUSIVE-OR.\\na == b True if a equals b\\na != b True if a is not equal to b\\na <= b, a < b True if a is less than (less than or equal) to b\\na > b, a >= b True if a is greater than (greater than or equal) to b\\na is b True if a and b reference same Python object\\na is not b True if a and b reference different Python objects\\nStrictness versus laziness\\nWhen using any programming language, it’s important to understand when expressions\\nare evaluated. Consider the simple expression:\\na = b = c = 5\\nd = a + b * c\\nIn Python, once these statements are evaluated, the calculation is immediately (or\\nstrictly) carried out, setting the value of d to 30. In another programming paradigm,\\nsuch as in a pure functional programming language like Haskell, the value of d might\\nnot be evaluated until it is actually used elsewhere. The idea of deferring computations\\nin this way is commonly known as lazy evaluation. Python, on the other hand, is a very \\nstrict (or eager) language. Nearly all of the time, computations and expressions are\\nevaluated immediately. Even in the above simple expression, the result of b * c is\\ncomputed as a separate step before adding it to a.\\nThere are Python techniques, especially using iterators and generators, which can be\\nused to achieve laziness. When performing very expensive computations which are only\\nnecessary some of the time, this can be an important technique in data-intensive ap-\\nplications.\\nMutable and immutable objects\\nMost objects in Python are mutable, such as lists, dicts, NumPy arrays, or most user-\\ndefined types (classes). This means that the object or values that they contain can be\\nmodified.\\nIn [274]: a_list = ['foo', 2, [4, 5]]\\nIn [275]: a_list[2] = (3, 4)\\nIn [276]: a_list\\nOut[276]: ['foo', 2, (3, 4)]\\n394 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 410, 'page_label': '395'}, page_content=\"Others, like strings and tuples, are immutable:\\nIn [277]: a_tuple = (3, 5, (4, 5))\\nIn [278]: a_tuple[1] = 'four'\\n---------------------------------------------------------------------------\\nTypeError                                 Traceback (most recent call last)\\n<ipython-input-278-b7966a9ae0f1> in <module>()\\n----> 1 a_tuple[1] = 'four'\\nTypeError: 'tuple' object does not support item assignment\\nRemember that just because you can mutate an object does not mean that you always\\nshould. Such actions are known in programming as side effects. For example, when\\nwriting a function, any side effects should be explicitly communicated to the user in\\nthe function’s documentation or comments. If possible, I recommend trying to avoid\\nside effects and favor immutability, even though there may be mutable objects involved.\\nScalar Types\\nPython has a small set of built-in types for handling numerical data, strings, boolean\\n(True or False) values, and dates and time. See Table A-2 for a list of the main scalar\\ntypes. Date and time handling will be discussed separately as these are provided by the \\ndatetime module in the standard library.\\nTable A-2. Standard Python Scalar Types\\nType Description\\nNone The Python “null” value (only one instance of the None object exists)\\nstr String type. ASCII-valued only in Python 2.x and Unicode in Python 3\\nunicode Unicode string type\\nfloat Double-precision (64-bit) floating point number. Note there is no separate double type.\\nbool A True or False value\\nint Signed integer with maximum value determined by the platform.\\nlong Arbitrary precision signed integer. Large int values are automatically converted to long.\\nNumeric types\\nThe primary Python types for numbers are int and float. The size of the integer which\\ncan be stored as an int is dependent on your platform (whether 32 or 64-bit), but Python\\nwill transparently convert a very large integer to long, which can store arbitrarily large\\nintegers.\\nIn [279]: ival = 17239871\\nIn [280]: ival ** 6\\nOut[280]: 26254519291092456596965462913230729701102721L\\nThe Basics | 395\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 411, 'page_label': '396'}, page_content='Floating point numbers are represented with the Python float type. Under the hood\\neach one is a double-precision (64 bits) value. They can also be expressed using scien-\\ntific notation:\\nIn [281]: fval = 7.243\\nIn [282]: fval2 = 6.78e-5\\nIn Python 3, integer division not resulting in a whole number will always yield a floating\\npoint number:\\nIn [284]: 3 / 2\\nOut[284]: 1.5\\nIn Python 2.7 and below (which some readers will likely be using), you can enable this\\nbehavior by default by putting the following cryptic-looking statement at the top of\\nyour module:\\nfrom __future__ import division\\nWithout this in place, you can always explicitly convert the denominator into a floating\\npoint number:\\nIn [285]: 3 / float(2)\\nOut[285]: 1.5\\nTo get C-style integer division (which drops the fractional part if the result is not a\\nwhole number), use the floor division operator //:\\nIn [286]: 3 // 2\\nOut[286]: 1\\nComplex numbers are written using j for the imaginary part:\\nIn [287]: cval = 1 + 2j\\nIn [288]: cval * (1 - 2j)\\nOut[288]: (5+0j)\\nStrings\\nMany people use Python for its powerful and flexible built-in string processing capa-\\nbilities. You can write string literal using either single quotes \\' or double quotes \":\\na = \\'one way of writing a string\\'\\nb = \"another way\"\\nFor multiline strings with line breaks, you can use triple quotes, either \\'\\'\\' or \"\"\":\\nc = \"\"\"\\nThis is a longer string that\\nspans multiple lines\\n\"\"\"\\nPython strings are immutable; you cannot modify a string without creating a new string:\\n396 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 412, 'page_label': '397'}, page_content=\"In [289]: a = 'this is a string'\\nIn [290]: a[10] = 'f'\\n---------------------------------------------------------------------------\\nTypeError                                 Traceback (most recent call last)\\n<ipython-input-290-5ca625d1e504> in <module>()\\n----> 1 a[10] = 'f'\\nTypeError: 'str' object does not support item assignment\\nIn [291]: b = a.replace('string', 'longer string')\\nIn [292]: b\\nOut[292]: 'this is a longer string'\\nMany Python objects can be converted to a string using the str function:\\nIn [293]: a = 5.6        In [294]: s = str(a)\\n                                             \\nIn [295]: s\\nOut[295]: '5.6'\\nStrings are a sequence of characters and therefore can be treated like other sequences,\\nsuch as lists and tuples:\\nIn [296]: s = 'python'        In [297]: list(s)                       \\n                              Out[297]: ['p', 'y', 't', 'h', 'o', 'n']\\n                                                                      \\nIn [298]: s[:3]\\nOut[298]: 'pyt'\\nThe backslash character \\\\ is an escape character, meaning that it is used to specify\\nspecial characters like newline \\\\n or unicode characters. To write a string literal with\\nbackslashes, you need to escape them:\\nIn [299]: s = '12\\\\\\\\34'\\nIn [300]: print s\\n12\\\\34\\nIf you have a string with a lot of backslashes and no special characters, you might find\\nthis a bit annoying. Fortunately you can preface the leading quote of the string with r\\nwhich means that the characters should be interpreted as is:\\nIn [301]: s = r'this\\\\has\\\\no\\\\special\\\\characters'\\nIn [302]: s\\nOut[302]: 'this\\\\\\\\has\\\\\\\\no\\\\\\\\special\\\\\\\\characters'\\nAdding two strings together concatenates them and produces a new string:\\nIn [303]: a = 'this is the first half '\\nIn [304]: b = 'and this is the second half'\\nIn [305]: a + b\\nOut[305]: 'this is the first half and this is the second half'\\nThe Basics | 397\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 413, 'page_label': '398'}, page_content=\"String templating or formatting is another important topic. The number of ways to do\\nso has expanded with the advent of Python 3, here I will briefly describe the mechanics\\nof one of the main interfaces. Strings with a % followed by one or more format characters\\nis a target for inserting a value into that string (this is quite similar to the printf function\\nin C). As an example, consider this string:\\nIn [306]: template = '%.2f %s are worth $%d'\\nIn this string, %s means to format an argument as a string, %.2f a number with 2 decimal\\nplaces, and %d an integer. To substitute arguments for these format parameters, use the\\nbinary operator % with a tuple of values:\\nIn [307]: template % (4.5560, 'Argentine Pesos', 1)\\nOut[307]: '4.56 Argentine Pesos are worth $1'\\nString formatting is a broad topic; there are multiple methods and numerous options\\nand tweaks available to control how values are formatted in the resulting string. To\\nlearn more, I recommend you seek out more information on the web.\\nI discuss general string processing as it relates to data analysis in more detail in Chap-\\nter 7.\\nBooleans\\nThe two boolean values in Python are written as True and False. Comparisons and\\nother conditional expressions evaluate to either True or False. Boolean values are com-\\nbined with the and and or keywords:\\nIn [308]: True and True\\nOut[308]: True\\nIn [309]: False or True\\nOut[309]: True\\nAlmost all built-in Python tops and any class defining the __nonzero__ magic method\\nhave a True or False interpretation in an if statement:\\nIn [310]: a = [1, 2, 3]\\n   .....: if a:\\n   .....:     print 'I found something!'\\n   .....:\\nI found something!\\nIn [311]: b = []\\n   .....: if not b:\\n   .....:     print 'Empty!'\\n   .....:\\nEmpty!\\nMost objects in Python have a notion of true- or falseness. For example, empty se-\\nquences (lists, dicts, tuples, etc.) are treated as False if used in control flow (as above\\nwith the empty list b). You can see exactly what boolean value an object coerces to by\\ninvoking bool on it:\\n398 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 414, 'page_label': '399'}, page_content=\"In [312]: bool([]), bool([1, 2, 3])\\nOut[312]: (False, True)\\nIn [313]: bool('Hello world!'), bool('')\\nOut[313]: (True, False)\\nIn [314]: bool(0), bool(1)\\nOut[314]: (False, True)\\nType casting\\nThe str, bool, int and float types are also functions which can be used to cast values\\nto those types:\\nIn [315]: s = '3.14159'\\nIn [316]: fval = float(s)        In [317]: type(fval)\\n                                 Out[317]: float     \\n                                                     \\nIn [318]: int(fval)        In [319]: bool(fval)        In [320]: bool(0)\\nOut[318]: 3                Out[319]: True              Out[320]: False\\nNone\\nNone is the Python null value type. If a function does not explicitly return a value, it\\nimplicitly returns None.\\nIn [321]: a = None      In [322]: a is None\\n                        Out[322]: True     \\n                                           \\nIn [323]: b = 5         In [324]: b is not None\\n                        Out[324]: True\\nNone is also a common default value for optional function arguments:\\ndef add_and_maybe_multiply(a, b, c=None):\\n    result = a + b\\n    if c is not None:\\n        result = result * c\\n    return result\\nWhile a technical point, it’s worth bearing in mind that None is not a reserved keyword\\nbut rather a unique instance of NoneType.\\nDates and times\\nThe built-in Python datetime module provides datetime, date, and time types. The\\ndatetime type as you may imagine combines the information stored in date and time\\nand is the most commonly used:\\nIn [325]: from datetime import datetime, date, time\\nIn [326]: dt = datetime(2011, 10, 29, 20, 30, 21)\\nThe Basics | 399\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 415, 'page_label': '400'}, page_content=\"In [327]: dt.day    In [328]: dt.minute\\nOut[327]: 29        Out[328]: 30\\nGiven a datetime instance, you can extract the equivalent date and time objects by\\ncalling methods on the datetime of the same name:\\nIn [329]: dt.date()                      In [330]: dt.time()                \\nOut[329]: datetime.date(2011, 10, 29)    Out[330]: datetime.time(20, 30, 21)\\nThe strftime method formats a datetime as a string:\\nIn [331]: dt.strftime('%m/%d/%Y %H:%M')\\nOut[331]: '10/29/2011 20:30'\\nStrings can be converted (parsed) into datetime objects using the strptime function:\\nIn [332]: datetime.strptime('20091031', '%Y%m%d')\\nOut[332]: datetime.datetime(2009, 10, 31, 0, 0)\\nSee Table 10-2 for a full list of format specifications.\\nWhen aggregating of otherwise grouping time series data, it will occasionally be useful\\nto replace fields of a series of datetimes, for example replacing the minute and second\\nfields with zero, producing a new object:\\nIn [333]: dt.replace(minute=0, second=0)\\nOut[333]: datetime.datetime(2011, 10, 29, 20, 0)\\nThe difference of two datetime objects produces a datetime.timedelta type:\\nIn [334]: dt2 = datetime(2011, 11, 15, 22, 30)\\nIn [335]: delta = dt2 - dt\\nIn [336]: delta                             In [337]: type(delta)       \\nOut[336]: datetime.timedelta(17, 7179)      Out[337]: datetime.timedelta\\nAdding a timedelta to a datetime produces a new shifted datetime:\\nIn [338]: dt\\nOut[338]: datetime.datetime(2011, 10, 29, 20, 30, 21)\\nIn [339]: dt + delta\\nOut[339]: datetime.datetime(2011, 11, 15, 22, 30)\\nControl Flow\\nif, elif, and else\\nThe if statement is one of the most well-known control flow statement types. It checks\\na condition which, if True, evaluates the code in the block that follows:\\nif x < 0:\\n    print 'It's negative'\\n400 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 416, 'page_label': '401'}, page_content=\"An if statement can be optionally followed by one or more elif blocks and a catch-all\\nelse block if all of the conditions are False:\\nif x < 0:\\n    print 'It's negative'\\nelif x == 0:\\n    print 'Equal to zero'\\nelif 0 < x < 5:\\n    print 'Positive but smaller than 5'\\nelse:\\n    print 'Positive and larger than or equal to 5'\\nIf any of the conditions is True, no further elif or else blocks will be reached. With a\\ncompound condition using and or or, conditions are evaluated left-to-right and will\\nshort circuit:\\nIn [340]: a = 5; b = 7\\nIn [341]: c = 8; d = 4\\nIn [342]: if a < b or c > d:\\n   .....:     print 'Made it'\\nMade it\\nIn this example, the comparison c > d never gets evaluated because the first comparison\\nwas True.\\nfor loops\\nfor loops are for iterating over a collection (like a list or tuple) or an iterater. The\\nstandard syntax for a for loop is:\\nfor value in collection:\\n    # do something with value\\nA for loop can be advanced to the next iteration, skipping the remainder of the block,\\nusing the continue keyword. Consider this code which sums up integers in a list and\\nskips None values:\\nsequence = [1, 2, None, 4, None, 5]\\ntotal = 0\\nfor value in sequence:\\n    if value is None:\\n        continue\\n    total += value\\nA for loop can be exited altogether using the break keyword. This code sums elements\\nof the list until a 5 is reached:\\nsequence = [1, 2, 0, 4, 6, 5, 2, 1]\\ntotal_until_5 = 0\\nfor value in sequence:\\n    if value == 5:\\n        break\\n    total_until_5 += value\\nThe Basics | 401\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 417, 'page_label': '402'}, page_content=\"As we will see in more detail, if the elements in the collection or iterator are sequences\\n(tuples or lists, say), they can be conveniently unpacked into variables in the for loop\\nstatement:\\nfor a, b, c in iterator:\\n    # do something\\nwhile loops\\nA while loop specifies a condition and a block of code that is to be executed until the\\ncondition evaluates to False or the loop is explicitly ended with break:\\nx = 256\\ntotal = 0\\nwhile x > 0:\\n    if total > 500:\\n        break\\n    total += x\\n    x = x // 2\\npass\\npass is the “no-op” statement in Python. It can be used in blocks where no action is to\\nbe taken; it is only required because Python uses whitespace to delimit blocks:\\nif x < 0:\\n    print 'negative!'\\nelif x == 0:\\n    # TODO: put something smart here\\n    pass\\nelse:\\n    print 'positive!'\\nIt’s common to use pass as a place-holder in code while working on a new piece of\\nfunctionality:\\ndef f(x, y, z):\\n    # TODO: implement this function!\\n    pass\\nException handling\\nHandling Python errors or exceptions gracefully is an important part of building robust\\nprograms. In data analysis applications, many functions only work on certain kinds of\\ninput. As an example, Python’s float function is capable of casting a string to a floating\\npoint number, but fails with ValueError on improper inputs:\\nIn [343]: float('1.2345')\\nOut[343]: 1.2345\\nIn [344]: float('something')\\n---------------------------------------------------------------------------\\nValueError                                Traceback (most recent call last)\\n<ipython-input-344-439904410854> in <module>()\\n402 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 418, 'page_label': '403'}, page_content=\"----> 1 float('something')\\nValueError: could not convert string to float: something\\nSuppose we wanted a version of float that fails gracefully, returning the input argu-\\nment. We can do this by writing a function that encloses the call to float in a try/\\nexcept block:\\ndef attempt_float(x):\\n    try:\\n        return float(x)\\n    except:\\n        return x\\nThe code in the except part of the block will only be executed if float(x) raises an\\nexception:\\nIn [346]: attempt_float('1.2345')\\nOut[346]: 1.2345\\nIn [347]: attempt_float('something')\\nOut[347]: 'something'\\nYou might notice that float can raise exceptions other than ValueError:\\nIn [348]: float((1, 2))\\n---------------------------------------------------------------------------\\nTypeError                                 Traceback (most recent call last)\\n<ipython-input-348-842079ebb635> in <module>()\\n----> 1 float((1, 2))\\nTypeError: float() argument must be a string or a number\\nYou might want to only suppress ValueError, since a TypeError (the input was not a\\nstring or numeric value) might indicate a legitimate bug in your program. To do that,\\nwrite the exception type after except:\\ndef attempt_float(x):\\n    try:\\n        return float(x)\\n    except ValueError:\\n        return x\\nWe have then:\\nIn [350]: attempt_float((1, 2))\\n---------------------------------------------------------------------------\\nTypeError                                 Traceback (most recent call last)\\n<ipython-input-350-9bdfd730cead> in <module>()\\n----> 1 attempt_float((1, 2))\\n<ipython-input-349-3e06b8379b6b> in attempt_float(x)\\n      1 def attempt_float(x):\\n      2     try:\\n----> 3         return float(x)\\n      4     except ValueError:\\n      5         return x\\nTypeError: float() argument must be a string or a number\\nThe Basics | 403\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 419, 'page_label': '404'}, page_content=\"You can catch multiple exception types by writing a tuple of exception types instead\\n(the parentheses are required):\\ndef attempt_float(x):\\n    try:\\n        return float(x)\\n    except (TypeError, ValueError):\\n        return x\\nIn some cases, you may not want to suppress an exception, but you want some code\\nto be executed regardless of whether the code in the try block succeeds or not. To do\\nthis, use finally:\\nf = open(path, 'w')\\ntry:\\n    write_to_file(f)\\nfinally:\\n    f.close()\\nHere, the file handle f will always get closed. Similarly, you can have code that executes\\nonly if the try: block succeeds using else:\\nf = open(path, 'w')\\ntry:\\n    write_to_file(f)\\nexcept:\\n    print 'Failed'\\nelse:\\n    print 'Succeeded'\\nfinally:\\n    f.close()\\nrange and xrange\\nThe range function produces a list of evenly-spaced integers:\\nIn [352]: range(10)\\nOut[352]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\\nBoth a start, end, and step can be given:\\nIn [353]: range(0, 20, 2)\\nOut[353]: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\\nAs you can see, range produces integers up to but not including the endpoint. A com-\\nmon use of range is for iterating through sequences by index:\\nseq = [1, 2, 3, 4]\\nfor i in range(len(seq)):\\n    val = seq[i]\\nFor very long ranges, it’s recommended to use xrange, which takes the same arguments\\nas range but returns an iterator that generates integers one by one rather than generating\\n404 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 420, 'page_label': '405'}, page_content=\"all of them up-front and storing them in a (potentially very large) list. This snippet sums\\nall numbers from 0 to 9999 that are multiples of 3 or 5:\\nsum = 0\\nfor i in xrange(10000):\\n    # % is the modulo operator\\n    if x % 3 == 0 or x % 5 == 0:\\n        sum += i\\nIn Python 3, range always returns an iterator, and thus it is not necessary\\nto use the xrange function\\nTernary Expressions\\nA ternary expression in Python allows you combine an if-else block which produces\\na value into a single line or expression. The syntax for this in Python is\\nvalue = true-expr if condition else\\nfalse-expr\\nHere, true-expr and false-expr can be any Python expressions. It has the identical\\neffect as the more verbose\\nif condition:\\n    value = true-expr\\nelse:\\n    value = false-expr\\nThis is a more concrete example:\\nIn [354]: x = 5\\nIn [355]: 'Non-negative' if x >= 0 else 'Negative'\\nOut[355]: 'Non-negative'\\nAs with if-else blocks, only one of the expressions will be evaluated. While it may be\\ntempting to always use ternary expressions to condense your code, realize that you may\\nsacrifice readability if the condition as well and the true and false expressions are very\\ncomplex.\\nData Structures and Sequences\\nPython’s data structures are simple, but powerful. Mastering their use is a critical part\\nof becoming a proficient Python programmer.\\nData Structures and Sequences | 405\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 421, 'page_label': '406'}, page_content=\"Tuple\\nA tuple is a one-dimensional, fixed-length, immutable sequence of Python objects. The\\neasiest way to create one is with a comma-separated sequence of values:\\nIn [356]: tup = 4, 5, 6\\nIn [357]: tup\\nOut[357]: (4, 5, 6)\\nWhen defining tuples in more complicated expressions, it’s often necessary to enclose\\nthe values in parentheses, as in this example of creating a tuple of tuples:\\nIn [358]: nested_tup = (4, 5, 6), (7, 8)\\nIn [359]: nested_tup\\nOut[359]: ((4, 5, 6), (7, 8))\\nAny sequence or iterator can be converted to a tuple by invoking tuple:\\nIn [360]: tuple([4, 0, 2])\\nOut[360]: (4, 0, 2)\\nIn [361]: tup = tuple('string')\\nIn [362]: tup\\nOut[362]: ('s', 't', 'r', 'i', 'n', 'g')\\nElements can be accessed with square brackets [] as with most other sequence types.\\nLike C, C++, Java, and many other languages, sequences are 0-indexed in Python:\\nIn [363]: tup[0]\\nOut[363]: 's'\\nWhile the objects stored in a tuple may be mutable themselves, once created it’s not\\npossible to modify which object is stored in each slot:\\nIn [364]: tup = tuple(['foo', [1, 2], True])\\nIn [365]: tup[2] = False\\n---------------------------------------------------------------------------\\nTypeError                                 Traceback (most recent call last)\\n<ipython-input-365-c7308343b841> in <module>()\\n----> 1 tup[2] = False\\nTypeError: 'tuple' object does not support item assignment\\n# however\\nIn [366]: tup[1].append(3)\\nIn [367]: tup\\nOut[367]: ('foo', [1, 2, 3], True)\\nTuples can be concatenated using the + operator to produce longer tuples:\\nIn [368]: (4, None, 'foo') + (6, 0) + ('bar',)\\nOut[368]: (4, None, 'foo', 6, 0, 'bar')\\n406 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 422, 'page_label': '407'}, page_content=\"Multiplying a tuple by an integer, as with lists, has the effect of concatenating together\\nthat many copies of the tuple.\\nIn [369]: ('foo', 'bar') * 4\\nOut[369]: ('foo', 'bar', 'foo', 'bar', 'foo', 'bar', 'foo', 'bar')\\nNote that the objects themselves are not copied, only the references to them.\\nUnpacking tuples\\nIf you try to assign to a tuple-like expression of variables, Python will attempt to un-\\npack the value on the right-hand side of the equals sign:\\nIn [370]: tup = (4, 5, 6)\\nIn [371]: a, b, c = tup\\nIn [372]: b\\nOut[372]: 5\\nEven sequences with nested tuples can be unpacked:\\nIn [373]: tup = 4, 5, (6, 7)\\nIn [374]: a, b, (c, d) = tup\\nIn [375]: d\\nOut[375]: 7\\nUsing this functionality it’s easy to swap variable names, a task which in many lan-\\nguages might look like:\\ntmp = a\\na = b\\nb = tmp\\nb, a = a, b\\nOne of the most common uses of variable unpacking when iterating over sequences of\\ntuples or lists:\\nseq = [(1, 2, 3), (4, 5, 6), (7, 8, 9)]\\nfor a, b, c in seq:\\n    pass\\nAnother common use is for returning multiple values from a function. More on this\\nlater.\\nTuple methods\\nSince the size and contents of a tuple cannot be modified, it is very light on instance\\nmethods. One particularly useful one (also available on lists) is count, which counts the\\nnumber of occurrences of a value:\\nIn [376]: a = (1, 2, 2, 2, 3, 4, 2)\\nData Structures and Sequences | 407\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 423, 'page_label': '408'}, page_content=\"In [377]: a.count(2)\\nOut[377]: 4\\nList\\nIn contrast with tuples, lists are variable-length and their contents can be modified.\\nThey can be defined using square brackets [] or using the list type function:\\nIn [378]: a_list = [2, 3, 7, None]\\nIn [379]: tup = ('foo', 'bar', 'baz')\\nIn [380]: b_list = list(tup)        In [381]: b_list               \\n                                    Out[381]: ['foo', 'bar', 'baz']\\n                                                                   \\nIn [382]: b_list[1] = 'peekaboo'    In [383]: b_list                    \\n                                    Out[383]: ['foo', 'peekaboo', 'baz']\\nLists and tuples are semantically similar as one-dimensional sequences of objects and\\nthus can be used interchangeably in many functions.\\nAdding and removing elements\\nElements can be appended to the end of the list with the append method:\\nIn [384]: b_list.append('dwarf')\\nIn [385]: b_list\\nOut[385]: ['foo', 'peekaboo', 'baz', 'dwarf']\\nUsing insert you can insert an element at a specific location in the list:\\nIn [386]: b_list.insert(1, 'red')\\nIn [387]: b_list\\nOut[387]: ['foo', 'red', 'peekaboo', 'baz', 'dwarf']\\ninsert is computationally expensive compared with append as references\\nto subsequent elements have to be shifted internally to make room for\\nthe new element.\\nThe inverse operation to insert is pop, which removes and returns an element at a\\nparticular index:\\nIn [388]: b_list.pop(2)\\nOut[388]: 'peekaboo'\\nIn [389]: b_list\\nOut[389]: ['foo', 'red', 'baz', 'dwarf']\\nElements can be removed by value using remove, which locates the first such value and\\nremoves it from the last:\\n408 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 424, 'page_label': '409'}, page_content=\"In [390]: b_list.append('foo')\\nIn [391]: b_list.remove('foo')\\nIn [392]: b_list\\nOut[392]: ['red', 'baz', 'dwarf', 'foo']\\nIf performance is not a concern, by using append and remove, a Python list can be used\\nas a perfectly suitable “multi-set” data structure.\\nYou can check if a list contains a value using the in keyword:\\nIn [393]: 'dwarf' in b_list\\nOut[393]: True\\nNote that checking whether a list contains a value is a lot slower than dicts and sets as\\nPython makes a linear scan across the values of the list, whereas the others (based on\\nhash tables) can make the check in constant time.\\nConcatenating and combining lists\\nSimilar to tuples, adding two lists together with + concatenates them:\\nIn [394]: [4, None, 'foo'] + [7, 8, (2, 3)]\\nOut[394]: [4, None, 'foo', 7, 8, (2, 3)]\\nIf you have a list already defined, you can append multiple elements to it using the \\nextend method:\\nIn [395]: x = [4, None, 'foo']\\nIn [396]: x.extend([7, 8, (2, 3)])\\nIn [397]: x\\nOut[397]: [4, None, 'foo', 7, 8, (2, 3)]\\nNote that list concatenation is a compartively expensive operation since a new list must\\nbe created and the objects copied over. Using extend to append elements to an existing\\nlist, especially if you are building up a large list, is usually preferable. Thus,\\neverything = []\\nfor chunk in list_of_lists:\\n    everything.extend(chunk)\\nis faster than than the concatenative alternative\\neverything = []\\nfor chunk in list_of_lists:\\n    everything = everything + chunk\\nSorting\\nA list can be sorted in-place (without creating a new object) by calling its sort function:\\nIn [398]: a = [7, 2, 5, 1, 3]\\nData Structures and Sequences | 409\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 425, 'page_label': '410'}, page_content=\"In [399]: a.sort()\\nIn [400]: a\\nOut[400]: [1, 2, 3, 5, 7]\\nsort has a few options that will occasionally come in handy. One is the ability to pass\\na secondary sort key, i.e. a function that produces a value to use to sort the objects. For\\nexample, we could sort a collection of strings by their lengths:\\nIn [401]: b = ['saw', 'small', 'He', 'foxes', 'six']\\nIn [402]: b.sort(key=len)\\nIn [403]: b\\nOut[403]: ['He', 'saw', 'six', 'small', 'foxes']\\nBinary search and maintaining a sorted list\\nThe built-in bisect module implements binary-search and insertion into a sorted list.\\nbisect.bisect finds the location where an element should be inserted to keep it sorted,\\nwhile bisect.insort actually inserts the element into that location:\\nIn [404]: import bisect\\nIn [405]: c = [1, 2, 2, 2, 3, 4, 7]\\nIn [406]: bisect.bisect(c, 2)        In [407]: bisect.bisect(c, 5)\\nOut[406]: 4                          Out[407]: 6                  \\n                                                                  \\nIn [408]: bisect.insort(c, 6)\\nIn [409]: c\\nOut[409]: [1, 2, 2, 2, 3, 4, 6, 7]\\nThe bisect module functions do not check whether the list is sorted as\\ndoing so would be computationally expensive. Thus, using them with\\nan unsorted list will succeed without error but may lead to incorrect\\nresults.\\nSlicing\\nYou can select sections of list-like types (arrays, tuples, NumPy arrays) by using slice\\nnotation, which in its basic form consists of start:stop passed to the indexing operator\\n[]:\\nIn [410]: seq = [7, 2, 3, 7, 5, 6, 0, 1]\\nIn [411]: seq[1:5]\\nOut[411]: [2, 3, 7, 5]\\nSlices can also be assigned to with a sequence:\\nIn [412]: seq[3:4] = [6, 3]\\n410 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 426, 'page_label': '411'}, page_content='In [413]: seq\\nOut[413]: [7, 2, 3, 6, 3, 5, 6, 0, 1]\\nWhile element at the start index is included, the stop index is not included, so that\\nthe number of elements in the result is stop - start.\\nEither the start or stop can be omitted in which case they default to the start of the\\nsequence and the end of the sequence, respectively:\\nIn [414]: seq[:5]                In [415]: seq[3:]           \\nOut[414]: [7, 2, 3, 6, 3]        Out[415]: [6, 3, 5, 6, 0, 1]\\nNegative indices slice the sequence relative to the end:\\nIn [416]: seq[-4:]            In [417]: seq[-6:-2]  \\nOut[416]: [5, 6, 0, 1]        Out[417]: [6, 3, 5, 6]\\nSlicing semantics takes a bit of getting used to, especially if you’re coming from R or\\nMATLAB. See Figure A-2 for a helpful illustrating of slicing with positive and negative\\nintegers.\\nA step can also be used after a second colon to, say, take every other element:\\nIn [418]: seq[::2]\\nOut[418]: [7, 3, 3, 6, 1]\\nA clever use of this is to pass -1 which has the useful effect of reversing a list or tuple:\\nIn [419]: seq[::-1]\\nOut[419]: [1, 0, 6, 5, 3, 6, 3, 2, 7]\\nFigure A-2. Illustration of Python slicing conventions\\nBuilt-in Sequence Functions\\nPython has a handful of useful sequence functions that you should familiarize yourself\\nwith and use at any opportunity.\\nData Structures and Sequences | 411\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 427, 'page_label': '412'}, page_content=\"enumerate\\nIt’s common when iterating over a sequence to want to keep track of the index of the\\ncurrent item. A do-it-yourself approach would look like:\\ni = 0\\nfor value in collection:\\n   # do something with value\\n   i += 1\\nSince this is so common, Python has a built-in function enumerate which returns a\\nsequence of (i, value) tuples:\\nfor i, value in enumerate(collection):\\n   # do something with value\\nWhen indexing data, a useful pattern that uses enumerate is computing a dict mapping\\nthe values of a sequence (which are assumed to be unique) to their locations in the\\nsequence:\\nIn [420]: some_list = ['foo', 'bar', 'baz']\\nIn [421]: mapping = dict((v, i) for i, v in enumerate(some_list))\\nIn [422]: mapping\\nOut[422]: {'bar': 1, 'baz': 2, 'foo': 0}\\nsorted\\nThe sorted function returns a new sorted list from the elements of any sequence:\\nIn [423]: sorted([7, 1, 2, 6, 0, 3, 2])\\nOut[423]: [0, 1, 2, 2, 3, 6, 7]\\nIn [424]: sorted('horse race')\\nOut[424]: [' ', 'a', 'c', 'e', 'e', 'h', 'o', 'r', 'r', 's']\\nA common pattern for getting a sorted list of the unique elements in a sequence is to\\ncombine sorted with set:\\nIn [425]: sorted(set('this is just some string'))\\nOut[425]: [' ', 'e', 'g', 'h', 'i', 'j', 'm', 'n', 'o', 'r', 's', 't', 'u']\\nzip\\nzip “pairs” up the elements of a number of lists, tuples, or other sequences, to create\\na list of tuples:\\nIn [426]: seq1 = ['foo', 'bar', 'baz']\\nIn [427]: seq2 = ['one', 'two', 'three']\\nIn [428]: zip(seq1, seq2)\\nOut[428]: [('foo', 'one'), ('bar', 'two'), ('baz', 'three')]\\n412 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 428, 'page_label': '413'}, page_content=\"zip can take an arbitrary number of sequences, and the number of elements it produces\\nis determined by the shortest sequence:\\nIn [429]: seq3 = [False, True]\\nIn [430]: zip(seq1, seq2, seq3)\\nOut[430]: [('foo', 'one', False), ('bar', 'two', True)]\\nA very common use of zip is for simultaneously iterating over multiple sequences,\\npossibly also combined with enumerate:\\nIn [431]: for i, (a, b) in enumerate(zip(seq1, seq2)):\\n   .....:     print('%d: %s, %s' % (i, a, b))\\n   .....:\\n0: foo, one\\n1: bar, two\\n2: baz, three\\nGiven a “zipped” sequence, zip can be applied in a clever way to “unzip” the sequence.\\nAnother way to think about this is converting a list of rows into a list of columns. The\\nsyntax, which looks a bit magical, is:\\nIn [432]: pitchers = [('Nolan', 'Ryan'), ('Roger', 'Clemens'),\\n   .....:             ('Schilling', 'Curt')]\\nIn [433]: first_names, last_names = zip(*pitchers)\\nIn [434]: first_names\\nOut[434]: ('Nolan', 'Roger', 'Schilling')\\nIn [435]: last_names\\nOut[435]: ('Ryan', 'Clemens', 'Curt')\\nWe’ll look in more detail at the use of * in a function call. It is equivalent to the fol-\\nlowing:\\nzip(seq[0], seq[1], ..., seq[len(seq) - 1])\\nreversed\\nreversed iterates over the elements of a sequence in reverse order:\\nIn [436]: list(reversed(range(10)))\\nOut[436]: [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\\nDict\\ndict is likely the most important built-in Python data structure. A more common name\\nfor it is hash map or associative array. It is a flexibly-sized collection of key-value pairs,\\nwhere key and value are Python objects. One way to create one is by using curly braces \\n{} and using colons to separate keys and values:\\nIn [437]: empty_dict = {}\\nIn [438]: d1 = {'a' : 'some value', 'b' : [1, 2, 3, 4]}\\nData Structures and Sequences | 413\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 429, 'page_label': '414'}, page_content=\"In [439]: d1\\nOut[439]: {'a': 'some value', 'b': [1, 2, 3, 4]}\\nElements can be accessed and inserted or set using the same syntax as accessing ele-\\nments of a list or tuple:\\nIn [440]: d1[7] = 'an integer'\\nIn [441]: d1\\nOut[441]: {7: 'an integer', 'a': 'some value', 'b': [1, 2, 3, 4]}\\nIn [442]: d1['b']\\nOut[442]: [1, 2, 3, 4]\\nYou can check if a dict contains a key using the same syntax as with checking whether\\na list or tuple contains a value:\\nIn [443]: 'b' in d1\\nOut[443]: True\\nValues can be deleted either using the del keyword or the pop method (which simulta-\\nneously returns the value and deletes the key):\\nIn [444]: d1[5] = 'some value'\\nIn [445]: d1['dummy'] = 'another value'\\nIn [446]: del d1[5]\\nIn [447]: ret = d1.pop('dummy')        In [448]: ret            \\n                                       Out[448]: 'another value'\\nThe keys and values method give you lists of the keys and values, respectively. While\\nthe key-value pairs are not in any particular order, these functions output the keys and\\nvalues in the same order:\\nIn [449]: d1.keys()            In [450]: d1.values()                               \\nOut[449]: ['a', 'b', 7]        Out[450]: ['some value', [1, 2, 3, 4], 'an integer']\\nIf you’re using Python 3, dict.keys() and dict.values() are iterators\\ninstead of lists.\\nOne dict can be merged into another using the update method:\\nIn [451]: d1.update({'b' : 'foo', 'c' : 12})\\nIn [452]: d1\\nOut[452]: {7: 'an integer', 'a': 'some value', 'b': 'foo', 'c': 12}\\n414 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 430, 'page_label': '415'}, page_content=\"Creating dicts from sequences\\nIt’s common to occasionally end up with two sequences that you want to pair up ele-\\nment-wise in a dict. As a first cut, you might write code like this:\\nmapping = {}\\nfor key, value in zip(key_list, value_list):\\n    mapping[key] = value\\nSince a dict is essentially a collection of 2-tuples, it should be no shock that the dict\\ntype function accepts a list of 2-tuples:\\nIn [453]: mapping = dict(zip(range(5), reversed(range(5))))\\nIn [454]: mapping\\nOut[454]: {0: 4, 1: 3, 2: 2, 3: 1, 4: 0}\\nIn a later section we’ll talk about dict comprehensions, another elegant way to construct\\ndicts.\\nDefault values\\nIt’s very common to have logic like:\\nif key in some_dict:\\n    value = some_dict[key]\\nelse:\\n    value = default_value\\nThus, the dict methods get and pop can take a default value to be returned, so that the\\nabove if-else block can be written simply as:\\nvalue = some_dict.get(key, default_value)\\nget by default will return None if the key is not present, while pop will raise an exception.\\nWith setting values, a common case is for the values in a dict to be other collections,\\nlike lists. For example, you could imagine categorizing a list of words by their first letters\\nas a dict of lists:\\nIn [455]: words = ['apple', 'bat', 'bar', 'atom', 'book']\\nIn [456]: by_letter = {}\\nIn [457]: for word in words:\\n   .....:     letter = word[0]\\n   .....:     if letter not in by_letter:\\n   .....:         by_letter[letter] = [word]\\n   .....:     else:\\n   .....:         by_letter[letter].append(word)\\n   .....:\\nIn [458]: by_letter\\nOut[458]: {'a': ['apple', 'atom'], 'b': ['bat', 'bar', 'book']}\\nThe setdefault dict method is for precisely this purpose. The if-else block above can\\nbe rewritten as:\\nData Structures and Sequences | 415\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 431, 'page_label': '416'}, page_content=\"by_letter.setdefault(letter, []).append(word)\\nThe built-in collections module has a useful class, defaultdict, which makes this even\\neasier. One is created by passing a type or function for generating the default value for\\neach slot in the dict:\\nfrom collections import defaultdict\\nby_letter = defaultdict(list)\\nfor word in words:\\n    by_letter[word[0]].append(word)\\nThe initializer to defaultdict only needs to be a callable object (e.g. any function), not\\nnecessarily a type. Thus, if you wanted the default value to be 4 you could pass a\\nfunction returning 4\\ncounts = defaultdict(lambda: 4)\\nValid dict key types\\nWhile the values of a dict can be any Python object, the keys have to be immutable\\nobjects like scalar types (int, float, string) or tuples (all the objects in the tuple need to\\nbe immutable, too). The technical term here is hashability. You can check whether an\\nobject is hashable (can be used as a key in a dict) with the hash function:\\nIn [459]: hash('string')\\nOut[459]: -9167918882415130555\\nIn [460]: hash((1, 2, (2, 3)))\\nOut[460]: 1097636502276347782\\nIn [461]: hash((1, 2, [2, 3])) # fails because lists are mutable\\n---------------------------------------------------------------------------\\nTypeError                                 Traceback (most recent call last)\\n<ipython-input-461-800cd14ba8be> in <module>()\\n----> 1 hash((1, 2, [2, 3])) # fails because lists are mutable\\nTypeError: unhashable type: 'list'\\nTo use a list as a key, an easy fix is to convert it to a tuple:\\nIn [462]: d = {}\\nIn [463]: d[tuple([1, 2, 3])] = 5\\nIn [464]: d\\nOut[464]: {(1, 2, 3): 5}\\nSet\\nA set is an unordered collection of unique elements. You can think of them like dicts,\\nbut keys only, no values. A set can be created in two ways: via the set function or using\\na set literal with curly braces:\\nIn [465]: set([2, 2, 2, 1, 3, 3])\\nOut[465]: set([1, 2, 3])\\n416 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 432, 'page_label': '417'}, page_content='In [466]: {2, 2, 2, 1, 3, 3}\\nOut[466]: set([1, 2, 3])\\nSets support mathematical set operations like union, intersection, difference, and sym-\\nmetric difference. See Table A-3 for a list of commonly used set methods.\\nIn [467]: a = {1, 2, 3, 4, 5}\\nIn [468]: b = {3, 4, 5, 6, 7, 8}\\nIn [469]: a | b  # union (or)\\nOut[469]: set([1, 2, 3, 4, 5, 6, 7, 8])\\nIn [470]: a & b  # intersection (and)\\nOut[470]: set([3, 4, 5])\\nIn [471]: a - b  # difference\\nOut[471]: set([1, 2])\\nIn [472]: a ^ b  # symmetric difference (xor)\\nOut[472]: set([1, 2, 6, 7, 8])\\nYou can also check if a set is a subset of (is contained in) or a superset of (contains all\\nelements of) another set:\\nIn [473]: a_set = {1, 2, 3, 4, 5}\\nIn [474]: {1, 2, 3}.issubset(a_set)\\nOut[474]: True\\nIn [475]: a_set.issuperset({1, 2, 3})\\nOut[475]: True\\nAs you might guess, sets are equal if their contents are equal:\\nIn [476]: {1, 2, 3} == {3, 2, 1}\\nOut[476]: True\\nTable A-3. Python Set Operations\\nFunction Alternate Syntax Description\\na.add(x) N/A Add element x to the set a\\na.remove(x) N/A Remove element x from the set a\\na.union(b) a | b All of the unique elements in a and b.\\na.intersection(b) a & b All of the elements in both  a and b.\\na.difference(b) a - b The elements in a that are not in b.\\na.symmetric_difference(b) a ^ b All of the elements in a or b but not both.\\na.issubset(b) N/A True if the elements of a are all contained in b.\\na.issuperset(b) N/A True if the elements of b are all contained in a.\\na.isdisjoint(b) N/A True if a and b have no elements in common.\\nData Structures and Sequences | 417\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 433, 'page_label': '418'}, page_content=\"List, Set, and Dict Comprehensions\\nList comprehensions are one of the most-loved Python language features. They allow\\nyou to concisely form a new list by filtering the elements of a collection and transforming\\nthe elements passing the filter in one conscise expression. They take the basic form:\\n[expr for val in collection if condition]\\nThis is equivalent to the following for loop:\\nresult = []\\nfor val in collection:\\n    if condition:\\n        result.append(expr)\\nThe filter condition can be omitted, leaving only the expression. For example, given a\\nlist of strings, we could filter out strings with length 2 or less and also convert them to\\nuppercase like this:\\nIn [477]: strings = ['a', 'as', 'bat', 'car', 'dove', 'python']\\nIn [478]: [x.upper() for x in strings if len(x) > 2]\\nOut[478]: ['BAT', 'CAR', 'DOVE', 'PYTHON']\\nSet and dict comprehensions are a natural extension, producing sets and dicts in a\\nidiomatically similar way instead of lists. A dict comprehension looks like this:\\ndict_comp = {key-expr : value-expr for value in collection\\n             if condition}\\nA set comprehension looks like the equivalent list comprehension except with curly\\nbraces instead of square brackets:\\nset_comp = {expr for value in collection if condition}\\nLike list comprehensions, set and dict comprehensions are just syntactic sugar, but they\\nsimilarly can make code both easier to write and read. Consider the list of strings above.\\nSuppose we wanted a set containing just the lengths of the strings contained in the\\ncollection; this could be easily computed using a set comprehension:\\nIn [479]: unique_lengths = {len(x) for x in strings}\\nIn [480]: unique_lengths\\nOut[480]: set([1, 2, 3, 4, 6])\\nAs a simple dict comprehension example, we could create a lookup map of these strings\\nto their locations in the list:\\nIn [481]: loc_mapping = {val : index for index, val in enumerate(strings)}\\nIn [482]: loc_mapping\\nOut[482]: {'a': 0, 'as': 1, 'bat': 2, 'car': 3, 'dove': 4, 'python': 5}\\nNote that this dict could be equivalently constructed by:\\nloc_mapping = dict((val, idx) for idx, val in enumerate(strings)}\\n418 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 434, 'page_label': '419'}, page_content=\"The dict comprehension version is shorter and cleaner in my opinion.\\nDict and set comprehensions were added to Python fairly recently in\\nPython 2.7 and Python 3.1+.\\nNested list comprehensions\\nSuppose we have a list of lists containing some boy and girl names:\\nIn [483]: all_data = [['Tom', 'Billy', 'Jefferson', 'Andrew', 'Wesley', 'Steven', 'Joe'],\\n   .....:             ['Susie', 'Casey', 'Jill', 'Ana', 'Eva', 'Jennifer', 'Stephanie']]\\nYou might have gotten these names from a couple of files and decided to keep the boy\\nand girl names separate. Now, suppose we wanted to get a single list containing all\\nnames with two or more e’s in them. We could certainly do this with a simple for loop:\\nnames_of_interest = []\\nfor names in all_data:\\n    enough_es = [name for name in names if name.count('e') > 2]\\n    names_of_interest.extend(enough_es)\\nYou can actually wrap this whole operation up in a single nested list comprehension,\\nwhich will look like:\\nIn [484]: result = [name for names in all_data for name in names\\n   .....:           if name.count('e') >= 2]\\nIn [485]: result\\nOut[485]: ['Jefferson', 'Wesley', 'Steven', 'Jennifer', 'Stephanie']\\nAt first, nested list comprehensions are a bit hard to wrap your head around. The for\\nparts of the list comprehension are arranged according to the order of nesting, and any\\nfilter condition is put at the end as before. Here is another example where we “flatten”\\na list of tuples of integers into a simple list of integers:\\nIn [486]: some_tuples = [(1, 2, 3), (4, 5, 6), (7, 8, 9)]\\nIn [487]: flattened = [x for tup in some_tuples for x in tup]\\nIn [488]: flattened\\nOut[488]: [1, 2, 3, 4, 5, 6, 7, 8, 9]\\nKeep in mind that the order of the for expressions would be the same if you wrote a\\nnested for loop instead of a list comprehension:\\nflattened = []\\nfor tup in some_tuples:\\n    for x in tup:\\n        flattened.append(x)\\nData Structures and Sequences | 419\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 435, 'page_label': '420'}, page_content='You can have arbitrarily many levels of nesting, though if you have more than two or\\nthree levels of nesting you should probably start to question your data structure design.\\nIt’s important to distinguish the above syntax from a list comprehension inside a list\\ncomprehension, which is also perfectly valid:\\nIn [229]: [[x for x in tup] for tup in some_tuples]\\nFunctions\\nFunctions are the primary and most important method of code organization and reuse\\nin Python. There may not be such a thing as having too many functions. In fact, I would\\nargue that most programmers doing data analysis don’t write enough functions! As you\\nhave likely inferred from prior examples, functions are declared using the def keyword\\nand returned from using the return keyword:\\ndef my_function(x, y, z=1.5):\\n    if z > 1:\\n        return z * (x + y)\\n    else:\\n        return z / (x + y)\\nThere is no issue with having multiple return statements. If the end of a function is\\nreached without encountering a return statement, None is returned.\\nEach function can have some number of positional arguments and some number of \\nkeyword arguments. Keyword arguments are most commonly used to specify default\\nvalues or optional arguments. In the above function, x and y are positional arguments\\nwhile z is a keyword argument. This means that it can be called in either of these\\nequivalent ways:\\nmy_function(5, 6, z=0.7)\\nmy_function(3.14, 7, 3.5)\\nThe main restriction on function arguments it that the keyword arguments must follow\\nthe positional arguments (if any). You can specify keyword arguments in any order;\\nthis frees you from having to remember which order the function arguments were\\nspecified in and only what their names are.\\nNamespaces, Scope, and Local Functions\\nFunctions can access variables in two different scopes: global and local. An alternate\\nand more descriptive name describing a variable scope in Python is a namespace. Any\\nvariables that are assigned within a function by default are assigned to the local name-\\nspace. The local namespace is created when the function is called and immediately\\npopulated by the function’s arguments. After the function is finished, the local name-\\nspace is destroyed (with some exceptions, see section on closures below). Consider the\\nfollowing function:\\n420 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 436, 'page_label': '421'}, page_content='def func():\\n    a = []\\n    for i in range(5):\\n        a.append(i)\\nUpon calling func(), the empty list a is created, 5 elements are appended, then a is\\ndestroyed when the function exits. Suppose instead we had declared a\\na = []\\ndef func():\\n    for i in range(5):\\n        a.append(i)\\nAssigning global variables within a function is possible, but those variables must be\\ndeclared as global using the global keyword:\\nIn [489]: a = None\\nIn [490]: def bind_a_variable():\\n   .....:     global a\\n   .....:     a = []\\n   .....: bind_a_variable()\\n   .....:\\nIn [491]: print a\\n[]\\nI generally discourage people from using the global keyword frequently.\\nTypically global variables are used to store some kind of state in a sys-\\ntem. If you find yourself using a lot of them, it’s probably a sign that\\nsome object-oriented programming (using classes) is in order.\\nFunctions can be declared anywhere, and there is no problem with having local func-\\ntions that are dynamically created when a function is called:\\ndef outer_function(x, y, z):\\n    def inner_function(a, b, c):\\n        pass\\n    pass\\nIn the above code, the inner_function will not exist until outer_function is called. As\\nsoon as outer_function is done executing, the inner_function is destroyed.\\nNested inner functions can access the local namespace of the enclosing function, but\\nthey cannot bind new variables in it. I’ll talk a bit more about this in the section on\\nclosures.\\nIn a strict sense, all functions are local to some scope, that scope may just be the module\\nlevel scope.\\nFunctions | 421\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 437, 'page_label': '422'}, page_content=\"Returning Multiple Values\\nWhen I first programmed in Python after having programmed in Java and C++, one of\\nmy favorite features was the ability to return multiple values from a function. Here’s a\\nsimple example:\\ndef f():\\n    a = 5\\n    b = 6\\n    c = 7\\n    return a, b, c\\na, b, c = f()\\nIn data analysis and other scientific applications, you will likely find yourself doing this\\nvery often as many functions may have multiple outputs, whether those are data struc-\\ntures or other auxiliary data computed inside the function. If you think about tuple\\npacking and unpacking from earlier in this chapter, you may realize that what’s hap-\\npening here is that the function is actually just returning one object, namely a tuple,\\nwhich is then being unpacked into the result variables. In the above example, we could\\nhave done instead:\\nreturn_value = f()\\nIn this case, return_value would be, as you may guess, a 3-tuple with the three returned\\nvariables. A potentially attractive alternative to returning multiple values like above\\nmight be to return a dict instead:\\ndef f():\\n    a = 5\\n    b = 6\\n    c = 7\\n    return {'a' : a, 'b' : b, 'c' : c}\\nFunctions Are Objects\\nSince Python functions are objects, many constructs can be easily expressed that are\\ndifficult to do in other languages. Suppose we were doing some data cleaning and\\nneeded to apply a bunch of transformations to the following list of strings:\\nstates = ['   Alabama ', 'Georgia!', 'Georgia', 'georgia', 'FlOrIda',\\n          'south   carolina##', 'West virginia?']\\nAnyone who has ever worked with user-submitted survey data can expect messy results\\nlike these. Lots of things need to happen to make this list of strings uniform and ready\\nfor analysis: whitespace stripping, removing punctuation symbols, and proper capital-\\nization. As a first pass, we might write some code like:\\nimport re  # Regular expression module\\ndef clean_strings(strings):\\n    result = []\\n422 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 438, 'page_label': '423'}, page_content=\"for value in strings:\\n        value = value.strip()\\n        value = re.sub('[!#?]', '', value) # remove punctuation\\n        value = value.title()\\n        result.append(value)\\n    return result\\nThe result looks like this:\\nIn [15]: clean_strings(states)\\nOut[15]:\\n['Alabama',\\n 'Georgia',\\n 'Georgia',\\n 'Georgia',\\n 'Florida',\\n 'South Carolina',\\n 'West Virginia']\\nAn alternate approach that you may find useful is to make a list of the operations you\\nwant to apply to a particular set of strings:\\ndef remove_punctuation(value):\\n    return re.sub('[!#?]', '', value)\\nclean_ops = [str.strip, remove_punctuation, str.title]\\ndef clean_strings(strings, ops):\\n    result = []\\n    for value in strings:\\n        for function in ops:\\n            value = function(value)\\n        result.append(value)\\n    return result\\nThen we have\\nIn [22]: clean_strings(states, clean_ops)\\nOut[22]:\\n['Alabama',\\n 'Georgia',\\n 'Georgia',\\n 'Georgia',\\n 'Florida',\\n 'South Carolina',\\n 'West Virginia']\\nA more functional pattern like this enables you to easily modify how the strings are\\ntransformed at a very high level. The clean_strings function is also now more reusable!\\nYou can naturally use functions as arguments to other functions like the built-in map\\nfunction, which applies a function to a collection of some kind:\\nIn [23]: map(remove_punctuation, states)\\nOut[23]:\\n['   Alabama ',\\n 'Georgia',\\nFunctions | 423\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 439, 'page_label': '424'}, page_content=\"'Georgia',\\n 'georgia',\\n 'FlOrIda',\\n 'south   carolina',\\n 'West virginia']\\nAnonymous (lambda) Functions\\nPython has support for so-called anonymous or lambda functions, which are really just\\nsimple functions consisting of a single statement, the result of which is the return value.\\nThey are defined using the lambda keyword, which has no meaning other than “we are\\ndeclaring an anonymous function.”\\ndef short_function(x):\\n    return x * 2\\nequiv_anon = lambda x: x * 2\\nI usually refer to these as lambda functions in the rest of the book. They are especially\\nconvenient in data analysis because, as you’ll see, there are many cases where data\\ntransformation functions will take functions as arguments. It’s often less typing (and\\nclearer) to pass a lambda function as opposed to writing a full-out function declaration\\nor even assigning the lambda function to a local variable. For example, consider this\\nsilly example:\\ndef apply_to_list(some_list, f):\\n    return [f(x) for x in some_list]\\nints = [4, 0, 1, 5, 6]\\napply_to_list(ints, lambda x: x * 2)\\nYou could also have written [x * 2 for x in ints], but here we were able to succintly\\npass a custom operator to the apply_to_list function.\\nAs another example, suppose you wanted to sort a collection of strings by the number\\nof distinct letters in each string:\\nIn [492]: strings = ['foo', 'card', 'bar', 'aaaa', 'abab']\\nHere we could pass a lambda function to the list’s sort method:\\nIn [493]: strings.sort(key=lambda x: len(set(list(x))))\\nIn [494]: strings\\nOut[494]: ['aaaa', 'foo', 'abab', 'bar', 'card']\\nOne reason lambda functions are called anonymous functions is that\\nthe function object itself is never given a name attribute.\\n424 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 440, 'page_label': '425'}, page_content=\"Closures: Functions that Return Functions\\nClosures are nothing to fear. They can actually be a very useful and powerful tool in\\nthe right circumstance! In a nutshell, a closure is any dynamically-generated function\\nreturned by another function. The key property is that the returned function has access\\nto the variables in the local namespace where it was created. Here is a very simple\\nexample:\\ndef make_closure(a):\\n    def closure():\\n        print('I know the secret: %d' % a)\\n    return closure\\nclosure = make_closure(5)\\nThe difference between a closure and a regular Python function is that the closure\\ncontinues to have access to the namespace (the function) where it was created, even\\nthough that function is done executing. So in the above case, the returned closure will\\nalways print I know the secret: 5 whenever you call it. While it’s common to create\\nclosures whose internal state (in this example, only the value of a) is static, you can just\\nas easily have a mutable object like a dict, set, or list that can be modified. For example,\\nhere’s a function that returns a function that keeps track of arguments it has been called\\nwith:\\ndef make_watcher():\\n    have_seen = {}\\n    def has_been_seen(x):\\n        if x in have_seen:\\n            return True\\n        else:\\n            have_seen[x] = True\\n            return False\\n    return has_been_seen\\nUsing this on a sequence of integers I obtain:\\nIn [496]: watcher = make_watcher()\\nIn [497]: vals = [5, 6, 1, 5, 1, 6, 3, 5]\\nIn [498]: [watcher(x) for x in vals]\\nOut[498]: [False, False, False, True, True, True, False, True]\\nHowever, one technical limitation to keep in mind is that while you can mutate any\\ninternal state objects (like adding key-value pairs to a dict), you cannot bind variables\\nin the enclosing function scope. One way to work around this is to modify a dict or list\\nrather than binding variables:\\ndef make_counter():\\n    count = [0]\\n    def counter():\\nFunctions | 425\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 441, 'page_label': '426'}, page_content='# increment and return the current count\\n        count[0] += 1\\n        return count[0]\\n    return counter\\ncounter = make_counter()\\nYou might be wondering why this is useful. In practice, you can write very general\\nfunctions with lots of options, then fabricate simpler, more specialized functions.\\nHere’s an example of creating a string formatting function:\\ndef format_and_pad(template, space):\\n    def formatter(x):\\n        return (template % x).rjust(space)\\n    return formatter\\nYou could then create a floating point formatter that always returns a length-15 string\\nlike so:\\nIn [500]: fmt = format_and_pad(\\'%.4f\\', 15)\\nIn [501]: fmt(1.756)\\nOut[501]: \\'         1.7560\\'\\nIf you learn more about object-oriented programming in Python, you might observe\\nthat these patterns also could be implemented (albeit more verbosely) using classes.\\nExtended Call Syntax with *args, **kwargs\\nThe way that function arguments work under the hood in Python is actually very sim-\\nple. When you write func(a, b, c, d=some, e=value) , the positional and keyword\\narguments are actually packed up into a tuple and dict, respectively. So the internal\\nfunction receives a tuple args and dict kwargs and internally does the equivalent of:\\na, b, c = args\\nd = kwargs.get(\\'d\\', d_default_value)\\ne = kwargs.get(\\'e\\', e_default_value)\\nThis all happens nicely behind the scenes. Of course, it also does some error checking\\nand allows you to specify some of the positional arguments as keywords also (even if\\nthey aren’t keyword in the function declaration!).\\ndef say_hello_then_call_f(f, *args, **kwargs):\\n    print \\'args is\\', args\\n    print \\'kwargs is\\', kwargs\\n    print(\"Hello! Now I\\'m going to call %s\" % f)\\n    return f(*args, **kwargs)\\ndef g(x, y, z=1):\\n    return (x + y) / z\\nThen if we call g with say_hello_then_call_f we get:\\n426 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 442, 'page_label': '427'}, page_content=\"In [8]:  say_hello_then_call_f(g, 1, 2, z=5.)\\nargs is (1, 2)\\nkwargs is {'z': 5.0}\\nHello! Now I'm going to call <function g at 0x2dd5cf8>\\nOut[8]: 0.6\\nCurrying: Partial Argument Application\\nCurrying is a fun computer science term which means deriving new functions from\\nexisting ones by partial argument application. For example, suppose we had a trivial\\nfunction that adds two numbers together:\\ndef add_numbers(x, y):\\n    return x + y\\nUsing this function, we could derive a new function of one variable, add_five, that adds\\n5 to its argument:\\nadd_five = lambda y: add_numbers(5, y)\\nThe second argument to add_numbers is said to be curried. There’s nothing very fancy\\nhere as we really only have defined a new function that calls an existing function. The\\nbuilt-in functools module can simplify this process using the partial function:\\nfrom functools import partial\\nadd_five = partial(add_numbers, 5)\\nWhen discussing pandas and time series data, we’ll use this technique to create speci-\\nalized functions for transforming data series\\n# compute 60-day moving average of time series x\\nma60 = lambda x: pandas.rolling_mean(x, 60)\\n# Take the 60-day moving average of of all time series in data\\ndata.apply(ma60)\\nGenerators\\nHaving a consistent way to iterate over sequences, like objects in a list or lines in a file,\\nis an important Python feature. This is accomplished by means of the iterator proto-\\ncol, a generic way to make objects iterable. For example, iterating over a dict yields the\\ndict keys:\\nIn [502]: some_dict = {'a': 1, 'b': 2, 'c': 3}\\nIn [503]: for key in some_dict:\\n   .....:     print key,\\na c b\\nWhen you write for key in some_dict, the Python interpreter first attempts to create\\nan iterator out of some_dict:\\nIn [504]: dict_iterator = iter(some_dict)\\nFunctions | 427\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 443, 'page_label': '428'}, page_content=\"In [505]: dict_iterator\\nOut[505]: <dictionary-keyiterator at 0x10a0a1578>\\nAny iterator is any object that will yield objects to the Python interpreter when used in\\na context like a for loop. Most methods expecting a list or list-like object will also accept\\nany iterable object. This includes built-in methods such as min, max, and sum, and type\\nconstructors like list and tuple:\\nIn [506]: list(dict_iterator)\\nOut[506]: ['a', 'c', 'b']\\nA generator is a simple way to construct a new iterable object. Whereas normal func-\\ntions execute and return a single value, generators return a sequence of values lazily,\\npausing after each one until the next one is requested. To create a generator, use the\\nyield keyword instead of return in a function:\\ndef squares(n=10):\\n    for i in xrange(1, n + 1):\\n        print 'Generating squares from 1 to %d' % (n ** 2)\\n        yield i ** 2\\nWhen you actually call the generator, no code is immediately executed:\\nIn [2]: gen = squares()\\nIn [3]: gen\\nOut[3]: <generator object squares at 0x34c8280>\\nIt is not until you request elements from the generator that it begins executing its code:\\nIn [4]: for x in gen:\\n   ...:     print x,\\n   ...:\\nGenerating squares from 0 to 100\\n1 4 9 16 25 36 49 64 81 100\\nAs a less trivial example, suppose we wished to find all unique ways to make change\\nfor $1 (100 cents) using an arbitrary set of coins. You can probably think of various\\nways to implement this and how to store the unique combinations as you come up with\\nthem. One way is to write a generator that yields lists of coins (represented as integers):\\ndef make_change(amount, coins=[1, 5, 10, 25], hand=None):\\n    hand = [] if hand is None else hand\\n    if amount == 0:\\n        yield hand\\n    for coin in coins:\\n        # ensures we don't give too much change, and combinations are unique\\n        if coin > amount or (len(hand) > 0 and hand[-1] < coin):\\n            continue\\n        for result in make_change(amount - coin, coins=coins,\\n                                  hand=hand + [coin]):\\n            yield result\\nThe details of the algorithm are not that important (can you think of a shorter way?).\\nThen we can write:\\n428 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 444, 'page_label': '429'}, page_content=\"In [508]: for way in make_change(100, coins=[10, 25, 50]):\\n   .....:     print way\\n[10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\\n[25, 25, 10, 10, 10, 10, 10]\\n[25, 25, 25, 25]\\n[50, 10, 10, 10, 10, 10]\\n[50, 25, 25]\\n[50, 50]\\nIn [509]: len(list(make_change(100)))\\nOut[509]: 242\\nGenerator expresssions\\nA simple way to make a generator is by using a generator expression. This is a generator\\nanalogue to list, dict and set comprehensions; to create one, enclose what would other-\\nwise be a list comprehension with parenthesis instead of brackets:\\nIn [510]: gen = (x ** 2 for x in xrange(100))\\nIn [511]: gen\\nOut[511]: <generator object <genexpr> at 0x10a0a31e0>\\nThis is completely equivalent to the following more verbose generator:\\ndef _make_gen():\\n    for x in xrange(100):\\n        yield x ** 2\\ngen = _make_gen()\\nGenerator expressions can be used inside any Python function that will accept a gen-\\nerator:\\nIn [512]: sum(x ** 2 for x in xrange(100))\\nOut[512]: 328350\\nIn [513]: dict((i, i **2) for i in xrange(5))\\nOut[513]: {0: 0, 1: 1, 2: 4, 3: 9, 4: 16}\\nitertools module\\nThe standard library itertools module has a collection of generators for many common\\ndata algorithms. For example, groupby takes any sequence and a function; this groups\\nconsecutive elements in the sequence by return value of the function. Here’s an exam-\\nple:\\nIn [514]: import itertools\\nIn [515]: first_letter = lambda x: x[0]\\nIn [516]: names = ['Alan', 'Adam', 'Wes', 'Will', 'Albert', 'Steven']\\nIn [517]: for letter, names in itertools.groupby(names, first_letter):\\n   .....:     print letter, list(names) # names is a generator\\nA ['Alan', 'Adam']\\nFunctions | 429\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 445, 'page_label': '430'}, page_content=\"W ['Wes', 'Will']\\nA ['Albert']\\nS ['Steven']\\nSee Table A-4 for a list of a few other itertools functions I’ve frequently found useful.\\nTable A-4. Some useful itertools functions\\nFunction Description\\nimap(func, *iterables) Generator version of the built-in map; applies func to each zipped tuple of\\nthe passed sequences.\\nifilter(func, iterable) Generator version of the built-in filter; yields elements x for which\\nfunc(x) is True.\\ncombinations(iterable, k) Generates a sequence of all possible k-tuples of elements in the iterable,\\nignoring order.\\npermutations(iterable, k) Generates a sequence of all possible k-tuples of elements in the iterable,\\nrespecting order.\\ngroupby(iterable[, keyfunc]) Generates (key, sub-iterator) for each unique key\\nIn Python 3, several built-in functions ( zip, map, filter) producing\\nlists have been replaced by their generator versions found in itertools\\nin Python 2.\\nFiles and the operating system\\nMost of this book uses high-level tools like pandas.read_csv to read data files from disk\\ninto Python data structures. However, it’s important to understand the basics of how\\nto work with files in Python. Fortunately, it’s very simple, which is part of why Python\\nis so popular for text and file munging.\\nTo open a file for reading or writing, use the built-in open function with either a relative\\nor absolute file path:\\nIn [518]: path = 'ch13/segismundo.txt'\\nIn [519]: f = open(path)\\nBy default, the file is opened in read-only mode 'r'. We can then treat the file handle\\nf like a list and iterate over the lines like so\\nfor line in f:\\n    pass\\nThe lines come out of the file with the end-of-line (EOL) markers intact, so you’ll often\\nsee code to get an EOL-free list of lines in a file like\\nIn [520]: lines = [x.rstrip() for x in open(path)]\\nIn [521]: lines\\n430 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 446, 'page_label': '431'}, page_content=\"Out[521]: \\n['Sue\\\\xc3\\\\xb1a el rico en su riqueza,',\\n 'que m\\\\xc3\\\\xa1s cuidados le ofrece;',\\n '',\\n 'sue\\\\xc3\\\\xb1a el pobre que padece',\\n 'su miseria y su pobreza;',\\n '',\\n 'sue\\\\xc3\\\\xb1a el que a medrar empieza,',\\n 'sue\\\\xc3\\\\xb1a el que afana y pretende,',\\n 'sue\\\\xc3\\\\xb1a el que agravia y ofende,',\\n '',\\n 'y en el mundo, en conclusi\\\\xc3\\\\xb3n,',\\n 'todos sue\\\\xc3\\\\xb1an lo que son,',\\n 'aunque ninguno lo entiende.',\\n '']\\nIf we had typed f = open(path, 'w'), a new file at ch13/segismundo.txt would have\\nbeen created, overwriting any one in its place. See below for a list of all valid file read/\\nwrite modes.\\nTable A-5. Python file modes\\nMode Description\\nr Read-only mode\\nw Write-only mode. Creates a new file (deleting any file with the same name)\\na Append to existing file (create it if it does not exist)\\nr+ Read and write\\nb Add to mode for binary files, that is 'rb' or 'wb'\\nU Use universal newline mode. Pass by itself 'U' or appended to one of the read modes like 'rU'\\nTo write text to a file, you can use either the file’s write or writelines methods. For\\nexample, we could create a version of prof_mod.py with no blank lines like so:\\nIn [522]: with open('tmp.txt', 'w') as handle:\\n   .....:     handle.writelines(x for x in open(path) if len(x) > 1)\\nIn [523]: open('tmp.txt').readlines()\\nOut[523]: \\n['Sue\\\\xc3\\\\xb1a el rico en su riqueza,\\\\n',\\n 'que m\\\\xc3\\\\xa1s cuidados le ofrece;\\\\n',\\n 'sue\\\\xc3\\\\xb1a el pobre que padece\\\\n',\\n 'su miseria y su pobreza;\\\\n',\\n 'sue\\\\xc3\\\\xb1a el que a medrar empieza,\\\\n',\\n 'sue\\\\xc3\\\\xb1a el que afana y pretende,\\\\n',\\n 'sue\\\\xc3\\\\xb1a el que agravia y ofende,\\\\n',\\n 'y en el mundo, en conclusi\\\\xc3\\\\xb3n,\\\\n',\\n 'todos sue\\\\xc3\\\\xb1an lo que son,\\\\n',\\n 'aunque ninguno lo entiende.\\\\n']\\nSee Table A-6 for many of the most commonly-used file methods.\\nFiles and the operating system | 431\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 447, 'page_label': '432'}, page_content='Table A-6. Important Python file methods or attributes\\nMethod Description\\nread([size]) Return data from file as a string, with optional size argument indicating the number of bytes\\nto read\\nreadlines([size]) Return list of lines in the file, with optional size argument\\nreadlines([size]) Return list of lines (as strings) in the file\\nwrite(str) Write passed string to file.\\nwritelines(strings) Write passed sequence of strings to the file.\\nclose() Close the handle\\nflush() Flush the internal I/O buffer to disk\\nseek(pos) Move to indicated file position (integer).\\ntell() Return current file position as integer.\\nclosed True is the file is closed.\\n432 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 448, 'page_label': '433'}, page_content='Index\\nSymbols\\n! character, 60, 61, 64\\n!= operator, 91\\n!cmd command, 60\\n\"two-language\" problem, 2–3\\n# (hash mark), 388\\n$PATH variable, 10\\n% character, 398\\n%a datetime format, 293\\n%A datetime format, 293\\n%alias magic function, 61\\n%automagic magic function, 55\\n%b datetime format, 293\\n%B datetime format, 293\\n%bookmark magic function, 60, 62\\n%c datetime format, 293\\n%cd magic function, 60\\n%cpaste magic function, 51–52, 55\\n%d datetime format, 292\\n%D datetime format, 293\\n%d format character, 398\\n%debug magic function, 54–55, 62\\n%dhist magic function, 60\\n%dirs magic function, 60\\n%env magic function, 60\\n%F datetime format, 293\\n%gui magic function, 57\\n%H datetime format, 292\\n%hist magic function, 55, 59\\n%I datetime format, 292\\n%logstart magic function, 60\\n%logstop magic function, 60\\n%lprun magic function, 70, 72\\n%m datetime format, 292\\n%M datetime format, 292\\n%magic magic function, 55\\n%p datetime format, 293\\n%page magic function, 55\\n%paste magic function, 51, 55\\n%pdb magic function, 54, 63\\n%popd magic function, 60\\n%prun magic function, 55, 70\\n%pushd magic function, 60\\n%pwd magic function, 60\\n%quickref magic function, 55\\n%reset magic function, 55, 59\\n%run magic function, 49–50, 55, 386\\n%S datetime format, 292\\n%s format character, 398\\n%time magic function, 55, 67\\n%timeit magic function, 54, 67, 68\\n%U datetime format, 293\\n%w datetime format, 292\\n%W datetime format, 293\\n%who magic function, 55\\n%whos magic function, 55\\n%who_ls magic function, 55\\n%x datetime format, 293\\n%X datetime format, 293\\n%xdel magic function, 55, 59\\n%xmode magic function, 54\\n%Y datetime format, 292\\n%y datetime format, 292\\n%z datetime format, 293\\n& operator, 91\\n* operator, 105\\n+ operator, 406, 409\\n2012 Federal Election Commission database\\nexample, 278–287\\nWe’d like to hear your suggestions for improving our indexes. Send email to index@oreilly.com.\\n433\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 449, 'page_label': '434'}, page_content='bucketing donation amounts, 283–285\\ndonation statistics by occupation and\\nemployer, 280–283\\ndonation statistics by state, 285–287\\n== operator, 393\\n>>> prompt, 386\\n? (question mark), 49\\n[] (brackets), 406, 408\\n\\\\ (backslash), 397\\n_ (underscore), 48, 58\\n__ (two underscores), 58\\n{} (braces), 413\\n| operator, 91\\nA\\na file mode, 431\\nabs function, 96\\naccumulate method, 368\\nadd method, 95, 130, 417\\nadd_patch method, 229\\nadd_subplot method, 221\\naggfunc option, 277\\naggregate method, 260, 262\\naggregations, 100\\nalgorithms for sorting, 375–376\\nalignment of data, 330–331\\nall method, 101, 368\\nalpha argument, 233\\nand keyword, 398, 401\\nannotating in matplotlib, 228–230\\nanonymous functions, 424\\nany method, 101, 110, 201\\nappend method, 122, 408\\napply method, 39, 132, 142, 266–268, 270\\napt package management tool, 10\\narange function, 82\\narccos function, 96\\narccosh function, 96\\narcsin function, 96\\narcsinh function, 96\\narctan function, 96\\narctanh function, 96\\nargmax method, 101\\nargmin method, 101, 139\\nargsort method, 135, 374\\narithmetic, 128–132\\noperations between DataFrame and Series,\\n130–132\\nwith fill values, 129–130\\narrays\\nboolean arrays, 101\\nboolean indexing for, 89–92\\nconditional logic as operation, 98–100\\ncreating, 81–82\\ncreating PeriodIndex from, 312\\ndata types for, 83–85\\nfancy indexing, 92–93\\nfile input and output with, 103–105\\nsaving and loading text files, 104–105\\nstoring on disk in binary format, 103–\\n104\\nfinding elements in sorted array, 376–377\\nin NumPy, 355–362\\nconcatenating, 357–359\\nc_ object, 359\\nlayout of in memory, 356–357\\nreplicating, 360–361\\nreshaping, 355–356\\nr_ object, 359\\nsaving to file, 379–380\\nsplitting, 357–359\\nsubsets for, 361–362\\nindexes for, 86–89\\noperations between, 85–86\\nsetting values by broadcasting, 367\\nslicing, 86–89\\nsorting, 101–102\\nstatistical methods for, 100\\nstructured arrays, 370–372\\nbenefits of, 372\\nmainpulating, 372\\nnested data types, 371–372\\nswapping axes in, 93–94\\ntransposing, 93–94\\nunique function, 102–103\\nwhere function, 98–100\\narrow function, 229\\nas keyword, 393\\nasarray function, 82, 379\\nasfreq method, 308, 318\\nasof method, 334–336\\nastype method, 84, 85\\nattributes\\nin Python, 391\\nstarting with underscore, 48\\naverage method, 136\\nax argument, 233\\naxes\\n434 | Index\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 450, 'page_label': '435'}, page_content='broadcasting over, 364–367\\nconcatenating along, 185–188\\nlabels for, 226–227\\nrenaming indexes for, 197–198\\nswapping in arrays, 93–94\\nAxesSubplot object, 221\\naxis argument, 188\\naxis method, 138\\nB\\nb file mode, 431\\nbackslash (\\\\), 397\\nbar plots, 235–238\\nBasemap object, 245, 246\\n.bashrc file, 10\\n.bash_profile file, 9\\nbbox_inches option, 231\\nbenefits\\nof Python, 2–3\\nglue for code, 2\\nsolving \"two-language\" problem with, 2–\\n3\\nof structured arrays, 372\\nbeta function, 107\\ndefined, 342\\nbetween_time method, 335\\nbfill method, 123\\nbin edges, 314\\nbinary data formats, 171–172\\nHDF5, 171–172\\nMicrosoft Excel files, 172\\nstoring arrays in, 103–104\\nbinary moving window functions, 324–325\\nbinary search of lists, 410\\nbinary universal functions, 96\\nbinding\\ndefined, 390\\nvariables, 425\\nbinomial function, 107\\nbisect module, 410\\nbookmarking directories in IPython, 62\\nBoolean\\narrays, 101\\ndata type, 84, 398\\nindexing for arrays, 89–92\\nbottleneck library, 324\\nbraces ({}), 413\\nbrackets ([]), 406, 408\\nbreak keyword, 401\\nbroadcasting, 362–367\\ndefined, 86, 360, 362\\nover other axes, 364–367\\nsetting array values by, 367\\nbucketing, 283–285\\nC\\ncalendar module, 290\\ncasting, 84\\ncat method, 156, 212\\nCategorical object, 199\\nceil function, 96\\ncenter method, 212\\nChaco, 248\\nchisquare function, 107\\nchunksize argument, 160, 161\\nclearing screen shortcut, 53\\nclipboard, executing code from, 50–52\\nclock function, 67\\nclose method, 220, 432\\nclosures, 425–426\\ncmd.exe, 7\\ncollections module, 416\\ncolons, 387\\ncols option, 277\\ncolumns, grouping on, 256–257\\ncolumn_stack function, 359\\ncombinations function, 430\\ncombine_first method, 177, 189\\ncombining\\ndata sources, 336–338\\ndata sources, with overlap, 188–189\\nlists, 409\\ncommands, 65\\n(see also magic commands)\\ndebugger, 65\\nhistory in IPython, 58–60\\ninput and output variables, 58–59\\nlogging of, 59–60\\nreusing command history, 58\\nsearching for, 53\\ncomment argument, 160\\ncomments in Python, 388\\ncompile method, 208\\ncomplex128 data type, 84\\ncomplex256 data type, 84\\ncomplex64 data type, 84\\nconcat function, 34, 177, 184, 185, 186, 267,\\n357, 359\\nIndex | 435\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 451, 'page_label': '436'}, page_content='concatenating\\nalong axis, 185–188\\narrays, 357–359\\nconditional logic as array operation, 98–100\\nconferences, 12\\nconfiguring matplotlib, 231–232\\nconforming, 122\\ncontains method, 212\\ncontiguous memory, 381–382\\ncontinue keyword, 401\\ncontinuous return, 348\\nconvention argument, 314\\nconverting\\nbetween string and datetime, 291–293\\ntimestamps to periods, 311\\ncoordinated universal time (UTC), 303\\ncopy argument, 181\\ncopy method, 118\\ncopysign function, 96\\ncorr method, 140\\ncorrelation, 139–141\\ncorrwith method, 140\\ncos function, 96\\ncosh function, 96\\ncount method, 139, 206, 212, 261, 407\\nCounter class, 21\\ncov method, 140\\ncovariance, 139–141\\nCPython, 7\\ncross-section, 329\\ncrosstab function, 277–278\\ncrowdsourcing, 241\\nCSV files, 163–165, 242\\nCtrl-A keyboard shortcut, 53\\nCtrl-B keyboard shortcut, 53\\nCtrl-C keyboard shortcut, 53\\nCtrl-E keyboard shortcut, 53\\nCtrl-F keyboard shortcut, 53\\nCtrl-K keyboard shortcut, 53\\nCtrl-L keyboard shortcut, 53\\nCtrl-N keyboard shortcut, 53\\nCtrl-P keyboard shortcut, 53\\nCtrl-R keyboard shortcut, 53\\nCtrl-Shift-V keyboard shortcut, 53\\nCtrl-U keyboard shortcut, 53\\ncummax method, 139\\ncummin method, 139\\ncumprod method, 100, 139\\ncumsum method, 100, 139\\ncumulative returns, 338–340\\ncurrying, 427\\ncursor, moving with keyboard, 53\\ncustom universal functions, 370\\ncut function, 199, 200, 201, 268, 283\\nCython project, 2, 382–383\\nc_ object, 359\\nD\\ndata aggregation, 259–264\\nreturning data in unindexed form, 264\\nusing multiple functions, 262–264\\ndata alignment, 128–132\\narithmetic methods with fill values, 129–\\n130\\noperations between DataFrame and Series,\\n130–132\\ndata munging, 329–340\\nasof method, 334–336\\ncombining data, 336–338\\nfor data alignment, 330–331\\nfor specialized frequencies, 332–334\\ndata structures for pandas, 112–121\\nDataFrame, 115–120\\nIndex objects, 120–121\\nPanel, 152–154\\nSeries, 112–115\\ndata types\\nfor arrays, 83–85\\nfor ndarray, 83–85\\nfor NumPy, 353–354\\nhierarchy of, 354\\nfor Python, 395–400\\nboolean data type, 398\\ndates and times, 399–400\\nNone data type, 399\\nnumeric data types, 395–396\\nstr data type, 396–398\\ntype casting in, 399\\nfor time series data, 290–293\\nconverting between string and datetime,\\n291–293\\nnested, 371–372\\ndata wrangling\\nmanipulating strings, 205–211\\nmethods for, 206–207\\nvectorized string methods, 210–211\\nwith regular expressions, 207–210\\nmerging data, 177–189\\n436 | Index\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 452, 'page_label': '437'}, page_content='combining data with overlap, 188–189\\nconcatenating along axis, 185–188\\nDataFrame merges, 178–181\\non index, 182–184\\npivoting, 192–193\\nreshaping, 190–191\\ntransforming data, 194–205\\ndiscretization, 199–201\\ndummy variables, 203–205\\nfiltering outliers, 201–202\\nmapping, 195–196\\npermutation, 202\\nremoving duplicates, 194–195\\nrenaming axis indexes, 197–198\\nreplacing values, 196–197\\nUSDA food database example, 212–217\\ndatabases\\nreading and writing to, 174–176\\nDataFrame data structure, 22, 27, 112, 115–\\n120\\narithmetic operations between Series and,\\n130–132\\nhierarchical indexing using, 150–151\\nmerging data with, 178–181\\ndates and times, 291\\n(see also time series data)\\ndata types for, 291, 399–400\\ndate ranges, 298\\ndatetime type, 291–293, 395, 399\\nDatetimeIndex Index object, 121\\ndateutil package, 291\\ndate_parser argument, 160\\ndate_range function, 298\\ndayfirst argument, 160\\ndebug function, 66\\ndebugger, IPython\\nin IPython, 62–66\\ndef keyword, 420\\ndefaults\\nprofiles, 77\\nvalues for dicts, 415–416\\ndel keyword, 59, 118, 414\\ndelete method, 122\\ndelimited formats, 163–165\\ndensity plots, 238–239\\ndescribe method, 138, 243, 267\\ndesign tips, 74–76\\nflat is better than nested, 75\\nkeeping relevant objects and data alive, 75\\novercoming fear of longer files, 75–76\\ndet function, 106\\ndevelopment tools in IPython, 62–72\\ndebugger, 62–66\\nprofiling code, 68–70\\nprofiling function line-by-line, 70–72\\ntiming code, 67–68\\ndiag function, 106\\ndicts, 413–416\\ncreating, 415\\ndefault values for, 415–416\\ndict comprehensions, 418–420\\ngrouping on, 257–258\\nkeys for, 416\\nreturning system environment variables as,\\n60\\ndiff method, 122, 139\\ndifference method, 417\\ndigitize function, 377\\ndirectories\\nbookmarking in IPython, 62\\nchanging, commands for, 60\\ndiscretization, 199–201\\ndiv method, 130\\ndivide function, 96\\n.dmg file, 9\\ndonation statistics\\nby occupation and employer, 280–283\\nby state, 285–287\\ndot function, 105, 106, 377\\ndoublequote option, 165\\ndownsampling, 312\\ndpi (dots-per-inch) option, 231\\ndreload function, 74\\ndrop method, 122, 125\\ndropna method, 143\\ndrop_duplicates method, 194\\ndsplit function, 359\\ndstack function, 359\\ndtype object (see data types)\\n“duck” typing in Python, 392\\ndummy variables, 203–205\\ndumps function, 165\\nduplicated method, 194\\nduplicates\\nindices, 296–297\\nremoving from data, 194–195\\ndynamically-generated functions, 425\\nIndex | 437\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 453, 'page_label': '438'}, page_content='E\\nedgecolo option, 231\\nedit-compile-run workflow, 45\\neig function, 106\\nelif blocks (see if statements)\\nelse block (see if statements)\\nempty function, 82, 83\\nempty namespace, 50\\nencoding argument, 160\\nendswith method, 207, 212\\nenumerate function, 412\\nenvironment variables, 8, 60\\nEPD (Enthought Python Distribution), 7–9\\nequal function, 96\\nescapechar option, 165\\newma function, 323\\newmcorr function, 323\\newmcov function, 323\\newmstd function, 323\\newmvar function, 323\\nExcelFile class, 172\\nexcept block, 403\\nexceptions\\nautomatically entering debugger after, 55\\ndefined, 402\\nhandling in Python, 402–404\\nexec keyword, 59\\nexecute-explore workflow, 45\\nexecution time\\nof code, 55\\nof single statement, 55\\nexit command, 386\\nexp function, 96\\nexpanding window mean, 322\\nexponentially-weighted functions, 324\\nextend method, 409\\nextensible markup language (XML) files, 169–\\n170\\neye function, 83\\nF\\nfabs function, 96\\nfacecolor option, 231\\nfactor analysis, 342–343\\nFactor object, 269\\nfactors, 342\\nfancy indexing\\ndefined, 361\\nfor arrays, 92–93\\nffill method, 123\\nfigsize argument, 234\\nFigure object, 220, 223\\nfile input/output\\nbinary data formats for, 171–172\\nHDF5, 171–172\\nMicrosoft Excel files, 172\\nfor arrays, 103–105\\nHDF5, 380\\nmemory-mapped files, 379–380\\nsaving and loading text files, 104–105\\nstoring on disk in binary format, 103–\\n104\\nin Python, 430–431\\nsaving plot to file, 231\\ntext files, 155–170\\ndelimited formats, 163–165\\nHTML files, 166–170\\nJSON data, 165–166\\nlxml library, 166–170\\nreading in pieces, 160–162\\nwriting to, 162–163\\nXML files, 169–170\\nwith databases, 174–176\\nwith Web APIs, 173–174\\nfilling in missing data, 145–146, 270–271\\nfillna method, 22, 143, 145, 146, 196, 270,\\n317\\nfill_method argument, 313\\nfill_value option, 277\\nfiltering\\nin pandas, 125–128\\nmissing data, 143–144\\noutliers, 201–202\\nfinancial applications\\ncumulative returns, 338–340\\ndata munging, 329–340\\nasof method, 334–336\\ncombining data, 336–338\\nfor data alignment, 330–331\\nfor specialized frequencies, 332–334\\nfuture contract rolling, 347–350\\ngrouping for, 340–345\\nfactor analysis with, 342–343\\nquartile analysis, 343–345\\nlinear regression, 350–351\\nreturn indexes, 338–340\\nrolling correlation, 350–351\\n438 | Index\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 454, 'page_label': '439'}, page_content='signal frontier analysis, 345–347\\nfind method, 206, 207\\nfindall method, 167, 208, 210, 212\\nfinditer method, 210\\nfirst crossing time, 109\\nfirst method, 136, 261\\nflat is better than nested, 75\\nflattening, 356\\nfloat data type, 83, 354, 395, 396, 399\\nfloat function, 402\\nfloat128 data type, 84\\nfloat16 data type, 84\\nfloat32 data type, 84\\nfloat64 data type, 84\\nfloor function, 96\\nfloor_divide function, 96\\nflow control, 400–405\\nexception handling, 402–404\\nfor loops, 401–402\\nif statements, 400–401\\npass statements, 402\\nrange function, 404–405\\nternary expressions, 405\\nwhile loops, 402\\nxrange function, 404–405\\nflush method, 432\\nfmax function, 96\\nfmin function, 96\\nfname option, 231\\nfor loops, 85, 100, 401–402, 418, 419\\nformat option, 231\\nfrequencies, 299–301\\nconverting, 308\\nspecialized frequencies, 332–334\\nweek of month dates, 301\\nfrompyfunc function, 370\\nfrom_csv method, 163\\nfunctions, 389, 420–430\\nanonymous functions, 424\\nare objects, 422–423\\nclosures, 425–426\\ncurrying of, 427\\nextended call syntax for, 426\\nlambda functions, 424\\nnamespaces for, 420–421\\nparsing in pandas, 155\\nreturning multiple values from, 422\\nscope of, 420–421\\nfunctools module, 427\\nfuture contract rolling, 347–350\\nfutures, 347\\nG\\ngamma function, 107\\ngcc command, 9, 11\\ngenerators, 427–430\\ndefined, 428\\ngenerator expressions, 429\\nitertools module for, 429–430\\nget method, 167, 172, 212, 415\\ngetattr function, 391\\nget_chunk method, 162\\nget_dummies function, 203, 205\\nget_value method, 128\\nget_xlim method, 226\\nGIL (global interpreter lock), 3\\nglobal scope, 420, 421\\nglue for code\\nPython as, 2\\n.gov domain, 17\\nGranger, Brian, 72\\ngraphics\\nChaco, 248\\nmayavi, 248\\ngreater function, 96\\ngreater_equal function, 96\\ngrid argument, 234\\ngroup keys, 268\\ngroupby method, 39, 252–259, 297, 316, 343,\\n377, 429\\niterating over groups, 255–256\\non column, 256–257\\non dict, 257–258\\non levels, 259\\nresampling with, 316\\nusing functions with, 258–259\\nwith Series, 257–258\\ngrouping\\n2012 Federal Election Commission database\\nexample, 278–287\\nbucketing donation amounts, 283–285\\ndonation statistics by occupation and\\nemployer, 280–283\\ndonation statistics by state, 285–287\\napply method, 266–268\\ndata aggregation, 259–264\\nreturning data in unindexed form, 264\\nusing multiple functions, 262–264\\nIndex | 439\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 455, 'page_label': '440'}, page_content='filling missing values with group-specific\\nvalues, 270–271\\nfor financial applications, 340–345\\nfactor analysis with, 342–343\\nquartile analysis, 343–345\\ngroup weighted average, 273–274\\ngroupby method, 252–259\\niterating over groups, 255–256\\non column, 256–257\\non dict, 257–258\\non levels, 259\\nusing functions with, 258–259\\nwith Series, 257–258\\nlinear regression for, 274–275\\npivot tables, 275–278\\ncross-tabulation, 277–278\\nquantile analysis with, 268–269\\nrandom sampling with, 271–272\\nH\\nHaiti earthquake crisis data example, 241–247\\nhalf-open, 314\\nhasattr function, 391\\nhash mark (#), 388\\nhashability, 416\\nHDF5 (hierarchical data format), 171–172,\\n380\\nHDFStore class, 171\\nheader argument, 160\\nheapsort sorting method, 376\\nhierarchical data format (HDF5), 171–172,\\n380\\nhierarchical indexing\\nin pandas, 147–151\\nsorting levels, 149–150\\nsummary statistics by level, 150\\nwith DataFrame columns, 150–151\\nreshaping data with, 190–191\\nhist method, 238\\nhistograms, 238–239\\nhistory of commands, searching, 53\\nhomogeneous data container, 370\\nhow argument, 181, 313, 316\\nhsplit function, 359\\nhstack function, 358\\nHTML files, 166–170\\nHTML Notebook in IPython, 72\\nHunter, John D., 5, 219\\nhyperbolic trigonometric functions, 96\\nI\\nicol method, 128, 152\\nIDEs (Integrated Development Environments),\\n11, 52\\nidxmax method, 138\\nidxmin method, 138\\nif statements, 400–401, 415\\nifilter function, 430\\niget_value method, 152\\nignore_index argument, 188\\nimap function, 430\\nimport directive\\nin Python, 392–393\\nusage of in this book, 13\\nimshow function, 98\\nin keyword, 409\\nin-place sort, 373\\nin1d method, 103\\nindentation\\nin Python, 387–388\\nIndentationError event, 51\\nindex method, 206, 207\\nIndex objects data structure, 120–121\\nindexes\\ndefined, 112\\nfor arrays, 86–89\\nfor axis, 197–198\\nfor TimeSeries class, 294–296\\nhierarchical indexing, 147–151\\nreshaping data with, 190–191\\nsorting levels, 149–150\\nsummary statistics by level, 150\\nwith DataFrame columns, 150–151\\nin pandas, 136\\ninteger indexing, 151–152\\nmerging data on, 182–184\\nindex_col argument, 160\\nindirect sorts, 374–375, 374\\ninput variables, 58–59\\ninsert method, 122, 408\\ninsort method, 410\\nint data type, 83, 395, 399\\nint16 data type, 84\\nint32 data type, 84\\nint64 data type, 84\\nInt64Index Index object, 121\\nint8 data type, 84\\ninteger arrays, indexing using (see fancy\\nindexing)\\n440 | Index\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 456, 'page_label': '441'}, page_content='integer indexing, 151–152\\nIntegrated Development Environments (IDEs),\\n11, 52\\ninterpreted languages\\ndefined, 386\\nPython interpreter, 386\\ninterrupting code, 50, 53\\nintersect1d method, 103\\nintersection method, 122, 417\\nintervals of time, 289\\ninv function, 106\\ninverse trigonometric functions, 96\\n.ipynb files, 72\\nIPython, 5\\nbookmarking directories, 62\\ncommand history in, 58–60\\ninput and output variables, 58–59\\nlogging of, 59–60\\nreusing command history, 58\\ndesign tips, 74–76\\nflat is better than nested, 75\\nkeeping relevant objects and data alive,\\n75\\novercoming fear of longer files, 75–76\\ndevelopment tools, 62–72\\ndebugger, 62–66\\nprofiling code, 68–70\\nprofiling function line-by-line, 70–72\\ntiming code, 67–68\\nexecuting code from clipboard, 50–52\\nHTML Notebook in, 72\\nintegration with IDEs and editors, 52\\nintegration with mathplotlib, 56–57\\nkeyboard shortcuts for, 52\\nmagic commands in, 54–55\\nmaking classes output correctly, 76\\nobject introspection in, 48–49\\nprofiles for, 77–78\\nQt console for, 55\\nQuick Reference Card for, 55\\nreloading module dependencies, 74\\n%run command in, 49–50\\nshell commands in, 60–61\\ntab completion in, 47–48\\ntracebacks in, 53–54\\nipython_config.py file, 77\\nirow method, 128, 152\\nis keyword, 393\\nisdisjoint method, 417\\nisfinite function, 96\\nisin method, 141–142\\nisinf function, 96\\nisinstance function, 391\\nisnull method, 96, 114, 143\\nissubdtype function, 354\\nissubset method, 417\\nissuperset method, 417\\nis_monotonic method, 122\\nis_unique method, 122\\niter function, 392\\niterating over groups, 255–256\\niterator argument, 160\\niterator protocol, 392, 427\\nitertools module, 429–430, 429\\nix_ function, 93\\nJ\\njoin method, 184, 206, 212\\nJSON (JavaScript Object Notation), 18, 165–\\n166, 213\\nK\\nKDE (kernel density estimate) plots, 239\\nkeep_date_col argument, 160\\nkernels, 239\\nkey-value pairs, 413\\nkeyboard shortcuts, 53\\nfor deleting text, 53\\nfor IPython, 52\\nKeyboardInterrupt event, 50\\nkeys\\nargument, 188\\nfor dicts, 416\\nmethod, 414\\nkeyword arguments, 389, 420\\nkind argument, 234, 314\\nkurt method, 139\\nL\\nlabel argument, 233, 313, 315\\nlambda functions, 211, 262, 424\\nlast method, 261\\nlayout of arrays in memory, 356–357\\nleft argument, 181\\nleft_index argument, 181\\nleft_on argument, 181\\nlegends in matplotlib, 228\\nIndex | 441\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 457, 'page_label': '442'}, page_content='len function, 212, 258\\nless function, 96\\nless_equal function, 96\\nlevel keyword, 259\\nlevels\\ndefined, 147\\ngrouping on, 259\\nsorting, 149–150\\nsummary statistics by, 150\\nlexicographical sort\\ndefined, 375\\nlexsort method, 374\\nlibraries, 3–6\\nIPython, 5\\nmatplotlib, 5\\nNumPy, 4\\npandas, 4–5\\nSciPy, 6\\nlimit argument, 313\\nlinalg function, 105\\nline plots, 232–235\\nlinear algebra, 105–106\\nlinear regression, 274–275, 350–351\\nlineterminator option, 164\\nline_profiler extension, 70\\nLinux, setting up on, 10–11\\nlist comprehensions, 418–420\\nnested list comprehensions, 419–420\\nlist function, 408\\nlists, 408–411\\nadding elements to, 408–409\\nbinary search of, 410\\ncombining, 409\\ninsertion into sorted, 410\\nlist comprehensions, 418–420\\nremoving elements from, 408–409\\nslicing, 410–411\\nsorting, 409–410\\nljust method, 207\\nload function, 103, 379\\nload method, 171\\nloads function, 18\\nlocal scope, 420\\nlocalizing time series data, 304–305\\nloffset argument, 313, 316\\nlog function, 96\\nlog1p function, 96\\nlog2 function, 96\\nlogging command history in IPython, 59–60\\nlogical_and function, 96\\nlogical_not function, 96\\nlogical_or function, 96\\nlogical_xor function, 96\\nlogy argument, 234\\nlong format, 192\\nlong type, 395\\nlonger files overcoming fear of, 75–76\\nlower method, 207, 212\\nlstrip method, 207, 212\\nlstsq function, 106\\nlxml library, 166–170\\nM\\nmad method, 139\\nmagic methods, 48, 54–55\\nmain function, 75\\nmainpulating structured arrays, 372\\nmany-to-many merge, 179\\nmany-to-one merge, 178\\nmap method, 133, 195–196, 211, 280, 423\\nmargins, 275\\nmarkers, 224\\nmatch method, 208–212\\nmatplotlib, 5, 219–232\\nannotating in, 228–230\\naxis labels in, 226–227\\nconfiguring, 231–232\\nintegrating with IPython, 56–57\\nlegends in, 228\\nsaving to file, 231\\nstyling for, 224–225\\nsubplots in, 220–224\\nticks in, 226–227\\ntitle in, 226–227\\nmatplotlibrc file, 232\\nmatrix operations in NumPy, 377–379\\nmax method, 101, 136, 139, 261, 428\\nmaximum function, 95, 96\\nmayavi, 248\\nmean method, 100, 139, 253, 259, 261, 265\\nmedian method, 139, 261\\nmemmap object, 379\\nmemory, layout of arrays in, 356–357\\nmemory-mapped files\\ndefined, 379\\nsaving arrays to file, 379–380\\nmergesort sorting method, 375, 376\\nmerging data, 177–189\\n442 | Index\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 458, 'page_label': '443'}, page_content='combining data with overlap, 188–189\\nconcatenating along axis, 185–188\\nDataFrame merges, 178–181\\non index, 182–184\\nmeshgrid function, 97\\nmethods\\ndefined, 389\\nfor tuples, 407\\nin Python, 389\\nstarting with underscore, 48\\nMicrosoft Excel files, 172\\n.mil domain, 17\\nmin method, 101, 136, 139, 261, 428\\nminimum function, 96\\nmissing data, 142–146\\nfilling in, 145–146\\nfiltering out, 143–144\\nmod function, 96\\nmodf function, 95\\nmodules, 392\\nmomentum, 343\\nMongoDB, 176\\nMovieLens 1M data set example, 26–31\\nmoving window functions, 320–326\\nbinary moving window functions, 324–325\\nexponentially-weighted functions, 324\\nuser-defined, 326\\n.mpkg file, 9\\nmro method, 354\\nmul method, 130\\nMultiIndex Index object, 121, 147, 149\\nmultiple profiles, 77\\nmultiply function, 96\\nmunging, 13\\nmutable objects, 394–395\\nN\\nNA data type, 143\\nnames argument, 160, 188\\nnamespaces\\ndefined, 420\\nin Python, 420–421\\nnaming trends\\nin US baby names 1880-2010 example, 36–\\n43\\nboy names that became girl names, 42–\\n43\\nmeasuring increase in diversity, 37–40\\nrevolution of last letter, 40–41\\nNaN (not a number), 101, 114, 143\\nna_values argument, 160\\nncols option, 223\\nndarray, 80\\nBoolean indexing, 89–92\\ncreating arrays, 81–82\\ndata types for, 83–85\\nfancy indexing, 92–93\\nindexes for, 86–89\\noperations between arrays, 85–86\\nslicing arrays, 86–89\\nswapping axes in, 93–94\\ntransposing, 93–94\\nnested code, 75\\nnested data types, 371–372\\nnested list comprehensions, 419–420\\nNew York MTA (Metropolitan Transportation\\nAuthority), 169\\nNone data type, 395, 399\\nnormal function, 107, 110\\nnormalized timestamps, 298\\nNoSQL databases, 176\\nnot a number (NaN), 101, 114, 143\\nNotebookCloud, 72\\nnotnull method, 114, 143\\nnot_equal function, 96\\n.npy files, 103\\n.npz files, 104\\nnrows argument, 160, 223\\nnuisance column, 254\\nnumeric data types, 395–396\\nNumPy, 4\\narrays in, 355–362\\nconcatenating, 357–359\\nc_ object, 359\\nlayout of in memory, 356–357\\nreplicating, 360–361\\nreshaping, 355–356\\nr_ object, 359\\nsaving to file, 379–380\\nsplitting, 357–359\\nsubsets for, 361–362\\nbroadcasting, 362–367\\nover other axes, 364–367\\nsetting array values by, 367\\ndata processing using\\nwhere function, 98–100\\ndata processing using arrays, 97–103\\nIndex | 443\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 459, 'page_label': '444'}, page_content='conditional logic as array operation, 98–\\n100\\nmethods for boolean arrays, 101\\nsorting arrays, 101–102\\nstatistical methods, 100\\nunique function, 102–103\\ndata types for, 353–354\\nfile input and output with arrays, 103–105\\nsaving and loading text files, 104–105\\nstoring on disk in binary format, 103–\\n104\\nlinear algebra, 105–106\\nmatrix operations in, 377–379\\nndarray arrays, 80\\nBoolean indexing, 89–92\\ncreating, 81–82\\ndata types for, 83–85\\nfancy indexing, 92–93\\nindexes for, 86–89\\noperations between arrays, 85–86\\nslicing arrays, 86–89\\nswapping axes in, 93–94\\ntransposing, 93–94\\nnumpy-discussion (mailing list), 12\\nperformance of, 380–383\\ncontiguous memory, 381–382\\nCython project, 382–383\\nrandom number generation, 106–107\\nrandom walks example, 108–110\\nsorting, 373–377\\nalgorithms for, 375–376\\nfinding elements in sorted array, 376–\\n377\\nindirect sorts, 374–375\\nstructured arrays in, 370–372\\nbenefits of, 372\\nmainpulating, 372\\nnested data types, 371–372\\nuniversal functions for, 95–96, 367–370\\ncustom, 370\\nin pandas, 132–133\\ninstance methods for, 368–369\\nO\\nobject introspection, 48–49\\nobject model, 388\\nobject type, 84\\nobjectify function, 166, 169\\nobjs argument, 188\\noffsets for time series data, 302–303\\nOHLC (Open-High-Low-Close) resampling,\\n316\\nols function, 351\\nOlson database, 303\\non argument, 181\\nones function, 82\\nopen function, 430\\nOpen-High-Low-Close (OHLC) resampling,\\n316\\noperators in Python, 393\\nor keyword, 401\\norder method, 375\\nOS X, setting up Python on, 9–10\\nouter method, 368, 369\\noutliers, filtering, 201–202\\noutput variables, 58–59\\nP\\npad method, 212\\npairs plot, 241\\npandas, 4–5\\narithmetic and data alignment, 128–132\\narithmetic methods with fill values, 129–\\n130\\noperations between DataFrame and\\nSeries, 130–132\\ndata structures for, 112–121\\nDataFrame, 115–120\\nIndex objects, 120–121\\nPanel, 152–154\\nSeries, 112–115\\ndrop function, 125\\nfiltering in, 125–128\\nhandling missing data, 142–146\\nfilling in, 145–146\\nfiltering out, 143–144\\nhierarchical indexing in, 147–151\\nsorting levels, 149–150\\nsummary statistics by level, 150\\nwith DataFrame columns, 150–151\\nindexes in, 136\\nindexing options, 125–128\\ninteger indexing, 151–152\\nNumPy universal functions with, 132–133\\nplotting with, 232\\nbar plots, 235–238\\ndensity plots, 238–239\\nhistograms, 238–239\\n444 | Index\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 460, 'page_label': '445'}, page_content='line plots, 232–235\\nscatter plots, 239–241\\nranking data in, 133–135\\nreductions in, 137–142\\nreindex function, 122–124\\nselecting in objects, 125–128\\nsorting in, 133–135\\nsummary statistics in\\ncorrelation and covariance, 139–141\\nisin function, 141–142\\nunique function, 141–142\\nvalue_counts function, 141–142\\nusa.gov data from bit.ly example with, 21–\\n26\\nPanel data structure, 152–154\\npanels, 329\\nparse method, 291\\nparse_dates argument, 160\\npartial function, 427\\npartial indexing, 147\\npass statements, 402\\npassing by reference, 390\\npasting\\nkeyboard shortcut for, 53\\nmagic command for, 55\\npatches, 229\\npath argument, 160\\nPath variable, 8\\npct_change method, 139\\npdb debugger, 62\\n.pdf files, 231\\npercentileofscore function, 326\\nPérez, Fernando, 45, 219\\nperformance\\nand time series data, 327–328\\nof NumPy, 380–383\\ncontiguous memory, 381–382\\nCython project, 382–383\\nPeriod class, 307\\nPeriodIndex Index object, 121, 311, 312\\nperiods, 307–312\\nconverting timestamps to, 311\\ncreating PeriodIndex from arrays, 312\\ndefined, 289, 307\\nfrequency conversion for, 308\\ninstead of timestamps, 333–334\\nquarterly periods, 309–310\\nresampling with, 318–319\\nperiod_range function, 307, 310\\npermutation, 202\\npickle serialization, 170\\npinv function, 106\\npivoting data\\ncross-tabulation, 277–278\\ndefined, 189\\npivot method, 192–193\\npivot_table method, 29, 275–278\\npivot_table aggregation type, 275\\nplot method, 23, 36, 41, 220, 224, 232, 239,\\n246, 319\\nplotting\\nHaiti earthquake crisis data example, 241–\\n247\\ntime series data, 319–320\\nwith matplotlib, 219–232\\nannotating in, 228–230\\naxis labels in, 226–227\\nconfiguring, 231–232\\nlegends in, 228\\nsaving to file, 231\\nstyling for, 224–225\\nsubplots in, 220–224\\nticks in, 226–227\\ntitle in, 226–227\\nwith pandas, 232\\nbar plots, 235–238\\ndensity plots, 238–239\\nhistograms, 238–239\\nline plots, 232–235\\nscatter plots, 239–241\\n.png files, 231\\npop method, 408, 414\\npositional arguments, 389\\npower function, 96\\npprint module, 76\\npretty printing\\nand displaying through pager, 55\\ndefined, 47\\nprivate attributes, 48\\nprivate methods, 48\\nprod method, 261\\nprofiles\\ndefined, 77\\nfor IPython, 77–78\\nprofile_default directory, 77\\nprofiling code\\nin IPython, 68–70\\npseudocode, 14\\nIndex | 445\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 461, 'page_label': '446'}, page_content='put function, 362\\nput method, 362\\n.py files, 50, 386, 392\\npydata (Google group), 12\\npylab mode, 219\\npymongo driver, 175\\npyplot module, 220\\npystatsmodels (mailing list), 12\\nPython\\nbenefits of using, 2–3\\nglue for code, 2\\nsolving \"two-language\" problem with, 2–\\n3\\ndata types for, 395–400\\nboolean data type, 398\\ndates and times, 399–400\\nNone data type, 399\\nnumeric data types, 395–396\\nstr data type, 396–398\\ntype casting in, 399\\ndict comprehensions in, 418–420\\ndicts in, 413–416\\ncreating, 415\\ndefault values for, 415–416\\nkeys for, 416\\nfile input/output in, 430–431\\nflow control in, 400–405\\nexception handling, 402–404\\nfor loops, 401–402\\nif statements, 400–401\\npass statements, 402\\nrange function, 404–405\\nternary expressions, 405\\nwhile loops, 402\\nxrange function, 404–405\\nfunctions in, 420–430\\nanonymous functions, 424\\nare objects, 422–423\\nclosures, 425–426\\ncurrying of, 427\\nextended call syntax for, 426\\nlambda functions, 424\\nnamespaces for, 420–421\\nreturning multiple values from, 422\\nscope of, 420–421\\ngenerators in, 427–430\\ngenerator expressions, 429\\nitertools module for, 429–430\\nIDEs for, 11\\ninterpreter for, 386\\nlist comprehensions in, 418–420\\nlists in, 408–411\\nadding elements to, 408–409\\nbinary search of, 410\\ncombining, 409\\ninsertion into sorted, 410\\nremoving elements from, 408–409\\nslicing, 410–411\\nsorting, 409–410\\nPython 2 vs. Python 3, 11\\nrequired libraries, 3–6\\nIPython, 5\\nmatplotlib, 5\\nNumPy, 4\\npandas, 4–5\\nSciPy, 6\\nsemantics of, 387–395\\nattributes in, 391\\ncomments in, 388\\nfunctions in, 389\\nimport directive, 392–393\\nindentation, 387–388\\nmethods in, 389\\nmutable objects in, 394–395\\nobject model, 388\\noperators for, 393\\nreferences in, 389–390\\nstrict evaluation, 394\\nstrongly-typed language, 390–391\\nvariables in, 389–390\\n“duck” typing, 392\\nsequence functions in, 411–413\\nenumerate function, 412\\nreversed function, 413\\nsorted function, 412\\nzip function, 412–413\\nset comprehensions in, 418–420\\nsets in, 416–417\\nsetting up, 6–11\\non Linux, 10–11\\non OS X, 9–10\\non Windows, 7–9\\ntuples in, 406–407\\nmethods for, 407\\nunpacking, 407\\npytz library, 303\\n446 | Index\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 462, 'page_label': '447'}, page_content='Q\\nqcut method, 200, 201, 268, 269, 343\\nqr function, 106\\nQt console for IPython, 55\\nquantile analysis, 268–269\\nquarterly periods, 309–310\\nquartile analysis, 343–345\\nquestion mark (?), 49\\nquicksort sorting method, 376\\nquotechar option, 164\\nquoting option, 164\\nR\\nr file mode, 431\\nr+ file mode, 431\\nRamachandran, Prabhu, 248\\nrand function, 107\\nrandint function, 107, 202\\nrandn function, 89, 107\\nrandom number generation, 106–107\\nrandom sampling with grouping, 271–272\\nrandom walks example, 108–110\\nrange function, 82, 404–405\\nranking data\\ndefined, 135\\nin pandas, 133–135\\nravel method, 356, 357\\nrc method, 231, 232\\nre module, 207\\nread method, 432\\nread-only mode, 431\\nreading\\nfrom databases, 174–176\\nfrom text files in pieces, 160–162\\nreadline functionality, 58\\nreadlines method, 432\\nreadshapefile method, 246\\nread_clipboard function, 155\\nread_csv function, 104, 155, 161, 163, 261,\\n430\\nread_frame function, 175\\nread_fwf function, 155\\nread_table function, 104, 155, 158, 163\\nrecfunctions module, 372\\nreduce method, 368, 369\\nreduceat method, 369\\nreductions, 137\\n(see also aggregations)\\ndefined, 137\\nin pandas, 137–142\\nreferences\\ndefined, 389, 390\\nin Python, 389–390\\nregress function, 274\\nregular expressions (regex)\\ndefined, 207\\nmanipulating strings with, 207–210\\nreindex method, 122–124, 317, 332\\nreload function, 74\\nremove method, 408, 417\\nrename method, 198\\nrenaming axis indexes, 197–198\\nrepeat method, 212, 360\\nreplace method, 196, 206, 212\\nreplicating arrays, 360–361\\nresampling, 312–319, 332\\ndefined, 312\\nOHLC (Open-High-Low-Close)\\nresampling, 316\\nupsampling, 316–317\\nwith groupby method, 316\\nwith periods, 318–319\\nreset_index function, 151\\nreshape method, 190–191, 355, 365\\nreshaping\\narrays, 355–356\\ndefined, 189\\nwith hierarchical indexing, 190–191\\nresources, 12\\nreturn statements, 420\\nreturns\\ncumulative returns, 338–340\\ndefined, 338\\nreturn indexes, 338–340\\nreversed function, 413\\nrfind method, 207\\nright argument, 181\\nright_index argument, 181\\nright_on argument, 181\\nrint function, 96\\nrjust method, 207\\nrollback method, 302\\nrollforward method, 302\\nrolling, 348\\nrolling correlation, 350–351\\nrolling_apply function, 323, 326\\nrolling_corr function, 323, 350\\nIndex | 447\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 463, 'page_label': '448'}, page_content='rolling_count function, 323\\nrolling_cov function, 323\\nrolling_kurt function, 323\\nrolling_mean function, 321, 323\\nrolling_median function, 323\\nrolling_min function, 323\\nrolling_mint function, 323\\nrolling_quantile function, 323, 326\\nrolling_skew function, 323\\nrolling_std function, 323\\nrolling_sum function, 323\\nrolling_var function, 323\\nrot argument, 234\\nrows option, 277\\nrow_stack function, 359\\nrstrip method, 207, 212\\nr_ object, 359\\nS\\nsave function, 103, 379\\nsave method, 171, 176\\nsavefig method, 231\\nsavez function, 104\\nsaving text files, 104–105\\nscatter method, 239\\nscatter plots, 239–241\\nscatter_matrix function, 241\\nScientific Python base, 7\\nSciPy library, 6\\nscipy-user (mailing list), 12\\nscope, 420–421\\nscreen, clearing, 53\\nscripting languages, 2\\nscripts, 2\\nsearch method, 208, 210\\nsearchsorted method, 376\\nseed function, 107\\nseek method, 432\\nsemantics, 387–395\\nattributes in, 391\\ncomments in, 388\\n“duck” typing, 392\\nfunctions in, 389\\nimport directive, 392–393\\nindentation, 387–388\\nmethods in, 389\\nmutable objects in, 394–395\\nobject model, 388\\noperators for, 393\\nreferences in, 389–390\\nstrict evaluation, 394\\nstrongly-typed language, 390–391\\nvariables in, 389–390\\nsemicolons, 388\\nsentinels, 143, 159\\nsep argument, 160\\nsequence functions, 411–413\\nenumerate function, 412\\nreversed function, 413\\nsorted function, 412\\nzip function, 412–413\\nSeries data structure, 112–115\\narithmetic operations between DataFrame\\nand, 130–132\\ngrouping with, 257–258\\nset comprehensions, 418–420\\nset function, 416\\nsetattr function, 391\\nsetdefault method, 415\\nsetdiff1d method, 103\\nsets/set comprehensions, 416–417\\nsetxor1d method, 103\\nset_index function, 151\\nset_index method, 193\\nset_title method, 226\\nset_trace function, 65\\nset_value method, 128\\nset_xlabel method, 226\\nset_xlim method, 226\\nset_xticklabels method, 226\\nset_xticks method, 226\\nshapefiles, 246\\nshapes, 80, 353\\nsharex option, 223, 234\\nsharey option, 223, 234\\nshell commands in IPython, 60–61\\nshifting in time series data, 301–303\\nshortcuts, keyboard, 53\\nfor deleting text, 53\\nfor IPython, 52\\nshuffle function, 107\\nsign function, 96, 202\\nsignal frontier analysis, 345–347\\nsin function, 96\\nsinh function, 96\\nsize method, 255\\nskew method, 139\\nskipinitialspace option, 165\\n448 | Index\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 464, 'page_label': '449'}, page_content='skipna method, 138\\nskipna option, 137\\nskiprows argument, 160\\nskip_footer argument, 160\\nslice method, 212\\nslicing\\narrays, 86–89\\nlists, 410–411\\nSocial Security Administration (SSA), 32\\nsolve function, 106\\nsort argument, 181\\nsort method, 101, 373, 409, 424\\nsorted function, 412\\nsorting\\narrays, 101–102\\nfinding elements in sorted array, 376–377\\nin NumPy, 373–377\\nalgorithms for, 375–376\\nfinding elements in sorted array, 376–\\n377\\nindirect sorts, 374–375\\nin pandas, 133–135\\nlevels, 149–150\\nlists, 409–410\\nsortlevel function, 149\\nsort_columns argument, 235\\nsort_index method, 133, 150, 375\\nspaces, structuring code with, 387–388\\nspacing around subplots, 223–224\\nspan, 324\\nspecialized frequencies\\ndata munging for, 332–334\\nsplit method, 165, 206, 210, 212, 358\\nsplit-apply-combine, 252\\nsplitting arrays, 357–359\\nSQL databases, 175\\nsql module, 175\\nSQLite databases, 174\\nsqrt function, 95, 96\\nsquare function, 96\\nsqueeze argument, 160\\nSSA (Social Security Administration), 32\\nstable sorting, 375\\nstacked format, 192\\nstart index, 411\\nstartswith method, 207, 212\\nstatistical methods, 100\\nstd method, 101, 139, 261\\nstdout, 162\\nstep index, 411\\nstop index, 411\\nstrftime method, 291, 400\\nstrict evaluation/language, 394\\nstrides/strided view, 353\\nstrings\\nconverting to datetime, 291–293\\ndata types for, 84, 396–398\\nmanipulating, 205–211\\nmethods for, 206–207\\nvectorized string methods, 210–211\\nwith regular expressions, 207–210\\nstrip method, 207, 212\\nstrongly-typed languages, 390–391, 390\\nstrptime method, 291, 400\\nstructs, 370\\nstructured arrays, 370–372\\nbenefits of, 372\\ndefined, 370\\nmainpulating, 372\\nnested data types, 371–372\\nstyle argument, 233\\nstyling for matplotlib, 224–225\\nsub method, 130, 209\\nsubn method, 210\\nsubperiod, 319\\nsubplots, 220–224\\nsubplots method, 222\\nsubplots_adjust method, 223\\nsubplot_kw option, 223\\nsubsets for arrays, 361–362\\nsubtract function, 96\\nsudo command, 11\\nsuffixes argument, 181\\nsum method, 100, 132, 137, 139, 259, 261, 330,\\n428\\nsummary statistics, 137\\nby level, 150\\ncorrelation and covariance, 139–141\\nisin function, 141–142\\nunique function, 141–142\\nvalue_counts function, 141–142\\nsuperperiod, 319\\nsvd function, 106\\nswapaxes method, 94\\nswaplevel function, 149\\nswapping axes in arrays, 93–94\\nsymmetric_difference method, 417\\nsyntactic sugar, 14\\nIndex | 449\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 465, 'page_label': '450'}, page_content='system commands, defining alias for, 60\\nT\\ntab completion in IPython, 47–48\\ntabs, structuring code with, 387–388\\ntake method, 202, 362\\ntan function, 96\\ntanh function, 96\\ntell method, 432\\nterminology, 13–14\\nternary expressions, 405\\ntext editors, integrating with IPython, 52\\ntext files, 155–170\\ndelimited formats, 163–165\\nHTML files, 166–170\\nJSON data, 165–166\\nlxml library, 166–170\\nreading in pieces, 160–162\\nsaving and loading, 104–105\\nwriting to, 162–163\\nXML files, 169–170\\nTextParser class, 160, 162, 168\\ntext_content method, 167\\nthousands argument, 160\\nthresh argument, 144\\nticks, 226–227\\ntile function, 360, 361\\ntime series data\\nand performance, 327–328\\ndata types for, 290–293\\nconverting between string and datetime,\\n291–293\\ndate ranges, 298\\nfrequencies, 299–301\\nweek of month dates, 301\\nmoving window functions, 320–326\\nbinary moving window functions, 324–\\n325\\nexponentially-weighted functions, 324\\nuser-defined, 326\\nperiods, 307–312\\nconverting timestamps to, 311\\ncreating PeriodIndex from arrays, 312\\nfrequency conversion for, 308\\nquarterly periods, 309–310\\nplotting, 319–320\\nresampling, 312–319\\nOHLC (Open-High-Low-Close)\\nresampling, 316\\nupsampling, 316–317\\nwith groupby method, 316\\nwith periods, 318–319\\nshifting in, 301–303\\nwith offsets, 302–303\\ntime zones in, 303–306\\nlocalizing objects, 304–305\\nmethods for time zone-aware objects,\\n305–306\\nTimeSeries class, 293–297\\nduplicate indices with, 296–297\\nindexes for, 294–296\\nselecting data in, 294–296\\ntimestamps\\nconverting to periods, 311\\ndefined, 289\\nusing periods instead of, 333–334\\ntiming code, 67–68\\ntitle in matplotlib, 226–227\\ntop method, 267, 282\\nto_csv method, 162, 163\\nto_datetime method, 292\\nto_panel method, 154\\nto_period method, 311\\ntrace function, 106\\ntracebacks, 53–54\\ntransform method, 264–266\\ntransforming data, 194–205\\ndiscretization, 199–201\\ndummy variables, 203–205\\nfiltering outliers, 201–202\\nmapping, 195–196\\npermutation, 202\\nremoving duplicates, 194–195\\nrenaming axis indexes, 197–198\\nreplacing values, 196–197\\ntranspose method, 93, 94\\ntransposing arrays, 93–94\\ntrellis package, 247\\ntrigonometric functions, 96\\ntruncate method, 296\\ntry/except block, 403, 404\\ntuples, 406–407\\nmethods for, 407\\nunpacking, 407\\ntype casting, 399\\ntype command, 156\\nTypeError event, 84, 403\\ntypes, 388\\n450 | Index\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 466, 'page_label': '451'}, page_content='tz_convert method, 305\\ntz_localize method, 304, 305\\nU\\nU file mode, 431\\nuint16 data type, 84\\nuint32 data type, 84\\nuint64 data type, 84\\nuint8 data type, 84\\nunary functions, 95\\nunderscore (_), 48, 58\\nunicode type, 19, 84, 395\\nuniform function, 107\\nunion method, 103, 122, 204, 417\\nunique method, 102–103, 122, 141–142, 279\\nuniversal functions, 95–96, 367–370\\ncustom, 370\\nin pandas, 132–133\\ninstance methods for, 368–369\\nuniversal newline mode, 431\\nunpacking tuples, 407\\nunstack function, 148\\nupdate method, 337\\nupper method, 207, 212\\nupsampling, 312, 316–317\\nUS baby names 1880-2010 example, 32–43\\nboy names that became girl names, 42–43\\nmeasuring increase in diversity, 37–40\\nrevolution of last letter, 40–41\\nusa.gov data from bit.ly example, 17–26\\nUSDA (US Department of Agriculture) food\\ndatabase example, 212–217\\nuse_index argument, 234\\nUTC (coordinated universal time), 303\\nV\\nValueError event, 402, 403\\nvalues method, 414\\nvalue_counts method, 141–142\\nvar method, 101, 139, 261\\nvariables, 55\\n(see also environment variables)\\ndeleting, 55\\ndisplaying, 55\\nin Python, 389–390\\nVaroquaux, Gaël, 248\\nvectorization, 85\\ndefined, 97\\nvectorize function, 370\\nvectorized string methods, 210–211\\nverbose argument, 160\\nverify_integrity argument, 188\\nviews, 86, 118\\nvisualization tools\\nChaco, 248\\nmayavi, 248\\nvsplit function, 359\\nvstack function, 358\\nW\\nw file mode, 431\\nWattenberg, Laura, 40\\nWeb APIs, file input/output with, 173–174\\nweek of month dates, 301\\nwhen expressions, 394\\nwhere function, 98–100, 188\\nwhile loops, 402\\nwhitespace, structuring code with, 387–388\\nWickham, Hadley, 252\\nWilliams, Ashley, 212\\nWindows, setting up Python on, 7–9\\nworking directory\\nchanging to passed directory, 60\\nof current system, returning, 60\\nwrangling (see data wrangling)\\nwrite method, 431\\nwrite-only mode, 431\\nwritelines method, 431\\nwriter method, 165\\nwriting\\nto databases, 174–176\\nto text files, 162–163\\nX\\nXcode, 9\\nxlim method, 225, 226\\nXML (extensible markup language) files, 169–\\n170\\nxrange function, 404–405\\nxs method, 128\\nxticklabels method, 225\\nY\\nyield keyword, 428\\nylim argument, 234\\nyticks argument, 234\\nIndex | 451\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 467, 'page_label': '452'}, page_content='Z\\nzeros function, 82\\nzip function, 412–413\\n452 | Index\\nwww.it-ebooks.info'),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 468, 'page_label': '453'}, page_content=\"About the Author\\nWes McKinney is a New York −based data hacker and entrepreneur. After finishing\\nhis undergraduate degree in mathematics at MIT in 2007, he went on to do quantitative\\nfinance work at AQR Capital Management in Greenwich, CT. Frustrated by cumber-\\nsome data analysis tools, he learned Python and in 2008, started building what would\\nlater become the pandas project. He's now an active member of the scientific Python\\ncommunity and is an advocate for the use of Python in data analysis, finance, and\\nstatistical computing applications.\\nColophon\\nThe animal on the cover of Python for Data Analysis is a golden-tailed, or pen-tailed,\\ntree shrew (Ptilocercus lowii). The golden-tailed tree shrew is the only one of its species\\nin the genus Ptilocercus and family Ptilocercidae; all the other tree shrews are of the\\nfamily Tupaiidae. Tree shrews are identified by their long tails and soft red-brown fur.\\nAs nicknamed, the golden-tailed tree shrew has a tail that resembles the feather on a\\nquill pen. Tree shrews are omnivores, feeding primarily on insects, fruit, seeds, and\\nsmall vertebrates.\\nFound predominantly in Indonesia, Malaysia, and Thailand, these wild mammals are\\nknown for their chronic consumption of alcohol. Malaysian tree shrews were found to\\nspend several hours consuming the naturally fermented nectar of the bertam palm,\\nequalling about 10 to 12 glasses of wine with 3.8% alcohol content. Despite this, no\\ngolden-tailed tree shrew has ever been intoxicated, thanks largely to their impressive\\nethanol breakdown, which includes metabolizing the alcohol in a way not used by\\nhumans. Also more impressive than any of their mammal counterparts, including hu-\\nmans? Brain to body mass ratio.\\nDespite these mammals’ name, the golden-tailed shrew is not a true shrew, instead\\nmore closely related to primates. Because of their close relation, tree shrews have be-\\ncome an alternative to primates in medical experimentation for myopia, psychosocial\\nstress, and hepatitis.\\nThe cover image is from Cassel’s Natural History. The cover font is Adobe ITC Gara-\\nmond. The text font is Linotype Birka; the heading font is Adobe Myriad Condensed;\\nand the code font is LucasFont’s TheSansMonoCondensed.\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={'producer': 'A-PDF Watermark 4.5.11', 'creator': 'XSL Formatter V4.3 R1 (4,3,2008,0424) for Linux', 'creationdate': '2012-10-04T09:08:10-05:00', 'author': 'Wes McKinney', 'moddate': '2012-11-06T02:50:08+04:00', 'title': 'Python for Data Analysis', 'trapped': '/False', 'www.it-ebooks.info': '{0C23F91E-8B0B-458C-831D-C7768D737A44}', 'source': 'data/DATA_ANALYSIS.pdf', 'total_pages': 470, 'page': 469, 'page_label': '454'}, page_content='www.it-ebooks.info')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8621882",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trimming down the extracted contecxt only to what requiredp parameter\n",
    "#we need only source and page_conetent from the extracted text\n",
    "\n",
    "from langchain.schema import Document\n",
    "\n",
    "def trim_extracted_data(extract_text):\n",
    "    \"\"\"\n",
    "    Triming the extracted data here , note is source is stored in metadata\n",
    "    and page_content is stored directly in page_content attribute.\n",
    "\n",
    "    Here i need to use Document class from langchain.schema to create new Document\n",
    "    so whenever i trim out the data it should be kept inside document \n",
    "    class only.\n",
    "    \n",
    "    \"\"\"\n",
    "    trimmed_data=[]\n",
    "    for i in extract_text:\n",
    "        trimmed_data.append(Document(\n",
    "            source=i.metadata.get(\"source\"),\n",
    "            page_content=i.page_content\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return trimmed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e407ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the trimmed data to a variable\n",
    "trimmed_extracted_doc=trim_extracted_data(extract_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b37c70e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'source': 'data/DATA_ANALYSIS.pdf', 'page_content': 'www.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf', 'page_content': 'www.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Python for Data Analysis\\nWes McKinney\\nBeijing • Cambridge • Farnham • Köln • Sebastopol • Tokyo\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Python for Data Analysis\\nby Wes McKinney\\nCopyright © 2013 Wes McKinney. All rights reserved.\\nPrinted in the United States of America.\\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions\\nare also available for most titles ( http://my.safaribooksonline.com). For more information, contact our\\ncorporate/institutional sales department: 800-998-9938 or corporate@oreilly.com.\\nEditors: Julie Steele and Meghan Blanchette\\nProduction Editor: Melanie Yarbrough\\nCopyeditor: Teresa Exley\\nProofreader: BIM Publishing Services\\nIndexer: BIM Publishing Services\\nCover Designer: Karen Montgomery\\nInterior Designer: David Futato\\nIllustrator: Rebecca Demarest\\nOctober 2012: First Edition. \\nRevision History for the First Edition:\\n2012-10-05 First release\\nSee http://oreilly.com/catalog/errata.csp?isbn=9781449319793 for release details.\\nNutshell Handbook, the Nutshell Handbook logo, and the O’Reilly logo are registered trademarks of\\nO’Reilly Media, Inc. Python for Data Analysis, the cover image of a golden-tailed tree shrew, and related\\ntrade dress are trademarks of O’Reilly Media, Inc.\\nMany of the designations used by manufacturers and sellers to distinguish their products are claimed as\\ntrademarks. Where those designations appear in this book, and O’Reilly Media, Inc., was aware of a\\ntrademark claim, the designations have been printed in caps or initial caps.\\nWhile every precaution has been taken in the preparation of this book, the publisher and author assume\\nno responsibility for errors or omissions, or for damages resulting from the use of the information con-\\ntained herein.\\nISBN: 978-1-449-31979-3\\n[LSI]\\n1349356084\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Table of Contents\\nPreface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xi\\n1. Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1\\nWhat Is This Book About? 1\\nWhy Python for Data Analysis? 2\\nPython as Glue 2\\nSolving the “Two-Language” Problem 2\\nWhy Not Python? 3\\nEssential Python Libraries 3\\nNumPy 4\\npandas 4\\nmatplotlib 5\\nIPython 5\\nSciPy 6\\nInstallation and Setup 6\\nWindows 7\\nApple OS X 9\\nGNU/Linux 10\\nPython 2 and Python 3 11\\nIntegrated Development Environments (IDEs) 11\\nCommunity and Conferences 12\\nNavigating This Book 12\\nCode Examples 13\\nData for Examples 13\\nImport Conventions 13\\nJargon 13\\nAcknowledgements 14\\n2. Introductory Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  17\\n1.usa.gov data from bit.ly 17\\nCounting Time Zones in Pure Python 19\\niii\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Counting Time Zones with pandas 21\\nMovieLens 1M Data Set 26\\nMeasuring rating disagreement 30\\nUS Baby Names 1880-2010 32\\nAnalyzing Naming Trends 36\\nConclusions and The Path Ahead 43\\n3. IPython: An Interactive Computing and Development Environment . . . . . . . . . . . .  45\\nIPython Basics 46\\nTab Completion 47\\nIntrospection 48\\nThe %run Command 49\\nExecuting Code from the Clipboard 50\\nKeyboard Shortcuts 52\\nExceptions and Tracebacks 53\\nMagic Commands 54\\nQt-based Rich GUI Console 55\\nMatplotlib Integration and Pylab Mode 56\\nUsing the Command History 58\\nSearching and Reusing the Command History 58\\nInput and Output Variables 58\\nLogging the Input and Output 59\\nInteracting with the Operating System 60\\nShell Commands and Aliases 60\\nDirectory Bookmark System 62\\nSoftware Development Tools 62\\nInteractive Debugger 62\\nTiming Code: %time and %timeit 67\\nBasic Profiling: %prun and %run -p 68\\nProfiling a Function Line-by-Line 70\\nIPython HTML Notebook 72\\nTips for Productive Code Development Using IPython 72\\nReloading Module Dependencies 74\\nCode Design Tips 74\\nAdvanced IPython Features 76\\nMaking Your Own Classes IPython-friendly 76\\nProfiles and Configuration 77\\nCredits 78\\n4. NumPy Basics: Arrays and Vectorized Computation . . . . . . . . . . . . . . . . . . . . . . . . . .  79\\nThe NumPy ndarray: A Multidimensional Array Object 80\\nCreating ndarrays 81\\nData Types for ndarrays 83\\niv | Table of Contents\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Operations between Arrays and Scalars 85\\nBasic Indexing and Slicing 86\\nBoolean Indexing 89\\nFancy Indexing 92\\nTransposing Arrays and Swapping Axes 93\\nUniversal Functions: Fast Element-wise Array Functions 95\\nData Processing Using Arrays 97\\nExpressing Conditional Logic as Array Operations 98\\nMathematical and Statistical Methods 100\\nMethods for Boolean Arrays 101\\nSorting 101\\nUnique and Other Set Logic 102\\nFile Input and Output with Arrays 103\\nStoring Arrays on Disk in Binary Format 103\\nSaving and Loading Text Files 104\\nLinear Algebra 105\\nRandom Number Generation 106\\nExample: Random Walks 108\\nSimulating Many Random Walks at Once 109\\n5. Getting Started with pandas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  111\\nIntroduction to pandas Data Structures 112\\nSeries 112\\nDataFrame 115\\nIndex Objects 120\\nEssential Functionality 122\\nReindexing 122\\nDropping entries from an axis 125\\nIndexing, selection, and filtering 125\\nArithmetic and data alignment 128\\nFunction application and mapping 132\\nSorting and ranking 133\\nAxis indexes with duplicate values 136\\nSummarizing and Computing Descriptive Statistics 137\\nCorrelation and Covariance 139\\nUnique Values, Value Counts, and Membership 141\\nHandling Missing Data 142\\nFiltering Out Missing Data 143\\nFilling in Missing Data 145\\nHierarchical Indexing 147\\nReordering and Sorting Levels 149\\nSummary Statistics by Level 150\\nUsing a DataFrame’s Columns 150\\nTable of Contents | v\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Other pandas Topics 151\\nInteger Indexing 151\\nPanel Data 152\\n6. Data Loading, Storage, and File Formats . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  155\\nReading and Writing Data in Text Format 155\\nReading Text Files in Pieces 160\\nWriting Data Out to Text Format 162\\nManually Working with Delimited Formats 163\\nJSON Data 165\\nXML and HTML: Web Scraping 166\\nBinary Data Formats 171\\nUsing HDF5 Format 171\\nReading Microsoft Excel Files 172\\nInteracting with HTML and Web APIs 173\\nInteracting with Databases 174\\nStoring and Loading Data in MongoDB 176\\n7. Data Wrangling: Clean, Transform, Merge, Reshape . . . . . . . . . . . . . . . . . . . . . . . .  177\\nCombining and Merging Data Sets 177\\nDatabase-style DataFrame Merges 178\\nMerging on Index 182\\nConcatenating Along an Axis 185\\nCombining Data with Overlap 188\\nReshaping and Pivoting 189\\nReshaping with Hierarchical Indexing 190\\nPivoting “long” to “wide” Format 192\\nData Transformation 194\\nRemoving Duplicates 194\\nTransforming Data Using a Function or Mapping 195\\nReplacing Values 196\\nRenaming Axis Indexes 197\\nDiscretization and Binning 199\\nDetecting and Filtering Outliers 201\\nPermutation and Random Sampling 202\\nComputing Indicator/Dummy Variables 203\\nString Manipulation 205\\nString Object Methods 206\\nRegular expressions 207\\nVectorized string functions in pandas 210\\nExample: USDA Food Database 212\\nvi | Table of Contents\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': '8. Plotting and Visualization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  219\\nA Brief matplotlib API Primer 219\\nFigures and Subplots 220\\nColors, Markers, and Line Styles 224\\nTicks, Labels, and Legends 225\\nAnnotations and Drawing on a Subplot 228\\nSaving Plots to File 231\\nmatplotlib Configuration 231\\nPlotting Functions in pandas 232\\nLine Plots 232\\nBar Plots 235\\nHistograms and Density Plots 238\\nScatter Plots 239\\nPlotting Maps: Visualizing Haiti Earthquake Crisis Data 241\\nPython Visualization Tool Ecosystem 247\\nChaco 248\\nmayavi 248\\nOther Packages 248\\nThe Future of Visualization Tools? 249\\n9. Data Aggregation and Group Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  251\\nGroupBy Mechanics 252\\nIterating Over Groups 255\\nSelecting a Column or Subset of Columns 256\\nGrouping with Dicts and Series 257\\nGrouping with Functions 258\\nGrouping by Index Levels 259\\nData Aggregation 259\\nColumn-wise and Multiple Function Application 262\\nReturning Aggregated Data in “unindexed” Form 264\\nGroup-wise Operations and Transformations 264\\nApply: General split-apply-combine 266\\nQuantile and Bucket Analysis 268\\nExample: Filling Missing Values with Group-specific Values 270\\nExample: Random Sampling and Permutation 271\\nExample: Group Weighted Average and Correlation 273\\nExample: Group-wise Linear Regression 274\\nPivot Tables and Cross-Tabulation 275\\nCross-Tabulations: Crosstab 277\\nExample: 2012 Federal Election Commission Database 278\\nDonation Statistics by Occupation and Employer 280\\nBucketing Donation Amounts 283\\nDonation Statistics by State 285\\nTable of Contents | vii\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': '10. Time Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  289\\nDate and Time Data Types and Tools 290\\nConverting between string and datetime 291\\nTime Series Basics 293\\nIndexing, Selection, Subsetting 294\\nTime Series with Duplicate Indices 296\\nDate Ranges, Frequencies, and Shifting 297\\nGenerating Date Ranges 298\\nFrequencies and Date Offsets 299\\nShifting (Leading and Lagging) Data 301\\nTime Zone Handling 303\\nLocalization and Conversion 304\\nOperations with Time Zone−aware Timestamp Objects 305\\nOperations between Different Time Zones 306\\nPeriods and Period Arithmetic 307\\nPeriod Frequency Conversion 308\\nQuarterly Period Frequencies 309\\nConverting Timestamps to Periods (and Back) 311\\nCreating a PeriodIndex from Arrays 312\\nResampling and Frequency Conversion 312\\nDownsampling 314\\nUpsampling and Interpolation 316\\nResampling with Periods 318\\nTime Series Plotting 319\\nMoving Window Functions 320\\nExponentially-weighted functions 324\\nBinary Moving Window Functions 324\\nUser-Defined Moving Window Functions 326\\nPerformance and Memory Usage Notes 327\\n11. Financial and Economic Data Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  329\\nData Munging Topics 329\\nTime Series and Cross-Section Alignment 330\\nOperations with Time Series of Different Frequencies 332\\nTime of Day and “as of” Data Selection 334\\nSplicing Together Data Sources 336\\nReturn Indexes and Cumulative Returns 338\\nGroup Transforms and Analysis 340\\nGroup Factor Exposures 342\\nDecile and Quartile Analysis 343\\nMore Example Applications 345\\nSignal Frontier Analysis 345\\nFuture Contract Rolling 347\\nviii | Table of Contents\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Rolling Correlation and Linear Regression 350\\n12. Advanced NumPy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  353\\nndarray Object Internals 353\\nNumPy dtype Hierarchy 354\\nAdvanced Array Manipulation 355\\nReshaping Arrays 355\\nC versus Fortran Order 356\\nConcatenating and Splitting Arrays 357\\nRepeating Elements: Tile and Repeat 360\\nFancy Indexing Equivalents: Take and Put 361\\nBroadcasting 362\\nBroadcasting Over Other Axes 364\\nSetting Array Values by Broadcasting 367\\nAdvanced ufunc Usage 367\\nufunc Instance Methods 368\\nCustom ufuncs 370\\nStructured and Record Arrays 370\\nNested dtypes and Multidimensional Fields 371\\nWhy Use Structured Arrays? 372\\nStructured Array Manipulations: numpy.lib.recfunctions 372\\nMore About Sorting 373\\nIndirect Sorts: argsort and lexsort 374\\nAlternate Sort Algorithms 375\\nnumpy.searchsorted: Finding elements in a Sorted Array 376\\nNumPy Matrix Class 377\\nAdvanced Array Input and Output 379\\nMemory-mapped Files 379\\nHDF5 and Other Array Storage Options 380\\nPerformance Tips 380\\nThe Importance of Contiguous Memory 381\\nOther Speed Options: Cython, f2py, C 382\\nAppendix: Python Language Essentials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  385\\nIndex . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  433\\nTable of Contents | ix\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf', 'page_content': 'www.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Preface\\nThe scientific Python ecosystem of open source libraries has grown substantially over\\nthe last 10 years. By late 2011, I had long felt that the lack of centralized learning\\nresources for data analysis and statistical applications was a stumbling block for new\\nPython programmers engaged in such work. Key projects for data analysis (especially\\nNumPy, IPython, matplotlib, and pandas) had also matured enough that a book written\\nabout them would likely not go out-of-date very quickly. Thus, I mustered the nerve\\nto embark on this writing project. This is the book that I wish existed when I started\\nusing Python for data analysis in 2007. I hope you find it useful and are able to apply\\nthese tools productively in your work.\\nConventions Used in This Book\\nThe following typographical conventions are used in this book:\\nItalic\\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\\nConstant width\\nUsed for program listings, as well as within paragraphs to refer to program elements\\nsuch as variable or function names, databases, data types, environment variables,\\nstatements, and keywords.\\nConstant width bold\\nShows commands or other text that should be typed literally by the user.\\nConstant width italic\\nShows text that should be replaced with user-supplied values or by values deter-\\nmined by context.\\nThis icon signifies a tip, suggestion, or general note.\\nxi\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'This icon indicates a warning or caution.\\nUsing Code Examples\\nThis book is here to help you get your job done. In general, you may use the code in\\nthis book in your programs and documentation. You do not need to contact us for\\npermission unless you’re reproducing a significant portion of the code. For example,\\nwriting a program that uses several chunks of code from this book does not require\\npermission. Selling or distributing a CD-ROM of examples from O’Reilly books does\\nrequire permission. Answering a question by citing this book and quoting example\\ncode does not require permission. Incorporating a significant amount of example code\\nfrom this book into your product’s documentation does require permission.\\nWe appreciate, but do not require, attribution. An attribution usually includes the title,\\nauthor, publisher, and ISBN. For example: “Python for Data Analysis by William Wes-\\nley McKinney (O’Reilly). Copyright 2012 William McKinney, 978-1-449-31979-3.”\\nIf you feel your use of code examples falls outside fair use or the permission given above,\\nfeel free to contact us at permissions@oreilly.com.\\nSafari® Books Online\\nSafari Books Online (www.safaribooksonline.com) is an on-demand digital\\nlibrary that delivers expert content in both book and video form from the\\nworld’s leading authors in technology and business.\\nTechnology professionals, software developers, web designers, and business and cre-\\native professionals use Safari Books Online as their primary resource for research,\\nproblem solving, learning, and certification training.\\nSafari Books Online offers a range of product mixes and pricing programs for organi-\\nzations, government agencies, and individuals. Subscribers have access to thousands\\nof books, training videos, and prepublication manuscripts in one fully searchable da-\\ntabase from publishers like O’Reilly Media, Prentice Hall Professional, Addison-Wesley\\nProfessional, Microsoft Press, Sams, Que, Peachpit Press, Focal Press, Cisco Press, John\\nWiley & Sons, Syngress, Morgan Kaufmann, IBM Redbooks, Packt, Adobe Press, FT\\nPress, Apress, Manning, New Riders, McGraw-Hill, Jones & Bartlett, Course Tech-\\nnology, and dozens more. For more information about Safari Books Online, please visit\\nus online.\\nxii | Preface\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'How to Contact Us\\nPlease address comments and questions concerning this book to the publisher:\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\n707-829-0515 (international or local)\\n707-829-0104 (fax)\\nWe have a web page for this book, where we list errata, examples, and any additional\\ninformation. You can access this page at http://oreil.ly/python_for_data_analysis.\\nTo comment or ask technical questions about this book, send email to\\nbookquestions@oreilly.com.\\nFor more information about our books, courses, conferences, and news, see our website\\nat http://www.oreilly.com.\\nFind us on Facebook: http://facebook.com/oreilly\\nFollow us on Twitter: http://twitter.com/oreillymedia\\nWatch us on YouTube: http://www.youtube.com/oreillymedia\\nPreface | xiii\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf', 'page_content': 'www.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'CHAPTER 1\\nPreliminaries\\nWhat Is This Book About?\\nThis book is concerned with the nuts and bolts of manipulating, processing, cleaning,\\nand crunching data in Python. It is also a practical, modern introduction to scientific\\ncomputing in Python, tailored for data-intensive applications. This is a book about the\\nparts of the Python language and libraries you’ll need to effectively solve a broad set of\\ndata analysis problems. This book is not an exposition on analytical methods using\\nPython as the implementation language.\\nWhen I say “data”, what am I referring to exactly? The primary focus is on structured\\ndata, a deliberately vague term that encompasses many different common forms of\\ndata, such as\\n• Multidimensional arrays (matrices)\\n• Tabular or spreadsheet-like data in which each column may be a different type\\n(string, numeric, date, or otherwise). This includes most kinds of data commonly\\nstored in relational databases or tab- or comma-delimited text files\\n• Multiple tables of data interrelated by key columns (what would be primary or\\nforeign keys for a SQL user)\\n• Evenly or unevenly spaced time series\\nThis is by no means a complete list. Even though it may not always be obvious, a large\\npercentage of data sets can be transformed into a structured form that is more suitable\\nfor analysis and modeling. If not, it may be possible to extract features from a data set\\ninto a structured form. As an example, a collection of news articles could be processed\\ninto a word frequency table which could then be used to perform sentiment analysis.\\nMost users of spreadsheet programs like Microsoft Excel, perhaps the most widely used\\ndata analysis tool in the world, will not be strangers to these kinds of data.\\n1\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Why Python for Data Analysis?\\nFor many people (myself among them), the Python language is easy to fall in love with.\\nSince its first appearance in 1991, Python has become one of the most popular dynamic,\\nprogramming languages, along with Perl, Ruby, and others. Python and Ruby have\\nbecome especially popular in recent years for building websites using their numerous\\nweb frameworks, like Rails (Ruby) and Django (Python). Such languages are often\\ncalled scripting languages as they can be used to write quick-and-dirty small programs,\\nor scripts. I don’t like the term “scripting language” as it carries a connotation that they\\ncannot be used for building mission-critical software. Among interpreted languages\\nPython is distinguished by its large and active scientific computing community. Adop-\\ntion of Python for scientific computing in both industry applications and academic\\nresearch has increased significantly since the early 2000s.\\nFor data analysis and interactive, exploratory computing and data visualization, Python\\nwill inevitably draw comparisons with the many other domain-specific open source\\nand commercial programming languages and tools in wide use, such as R, MATLAB,\\nSAS, Stata, and others. In recent years, Python’s improved library support (primarily\\npandas) has made it a strong alternative for data manipulation tasks. Combined with\\nPython’s strength in general purpose programming, it is an excellent choice as a single\\nlanguage for building data-centric applications.\\nPython as Glue\\nPart of Python’s success as a scientific computing platform is the ease of integrating C,\\nC++, and FORTRAN code. Most modern computing environments share a similar set\\nof legacy FORTRAN and C libraries for doing linear algebra, optimization, integration,\\nfast fourier transforms, and other such algorithms. The same story has held true for\\nmany companies and national labs that have used Python to glue together 30 years’\\nworth of legacy software.\\nMost programs consist of small portions of code where most of the time is spent, with\\nlarge amounts of “glue code” that doesn’t run often. In many cases, the execution time\\nof the glue code is insignificant; effort is most fruitfully invested in optimizing the\\ncomputational bottlenecks, sometimes by moving the code to a lower-level language\\nlike C.\\nIn the last few years, the Cython project ( http://cython.org) has become one of the\\npreferred ways of both creating fast compiled extensions for Python and also interfacing\\nwith C and C++ code.\\nSolving the “Two-Language” Problem\\nIn many organizations, it is common to research, prototype, and test new ideas using\\na more domain-specific computing language like MATLAB or R then later port those\\n2 | Chapter 1: \\u2002Preliminaries\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'ideas to be part of a larger production system written in, say, Java, C#, or C++. What\\npeople are increasingly finding is that Python is a suitable language not only for doing\\nresearch and prototyping but also building the production systems, too. I believe that\\nmore and more companies will go down this path as there are often significant organ-\\nizational benefits to having both scientists and technologists using the same set of pro-\\ngrammatic tools.\\nWhy Not Python?\\nWhile Python is an excellent environment for building computationally-intensive sci-\\nentific applications and building most kinds of general purpose systems, there are a\\nnumber of uses for which Python may be less suitable.\\nAs Python is an interpreted programming language, in general most Python code will\\nrun substantially slower than code written in a compiled language like Java or C++. As\\nprogrammer time is typically more valuable than CPU time, many are happy to make\\nthis tradeoff. However, in an application with very low latency requirements (for ex-\\nample, a high frequency trading system), the time spent programming in a lower-level,\\nlower-productivity language like C++ to achieve the maximum possible performance\\nmight be time well spent.\\nPython is not an ideal language for highly concurrent, multithreaded applications, par-\\nticularly applications with many CPU-bound threads. The reason for this is that it has\\nwhat is known as the global interpreter lock  (GIL), a mechanism which prevents the\\ninterpreter from executing more than one Python bytecode instruction at a time. The\\ntechnical reasons for why the GIL exists are beyond the scope of this book, but as of\\nthis writing it does not seem likely that the GIL will disappear anytime soon. While it\\nis true that in many big data processing applications, a cluster of computers may be\\nrequired to process a data set in a reasonable amount of time, there are still situations\\nwhere a single-process, multithreaded system is desirable.\\nThis is not to say that Python cannot execute truly multithreaded, parallel code; that\\ncode just cannot be executed in a single Python process. As an example, the Cython\\nproject features easy integration with OpenMP, a C framework for parallel computing,\\nin order to to parallelize loops and thus significantly speed up numerical algorithms.\\nEssential Python Libraries\\nFor those who are less familiar with the scientific Python ecosystem and the libraries\\nused throughout the book, I present the following overview of each library.\\nEssential Python Libraries | 3\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'NumPy\\nNumPy, short for Numerical Python, is the foundational package for scientific com-\\nputing in Python. The majority of this book will be based on NumPy and libraries built\\non top of NumPy. It provides, among other things:\\n• A fast and efficient multidimensional array object ndarray\\n• Functions for performing element-wise computations with arrays or mathematical\\noperations between arrays\\n• Tools for reading and writing array-based data sets to disk\\n• Linear algebra operations, Fourier transform, and random number generation\\n• Tools for integrating connecting C, C++, and Fortran code to Python\\nBeyond the fast array-processing capabilities that NumPy adds to Python, one of its\\nprimary purposes with regards to data analysis is as the primary container for data to\\nbe passed between algorithms. For numerical data, NumPy arrays are a much more\\nefficient way of storing and manipulating data than the other built-in Python data\\nstructures. Also, libraries written in a lower-level language, such as C or Fortran, can\\noperate on the data stored in a NumPy array without copying any data.\\npandas\\npandas provides rich data structures and functions designed to make working with\\nstructured data fast, easy, and expressive. It is, as you will see, one of the critical in-\\ngredients enabling Python to be a powerful and productive data analysis environment.\\nThe primary object in pandas that will be used in this book is the DataFrame, a two-\\ndimensional tabular, column-oriented data structure with both row and column labels:\\n>>> frame\\n    total_bill  tip   sex     smoker  day  time    size\\n1   16.99       1.01  Female  No      Sun  Dinner  2\\n2   10.34       1.66  Male    No      Sun  Dinner  3\\n3   21.01       3.5   Male    No      Sun  Dinner  3\\n4   23.68       3.31  Male    No      Sun  Dinner  2\\n5   24.59       3.61  Female  No      Sun  Dinner  4\\n6   25.29       4.71  Male    No      Sun  Dinner  4\\n7   8.77        2     Male    No      Sun  Dinner  2\\n8   26.88       3.12  Male    No      Sun  Dinner  4\\n9   15.04       1.96  Male    No      Sun  Dinner  2\\n10  14.78       3.23  Male    No      Sun  Dinner  2\\npandas combines the high performance array-computing features of NumPy with the\\nflexible data manipulation capabilities of spreadsheets and relational databases (such\\nas SQL). It provides sophisticated indexing functionality to make it easy to reshape,\\nslice and dice, perform aggregations, and select subsets of data. pandas is the primary\\ntool that we will use in this book.\\n4 | Chapter 1: \\u2002Preliminaries\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'For financial users, pandas features rich, high-performance time series functionality\\nand tools well-suited for working with financial data. In fact, I initially designed pandas\\nas an ideal tool for financial data analysis applications.\\nFor users of the R language for statistical computing, the DataFrame name will be\\nfamiliar, as the object was named after the similar R data.frame object. They are not\\nthe same, however; the functionality provided by data.frame in R is essentially a strict\\nsubset of that provided by the pandas DataFrame. While this is a book about Python, I\\nwill occasionally draw comparisons with R as it is one of the most widely-used open\\nsource data analysis environments and will be familiar to many readers.\\nThe pandas name itself is derived from panel data, an econometrics term for multidi-\\nmensional structured data sets, and Python data analysis itself.\\nmatplotlib\\nmatplotlib is the most popular Python library for producing plots and other 2D data\\nvisualizations. It was originally created by John D. Hunter (JDH) and is now maintained\\nby a large team of developers. It is well-suited for creating plots suitable for publication.\\nIt integrates well with IPython (see below), thus providing a comfortable interactive\\nenvironment for plotting and exploring data. The plots are also interactive; you can\\nzoom in on a section of the plot and pan around the plot using the toolbar in the plot\\nwindow.\\nIPython\\nIPython is the component in the standard scientific Python toolset that ties everything\\ntogether. It provides a robust and productive environment for interactive and explor-\\natory computing. It is an enhanced Python shell designed to accelerate the writing,\\ntesting, and debugging of Python code. It is particularly useful for interactively working\\nwith data and visualizing data with matplotlib. IPython is usually involved with the\\nmajority of my Python work, including running, debugging, and testing code.\\nAside from the standard terminal-based IPython shell, the project also provides\\n• A Mathematica-like HTML notebook for connecting to IPython through a web\\nbrowser (more on this later).\\n• A Qt framework-based GUI console with inline plotting, multiline editing, and\\nsyntax highlighting\\n• An infrastructure for interactive parallel and distributed computing\\nI will devote a chapter to IPython and how to get the most out of its features. I strongly\\nrecommend using it while working through this book.\\nEssential Python Libraries | 5\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'SciPy\\nSciPy is a collection of packages addressing a number of different standard problem\\ndomains in scientific computing. Here is a sampling of the packages included:\\n• scipy.integrate: numerical integration routines and differential equation solvers\\n• scipy.linalg: linear algebra routines and matrix decompositions extending be-\\nyond those provided in numpy.linalg.\\n• scipy.optimize: function optimizers (minimizers) and root finding algorithms\\n• scipy.signal: signal processing tools\\n• scipy.sparse: sparse matrices and sparse linear system solvers\\n• scipy.special: wrapper around SPECFUN, a Fortran library implementing many\\ncommon mathematical functions, such as the gamma function\\n• scipy.stats: standard continuous and discrete probability distributions (density\\nfunctions, samplers, continuous distribution functions), various statistical tests,\\nand more descriptive statistics\\n• scipy.weave: tool for using inline C++ code to accelerate array computations\\nTogether NumPy and SciPy form a reasonably complete computational replacement\\nfor much of MATLAB along with some of its add-on toolboxes.\\nInstallation and Setup\\nSince everyone uses Python for different applications, there is no single solution for\\nsetting up Python and required add-on packages. Many readers will not have a complete\\nscientific Python environment suitable for following along with this book, so here I will\\ngive detailed instructions to get set up on each operating system. I recommend using\\none of the following base Python distributions:\\n• Enthought Python Distribution: a scientific-oriented Python distribution from En-\\nthought (http://www.enthought.com). This includes EPDFree, a free base scientific\\ndistribution (with NumPy, SciPy, matplotlib, Chaco, and IPython) and EPD Full,\\na comprehensive suite of more than 100 scientific packages across many domains.\\nEPD Full is free for academic use but has an annual subscription for non-academic\\nusers.\\n• Python(x,y) ( http://pythonxy.googlecode.com): A free scientific-oriented Python\\ndistribution for Windows.\\nI will be using EPDFree for the installation guides, though you are welcome to take\\nanother approach depending on your needs. At the time of this writing, EPD includes\\nPython 2.7, though this might change at some point in the future. After installing, you\\nwill have the following packages installed and importable:\\n6 | Chapter 1: \\u2002Preliminaries\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': '• Scientific Python base: NumPy, SciPy, matplotlib, and IPython. These are all in-\\ncluded in EPDFree.\\n• IPython Notebook dependencies: tornado and pyzmq. These are included in EPD-\\nFree.\\n• pandas (version 0.8.2 or higher).\\nAt some point while reading you may wish to install one or more of the following\\npackages: statsmodels, PyTables, PyQt (or equivalently, PySide), xlrd, lxml, basemap,\\npymongo, and requests. These are used in various examples. Installing these optional\\nlibraries is not necessary, and I would would suggest waiting until you need them. For\\nexample, installing PyQt or PyTables from source on OS X or Linux can be rather\\narduous. For now, it’s most important to get up and running with the bare minimum:\\nEPDFree and pandas.\\nFor information on each Python package and links to binary installers or other help,\\nsee the Python Package Index (PyPI, http://pypi.python.org). This is also an excellent\\nresource for finding new Python packages.\\nTo avoid confusion and to keep things simple, I am avoiding discussion\\nof more complex environment management tools like pip and virtua-\\nlenv. There are many excellent guides available for these tools on the\\nInternet.\\nSome users may be interested in alternate Python implementations, such\\nas IronPython, Jython, or PyPy. To make use of the tools presented in\\nthis book, it is (currently) necessary to use the standard C-based Python\\ninterpreter, known as CPython.\\nWindows\\nTo get started on Windows, download the EPDFree installer from http://www.en\\nthought.com, which should be an MSI installer named like epd_free-7.3-1-win-\\nx86.msi. Run the installer and accept the default installation location C:\\\\Python27. If\\nyou had previously installed Python in this location, you may want to delete it manually\\nfirst (or using Add/Remove Programs).\\nNext, you need to verify that Python has been successfully added to the system path\\nand that there are no conflicts with any prior-installed Python versions. First, open a\\ncommand prompt by going to the Start Menu and starting the Command Prompt ap-\\nplication, also known as cmd.exe. Try starting the Python interpreter by typing\\npython. You should see a message that matches the version of EPDFree you installed:\\nC:\\\\Users\\\\Wes>python\\nPython 2.7.3 |EPD_free 7.3-1 (32-bit)| (default, Apr 12 2012, 14:30:37) on win32\\nType \"credits\", \"demo\" or \"enthought\" for more information.\\n>>>\\nInstallation and Setup | 7\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'If you see a message for a different version of EPD or it doesn’t work at all, you will\\nneed to clean up your Windows environment variables. On Windows 7 you can start\\ntyping “environment variables” in the programs search field and select Edit environ\\nment variables for your account . On Windows XP, you will have to go to Control\\nPanel > System > Advanced > Environment Variables . On the window that pops up,\\nyou are looking for the Path variable. It needs to contain the following two directory\\npaths, separated by semicolons:\\nC:\\\\Python27;C:\\\\Python27\\\\Scripts\\nIf you installed other versions of Python, be sure to delete any other Python-related\\ndirectories from both the system and user Path variables. After making a path alterna-\\ntion, you have to restart the command prompt for the changes to take effect.\\nOnce you can launch Python successfully from the command prompt, you need to\\ninstall pandas. The easiest way is to download the appropriate binary installer from\\nhttp://pypi.python.org/pypi/pandas. For EPDFree, this should be pandas-0.9.0.win32-\\npy2.7.exe. After you run this, let’s launch IPython and check that things are installed\\ncorrectly by importing pandas and making a simple matplotlib plot:\\nC:\\\\Users\\\\Wes>ipython --pylab\\nPython 2.7.3 |EPD_free 7.3-1 (32-bit)|\\nType \"copyright\", \"credits\" or \"license\" for more information.\\nIPython 0.12.1 -- An enhanced Interactive Python.\\n?         -> Introduction and overview of IPython\\'s features.\\n%quickref -> Quick reference.\\nhelp      -> Python\\'s own help system.\\nobject?   -> Details about \\'object\\', use \\'object??\\' for extra details.\\nWelcome to pylab, a matplotlib-based Python environment [backend: WXAgg].\\nFor more information, type \\'help(pylab)\\'.\\nIn [1]: import pandas\\nIn [2]: plot(arange(10))\\nIf successful, there should be no error messages and a plot window will appear. You\\ncan also check that the IPython HTML notebook can be successfully run by typing:\\n$ ipython notebook --pylab=inline\\nIf you use the IPython notebook application on Windows and normally\\nuse Internet Explorer, you will likely need to install and run Mozilla\\nFirefox or Google Chrome instead.\\nEPDFree on Windows contains only 32-bit executables. If you want or need a 64-bit\\nsetup on Windows, using EPD Full is the most painless way to accomplish that. If you\\nwould rather install from scratch and not pay for an EPD subscription, Christoph\\nGohlke at the University of California, Irvine, publishes unofficial binary installers for\\n8 | Chapter 1: \\u2002Preliminaries\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'all of the book’s necessary packages (http://www.lfd.uci.edu/~gohlke/pythonlibs/) for 32-\\nand 64-bit Windows.\\nApple OS X\\nTo get started on OS X, you must first install Xcode, which includes Apple’s suite of\\nsoftware development tools. The necessary component for our purposes is the gcc C\\nand C++ compiler suite. The Xcode installer can be found on the OS X install DVD\\nthat came with your computer or downloaded from Apple directly.\\nOnce you’ve installed Xcode, launch the terminal (Terminal.app) by navigating to\\nApplications > Utilities. Type gcc and press enter. You should hopefully see some-\\nthing like:\\n$ gcc\\ni686-apple-darwin10-gcc-4.2.1: no input files\\nNow you need to install EPDFree. Download the installer which should be a disk image\\nnamed something like epd_free-7.3-1-macosx-i386.dmg. Double-click the .dmg file to\\nmount it, then double-click the .mpkg file inside to run the installer.\\nWhen the installer runs, it automatically appends the EPDFree executable path to\\nyour .bash_profile file. This is located at /Users/your_uname/.bash_profile:\\n# Setting PATH for EPD_free-7.3-1\\nPATH=\"/Library/Frameworks/Python.framework/Versions/Current/bin:${PATH}\"\\nexport PATH\\nShould you encounter any problems in the following steps, you’ll want to inspect\\nyour .bash_profile and potentially add the above directory to your path.\\nNow, it’s time to install pandas. Execute this command in the terminal:\\n$ sudo easy_install pandas\\nSearching for pandas\\nReading http://pypi.python.org/simple/pandas/\\nReading http://pandas.pydata.org\\nReading http://pandas.sourceforge.net\\nBest match: pandas 0.9.0\\nDownloading http://pypi.python.org/packages/source/p/pandas/pandas-0.9.0.zip\\nProcessing pandas-0.9.0.zip\\nWriting /tmp/easy_install-H5mIX6/pandas-0.9.0/setup.cfg\\nRunning pandas-0.9.0/setup.py -q bdist_egg --dist-dir /tmp/easy_install-H5mIX6/\\npandas-0.9.0/egg-dist-tmp-RhLG0z\\nAdding pandas 0.9.0 to easy-install.pth file\\nInstalled /Library/Frameworks/Python.framework/Versions/7.3/lib/python2.7/\\nsite-packages/pandas-0.9.0-py2.7-macosx-10.5-i386.egg\\nProcessing dependencies for pandas\\nFinished processing dependencies for pandas\\nTo verify everything is working, launch IPython in Pylab mode and test importing pan-\\ndas then making a plot interactively:\\nInstallation and Setup | 9\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': '$ ipython --pylab\\n22:29 ~/VirtualBox VMs/WindowsXP $ ipython\\nPython 2.7.3 |EPD_free 7.3-1 (32-bit)| (default, Apr 12 2012, 11:28:34)\\nType \"copyright\", \"credits\" or \"license\" for more information.\\nIPython 0.12.1 -- An enhanced Interactive Python.\\n?         -> Introduction and overview of IPython\\'s features.\\n%quickref -> Quick reference.\\nhelp      -> Python\\'s own help system.\\nobject?   -> Details about \\'object\\', use \\'object??\\' for extra details.\\nWelcome to pylab, a matplotlib-based Python environment [backend: WXAgg].\\nFor more information, type \\'help(pylab)\\'.\\nIn [1]: import pandas\\nIn [2]: plot(arange(10))\\nIf this succeeds, a plot window with a straight line should pop up.\\nGNU/Linux\\nSome, but not all, Linux distributions include sufficiently up-to-date\\nversions of all the required Python packages and can be installed using\\nthe built-in package management tool like apt. I detail setup using EPD-\\nFree as it\\'s easily reproducible across distributions.\\nLinux details will vary a bit depending on your Linux flavor, but here I give details for\\nDebian-based GNU/Linux systems like Ubuntu and Mint. Setup is similar to OS X with\\nthe exception of how EPDFree is installed. The installer is a shell script that must be\\nexecuted in the terminal. Depending on whether you have a 32-bit or 64-bit system,\\nyou will either need to install the x86 (32-bit) or x86_64 (64-bit) installer. You will then\\nhave a file named something similar to epd_free-7.3-1-rh5-x86_64.sh. To install it,\\nexecute this script with bash:\\n$ bash epd_free-7.3-1-rh5-x86_64.sh\\nAfter accepting the license, you will be presented with a choice of where to put the\\nEPDFree files. I recommend installing the files in your home directory, say /home/wesm/\\nepd (substituting your own username for wesm).\\nOnce the installer has finished, you need to add EPDFree’s bin directory to your \\n$PATH variable. If you are using the bash shell (the default in Ubuntu, for example), this\\nmeans adding the following path addition in your .bashrc:\\nexport PATH=/home/wesm/epd/bin:$PATH\\nObviously, substitute the installation directory you used for /home/wesm/epd/. After\\ndoing this you can either start a new terminal process or execute your .bashrc again\\nwith source ~/.bashrc.\\n10 | Chapter 1: \\u2002Preliminaries\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'You need a C compiler such as gcc to move forward; many Linux distributions include\\ngcc, but others may not. On Debian systems, you can install gcc by executing:\\nsudo apt-get install gcc\\nIf you type gcc on the command line it should say something like:\\n$ gcc\\ngcc: no input files\\nNow, time to install pandas:\\n$ easy_install pandas\\nIf you installed EPDFree as root, you may need to add sudo to the command and enter\\nthe sudo or root password. To verify things are working, perform the same checks as\\nin the OS X section.\\nPython 2 and Python 3\\nThe Python community is currently undergoing a drawn-out transition from the Python\\n2 series of interpreters to the Python 3 series. Until the appearance of Python 3.0, all\\nPython code was backwards compatible. The community decided that in order to move\\nthe language forward, certain backwards incompatible changes were necessary.\\nI am writing this book with Python 2.7 as its basis, as the majority of the scientific\\nPython community has not yet transitioned to Python 3. The good news is that, with\\na few exceptions, you should have no trouble following along with the book if you\\nhappen to be using Python 3.2.\\nIntegrated Development Environments (IDEs)\\nWhen asked about my standard development environment, I almost always say “IPy-\\nthon plus a text editor”. I typically write a program and iteratively test and debug each\\npiece of it in IPython. It is also useful to be able to play around with data interactively\\nand visually verify that a particular set of data manipulations are doing the right thing.\\nLibraries like pandas and NumPy are designed to be easy-to-use in the shell.\\nHowever, some will still prefer to work in an IDE instead of a text editor. They do\\nprovide many nice “code intelligence” features like completion or quickly pulling up\\nthe documentation associated with functions and classes. Here are some that you can\\nexplore:\\n• Eclipse with PyDev Plugin\\n• Python Tools for Visual Studio (for Windows users)\\n• PyCharm\\n• Spyder\\n• Komodo IDE\\nInstallation and Setup | 11\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Community and Conferences\\nOutside of an Internet search, the scientific Python mailing lists are generally helpful\\nand responsive to questions. Some ones to take a look at are:\\n• pydata: a Google Group list for questions related to Python for data analysis and\\npandas\\n• pystatsmodels: for statsmodels or pandas-related questions\\n• numpy-discussion: for NumPy-related questions\\n• scipy-user: for general SciPy or scientific Python questions\\nI deliberately did not post URLs for these in case they change. They can be easily located\\nvia Internet search.\\nEach year many conferences are held all over the world for Python programmers. PyCon\\nand EuroPython are the two main general Python conferences in the United States and\\nEurope, respectively. SciPy and EuroSciPy are scientific-oriented Python conferences\\nwhere you will likely find many “birds of a feather” if you become more involved with\\nusing Python for data analysis after reading this book.\\nNavigating This Book\\nIf you have never programmed in Python before, you may actually want to start at the\\nend of the book, where I have placed a condensed tutorial on Python syntax, language\\nfeatures, and built-in data structures like tuples, lists, and dicts. These things are con-\\nsidered prerequisite knowledge for the remainder of the book.\\nThe book starts by introducing you to the IPython environment. Next, I give a short\\nintroduction to the key features of NumPy, leaving more advanced NumPy use for\\nanother chapter at the end of the book. Then, I introduce pandas and devote the rest\\nof the book to data analysis topics applying pandas, NumPy, and matplotlib (for vis-\\nualization). I have structured the material in the most incremental way possible, though\\nthere is occasionally some minor cross-over between chapters.\\nData files and related material for each chapter are hosted as a git repository on GitHub:\\nhttp://github.com/pydata/pydata-book\\nI encourage you to download the data and use it to replicate the book’s code examples\\nand experiment with the tools presented in each chapter. I will happily accept contri-\\nbutions, scripts, IPython notebooks, or any other materials you wish to contribute to\\nthe book's repository for all to enjoy.\\n12 | Chapter 1: \\u2002Preliminaries\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Code Examples\\nMost of the code examples in the book are shown with input and output as it would\\nappear executed in the IPython shell.\\nIn [5]: code\\nOut[5]: output\\nAt times, for clarity, multiple code examples will be shown side by side. These should\\nbe read left to right and executed separately.\\nIn [5]: code         In [6]: code2\\nOut[5]: output       Out[6]: output2\\nData for Examples\\nData sets for the examples in each chapter are hosted in a repository on GitHub: http:\\n//github.com/pydata/pydata-book. You can download this data either by using the git\\nrevision control command-line program or by downloading a zip file of the repository\\nfrom the website.\\nI have made every effort to ensure that it contains everything necessary to reproduce\\nthe examples, but I may have made some mistakes or omissions. If so, please send me\\nan e-mail: wesmckinn@gmail.com.\\nImport Conventions\\nThe Python community has adopted a number of naming conventions for commonly-\\nused modules:\\nimport numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nThis means that when you see np.arange, this is a reference to the arange function in\\nNumPy. This is done as it’s considered bad practice in Python software development\\nto import everything (from numpy import *) from a large package like NumPy.\\nJargon\\nI’ll use some terms common both to programming and data science that you may not\\nbe familiar with. Thus, here are some brief definitions:\\nMunge/Munging/Wrangling\\nDescribes the overall process of manipulating unstructured and/or messy data into\\na structured or clean form. The word has snuck its way into the jargon of many\\nmodern day data hackers. Munge rhymes with “lunge”.\\nNavigating This Book | 13\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Pseudocode\\nA description of an algorithm or process that takes a code-like form while likely\\nnot being actual valid source code.\\nSyntactic sugar\\nProgramming syntax which does not add new features, but makes something more\\nconvenient or easier to type.\\nAcknowledgements\\nIt would have been difficult for me to write this book without the support of a large\\nnumber of people.\\nOn the O’Reilly staff, I’m very grateful for my editors Meghan Blanchette and Julie\\nSteele who guided me through the process. Mike Loukides also worked with me in the\\nproposal stages and helped make the book a reality.\\nI received a wealth of technical review from a large cast of characters. In particular,\\nMartin Blais and Hugh White were incredibly helpful in improving the book’s exam-\\nples, clarity, and organization from cover to cover. James Long, Drew Conway, Fer-\\nnando Pérez, Brian Granger, Thomas Kluyver, Adam Klein, Josh Klein, Chang She, and\\nStéfan van der Walt each reviewed one or more chapters, providing pointed feedback\\nfrom many different perspectives.\\nI got many great ideas for examples and data sets from friends and colleagues in the\\ndata community, among them: Mike Dewar, Jeff Hammerbacher, James Johndrow,\\nKristian Lum, Adam Klein, Hilary Mason, Chang She, and Ashley Williams.\\nI am of course indebted to the many leaders in the open source scientific Python com-\\nmunity who’ve built the foundation for my development work and gave encouragement\\nwhile I was writing this book: the IPython core team (Fernando Pérez, Brian Granger,\\nMin Ragan-Kelly, Thomas Kluyver, and others), John Hunter, Skipper Seabold, Travis\\nOliphant, Peter Wang, Eric Jones, Robert Kern, Josef Perktold, Francesc Alted, Chris\\nFonnesbeck, and too many others to mention. Several other people provided a great\\ndeal of support, ideas, and encouragement along the way: Drew Conway, Sean Taylor,\\nGiuseppe Paleologo, Jared Lander, David Epstein, John Krowas, Joshua Bloom, Den\\nPilsworth, John Myles-White, and many others I’ve forgotten.\\nI’d also like to thank a number of people from my formative years. First, my former\\nAQR colleagues who’ve cheered me on in my pandas work over the years: Alex Reyf-\\nman, Michael Wong, Tim Sargen, Oktay Kurbanov, Matthew Tschantz, Roni Israelov,\\nMichael Katz, Chris Uga, Prasad Ramanan, Ted Square, and Hoon Kim. Lastly, my\\nacademic advisors Haynes Miller (MIT) and Mike West (Duke).\\nOn the personal side, Casey Dinkin provided invaluable day-to-day support during the\\nwriting process, tolerating my highs and lows as I hacked together the final draft on\\n14 | Chapter 1: \\u2002Preliminaries\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'top of an already overcommitted schedule. Lastly, my parents, Bill and Kim, taught me\\nto always follow my dreams and to never settle for less.\\nAcknowledgements | 15\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf', 'page_content': 'www.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'CHAPTER 2\\nIntroductory Examples\\nThis book teaches you the Python tools to work productively with data. While readers\\nmay have many different end goals for their work, the tasks required generally fall into\\na number of different broad groups:\\nInteracting with the outside world\\nReading and writing with a variety of file formats and databases.\\nPreparation\\nCleaning, munging, combining, normalizing, reshaping, slicing and dicing, and\\ntransforming data for analysis.\\nTransformation\\nApplying mathematical and statistical operations to groups of data sets to derive\\nnew data sets. For example, aggregating a large table by group variables.\\nModeling and computation\\nConnecting your data to statistical models, machine learning algorithms, or other\\ncomputational tools\\nPresentation\\nCreating interactive or static graphical visualizations or textual summaries\\nIn this chapter I will show you a few data sets and some things we can do with them.\\nThese examples are just intended to pique your interest and thus will only be explained\\nat a high level. Don’t worry if you have no experience with any of these tools; they will\\nbe discussed in great detail throughout the rest of the book. In the code examples you’ll\\nsee input and output prompts like In [15]:; these are from the IPython shell.\\n1.usa.gov data from bit.ly\\nIn 2011, URL shortening service bit.ly partnered with the United States government\\nwebsite usa.gov to provide a feed of anonymous data gathered from users who shorten\\nlinks ending with .gov or .mil. As of this writing, in addition to providing a live feed,\\nhourly snapshots are available as downloadable text files.1\\n17\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'In the case of the hourly snapshots, each line in each file contains a common form of\\nweb data known as JSON, which stands for JavaScript Object Notation. For example,\\nif we read just the first line of a file you may see something like\\nIn [15]: path = \\'ch02/usagov_bitly_data2012-03-16-1331923249.txt\\'\\nIn [16]: open(path).readline()\\nOut[16]: \\'{ \"a\": \"Mozilla\\\\\\\\/5.0 (Windows NT 6.1; WOW64) AppleWebKit\\\\\\\\/535.11\\n(KHTML, like Gecko) Chrome\\\\\\\\/17.0.963.78 Safari\\\\\\\\/535.11\", \"c\": \"US\", \"nk\": 1,\\n\"tz\": \"America\\\\\\\\/New_York\", \"gr\": \"MA\", \"g\": \"A6qOVH\", \"h\": \"wfLQtf\", \"l\":\\n\"orofrog\", \"al\": \"en-US,en;q=0.8\", \"hh\": \"1.usa.gov\", \"r\":\\n\"http:\\\\\\\\/\\\\\\\\/www.facebook.com\\\\\\\\/l\\\\\\\\/7AQEFzjSi\\\\\\\\/1.usa.gov\\\\\\\\/wfLQtf\", \"u\":\\n\"http:\\\\\\\\/\\\\\\\\/www.ncbi.nlm.nih.gov\\\\\\\\/pubmed\\\\\\\\/22415991\", \"t\": 1331923247, \"hc\":\\n1331822918, \"cy\": \"Danvers\", \"ll\": [ 42.576698, -70.954903 ] }\\\\n\\'\\nPython has numerous built-in and 3rd party modules for converting a JSON string into\\na Python dictionary object. Here I’ll use the json module and its loads function invoked\\non each line in the sample file I downloaded:\\nimport json\\npath = \\'ch02/usagov_bitly_data2012-03-16-1331923249.txt\\'\\nrecords = [json.loads(line) for line in open(path)]\\nIf you’ve never programmed in Python before, the last expression here is called a list\\ncomprehension, which is a concise way of applying an operation (like json.loads) to a\\ncollection of strings or other objects. Conveniently, iterating over an open file handle\\ngives you a sequence of its lines. The resulting object records is now a list of Python\\ndicts:\\nIn [18]: records[0]\\nOut[18]:\\n{u\\'a\\': u\\'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like\\nGecko) Chrome/17.0.963.78 Safari/535.11\\',\\n u\\'al\\': u\\'en-US,en;q=0.8\\',\\n u\\'c\\': u\\'US\\',\\n u\\'cy\\': u\\'Danvers\\',\\n u\\'g\\': u\\'A6qOVH\\',\\n u\\'gr\\': u\\'MA\\',\\n u\\'h\\': u\\'wfLQtf\\',\\n u\\'hc\\': 1331822918,\\n u\\'hh\\': u\\'1.usa.gov\\',\\n u\\'l\\': u\\'orofrog\\',\\n u\\'ll\\': [42.576698, -70.954903],\\n u\\'nk\\': 1,\\n u\\'r\\': u\\'http://www.facebook.com/l/7AQEFzjSi/1.usa.gov/wfLQtf\\',\\n u\\'t\\': 1331923247,\\n u\\'tz\\': u\\'America/New_York\\',\\n u\\'u\\': u\\'http://www.ncbi.nlm.nih.gov/pubmed/22415991\\'}\\n1. http://www.usa.gov/About/developer-resources/1usagov.shtml\\n18 | Chapter 2: \\u2002Introductory Examples\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Note that Python indices start at 0 and not 1 like some other languages (like R). It’s\\nnow easy to access individual values within records by passing a string for the key you\\nwish to access:\\nIn [19]: records[0]['tz']\\nOut[19]: u'America/New_York'\\nThe u here in front of the quotation stands for unicode, a standard form of string en-\\ncoding. Note that IPython shows the time zone string object representation here rather\\nthan its print equivalent:\\nIn [20]: print records[0]['tz']\\nAmerica/New_York\\nCounting Time Zones in Pure Python\\nSuppose we were interested in the most often-occurring time zones in the data set (the\\ntz field). There are many ways we could do this. First, let’s extract a list of time zones\\nagain using a list comprehension:\\nIn [25]: time_zones = [rec['tz'] for rec in records]\\n---------------------------------------------------------------------------\\nKeyError                                  Traceback (most recent call last)\\n/home/wesm/book_scripts/whetting/<ipython> in <module>()\\n----> 1 time_zones = [rec['tz'] for rec in records]\\nKeyError: 'tz'\\nOops! Turns out that not all of the records have a time zone field. This is easy to handle\\nas we can add the check if 'tz' in rec at the end of the list comprehension:\\nIn [26]: time_zones = [rec['tz'] for rec in records if 'tz' in rec]\\nIn [27]: time_zones[:10]\\nOut[27]:\\n[u'America/New_York',\\n u'America/Denver',\\n u'America/New_York',\\n u'America/Sao_Paulo',\\n u'America/New_York',\\n u'America/New_York',\\n u'Europe/Warsaw',\\n u'',\\n u'',\\n u'']\\nJust looking at the first 10 time zones we see that some of them are unknown (empty).\\nYou can filter these out also but I’ll leave them in for now. Now, to produce counts by\\ntime zone I’ll show two approaches: the harder way (using just the Python standard\\nlibrary) and the easier way (using pandas). One way to do the counting is to use a dict\\nto store counts while we iterate through the time zones:\\ndef get_counts(sequence):\\n    counts = {}\\n1.usa.gov data from bit.ly | 19\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"for x in sequence:\\n        if x in counts:\\n            counts[x] += 1\\n        else:\\n            counts[x] = 1\\n    return counts\\nIf you know a bit more about the Python standard library, you might prefer to write\\nthe same thing more briefly:\\nfrom collections import defaultdict\\ndef get_counts2(sequence):\\n    counts = defaultdict(int) # values will initialize to 0\\n    for x in sequence:\\n        counts[x] += 1\\n    return counts\\nI put this logic in a function just to make it more reusable. To use it on the time zones,\\njust pass the time_zones list:\\nIn [31]: counts = get_counts(time_zones)\\nIn [32]: counts['America/New_York']\\nOut[32]: 1251\\nIn [33]: len(time_zones)\\nOut[33]: 3440\\nIf we wanted the top 10 time zones and their counts, we have to do a little bit of dic-\\ntionary acrobatics:\\ndef top_counts(count_dict, n=10):\\n    value_key_pairs = [(count, tz) for tz, count in count_dict.items()]\\n    value_key_pairs.sort()\\n    return value_key_pairs[-n:]\\nWe have then:\\nIn [35]: top_counts(counts)\\nOut[35]:\\n[(33, u'America/Sao_Paulo'),\\n (35, u'Europe/Madrid'),\\n (36, u'Pacific/Honolulu'),\\n (37, u'Asia/Tokyo'),\\n (74, u'Europe/London'),\\n (191, u'America/Denver'),\\n (382, u'America/Los_Angeles'),\\n (400, u'America/Chicago'),\\n (521, u''),\\n (1251, u'America/New_York')]\\n20 | Chapter 2: \\u2002Introductory Examples\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"If you search the Python standard library, you may find the collections.Counter class,\\nwhich makes this task a lot easier:\\nIn [49]: from collections import Counter\\nIn [50]: counts = Counter(time_zones)\\nIn [51]: counts.most_common(10)\\nOut[51]:\\n[(u'America/New_York', 1251),\\n (u'', 521),\\n (u'America/Chicago', 400),\\n (u'America/Los_Angeles', 382),\\n (u'America/Denver', 191),\\n (u'Europe/London', 74),\\n (u'Asia/Tokyo', 37),\\n (u'Pacific/Honolulu', 36),\\n (u'Europe/Madrid', 35),\\n (u'America/Sao_Paulo', 33)]\\nCounting Time Zones with pandas\\nThe main pandas data structure is the DataFrame, which you can think of as repre-\\nsenting a table or spreadsheet of data. Creating a DataFrame from the original set of\\nrecords is simple:\\nIn [289]: from pandas import DataFrame, Series\\nIn [290]: import pandas as pd\\nIn [291]: frame = DataFrame(records)\\nIn [292]: frame\\nOut[292]: \\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 3560 entries, 0 to 3559\\nData columns:\\n_heartbeat_    120  non-null values\\na              3440  non-null values\\nal             3094  non-null values\\nc              2919  non-null values\\ncy             2919  non-null values\\ng              3440  non-null values\\ngr             2919  non-null values\\nh              3440  non-null values\\nhc             3440  non-null values\\nhh             3440  non-null values\\nkw             93  non-null values\\nl              3440  non-null values\\nll             2919  non-null values\\nnk             3440  non-null values\\nr              3440  non-null values\\nt              3440  non-null values\\ntz             3440  non-null values\\n1.usa.gov data from bit.ly | 21\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"u              3440  non-null values\\ndtypes: float64(4), object(14)\\nIn [293]: frame['tz'][:10]\\nOut[293]: \\n0     America/New_York\\n1       America/Denver\\n2     America/New_York\\n3    America/Sao_Paulo\\n4     America/New_York\\n5     America/New_York\\n6        Europe/Warsaw\\n7                     \\n8                     \\n9                     \\nName: tz\\nThe output shown for the frame is the summary view, shown for large DataFrame ob-\\njects. The Series object returned by frame['tz'] has a method value_counts that gives\\nus what we’re looking for:\\nIn [294]: tz_counts = frame['tz'].value_counts()\\nIn [295]: tz_counts[:10]\\nOut[295]: \\nAmerica/New_York       1251\\n                        521\\nAmerica/Chicago         400\\nAmerica/Los_Angeles     382\\nAmerica/Denver          191\\nEurope/London            74\\nAsia/Tokyo               37\\nPacific/Honolulu         36\\nEurope/Madrid            35\\nAmerica/Sao_Paulo        33\\nThen, we might want to make a plot of this data using plotting library, matplotlib. You\\ncan do a bit of munging to fill in a substitute value for unknown and missing time zone\\ndata in the records. The fillna function can replace missing (NA) values and unknown\\n(empty strings) values can be replaced by boolean array indexing:\\nIn [296]: clean_tz = frame['tz'].fillna('Missing')\\nIn [297]: clean_tz[clean_tz == ''] = 'Unknown'\\nIn [298]: tz_counts = clean_tz.value_counts()\\nIn [299]: tz_counts[:10]\\nOut[299]: \\nAmerica/New_York       1251\\nUnknown                 521\\nAmerica/Chicago         400\\nAmerica/Los_Angeles     382\\nAmerica/Denver          191\\nMissing                 120\\n22 | Chapter 2: \\u2002Introductory Examples\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Europe/London            74\\nAsia/Tokyo               37\\nPacific/Honolulu         36\\nEurope/Madrid            35\\nMaking a horizontal bar plot can be accomplished using the plot method on the\\ncounts objects:\\nIn [301]: tz_counts[:10].plot(kind='barh', rot=0)\\nSee Figure 2-1 for the resulting figure. We’ll explore more tools for working with this\\nkind of data. For example, the a field contains information about the browser, device,\\nor application used to perform the URL shortening:\\nIn [302]: frame['a'][1]\\nOut[302]: u'GoogleMaps/RochesterNY'\\nIn [303]: frame['a'][50]\\nOut[303]: u'Mozilla/5.0 (Windows NT 5.1; rv:10.0.2) Gecko/20100101 Firefox/10.0.2'\\nIn [304]: frame['a'][51]\\nOut[304]: u'Mozilla/5.0 (Linux; U; Android 2.2.2; en-us; LG-P925/V10e Build/FRG83G) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1'\\nFigure 2-1. Top time zones in the 1.usa.gov sample data\\nParsing all of the interesting information in these “agent” strings may seem like a\\ndaunting task. Luckily, once you have mastered Python’s built-in string functions and\\nregular expression capabilities, it is really not so bad. For example, we could split off\\nthe first token in the string (corresponding roughly to the browser capability) and make\\nanother summary of the user behavior:\\nIn [305]: results = Series([x.split()[0] for x in frame.a.dropna()])\\nIn [306]: results[:5]\\nOut[306]: \\n0               Mozilla/5.0\\n1    GoogleMaps/RochesterNY\\n2               Mozilla/4.0\\n3               Mozilla/5.0\\n4               Mozilla/5.0\\n1.usa.gov data from bit.ly | 23\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [307]: results.value_counts()[:8]\\nOut[307]: \\nMozilla/5.0                 2594\\nMozilla/4.0                  601\\nGoogleMaps/RochesterNY       121\\nOpera/9.80                    34\\nTEST_INTERNET_AGENT           24\\nGoogleProducer                21\\nMozilla/6.0                    5\\nBlackBerry8520/5.0.0.681       4\\nNow, suppose you wanted to decompose the top time zones into Windows and non-\\nWindows users. As a simplification, let’s say that a user is on Windows if the string\\n'Windows' is in the agent string. Since some of the agents are missing, I’ll exclude these\\nfrom the data:\\nIn [308]: cframe = frame[frame.a.notnull()]\\nWe want to then compute a value whether each row is Windows or not:\\nIn [309]: operating_system = np.where(cframe['a'].str.contains('Windows'),\\n   .....:                             'Windows', 'Not Windows')\\nIn [310]: operating_system[:5]\\nOut[310]: \\n0        Windows\\n1    Not Windows\\n2        Windows\\n3    Not Windows\\n4        Windows\\nName: a\\nThen, you can group the data by its time zone column and this new list of operating\\nsystems:\\nIn [311]: by_tz_os = cframe.groupby(['tz', operating_system])\\nThe group counts, analogous to the value_counts function above, can be computed\\nusing size. This result is then reshaped into a table with unstack:\\nIn [312]: agg_counts = by_tz_os.size().unstack().fillna(0)\\nIn [313]: agg_counts[:10]\\nOut[313]: \\na                               Not Windows  Windows\\ntz                                                  \\n                                        245      276\\nAfrica/Cairo                              0        3\\nAfrica/Casablanca                         0        1\\nAfrica/Ceuta                              0        2\\nAfrica/Johannesburg                       0        1\\nAfrica/Lusaka                             0        1\\nAmerica/Anchorage                         4        1\\nAmerica/Argentina/Buenos_Aires            1        0\\n24 | Chapter 2: \\u2002Introductory Examples\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"America/Argentina/Cordoba                 0        1\\nAmerica/Argentina/Mendoza                 0        1\\nFinally, let’s select the top overall time zones. To do so, I construct an indirect index\\narray from the row counts in agg_counts:\\n# Use to sort in ascending order\\nIn [314]: indexer = agg_counts.sum(1).argsort()\\nIn [315]: indexer[:10]\\nOut[315]: \\ntz\\n                                  24\\nAfrica/Cairo                      20\\nAfrica/Casablanca                 21\\nAfrica/Ceuta                      92\\nAfrica/Johannesburg               87\\nAfrica/Lusaka                     53\\nAmerica/Anchorage                 54\\nAmerica/Argentina/Buenos_Aires    57\\nAmerica/Argentina/Cordoba         26\\nAmerica/Argentina/Mendoza         55\\nI then use take to select the rows in that order, then slice off the last 10 rows:\\nIn [316]: count_subset = agg_counts.take(indexer)[-10:]\\nIn [317]: count_subset\\nOut[317]: \\na                    Not Windows  Windows\\ntz                                       \\nAmerica/Sao_Paulo             13       20\\nEurope/Madrid                 16       19\\nPacific/Honolulu               0       36\\nAsia/Tokyo                     2       35\\nEurope/London                 43       31\\nAmerica/Denver               132       59\\nAmerica/Los_Angeles          130      252\\nAmerica/Chicago              115      285\\n                             245      276\\nAmerica/New_York             339      912\\nThen, as shown in the preceding code block, this can be plotted in a bar plot; I’ll make\\nit a stacked bar plot by passing stacked=True (see Figure 2-2) :\\nIn [319]: count_subset.plot(kind='barh', stacked=True)\\nThe plot doesn’t make it easy to see the relative percentage of Windows users in the\\nsmaller groups, but the rows can easily be normalized to sum to 1 then plotted again\\n(see Figure 2-3):\\nIn [321]: normed_subset = count_subset.div(count_subset.sum(1), axis=0)\\nIn [322]: normed_subset.plot(kind='barh', stacked=True)\\n1.usa.gov data from bit.ly | 25\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'All of the methods employed here will be examined in great detail throughout the rest\\nof the book.\\nMovieLens 1M Data Set\\nGroupLens Research (http://www.grouplens.org/node/73) provides a number of collec-\\ntions of movie ratings data collected from users of MovieLens in the late 1990s and\\nFigure 2-2. Top time zones by Windows and non-Windows users\\nFigure 2-3. Percentage Windows and non-Windows users in top-occurring time zones\\n26 | Chapter 2: \\u2002Introductory Examples\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"early 2000s. The data provide movie ratings, movie metadata (genres and year), and\\ndemographic data about the users (age, zip code, gender, and occupation). Such data\\nis often of interest in the development of recommendation systems based on machine\\nlearning algorithms. While I will not be exploring machine learning techniques in great\\ndetail in this book, I will show you how to slice and dice data sets like these into the\\nexact form you need.\\nThe MovieLens 1M data set contains 1 million ratings collected from 6000 users on\\n4000 movies. It’s spread across 3 tables: ratings, user information, and movie infor-\\nmation. After extracting the data from the zip file, each table can be loaded into a pandas\\nDataFrame object using pandas.read_table:\\nimport pandas as pd\\nunames = ['user_id', 'gender', 'age', 'occupation', 'zip']\\nusers = pd.read_table('ml-1m/users.dat', sep='::', header=None,\\n                      names=unames)\\nrnames = ['user_id', 'movie_id', 'rating', 'timestamp']\\nratings = pd.read_table('ml-1m/ratings.dat', sep='::', header=None,\\n                        names=rnames)\\nmnames = ['movie_id', 'title', 'genres']\\nmovies = pd.read_table('ml-1m/movies.dat', sep='::', header=None,\\n                        names=mnames)\\nYou can verify that everything succeeded by looking at the first few rows of each Da-\\ntaFrame with Python's slice syntax:\\nIn [334]: users[:5]\\nOut[334]: \\n   user_id gender  age  occupation    zip\\n0        1      F    1          10  48067\\n1        2      M   56          16  70072\\n2        3      M   25          15  55117\\n3        4      M   45           7  02460\\n4        5      M   25          20  55455\\nIn [335]: ratings[:5]\\nOut[335]: \\n   user_id  movie_id  rating  timestamp\\n0        1      1193       5  978300760\\n1        1       661       3  978302109\\n2        1       914       3  978301968\\n3        1      3408       4  978300275\\n4        1      2355       5  978824291\\nIn [336]: movies[:5]\\nOut[336]: \\n   movie_id                               title                        genres\\n0         1                    Toy Story (1995)   Animation|Children's|Comedy\\n1         2                      Jumanji (1995)  Adventure|Children's|Fantasy\\n2         3             Grumpier Old Men (1995)                Comedy|Romance\\n3         4            Waiting to Exhale (1995)                  Comedy|Drama\\nMovieLens 1M Data Set | 27\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"4         5  Father of the Bride Part II (1995)                        Comedy\\nIn [337]: ratings\\nOut[337]: \\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 1000209 entries, 0 to 1000208\\nData columns:\\nuser_id      1000209  non-null values\\nmovie_id     1000209  non-null values\\nrating       1000209  non-null values\\ntimestamp    1000209  non-null values\\ndtypes: int64(4)\\nNote that ages and occupations are coded as integers indicating groups described in\\nthe data set’s README file. Analyzing the data spread across three tables is not a simple\\ntask; for example, suppose you wanted to compute mean ratings for a particular movie\\nby sex and age. As you will see, this is much easier to do with all of the data merged\\ntogether into a single table. Using pandas’s merge function, we first merge ratings with\\nusers then merging that result with the movies data. pandas infers which columns to\\nuse as the merge (or join) keys based on overlapping names:\\nIn [338]: data = pd.merge(pd.merge(ratings, users), movies)\\nIn [339]: data\\nOut[339]: \\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 1000209 entries, 0 to 1000208\\nData columns:\\nuser_id       1000209  non-null values\\nmovie_id      1000209  non-null values\\nrating        1000209  non-null values\\ntimestamp     1000209  non-null values\\ngender        1000209  non-null values\\nage           1000209  non-null values\\noccupation    1000209  non-null values\\nzip           1000209  non-null values\\ntitle         1000209  non-null values\\ngenres        1000209  non-null values\\ndtypes: int64(6), object(4)\\nIn [340]: data.ix[0]\\nOut[340]: \\nuser_id                                 1\\nmovie_id                                1\\nrating                                  5\\ntimestamp                       978824268\\ngender                                  F\\nage                                     1\\noccupation                             10\\nzip                                 48067\\ntitle                    Toy Story (1995)\\ngenres        Animation|Children's|Comedy\\nName: 0\\n28 | Chapter 2: \\u2002Introductory Examples\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In this form, aggregating the ratings grouped by one or more user or movie attributes\\nis straightforward once you build some familiarity with pandas. To get mean movie\\nratings for each film grouped by gender, we can use the pivot_table method:\\nIn [341]: mean_ratings = data.pivot_table('rating', rows='title',\\n   .....:                                 cols='gender', aggfunc='mean')\\nIn [342]: mean_ratings[:5]\\nOut[342]: \\ngender                                F         M\\ntitle                                            \\n$1,000,000 Duck (1971)         3.375000  2.761905\\n'Night Mother (1986)           3.388889  3.352941\\n'Til There Was You (1997)      2.675676  2.733333\\n'burbs, The (1989)             2.793478  2.962085\\n...And Justice for All (1979)  3.828571  3.689024\\nThis produced another DataFrame containing mean ratings with movie totals as row\\nlabels and gender as column labels. First, I’m going to filter down to movies that re-\\nceived at least 250 ratings (a completely arbitrary number); to do this, I group the data\\nby title and use size() to get a Series of group sizes for each title:\\nIn [343]: ratings_by_title = data.groupby('title').size()\\nIn [344]: ratings_by_title[:10]\\nOut[344]: \\ntitle\\n$1,000,000 Duck (1971)                37\\n'Night Mother (1986)                  70\\n'Til There Was You (1997)             52\\n'burbs, The (1989)                   303\\n...And Justice for All (1979)        199\\n1-900 (1994)                           2\\n10 Things I Hate About You (1999)    700\\n101 Dalmatians (1961)                565\\n101 Dalmatians (1996)                364\\n12 Angry Men (1957)                  616\\nIn [345]: active_titles = ratings_by_title.index[ratings_by_title >= 250]\\nIn [346]: active_titles\\nOut[346]: \\nIndex(['burbs, The (1989), 10 Things I Hate About You (1999),\\n       101 Dalmatians (1961), ..., Young Sherlock Holmes (1985),\\n       Zero Effect (1998), eXistenZ (1999)], dtype=object)\\nThe index of titles receiving at least 250 ratings can then be used to select rows from\\nmean_ratings above:\\nIn [347]: mean_ratings = mean_ratings.ix[active_titles]\\nIn [348]: mean_ratings\\nOut[348]: \\n<class 'pandas.core.frame.DataFrame'>\\nIndex: 1216 entries, 'burbs, The (1989) to eXistenZ (1999)\\nMovieLens 1M Data Set | 29\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Data columns:\\nF    1216  non-null values\\nM    1216  non-null values\\ndtypes: float64(2)\\nTo see the top films among female viewers, we can sort by the F column in descending\\norder:\\nIn [350]: top_female_ratings = mean_ratings.sort_index(by='F', ascending=False)\\nIn [351]: top_female_ratings[:10]\\nOut[351]: \\ngender                                                         F         M\\nClose Shave, A (1995)                                   4.644444  4.473795\\nWrong Trousers, The (1993)                              4.588235  4.478261\\nSunset Blvd. (a.k.a. Sunset Boulevard) (1950)           4.572650  4.464589\\nWallace & Gromit: The Best of Aardman Animation (1996)  4.563107  4.385075\\nSchindler's List (1993)                                 4.562602  4.491415\\nShawshank Redemption, The (1994)                        4.539075  4.560625\\nGrand Day Out, A (1992)                                 4.537879  4.293255\\nTo Kill a Mockingbird (1962)                            4.536667  4.372611\\nCreature Comforts (1990)                                4.513889  4.272277\\nUsual Suspects, The (1995)                              4.513317  4.518248\\nMeasuring rating disagreement\\nSuppose you wanted to find the movies that are most divisive between male and female\\nviewers. One way is to add a column to mean_ratings containing the difference in\\nmeans, then sort by that:\\nIn [352]: mean_ratings['diff'] = mean_ratings['M'] - mean_ratings['F']\\nSorting by 'diff' gives us the movies with the greatest rating difference and which were\\npreferred by women:\\nIn [353]: sorted_by_diff = mean_ratings.sort_index(by='diff')\\nIn [354]: sorted_by_diff[:15]\\nOut[354]: \\ngender                                        F         M      diff\\nDirty Dancing (1987)                   3.790378  2.959596 -0.830782\\nJumpin' Jack Flash (1986)              3.254717  2.578358 -0.676359\\nGrease (1978)                          3.975265  3.367041 -0.608224\\nLittle Women (1994)                    3.870588  3.321739 -0.548849\\nSteel Magnolias (1989)                 3.901734  3.365957 -0.535777\\nAnastasia (1997)                       3.800000  3.281609 -0.518391\\nRocky Horror Picture Show, The (1975)  3.673016  3.160131 -0.512885\\nColor Purple, The (1985)               4.158192  3.659341 -0.498851\\nAge of Innocence, The (1993)           3.827068  3.339506 -0.487561\\nFree Willy (1993)                      2.921348  2.438776 -0.482573\\nFrench Kiss (1995)                     3.535714  3.056962 -0.478752\\nLittle Shop of Horrors, The (1960)     3.650000  3.179688 -0.470312\\nGuys and Dolls (1955)                  4.051724  3.583333 -0.468391\\nMary Poppins (1964)                    4.197740  3.730594 -0.467147\\nPatch Adams (1998)                     3.473282  3.008746 -0.464536\\n30 | Chapter 2: \\u2002Introductory Examples\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Reversing the order of the rows and again slicing off the top 15 rows, we get the movies\\npreferred by men that women didn’t rate as highly:\\n# Reverse order of rows, take first 15 rows\\nIn [355]: sorted_by_diff[::-1][:15]\\nOut[355]: \\ngender                                         F         M      diff\\nGood, The Bad and The Ugly, The (1966)  3.494949  4.221300  0.726351\\nKentucky Fried Movie, The (1977)        2.878788  3.555147  0.676359\\nDumb & Dumber (1994)                    2.697987  3.336595  0.638608\\nLongest Day, The (1962)                 3.411765  4.031447  0.619682\\nCable Guy, The (1996)                   2.250000  2.863787  0.613787\\nEvil Dead II (Dead By Dawn) (1987)      3.297297  3.909283  0.611985\\nHidden, The (1987)                      3.137931  3.745098  0.607167\\nRocky III (1982)                        2.361702  2.943503  0.581801\\nCaddyshack (1980)                       3.396135  3.969737  0.573602\\nFor a Few Dollars More (1965)           3.409091  3.953795  0.544704\\nPorky's (1981)                          2.296875  2.836364  0.539489\\nAnimal House (1978)                     3.628906  4.167192  0.538286\\nExorcist, The (1973)                    3.537634  4.067239  0.529605\\nFright Night (1985)                     2.973684  3.500000  0.526316\\nBarb Wire (1996)                        1.585366  2.100386  0.515020\\nSuppose instead you wanted the movies that elicited the most disagreement among\\nviewers, independent of gender. Disagreement can be measured by the variance or\\nstandard deviation of the ratings:\\n# Standard deviation of rating grouped by title\\nIn [356]: rating_std_by_title = data.groupby('title')['rating'].std()\\n# Filter down to active_titles\\nIn [357]: rating_std_by_title = rating_std_by_title.ix[active_titles]\\n# Order Series by value in descending order\\nIn [358]: rating_std_by_title.order(ascending=False)[:10]\\nOut[358]: \\ntitle\\nDumb & Dumber (1994)                     1.321333\\nBlair Witch Project, The (1999)          1.316368\\nNatural Born Killers (1994)              1.307198\\nTank Girl (1995)                         1.277695\\nRocky Horror Picture Show, The (1975)    1.260177\\nEyes Wide Shut (1999)                    1.259624\\nEvita (1996)                             1.253631\\nBilly Madison (1995)                     1.249970\\nFear and Loathing in Las Vegas (1998)    1.246408\\nBicentennial Man (1999)                  1.245533\\nName: rating\\nYou may have noticed that movie genres are given as a pipe-separated (|) string. If you\\nwanted to do some analysis by genre, more work would be required to transform the\\ngenre information into a more usable form. I will revisit this data later in the book to\\nillustrate such a transformation.\\nMovieLens 1M Data Set | 31\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'US Baby Names 1880-2010\\nThe United States Social Security Administration (SSA) has made available data on the\\nfrequency of baby names from 1880 through the present. Hadley Wickham, an author\\nof several popular R packages, has often made use of this data set in illustrating data\\nmanipulation in R.\\nIn [4]: names.head(10)\\nOut[4]:\\n        name sex  births  year\\n0       Mary   F    7065  1880\\n1       Anna   F    2604  1880\\n2       Emma   F    2003  1880\\n3  Elizabeth   F    1939  1880\\n4     Minnie   F    1746  1880\\n5   Margaret   F    1578  1880\\n6        Ida   F    1472  1880\\n7      Alice   F    1414  1880\\n8     Bertha   F    1320  1880\\n9      Sarah   F    1288  1880\\nThere are many things you might want to do with the data set:\\n• Visualize the proportion of babies given a particular name (your own, or another\\nname) over time.\\n• Determine the relative rank of a name.\\n• Determine the most popular names in each year or the names with largest increases\\nor decreases.\\n• Analyze trends in names: vowels, consonants, length, overall diversity, changes in\\nspelling, first and last letters\\n• Analyze external sources of trends: biblical names, celebrities, demographic\\nchanges\\nUsing the tools we’ve looked at so far, most of these kinds of analyses are very straight-\\nforward, so I will walk you through many of them. I encourage you to download and\\nexplore the data yourself. If you find an interesting pattern in the data, I would love to\\nhear about it.\\nAs of this writing, the US Social Security Administration makes available data files, one\\nper year, containing the total number of births for each sex/name combination. The\\nraw archive of these files can be obtained here:\\nhttp://www.ssa.gov/oact/babynames/limits.html\\nIn the event that this page has been moved by the time you’re reading this, it can most\\nlikely be located again by Internet search. After downloading the “National data” file\\nnames.zip and unzipping it, you will have a directory containing a series of files like\\nyob1880.txt. I use the UNIX head command to look at the first 10 lines of one of the\\nfiles (on Windows, you can use the more command or open it in a text editor):\\n32 | Chapter 2: \\u2002Introductory Examples\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [367]: !head -n 10 names/yob1880.txt\\nMary,F,7065\\nAnna,F,2604\\nEmma,F,2003\\nElizabeth,F,1939\\nMinnie,F,1746\\nMargaret,F,1578\\nIda,F,1472\\nAlice,F,1414\\nBertha,F,1320\\nSarah,F,1288\\nAs this is a nicely comma-separated form, it can be loaded into a DataFrame with\\npandas.read_csv:\\nIn [368]: import pandas as pd\\nIn [369]: names1880 = pd.read_csv('names/yob1880.txt', names=['name', 'sex', 'births'])\\nIn [370]: names1880\\nOut[370]: \\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 2000 entries, 0 to 1999\\nData columns:\\nname      2000  non-null values\\nsex       2000  non-null values\\nbirths    2000  non-null values\\ndtypes: int64(1), object(2)\\nThese files only contain names with at least 5 occurrences in each year, so for simplic-\\nity’s sake we can use the sum of the births column by sex as the total number of births\\nin that year:\\nIn [371]: names1880.groupby('sex').births.sum()\\nOut[371]: \\nsex\\nF       90993\\nM      110493\\nName: births\\nSince the data set is split into files by year, one of the first things to do is to assemble\\nall of the data into a single DataFrame and further to add a year field. This is easy to\\ndo using pandas.concat:\\n# 2010 is the last available year right now\\nyears = range(1880, 2011)\\npieces = []\\ncolumns = ['name', 'sex', 'births']\\nfor year in years:\\n    path = 'names/yob%d.txt' % year\\n    frame = pd.read_csv(path, names=columns)\\n    frame['year'] = year\\n    pieces.append(frame)\\nUS Baby Names 1880-2010 | 33\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"# Concatenate everything into a single DataFrame\\nnames = pd.concat(pieces, ignore_index=True)\\nThere are a couple things to note here. First, remember that concat glues the DataFrame\\nobjects together row-wise by default. Secondly, you have to pass ignore_index=True\\nbecause we’re not interested in preserving the original row numbers returned from\\nread_csv. So we now have a very large DataFrame containing all of the names data:\\nNow the names DataFrame looks like:\\nIn [373]: names\\nOut[373]: \\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 1690784 entries, 0 to 1690783\\nData columns:\\nname      1690784  non-null values\\nsex       1690784  non-null values\\nbirths    1690784  non-null values\\nyear      1690784  non-null values\\ndtypes: int64(2), object(2)\\nWith this data in hand, we can already start aggregating the data at the year and sex\\nlevel using groupby or pivot_table, see Figure 2-4:\\nIn [374]: total_births = names.pivot_table('births', rows='year',\\n   .....:                                  cols='sex', aggfunc=sum)\\nIn [375]: total_births.tail()\\nOut[375]: \\nsex         F        M\\nyear                  \\n2006  1896468  2050234\\n2007  1916888  2069242\\n2008  1883645  2032310\\n2009  1827643  1973359\\n2010  1759010  1898382\\nIn [376]: total_births.plot(title='Total births by sex and year')\\nNext, let’s insert a column prop with the fraction of babies given each name relative to\\nthe total number of births. A prop value of 0.02 would indicate that 2 out of every 100\\nbabies was given a particular name. Thus, we group the data by year and sex, then add\\nthe new column to each group:\\ndef add_prop(group):\\n    # Integer division floors\\n    births = group.births.astype(float)\\n    group['prop'] = births / births.sum()\\n    return group\\nnames = names.groupby(['year', 'sex']).apply(add_prop)\\n34 | Chapter 2: \\u2002Introductory Examples\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Remember that because births is of integer type, we have to cast either\\nthe numerator or denominator to floating point to compute a fraction\\n(unless you are using Python 3!).\\nThe resulting complete data set now has the following columns:\\nIn [378]: names\\nOut[378]: \\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 1690784 entries, 0 to 1690783\\nData columns:\\nname      1690784  non-null values\\nsex       1690784  non-null values\\nbirths    1690784  non-null values\\nyear      1690784  non-null values\\nprop      1690784  non-null values\\ndtypes: float64(1), int64(2), object(2)\\nWhen performing a group operation like this, it's often valuable to do a sanity check,\\nlike verifying that the prop column sums to 1 within all the groups. Since this is floating\\npoint data, use np.allclose to check that the group sums are sufficiently close to (but\\nperhaps not exactly equal to) 1:\\nIn [379]: np.allclose(names.groupby(['year', 'sex']).prop.sum(), 1)\\nOut[379]: True\\nNow that this is done, I’m going to extract a subset of the data to facilitate further\\nanalysis: the top 1000 names for each sex/year combination. This is yet another group\\noperation:\\ndef get_top1000(group):\\n    return group.sort_index(by='births', ascending=False)[:1000]\\nFigure 2-4. Total births by sex and year\\nUS Baby Names 1880-2010 | 35\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'grouped = names.groupby([\\'year\\', \\'sex\\'])\\ntop1000 = grouped.apply(get_top1000)\\nIf you prefer a do-it-yourself approach, you could also do:\\npieces = []\\nfor year, group in names.groupby([\\'year\\', \\'sex\\']):\\n    pieces.append(group.sort_index(by=\\'births\\', ascending=False)[:1000])\\ntop1000 = pd.concat(pieces, ignore_index=True)\\nThe resulting data set is now quite a bit smaller:\\nIn [382]: top1000\\nOut[382]: \\n<class \\'pandas.core.frame.DataFrame\\'>\\nInt64Index: 261877 entries, 0 to 261876\\nData columns:\\nname      261877  non-null values\\nsex       261877  non-null values\\nbirths    261877  non-null values\\nyear      261877  non-null values\\nprop      261877  non-null values\\ndtypes: float64(1), int64(2), object(2)\\nWe’ll use this Top 1,000 data set in the following investigations into the data.\\nAnalyzing Naming Trends\\nWith the full data set and Top 1,000 data set in hand, we can start analyzing various\\nnaming trends of interest. Splitting the Top 1,000 names into the boy and girl portions\\nis easy to do first:\\nIn [383]: boys = top1000[top1000.sex == \\'M\\']\\nIn [384]: girls = top1000[top1000.sex == \\'F\\']\\nSimple time series, like the number of Johns or Marys for each year can be plotted but\\nrequire a bit of munging to be a bit more useful. Let’s form a pivot table of the total\\nnumber of births by year and name:\\nIn [385]: total_births = top1000.pivot_table(\\'births\\', rows=\\'year\\', cols=\\'name\\',\\n   .....:                                    aggfunc=sum)\\nNow, this can be plotted for a handful of names using DataFrame’s plot method:\\nIn [386]: total_births\\nOut[386]: \\n<class \\'pandas.core.frame.DataFrame\\'>\\nInt64Index: 131 entries, 1880 to 2010\\nColumns: 6865 entries, Aaden to Zuri\\ndtypes: float64(6865)\\nIn [387]: subset = total_births[[\\'John\\', \\'Harry\\', \\'Mary\\', \\'Marilyn\\']]\\nIn [388]: subset.plot(subplots=True, figsize=(12, 10), grid=False,\\n   .....:             title=\"Number of births per year\")\\n36 | Chapter 2: \\u2002Introductory Examples\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"See Figure 2-5 for the result. On looking at this, you might conclude that these names\\nhave grown out of favor with the American population. But the story is actually more\\ncomplicated than that, as will be explored in the next section.\\nFigure 2-5. A few boy and girl names over time\\nMeasuring the increase in naming diversity\\nOne explanation for the decrease in plots above is that fewer parents are choosing\\ncommon names for their children. This hypothesis can be explored and confirmed in\\nthe data. One measure is the proportion of births represented by the top 1000 most\\npopular names, which I aggregate and plot by year and sex:\\nIn [390]: table = top1000.pivot_table('prop', rows='year',\\n   .....:                             cols='sex', aggfunc=sum)\\nIn [391]: table.plot(title='Sum of table1000.prop by year and sex',\\n   .....:            yticks=np.linspace(0, 1.2, 13), xticks=range(1880, 2020, 10))\\nSee Figure 2-6 for this plot. So you can see that, indeed, there appears to be increasing\\nname diversity (decreasing total proportion in the top 1,000). Another interesting met-\\nric is the number of distinct names, taken in order of popularity from highest to lowest,\\nin the top 50% of births. This number is a bit more tricky to compute. Let’s consider\\njust the boy names from 2010:\\nIn [392]: df = boys[boys.year == 2010]\\nIn [393]: df\\nOut[393]: \\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 1000 entries, 260877 to 261876\\nData columns:\\nUS Baby Names 1880-2010 | 37\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"name      1000  non-null values\\nsex       1000  non-null values\\nbirths    1000  non-null values\\nyear      1000  non-null values\\nprop      1000  non-null values\\ndtypes: float64(1), int64(2), object(2)\\nFigure 2-6. Proportion of births represented in top 1000 names by sex\\nAfter sorting prop in descending order, we want to know how many of the most popular\\nnames it takes to reach 50%. You could write a for loop to do this, but a vectorized\\nNumPy way is a bit more clever. Taking the cumulative sum, cumsum, of prop then calling\\nthe method searchsorted returns the position in the cumulative sum at which 0.5 would\\nneed to be inserted to keep it in sorted order:\\nIn [394]: prop_cumsum = df.sort_index(by='prop', ascending=False).prop.cumsum()\\nIn [395]: prop_cumsum[:10]\\nOut[395]: \\n260877    0.011523\\n260878    0.020934\\n260879    0.029959\\n260880    0.038930\\n260881    0.047817\\n260882    0.056579\\n260883    0.065155\\n260884    0.073414\\n260885    0.081528\\n260886    0.089621\\nIn [396]: prop_cumsum.searchsorted(0.5)\\nOut[396]: 116\\n38 | Chapter 2: \\u2002Introductory Examples\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Since arrays are zero-indexed, adding 1 to this result gives you a result of 117. By con-\\ntrast, in 1900 this number was much smaller:\\nIn [397]: df = boys[boys.year == 1900]\\nIn [398]: in1900 = df.sort_index(by=\\'prop\\', ascending=False).prop.cumsum()\\nIn [399]: in1900.searchsorted(0.5) + 1\\nOut[399]: 25\\nIt should now be fairly straightforward to apply this operation to each year/sex com-\\nbination; groupby those fields and apply a function returning the count for each group:\\ndef get_quantile_count(group, q=0.5):\\n    group = group.sort_index(by=\\'prop\\', ascending=False)\\n    return group.prop.cumsum().searchsorted(q) + 1\\ndiversity = top1000.groupby([\\'year\\', \\'sex\\']).apply(get_quantile_count)\\ndiversity = diversity.unstack(\\'sex\\')\\nThis resulting DataFrame diversity now has two time series, one for each sex, indexed\\nby year. This can be inspected in IPython and plotted as before (see Figure 2-7):\\nIn [401]: diversity.head()\\nOut[401]: \\nsex    F   M\\nyear        \\n1880  38  14\\n1881  38  14\\n1882  38  15\\n1883  39  15\\n1884  39  16\\nIn [402]: diversity.plot(title=\"Number of popular names in top 50%\")\\nFigure 2-7. Plot of diversity metric by year\\nUS Baby Names 1880-2010 | 39\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"As you can see, girl names have always been more diverse than boy names, and they\\nhave only become more so over time. Further analysis of what exactly is driving the\\ndiversity, like the increase of alternate spellings, is left to the reader.\\nThe “Last letter” Revolution\\nIn 2007, a baby name researcher Laura Wattenberg pointed out on her website ( http:\\n//www.babynamewizard.com) that the distribution of boy names by final letter has\\nchanged significantly over the last 100 years. To see this, I first aggregate all of the births\\nin the full data set by year, sex, and final letter:\\n# extract last letter from name column\\nget_last_letter = lambda x: x[-1]\\nlast_letters = names.name.map(get_last_letter)\\nlast_letters.name = 'last_letter'\\ntable = names.pivot_table('births', rows=last_letters,\\n                          cols=['sex', 'year'], aggfunc=sum)\\nThen, I select out three representative years spanning the history and print the first few\\nrows:\\nIn [404]: subtable = table.reindex(columns=[1910, 1960, 2010], level='year')\\nIn [405]: subtable.head()\\nOut[405]: \\nsex               F                      M                \\nyear           1910    1960    2010   1910    1960    2010\\nlast_letter                                               \\na            108376  691247  670605    977    5204   28438\\nb               NaN     694     450    411    3912   38859\\nc                 5      49     946    482   15476   23125\\nd              6750    3729    2607  22111  262112   44398\\ne            133569  435013  313833  28655  178823  129012\\nNext, normalize the table by total births to compute a new table containing proportion\\nof total births for each sex ending in each letter:\\nIn [406]: subtable.sum()\\nOut[406]: \\nsex  year\\nF    1910     396416\\n     1960    2022062\\n     2010    1759010\\nM    1910     194198\\n     1960    2132588\\n     2010    1898382\\nIn [407]: letter_prop = subtable / subtable.sum().astype(float)\\nWith the letter proportions now in hand, I can make bar plots for each sex broken\\ndown by year. See Figure 2-8:\\nimport matplotlib.pyplot as plt\\n40 | Chapter 2: \\u2002Introductory Examples\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"fig, axes = plt.subplots(2, 1, figsize=(10, 8))\\nletter_prop['M'].plot(kind='bar', rot=0, ax=axes[0], title='Male')\\nletter_prop['F'].plot(kind='bar', rot=0, ax=axes[1], title='Female',\\n                      legend=False)\\nFigure 2-8. Proportion of boy and girl names ending in each letter\\nAs you can see, boy names ending in “n” have experienced significant growth since the\\n1960s. Going back to the full table created above, I again normalize by year and sex\\nand select a subset of letters for the boy names, finally transposing to make each column\\na time series:\\nIn [410]: letter_prop = table / table.sum().astype(float)\\nIn [411]: dny_ts = letter_prop.ix[['d', 'n', 'y'], 'M'].T\\nIn [412]: dny_ts.head()\\nOut[412]: \\n             d         n         y\\nyear                              \\n1880  0.083055  0.153213  0.075760\\n1881  0.083247  0.153214  0.077451\\n1882  0.085340  0.149560  0.077537\\n1883  0.084066  0.151646  0.079144\\n1884  0.086120  0.149915  0.080405\\nWith this DataFrame of time series in hand, I can make a plot of the trends over time\\nagain with its plot method (see Figure 2-9):\\nIn [414]: dny_ts.plot()\\nUS Baby Names 1880-2010 | 41\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Figure 2-9. Proportion of boys born with names ending in d/n/y over time\\nBoy names that became girl names (and vice versa)\\nAnother fun trend is looking at boy names that were more popular with one sex earlier\\nin the sample but have “changed sexes” in the present. One example is the name Lesley\\nor Leslie. Going back to the top1000 dataset, I compute a list of names occurring in the\\ndataset starting with 'lesl':\\nIn [415]: all_names = top1000.name.unique()\\nIn [416]: mask = np.array(['lesl' in x.lower() for x in all_names])\\nIn [417]: lesley_like = all_names[mask]\\nIn [418]: lesley_like\\nOut[418]: array([Leslie, Lesley, Leslee, Lesli, Lesly], dtype=object)\\nFrom there, we can filter down to just those names and sum births grouped by name\\nto see the relative frequencies:\\nIn [419]: filtered = top1000[top1000.name.isin(lesley_like)]\\nIn [420]: filtered.groupby('name').births.sum()\\nOut[420]: \\nname\\nLeslee      1082\\nLesley     35022\\nLesli        929\\nLeslie    370429\\nLesly      10067\\nName: births\\nNext, let’s aggregate by sex and year and normalize within year:\\n42 | Chapter 2: \\u2002Introductory Examples\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [421]: table = filtered.pivot_table('births', rows='year',\\n   .....:                              cols='sex', aggfunc='sum')\\nIn [422]: table = table.div(table.sum(1), axis=0)\\nIn [423]: table.tail()\\nOut[423]: \\nsex   F   M\\nyear       \\n2006  1 NaN\\n2007  1 NaN\\n2008  1 NaN\\n2009  1 NaN\\n2010  1 NaN\\nLastly, it’s now easy to make a plot of the breakdown by sex over time (Figure 2-10):\\nIn [425]: table.plot(style={'M': 'k-', 'F': 'k--'})\\nFigure 2-10. Proportion of male/female Lesley-like names over time\\nConclusions and The Path Ahead\\nThe examples in this chapter are rather simple, but they’re here to give you a bit of a\\nflavor of what sorts of things you can expect in the upcoming chapters. The focus of\\nthis book is on tools as opposed to presenting more sophisticated analytical methods.\\nMastering the techniques in this book will enable you to implement your own analyses\\n(assuming you know what you want to do!) in short order.\\nConclusions and The Path Ahead | 43\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf', 'page_content': 'www.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'CHAPTER 3\\nIPython: An Interactive Computing and\\nDevelopment Environment\\nAct without doing; work without effort. Think of the small as large and the few as many.\\nConfront the difficult while it is still easy; accomplish the great task by a series of small\\nacts.\\n—Laozi\\nPeople often ask me, “What is your Python development environment?” My answer is\\nalmost always the same, “IPython and a text editor”. You may choose to substitute an\\nIntegrated Development Environment (IDE) for a text editor in order to take advantage\\nof more advanced graphical tools and code completion capabilities. Even if so, I strongly\\nrecommend making IPython an important part of your workflow. Some IDEs even\\nprovide IPython integration, so it’s possible to get the best of both worlds.\\nThe IPython project began in 2001 as Fernando Pérez’s side project to make a better\\ninteractive Python interpreter. In the subsequent 11 years it has grown into what’s\\nwidely considered one of the most important tools in the modern scientific Python\\ncomputing stack. While it does not provide any computational or data analytical tools\\nby itself, IPython is designed from the ground up to maximize your productivity in both\\ninteractive computing and software development. It encourages an execute-explore\\nworkflow instead of the typical edit-compile-run workflow of many other programming\\nlanguages. It also provides very tight integration with the operating system’s shell and\\nfile system. Since much of data analysis coding involves exploration, trial and error,\\nand iteration, IPython will, in almost all cases, help you get the job done faster.\\nOf course, the IPython project now encompasses a great deal more than just an en-\\nhanced, interactive Python shell. It also includes a rich GUI console with inline plotting,\\na web-based interactive notebook format, and a lightweight, fast parallel computing\\nengine. And, as with so many other tools designed for and by programmers, it is highly\\ncustomizable. I’ll discuss some of these features later in the chapter.\\n45\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Since IPython has interactivity at its core, some of the features in this chapter are dif-\\nficult to fully illustrate without a live console. If this is your first time learning about\\nIPython, I recommend that you follow along with the examples to get a feel for how\\nthings work. As with any keyboard-driven console-like environment, developing mus-\\ncle-memory for the common commands is part of the learning curve.\\nMany parts of this chapter (for example: profiling and debugging) can\\nbe safely omitted on a first reading as they are not necessary for under-\\nstanding the rest of the book. This chapter is intended to provide a\\nstandalone, rich overview of the functionality provided by IPython.\\nIPython Basics\\nYou can launch IPython on the command line just like launching the regular Python\\ninterpreter except with the ipython command:\\n$ ipython\\nPython 2.7.2 (default, May 27 2012, 21:26:12)\\nType \"copyright\", \"credits\" or \"license\" for more information.\\nIPython 0.12 -- An enhanced Interactive Python.\\n?         -> Introduction and overview of IPython\\'s features.\\n%quickref -> Quick reference.\\nhelp      -> Python\\'s own help system.\\nobject?   -> Details about \\'object\\', use \\'object??\\' for extra details.\\nIn [1]: a = 5\\nIn [2]: a\\nOut[2]: 5\\nYou can execute arbitrary Python statements by typing them in and pressing\\n<return>. When typing just a variable into IPython, it renders a string representation\\nof the object:\\nIn [542]: data = {i : randn() for i in range(7)}\\nIn [543]: data\\nOut[543]: \\n{0: 0.6900018528091594,\\n 1: 1.0015434424937888,\\n 2: -0.5030873913603446,\\n 3: -0.6222742250596455,\\n 4: -0.9211686080130108,\\n 5: -0.726213492660829,\\n 6: 0.2228955458351768}\\n46 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Many kinds of Python objects are formatted to be more readable, or pretty-printed,\\nwhich is distinct from normal printing with print. If you printed a dict like the above\\nin the standard Python interpreter, it would be much less readable:\\n>>> from numpy.random import randn\\n>>> data = {i : randn() for i in range(7)}\\n>>> print data\\n{0: -1.5948255432744511, 1: 0.10569006472787983, 2: 1.972367135977295,\\n3: 0.15455217573074576, 4: -0.24058577449429575, 5: -1.2904897053651216,\\n6: 0.3308507317325902}\\nIPython also provides facilities to make it easy to execute arbitrary blocks of code (via\\nsomewhat glorified copy-and-pasting) and whole Python scripts. These will be dis-\\ncussed shortly.\\nTab Completion\\nOn the surface, the IPython shell looks like a cosmetically slightly-different interactive\\nPython interpreter. Users of Mathematica may find the enumerated input and output\\nprompts familiar. One of the major improvements over the standard Python shell is\\ntab completion , a feature common to most interactive data analysis environments.\\nWhile entering expressions in the shell, pressing <Tab> will search the namespace for\\nany variables (objects, functions, etc.) matching the characters you have typed so far:\\nIn [1]: an_apple = 27\\nIn [2]: an_example = 42\\nIn [3]: an<Tab>\\nan_apple    and         an_example  any\\nIn this example, note that IPython displayed both the two variables I defined as well as\\nthe Python keyword and and built-in function any. Naturally, you can also complete\\nmethods and attributes on any object after typing a period:\\nIn [3]: b = [1, 2, 3]\\nIn [4]: b.<Tab>\\nb.append   b.extend   b.insert   b.remove   b.sort\\nb.count    b.index    b.pop      b.reverse\\nThe same goes for modules:\\nIn [1]: import datetime\\nIn [2]: datetime.<Tab>\\ndatetime.date           datetime.MAXYEAR        datetime.timedelta\\ndatetime.datetime       datetime.MINYEAR        datetime.tzinfo\\ndatetime.datetime_CAPI  datetime.time\\nIPython Basics | 47\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Note that IPython by default hides methods and attributes starting with\\nunderscores, such as magic methods and internal “private” methods\\nand attributes, in order to avoid cluttering the display (and confusing\\nnew Python users!). These, too, can be tab-completed but you must first\\ntype an underscore to see them. If you prefer to always see such methods\\nin tab completion, you can change this setting in the IPython configu-\\nration.\\nTab completion works in many contexts outside of searching the interactive namespace\\nand completing object or module attributes.When typing anything that looks like a file\\npath (even in a Python string), pressing <Tab> will complete anything on your com-\\nputer’s file system matching what you’ve typed:\\nIn [3]: book_scripts/<Tab>\\nbook_scripts/cprof_example.py        book_scripts/ipython_script_test.py\\nbook_scripts/ipython_bug.py          book_scripts/prof_mod.py\\nIn [3]: path = \\'book_scripts/<Tab>\\nbook_scripts/cprof_example.py        book_scripts/ipython_script_test.py\\nbook_scripts/ipython_bug.py          book_scripts/prof_mod.py\\nCombined with the %run command (see later section), this functionality will undoubt-\\nedly save you many keystrokes.\\nAnother area where tab completion saves time is in the completion of function keyword\\narguments (including the = sign!).\\nIntrospection\\nUsing a question mark (?) before or after a variable will display some general informa-\\ntion about the object:\\nIn [545]: b?\\nType:       list\\nString Form:[1, 2, 3]\\nLength:     3\\nDocstring:\\nlist() -> new empty list\\nlist(iterable) -> new list initialized from iterable\\'s items\\nThis is referred to as object introspection. If the object is a function or instance method,\\nthe docstring, if defined, will also be shown. Suppose we’d written the following func-\\ntion:\\ndef add_numbers(a, b):\\n    \"\"\"\\n    Add two numbers together\\n    Returns\\n    -------\\n    the_sum : type of arguments\\n48 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': '\"\"\"\\n    return a + b\\nThen using ? shows us the docstring:\\nIn [547]: add_numbers?\\nType:       function\\nString Form:<function add_numbers at 0x5fad848>\\nFile:       book_scripts/<ipython-input-546-5473012eeb65>\\nDefinition: add_numbers(a, b)\\nDocstring:\\nAdd two numbers together\\nReturns\\n-------\\nthe_sum : type of arguments\\nUsing ?? will also show the function’s source code if possible:\\nIn [548]: add_numbers??\\nType:       function\\nString Form:<function add_numbers at 0x5fad848>\\nFile:       book_scripts/<ipython-input-546-5473012eeb65>\\nDefinition: add_numbers(a, b)\\nSource:\\ndef add_numbers(a, b):\\n    \"\"\"\\n    Add two numbers together\\n    Returns\\n    -------\\n    the_sum : type of arguments\\n    \"\"\"\\n    return a + b\\n? has a final usage, which is for searching the IPython namespace in a manner similar\\nto the standard UNIX or Windows command line. A number of characters combined\\nwith the wildcard ( *) will show all names matching the wildcard expression. For ex-\\nample, we could get a list of all functions in the top level NumPy namespace containing\\nload:\\nIn [549]: np.*load*?\\nnp.load\\nnp.loads\\nnp.loadtxt\\nnp.pkgload\\nThe %run Command\\nAny file can be run as a Python program inside the environment of your IPython session\\nusing the %run command. Suppose you had the following simple script stored in ipy\\nthon_script_test.py:\\ndef f(x, y, z):\\n    return (x + y) / z\\na = 5\\nIPython Basics | 49\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'b = 6\\nc = 7.5\\nresult = f(a, b, c)\\nThis can be executed by passing the file name to %run:\\nIn [550]: %run ipython_script_test.py\\nThe script is run in an empty namespace (with no imports or other variables defined)\\nso that the behavior should be identical to running the program on the command line\\nusing python script.py. All of the variables (imports, functions, and globals) defined\\nin the file (up until an exception, if any, is raised) will then be accessible in the IPython\\nshell:\\nIn [551]: c\\nOut[551]: 7.5\\nIn [552]: result\\nOut[552]: 1.4666666666666666\\nIf a Python script expects command line arguments (to be found in sys.argv), these\\ncan be passed after the file path as though run on the command line.\\nShould you wish to give a script access to variables already defined in\\nthe interactive IPython namespace, use %run -i instead of plain %run.\\nInterrupting running code\\nPressing <Ctrl-C> while any code is running, whether a script through %run or a long-\\nrunning command, will cause a KeyboardInterrupt to be raised. This will cause nearly\\nall Python programs to stop immediately except in very exceptional cases.\\nWhen a piece of Python code has called into some compiled extension\\nmodules, pressing <Ctrl-C> will not cause the program execution to stop\\nimmediately in all cases. In such cases, you will have to either wait until\\ncontrol is returned to the Python interpreter, or, in more dire circum-\\nstances, forcibly terminate the Python process via the OS task manager.\\nExecuting Code from the Clipboard\\nA quick-and-dirty way to execute code in IPython is via pasting from the clipboard.\\nThis might seem fairly crude, but in practice it is very useful. For example, while de-\\nveloping a complex or time-consuming application, you may wish to execute a script\\npiece by piece, pausing at each stage to examine the currently loaded data and results.\\nOr, you might find a code snippet on the Internet that you want to run and play around\\nwith, but you’d rather not create a new .py file for it.\\n50 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Code snippets can be pasted from the clipboard in many cases by pressing <Ctrl-Shift-\\nV>. Note that it is not completely robust as this mode of pasting mimics typing each\\nline into IPython, and line breaks are treated as <return>. This means that if you paste\\ncode with an indented block and there is a blank line, IPython will think that the in-\\ndented block is over. Once the next line in the block is executed, an IndentationEr\\nror will be raised. For example the following code:\\nx = 5\\ny = 7\\nif x > 5:\\n    x += 1\\n    y = 8\\nwill not work if simply pasted:\\nIn [1]: x = 5\\nIn [2]: y = 7\\nIn [3]: if x > 5:\\n   ...:         x += 1\\n   ...:\\nIn [4]:     y = 8\\nIndentationError: unexpected indent\\nIf you want to paste code into IPython, try the %paste and %cpaste\\nmagic functions.\\nAs the error message suggests, we should instead use the %paste and %cpaste magic\\nfunctions. %paste takes whatever text is in the clipboard and executes it as a single block\\nin the shell:\\nIn [6]: %paste\\nx = 5\\ny = 7\\nif x > 5:\\n    x += 1\\n    y = 8\\n## -- End pasted text --\\nDepending on your platform and how you installed Python, there’s a\\nsmall chance that %paste will not work. Packaged distributions like\\nEPDFree (as described in in the intro) should not be a problem.\\n%cpaste is similar, except that it gives you a special prompt for pasting code into:\\nIn [7]: %cpaste\\nPasting code; enter '--' alone on the line to stop or use Ctrl-D.\\n:x = 5\\n:y = 7\\n:if x > 5:\\nIPython Basics | 51\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': ':    x += 1\\n:\\n:    y = 8\\n:--\\nWith the %cpaste block, you have the freedom to paste as much code as you like before\\nexecuting it. You might decide to use %cpaste in order to look at the pasted code before\\nexecuting it. If you accidentally paste the wrong code, you can break out of the \\n%cpaste prompt by pressing <Ctrl-C>.\\nLater, I’ll introduce the IPython HTML Notebook which brings a new level of sophis-\\ntication for developing analyses block-by-block in a browser-based notebook format\\nwith executable code cells.\\nIPython interaction with editors and IDEs\\nSome text editors, such as Emacs and vim, have 3rd party extensions enabling blocks\\nof code to be sent directly from the editor to a running IPython shell. Refer to the\\nIPython website or do an Internet search to find out more.\\nSome IDEs, such as the PyDev plugin for Eclipse and Python Tools for Visual Studio\\nfrom Microsoft (and possibly others), have integration with the IPython terminal ap-\\nplication. If you want to work in an IDE but don’t want to give up the IPython console\\nfeatures, this may be a good option for you.\\nKeyboard Shortcuts\\nIPython has many keyboard shortcuts for navigating the prompt (which will be familiar\\nto users of the Emacs text editor or the UNIX bash shell) and interacting with the shell’s\\ncommand history (see later section). Table 3-1 summarizes some of the most commonly\\nused shortcuts. See Figure 3-1 for an illustration of a few of these, such as cursor move-\\nment.\\nFigure 3-1. Illustration of some of IPython’s keyboard shortcuts\\n52 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Table 3-1. Standard IPython Keyboard Shortcuts\\nCommand Description\\nCtrl-P or up-arrow Search backward in command history for commands starting with currently-entered text\\nCtrl-N or down-arrow Search forward in command history for commands starting with currently-entered text\\nCtrl-R Readline-style reverse history search (partial matching)\\nCtrl-Shift-V Paste text from clipboard\\nCtrl-C Interrupt currently-executing code\\nCtrl-A Move cursor to beginning of line\\nCtrl-E Move cursor to end of line\\nCtrl-K Delete text from cursor until end of line\\nCtrl-U Discard all text on current line\\nCtrl-F Move cursor forward one character\\nCtrl-B Move cursor back one character\\nCtrl-L Clear screen\\nExceptions and Tracebacks\\nIf an exception is raised while %run-ing a script or executing any statement, IPython will\\nby default print a full call stack trace (traceback) with a few lines of context around the\\nposition at each point in the stack.\\nIn [553]: %run ch03/ipython_bug.py\\n---------------------------------------------------------------------------\\nAssertionError                            Traceback (most recent call last)\\n/home/wesm/code/ipython/IPython/utils/py3compat.pyc in execfile(fname, *where)\\n    176             else:\\n    177                 filename = fname\\n--> 178             __builtin__.execfile(filename, *where)\\nbook_scripts/ch03/ipython_bug.py in <module>()\\n     13     throws_an_exception()\\n     14 \\n---> 15 calling_things()\\nbook_scripts/ch03/ipython_bug.py in calling_things()\\n     11 def calling_things():\\n     12     works_fine()\\n---> 13     throws_an_exception()\\n     14 \\n     15 calling_things()\\nbook_scripts/ch03/ipython_bug.py in throws_an_exception()\\n      7     a = 5\\n      8     b = 6\\n----> 9     assert(a + b == 10)\\n     10 \\n     11 def calling_things():\\nAssertionError:\\nIPython Basics | 53\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Having additional context by itself is a big advantage over the standard Python inter-\\npreter (which does not provide any additional context). The amount of context shown\\ncan be controlled using the %xmode magic command, from minimal (same as the stan-\\ndard Python interpreter) to verbose (which inlines function argument values and more).\\nAs you will see later in the chapter, you can step into the stack (using the %debug or \\n%pdb magics) after an error has occurred for interactive post-mortem debugging.\\nMagic Commands\\nIPython has many special commands, known as “magic” commands, which are de-\\nsigned to faciliate common tasks and enable you to easily control the behavior of the\\nIPython system. A magic command is any command prefixed by the the percent symbol\\n%. For example, you can check the execution time of any Python statement, such as a\\nmatrix multiplication, using the %timeit magic function (which will be discussed in\\nmore detail later):\\nIn [554]: a = np.random.randn(100, 100)\\nIn [555]: %timeit np.dot(a, a)\\n10000 loops, best of 3: 69.1 us per loop\\nMagic commands can be viewed as command line programs to be run within the IPy-\\nthon system. Many of them have additional “command line” options, which can all be\\nviewed (as you might expect) using ?:\\nIn [1]: %reset?\\nResets the namespace by removing all names defined by the user.\\nParameters\\n----------\\n  -f : force reset without asking for confirmation.\\n  -s : 'Soft' reset: Only clears your namespace, leaving history intact.\\n  References to objects may be kept. By default (without this option),\\n  we do a 'hard' reset, giving you a new session and removing all\\n  references to objects from the current session.\\nExamples\\n--------\\nIn [6]: a = 1\\nIn [7]: a\\nOut[7]: 1\\nIn [8]: 'a' in _ip.user_ns\\nOut[8]: True\\nIn [9]: %reset -f\\nIn [1]: 'a' in _ip.user_ns\\nOut[1]: False\\n54 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Magic functions can be used by default without the percent sign, as long as no variable\\nis defined with the same name as the magic function in question. This feature is called\\nautomagic and can be enabled or disabled using %automagic.\\nSince IPython’s documentation is easily accessible from within the system, I encourage\\nyou to explore all of the special commands available by typing %quickref or %magic. I\\nwill highlight a few more of the most critical ones for being productive in interactive\\ncomputing and Python development in IPython.\\nTable 3-2. Frequently-used IPython Magic Commands\\nCommand Description\\n%quickref Display the IPython Quick Reference Card\\n%magic Display detailed documentation for all of the available magic commands\\n%debug Enter the interactive debugger at the bottom of the last exception traceback\\n%hist Print command input (and optionally output) history\\n%pdb Automatically enter debugger after any exception\\n%paste Execute pre-formatted Python code from clipboard\\n%cpaste Open a special prompt for manually pasting Python code to be executed\\n%reset Delete all variables / names defined in interactive namespace\\n%page OBJECT Pretty print the object and display it through a pager\\n%run script.py Run a Python script inside IPython\\n%prun statement Execute statement with cProfile and report the profiler output\\n%time statement Report the execution time of single statement\\n%timeit statement Run a statement multiple times to compute an emsemble average execution time. Useful for\\ntiming code with very short execution time\\n%who, %who_ls, %whos Display variables defined in interactive namespace, with varying levels of information / verbosity\\n%xdel variable Delete a variable and attempt to clear any references to the object in the IPython internals\\nQt-based Rich GUI Console\\nThe IPython team has developed a Qt framework-based GUI console, designed to wed\\nthe features of the terminal-only applications with the features provided by a rich text\\nwidget, like embedded images, multiline editing, and syntax highlighting. If you have\\neither PyQt or PySide installed, the application can be launched with inline plotting by\\nrunning this on the command line:\\nipython qtconsole --pylab=inline\\nThe Qt console can launch multiple IPython processes in tabs, enabling you to switch\\nbetween tasks. It can also share a process with the IPython HTML Notebook applica-\\ntion, which I’ll highlight later.\\nIPython Basics | 55\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Matplotlib Integration and Pylab Mode\\nPart of why IPython is so widely used in scientific computing is that it is designed as a\\ncompanion to libraries like matplotlib and other GUI toolkits. Don’t worry if you have\\nnever used matplotlib before; it will be discussed in much more detail later in this book.\\nIf you create a matplotlib plot window in the regular Python shell, you’ll be sad to find\\nthat the GUI event loop “takes control” of the Python session until the plot window is\\nclosed. That won’t work for interactive data analysis and visualization, so IPython has\\nFigure 3-2. IPython Qt Console\\n56 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'implemented special handling for each GUI framework so that it will work seamlessly\\nwith the shell.\\nThe typical way to launch IPython with matplotlib integration is by adding the --\\npylab flag (two dashes).\\n$ ipython --pylab\\nThis will cause several things to happen. First IPython will launch with the default GUI\\nbackend integration enabled so that matplotlib plot windows can be created with no\\nissues. Secondly, most of NumPy and matplotlib will be imported into the top level\\ninteractive namespace to produce an interactive computing environment reminiscent\\nof MATLAB and other domain-specific scientific computing environments. It’s possi-\\nble to do this setup by hand by using %gui, too (try running %gui? to find out how).\\nFigure 3-3. Pylab mode: IPython with matplotlib windows\\nIPython Basics | 57\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Using the Command History\\nIPython maintains a small on-disk database containing the text of each command that\\nyou execute. This serves various purposes:\\n• Searching, completing, and executing previously-executed commands with mini-\\nmal typing\\n• Persisting the command history between sessions.\\n• Logging the input/output history to a file\\nSearching and Reusing the Command History\\nBeing able to search and execute previous commands is, for many people, the most\\nuseful feature. Since IPython encourages an iterative, interactive code development\\nworkflow, you may often find yourself repeating the same commands, such as a %run\\ncommand or some other code snippet. Suppose you had run:\\nIn[7]: %run first/second/third/data_script.py\\nand then explored the results of the script (assuming it ran successfully), only to find\\nthat you made an incorrect calculation. After figuring out the problem and modifying\\ndata_script.py, you can start typing a few letters of the %run command then press either\\nthe <Ctrl-P> key combination or the <up arrow> key. This will search the command\\nhistory for the first prior command matching the letters you typed. Pressing either\\n<Ctrl-P> or <up arrow> multiple times will continue to search through the history. If\\nyou pass over the command you wish to execute, fear not. You can move forward\\nthrough the command history by pressing either <Ctrl-N> or <down arrow>. After doing\\nthis a few times you may start pressing these keys without thinking!\\nUsing <Ctrl-R> gives you the same partial incremental searching capability provided\\nby the readline used in UNIX-style shells, such as the bash shell. On Windows, read\\nline functionality is emulated by IPython. To use this, press <Ctrl-R> then type a few\\ncharacters contained in the input line you want to search for:\\nIn [1]: a_command = foo(x, y, z)\\n(reverse-i-search)`com': a_command = foo(x, y, z)\\nPressing <Ctrl-R> will cycle through the history for each line matching the characters\\nyou’ve typed.\\nInput and Output Variables\\nForgetting to assign the result of a function call to a variable can be very annoying.\\nFortunately, IPython stores references to both the input (the text that you type) and\\noutput (the object that is returned) in special variables. The previous two outputs are\\nstored in the _ (one underscore) and __ (two underscores) variables, respectively:\\n58 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [556]: 2 ** 27\\nOut[556]: 134217728\\nIn [557]: _\\nOut[557]: 134217728\\nInput variables are stored in variables named like _iX, where X is the input line number.\\nFor each such input variables there is a corresponding output variable _X. So after input\\nline 27, say, there will be two new variables _27 (for the output) and _i27 for the input.\\nIn [26]: foo = 'bar'\\nIn [27]: foo\\nOut[27]: 'bar'\\nIn [28]: _i27\\nOut[28]: u'foo'\\nIn [29]: _27\\nOut[29]: 'bar'\\nSince the input variables are strings, that can be executed again using the Python \\nexec keyword:\\nIn [30]: exec _i27\\nSeveral magic functions allow you to work with the input and output history. %hist is\\ncapable of printing all or part of the input history, with or without line numbers. \\n%reset is for clearing the interactive namespace and optionally the input and output\\ncaches. The %xdel magic function is intended for removing all references to a particu-\\nlar object from the IPython machinery. See the documentation for both of these magics\\nfor more details.\\nWhen working with very large data sets, keep in mind that IPython’s\\ninput and output history causes any object referenced there to not be\\ngarbage collected (freeing up the memory), even if you delete the vari-\\nables from the interactive namespace using the del keyword. In such\\ncases, careful usage of %xdel and %reset can help you avoid running into\\nmemory problems.\\nLogging the Input and Output\\nIPython is capable of logging the entire console session including input and output.\\nLogging is turned on by typing %logstart:\\nIn [3]: %logstart\\nActivating auto-logging. Current session state plus future input saved.\\nFilename       : ipython_log.py\\nMode           : rotate\\nOutput logging : False\\nRaw input log  : False\\nUsing the Command History | 59\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Timestamping   : False\\nState          : active\\nIPython logging can be enabled at any time and it will record your entire session (in-\\ncluding previous commands). Thus, if you are working on something and you decide\\nyou want to save everything you did, you can simply enable logging. See the docstring\\nof %logstart for more options (including changing the output file path), as well as the\\ncompanion functions %logoff, %logon, %logstate, and %logstop.\\nInteracting with the Operating System\\nAnother important feature of IPython is that it provides very strong integration with\\nthe operating system shell. This means, among other things, that you can perform most\\nstandard command line actions as you would in the Windows or UNIX (Linux, OS X)\\nshell without having to exit IPython. This includes executing shell commands, changing\\ndirectories, and storing the results of a command in a Python object (list or string).\\nThere are also simple shell command aliasing and directory bookmarking features.\\nSee Table 3-3 for a summary of magic functions and syntax for calling shell commands.\\nI’ll briefly visit these features in the next few sections.\\nTable 3-3. IPython system-related commands\\nCommand Description\\n!cmd Execute cmd in the system shell\\noutput = !cmd args Run cmd and store the stdout in output\\n%alias alias_name cmd Define an alias for a system (shell) command\\n%bookmark Utilize IPython’s directory bookmarking system\\n%cd directory Change system working directory to passed directory\\n%pwd Return the current system working directory\\n%pushd directory Place current directory on stack and change to target directory\\n%popd Change to directory popped off the top of the stack\\n%dirs Return a list containing the current directory stack\\n%dhist Print the history of visited directories\\n%env Return the system environment variables as a dict\\nShell Commands and Aliases\\nStarting a line in IPython with an exclamation point !, or bang, tells IPython to execute\\neverything after the bang in the system shell. This means that you can delete files (using\\nrm or del, depending on your OS), change directories, or execute any other process. It’s\\neven possible to start processes that take control away from IPython, even another\\nPython interpreter:\\n60 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'In [2]: !python\\nPython 2.7.2 |EPD 7.1-2 (64-bit)| (default, Jul  3 2011, 15:17:51)\\n[GCC 4.1.2 20080704 (Red Hat 4.1.2-44)] on linux2\\nType \"packages\", \"demo\" or \"enthought\" for more information.\\n>>>\\nThe console output of a shell command can be stored in a variable by assigning the !-\\nescaped expression to a variable. For example, on my Linux-based machine connected\\nto the Internet via ethernet, I can get my IP address as a Python variable:\\nIn [1]: ip_info = !ifconfig eth0 | grep \"inet \"\\nIn [2]: ip_info[0].strip()\\nOut[2]: \\'inet addr:192.168.1.137  Bcast:192.168.1.255  Mask:255.255.255.0\\'\\nThe returned Python object ip_info is actually a custom list type containing various\\nversions of the console output.\\nIPython can also substitute in Python values defined in the current environment when\\nusing !. To do this, preface the variable name by the dollar sign $:\\nIn [3]: foo = \\'test*\\'\\nIn [4]: !ls $foo\\ntest4.py  test.py  test.xml\\nThe %alias magic function can define custom shortcuts for shell commands. As a simple\\nexample:\\nIn [1]: %alias ll ls -l\\nIn [2]: ll /usr\\ntotal 332\\ndrwxr-xr-x   2 root root  69632 2012-01-29 20:36 bin/\\ndrwxr-xr-x   2 root root   4096 2010-08-23 12:05 games/\\ndrwxr-xr-x 123 root root  20480 2011-12-26 18:08 include/\\ndrwxr-xr-x 265 root root 126976 2012-01-29 20:36 lib/\\ndrwxr-xr-x  44 root root  69632 2011-12-26 18:08 lib32/\\nlrwxrwxrwx   1 root root      3 2010-08-23 16:02 lib64 -> lib/\\ndrwxr-xr-x  15 root root   4096 2011-10-13 19:03 local/\\ndrwxr-xr-x   2 root root  12288 2012-01-12 09:32 sbin/\\ndrwxr-xr-x 387 root root  12288 2011-11-04 22:53 share/\\ndrwxrwsr-x  24 root src    4096 2011-07-17 18:38 src/\\nMultiple commands can be executed just as on the command line by separating them\\nwith semicolons:\\nIn [558]: %alias test_alias (cd ch08; ls; cd ..)\\nIn [559]: test_alias\\nmacrodata.csv  spx.csv    tips.csv\\nYou’ll notice that IPython “forgets” any aliases you define interactively as soon as the\\nsession is closed. To create permanent aliases, you will need to use the configuration\\nsystem. See later in the chapter.\\nInteracting with the Operating System | 61\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Directory Bookmark System\\nIPython has a simple directory bookmarking system to enable you to save aliases for\\ncommon directories so that you can jump around very easily. For example, I’m an avid\\nuser of Dropbox, so I can define a bookmark to make it easy to change directories to\\nmy Dropbox:\\nIn [6]: %bookmark db /home/wesm/Dropbox/\\nOnce I’ve done this, when I use the %cd magic, I can use any bookmarks I’ve defined\\nIn [7]: cd db\\n(bookmark:db) -> /home/wesm/Dropbox/\\n/home/wesm/Dropbox\\nIf a bookmark name conflicts with a directory name in your current working directory,\\nyou can use the -b flag to override and use the bookmark location. Using the -l option\\nwith %bookmark lists all of your bookmarks:\\nIn [8]: %bookmark -l\\nCurrent bookmarks:\\ndb -> /home/wesm/Dropbox/\\nBookmarks, unlike aliases, are automatically persisted between IPython sessions.\\nSoftware Development Tools\\nIn addition to being a comfortable environment for interactive computing and data\\nexploration, IPython is well suited as a software development environment. In data\\nanalysis applications, it’s important first to have correct code. Fortunately, IPython has\\nclosely integrated and enhanced the built-in Python pdb debugger. Secondly you want\\nyour code to be fast. For this IPython has easy-to-use code timing and profiling tools.\\nI will give an overview of these tools in detail here.\\nInteractive Debugger\\nIPython’s debugger enhances pdb with tab completion, syntax highlighting, and context\\nfor each line in exception tracebacks. One of the best times to debug code is right after\\nan error has occurred. The %debug command, when entered immediately after an ex-\\nception, invokes the “post-mortem” debugger and drops you into the stack frame where\\nthe exception was raised:\\nIn [2]: run ch03/ipython_bug.py\\n---------------------------------------------------------------------------\\nAssertionError                            Traceback (most recent call last)\\n/home/wesm/book_scripts/ch03/ipython_bug.py in <module>()\\n     13     throws_an_exception()\\n     14\\n---> 15 calling_things()\\n/home/wesm/book_scripts/ch03/ipython_bug.py in calling_things()\\n62 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"11 def calling_things():\\n     12     works_fine()\\n---> 13     throws_an_exception()\\n     14\\n     15 calling_things()\\n/home/wesm/book_scripts/ch03/ipython_bug.py in throws_an_exception()\\n      7     a = 5\\n      8     b = 6\\n----> 9     assert(a + b == 10)\\n     10\\n     11 def calling_things():\\nAssertionError:\\nIn [3]: %debug\\n> /home/wesm/book_scripts/ch03/ipython_bug.py(9)throws_an_exception()\\n      8     b = 6\\n----> 9     assert(a + b == 10)\\n     10\\nipdb>\\nOnce inside the debugger, you can execute arbitrary Python code and explore all of the\\nobjects and data (which have been “kept alive” by the interpreter) inside each stack\\nframe. By default you start in the lowest level, where the error occurred. By pressing\\nu (up) and d (down), you can switch between the levels of the stack trace:\\nipdb> u\\n> /home/wesm/book_scripts/ch03/ipython_bug.py(13)calling_things()\\n     12     works_fine()\\n---> 13     throws_an_exception()\\n     14\\nExecuting the %pdb command makes it so that IPython automatically invokes the de-\\nbugger after any exception, a mode that many users will find especially useful.\\nIt’s also easy to use the debugger to help develop code, especially when you wish to set\\nbreakpoints or step through the execution of a function or script to examine the state\\nat each stage. There are several ways to accomplish this. The first is by using %run with\\nthe -d flag, which invokes the debugger before executing any code in the passed script.\\nYou must immediately press s (step) to enter the script:\\nIn [5]: run -d ch03/ipython_bug.py\\nBreakpoint 1 at /home/wesm/book_scripts/ch03/ipython_bug.py:1\\nNOTE: Enter 'c' at the ipdb>  prompt to start your script.\\n> <string>(1)<module>()\\nipdb> s\\n--Call--\\n> /home/wesm/book_scripts/ch03/ipython_bug.py(1)<module>()\\n1---> 1 def works_fine():\\n      2     a = 5\\n      3     b = 6\\nSoftware Development Tools | 63\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'After this point, it’s up to you how you want to work your way through the file. For\\nexample, in the above exception, we could set a breakpoint right before calling the\\nworks_fine method and run the script until we reach the breakpoint by pressing c\\n(continue):\\nipdb> b 12\\nipdb> c\\n> /home/wesm/book_scripts/ch03/ipython_bug.py(12)calling_things()\\n     11 def calling_things():\\n2--> 12     works_fine()\\n     13     throws_an_exception()\\nAt this point, you can step into works_fine() or execute works_fine() by pressing n\\n(next) to advance to the next line:\\nipdb> n\\n> /home/wesm/book_scripts/ch03/ipython_bug.py(13)calling_things()\\n2    12     works_fine()\\n---> 13     throws_an_exception()\\n     14\\nThen, we could step into throws_an_exception and advance to the line where the error\\noccurs and look at the variables in the scope. Note that debugger commands take\\nprecedence over variable names; in such cases preface the variables with ! to examine\\ntheir contents.\\nipdb> s\\n--Call--\\n> /home/wesm/book_scripts/ch03/ipython_bug.py(6)throws_an_exception()\\n      5\\n----> 6 def throws_an_exception():\\n      7     a = 5\\nipdb> n\\n> /home/wesm/book_scripts/ch03/ipython_bug.py(7)throws_an_exception()\\n      6 def throws_an_exception():\\n----> 7     a = 5\\n      8     b = 6\\nipdb> n\\n> /home/wesm/book_scripts/ch03/ipython_bug.py(8)throws_an_exception()\\n      7     a = 5\\n----> 8     b = 6\\n      9     assert(a + b == 10)\\nipdb> n\\n> /home/wesm/book_scripts/ch03/ipython_bug.py(9)throws_an_exception()\\n      8     b = 6\\n----> 9     assert(a + b == 10)\\n     10\\nipdb> !a\\n5\\nipdb> !b\\n6\\n64 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Becoming proficient in the interactive debugger is largely a matter of practice and ex-\\nperience. See Table 3-3 for a full catalogue of the debugger commands. If you are used\\nto an IDE, you might find the terminal-driven debugger to be a bit bewildering at first,\\nbut that will improve in time. Most of the Python IDEs have excellent GUI debuggers,\\nbut it is usually a significant productivity gain to remain in IPython for your debugging.\\nTable 3-4. (I)Python debugger commands\\nCommand Action\\nh(elp) Display command list\\nhelp command Show documentation for command\\nc(ontinue) Resume program execution\\nq(uit) Exit debugger without executing any more code\\nb(reak) number Set breakpoint at number in current file\\nb path/to/file.py:number Set breakpoint at line number in specified file\\ns(tep) Step into function call\\nn(ext) Execute current line and advance to next line at current level\\nu(p) / d(own) Move up/down in function call stack\\na(rgs) Show arguments for current function\\ndebug statement Invoke statement statement in new (recursive) debugger\\nl(ist) statement Show current position and context at current level of stack\\nw(here) Print full stack trace with context at current position\\nOther ways to make use of the debugger\\nThere are a couple of other useful ways to invoke the debugger. The first is by using a\\nspecial set_trace function (named after pdb.set_trace), which is basically a “poor\\nman’s breakpoint”. Here are two small recipes you might want to put somewhere for\\nyour general use (potentially adding them to your IPython profile as I do):\\ndef set_trace():\\n    from IPython.core.debugger import Pdb\\n    Pdb(color_scheme='Linux').set_trace(sys._getframe().f_back)\\ndef debug(f, *args, **kwargs):\\n    from IPython.core.debugger import Pdb\\n    pdb = Pdb(color_scheme='Linux')\\n    return pdb.runcall(f, *args, **kwargs)\\nThe first function, set_trace, is very simple. Put set_trace() anywhere in your code\\nthat you want to stop and take a look around (for example, right before an exception\\noccurs):\\nIn [7]: run ch03/ipython_bug.py\\n> /home/wesm/book_scripts/ch03/ipython_bug.py(16)calling_things()\\n     15     set_trace()\\nSoftware Development Tools | 65\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"---> 16     throws_an_exception()\\n     17\\nPressing c (continue) will cause the code to resume normally with no harm done.\\nThe debug function above enables you to invoke the interactive debugger easily on an\\narbitrary function call. Suppose we had written a function like\\ndef f(x, y, z=1):\\n    tmp = x + y\\n    return tmp / z\\nand we wished to step through its logic. Ordinarily using f would look like f(1, 2,\\nz=3). To instead step into f, pass f as the first argument to debug followed by the po-\\nsitional and keyword arguments to be passed to f:\\nIn [6]: debug(f, 1, 2, z=3)\\n> <ipython-input>(2)f()\\n      1 def f(x, y, z):\\n----> 2     tmp = x + y\\n      3     return tmp / z\\nipdb>\\nI find that these two simple recipes save me a lot of time on a day-to-day basis.\\nLastly, the debugger can be used in conjunction with %run. By running a script with\\n%run -d, you will be dropped directly into the debugger, ready to set any breakpoints\\nand start the script:\\nIn [1]: %run -d ch03/ipython_bug.py\\nBreakpoint 1 at /home/wesm/book_scripts/ch03/ipython_bug.py:1\\nNOTE: Enter 'c' at the ipdb>  prompt to start your script.\\n> <string>(1)<module>()\\nipdb>\\nAdding -b with a line number starts the debugger with a breakpoint set already:\\nIn [2]: %run -d -b2 ch03/ipython_bug.py\\nBreakpoint 1 at /home/wesm/book_scripts/ch03/ipython_bug.py:2\\nNOTE: Enter 'c' at the ipdb>  prompt to start your script.\\n> <string>(1)<module>()\\nipdb> c\\n> /home/wesm/book_scripts/ch03/ipython_bug.py(2)works_fine()\\n      1 def works_fine():\\n1---> 2     a = 5\\n      3     b = 6\\nipdb>\\n66 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Timing Code: %time and %timeit\\nFor larger-scale or longer-running data analysis applications, you may wish to measure\\nthe execution time of various components or of individual statements or function calls.\\nYou may want a report of which functions are taking up the most time in a complex\\nprocess. Fortunately, IPython enables you to get this information very easily while you\\nare developing and testing your code.\\nTiming code by hand using the built-in time module and its functions time.clock and\\ntime.time is often tedious and repetitive, as you must write the same uninteresting\\nboilerplate code:\\nimport time\\nstart = time.time()\\nfor i in range(iterations):\\n    # some code to run here\\nelapsed_per = (time.time() - start) / iterations\\nSince this is such a common operation, IPython has two magic functions %time and \\n%timeit to automate this process for you. %time runs a statement once, reporting the\\ntotal execution time. Suppose we had a large list of strings and we wanted to compare\\ndifferent methods of selecting all strings starting with a particular prefix. Here is a\\nsimple list of 700,000 strings and two identical methods of selecting only the ones that\\nstart with 'foo':\\n# a very large list of strings\\nstrings = ['foo', 'foobar', 'baz', 'qux',\\n           'python', 'Guido Van Rossum'] * 100000\\nmethod1 = [x for x in strings if x.startswith('foo')]\\nmethod2 = [x for x in strings if x[:3] == 'foo']\\nIt looks like they should be about the same performance-wise, right? We can check for\\nsure using %time:\\nIn [561]: %time method1 = [x for x in strings if x.startswith('foo')]\\nCPU times: user 0.19 s, sys: 0.00 s, total: 0.19 s\\nWall time: 0.19 s\\nIn [562]: %time method2 = [x for x in strings if x[:3] == 'foo']\\nCPU times: user 0.09 s, sys: 0.00 s, total: 0.09 s\\nWall time: 0.09 s\\nThe Wall time is the main number of interest. So, it looks like the first method takes\\nmore than twice as long, but it’s not a very precise measurement. If you try %time-ing\\nthose statements multiple times yourself, you’ll find that the results are somewhat\\nvariable. To get a more precise measurement, use the %timeit magic function. Given\\nan arbitrary statement, it has a heuristic to run a statement multiple times to produce\\na fairly accurate average runtime.\\nIn [563]: %timeit [x for x in strings if x.startswith('foo')]\\n10 loops, best of 3: 159 ms per loop\\nSoftware Development Tools | 67\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [564]: %timeit [x for x in strings if x[:3] == 'foo']\\n10 loops, best of 3: 59.3 ms per loop\\nThis seemingly innocuous example illustrates that it is worth understanding the per-\\nformance characteristics of the Python standard library, NumPy, pandas, and other\\nlibraries used in this book. In larger-scale data analysis applications, those milliseconds\\nwill start to add up!\\n%timeit is especially useful for analyzing statements and functions with very short ex-\\necution times, even at the level of microseconds (1e-6 seconds) or nanoseconds (1e-9\\nseconds). These may seem like insignificant amounts of time, but of course a 20 mi-\\ncrosecond function invoked 1 million times takes 15 seconds longer than a 5 micro-\\nsecond function. In the above example, we could very directly compare the two string\\noperations to understand their performance characteristics:\\nIn [565]: x = 'foobar'\\nIn [566]: y = 'foo'\\nIn [567]: %timeit x.startswith(y)\\n1000000 loops, best of 3: 267 ns per loop\\nIn [568]: %timeit x[:3] == y\\n10000000 loops, best of 3: 147 ns per loop\\nBasic Profiling: %prun and %run -p\\nProfiling code is closely related to timing code, except it is concerned with determining\\nwhere time is spent. The main Python profiling tool is the cProfile module, which is\\nnot specific to IPython at all. cProfile executes a program or any arbitrary block of\\ncode while keeping track of how much time is spent in each function.\\nA common way to use cProfile is on the command line, running an entire program\\nand outputting the aggregated time per function. Suppose we had a simple script which\\ndoes some linear algebra in a loop (computing the maximum absolute eigenvalues of\\na series of 100 x 100 matrices):\\nimport numpy as np\\nfrom numpy.linalg import eigvals\\ndef run_experiment(niter=100):\\n    K = 100\\n    results = []\\n    for _ in xrange(niter):\\n        mat = np.random.randn(K, K)\\n        max_eigenvalue = np.abs(eigvals(mat)).max()\\n        results.append(max_eigenvalue)\\n    return results\\nsome_results = run_experiment()\\nprint 'Largest one we saw: %s' % np.max(some_results)\\n68 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Don’t worry if you are not familiar with NumPy. You can run this script through\\ncProfile by running the following in the command line:\\npython -m cProfile cprof_example.py\\nIf you try that, you’ll find that the results are outputted sorted by function name. This\\nmakes it a bit hard to get an idea of where the most time is spent, so it’s very common\\nto specify a sort order using the -s flag:\\n$ python -m cProfile -s cumulative cprof_example.py\\nLargest one we saw: 11.923204422\\n    15116 function calls (14927 primitive calls) in 0.720 seconds\\nOrdered by: cumulative time\\nncalls  tottime  percall  cumtime  percall filename:lineno(function)\\n     1    0.001    0.001    0.721    0.721 cprof_example.py:1(<module>)\\n   100    0.003    0.000    0.586    0.006 linalg.py:702(eigvals)\\n   200    0.572    0.003    0.572    0.003 {numpy.linalg.lapack_lite.dgeev}\\n     1    0.002    0.002    0.075    0.075 __init__.py:106(<module>)\\n   100    0.059    0.001    0.059    0.001 {method 'randn')\\n     1    0.000    0.000    0.044    0.044 add_newdocs.py:9(<module>)\\n     2    0.001    0.001    0.037    0.019 __init__.py:1(<module>)\\n     2    0.003    0.002    0.030    0.015 __init__.py:2(<module>)\\n     1    0.000    0.000    0.030    0.030 type_check.py:3(<module>)\\n     1    0.001    0.001    0.021    0.021 __init__.py:15(<module>)\\n     1    0.013    0.013    0.013    0.013 numeric.py:1(<module>)\\n     1    0.000    0.000    0.009    0.009 __init__.py:6(<module>)\\n     1    0.001    0.001    0.008    0.008 __init__.py:45(<module>)\\n   262    0.005    0.000    0.007    0.000 function_base.py:3178(add_newdoc)\\n   100    0.003    0.000    0.005    0.000 linalg.py:162(_assertFinite)\\n   ...\\nOnly the first 15 rows of the output are shown. It’s easiest to read by scanning down\\nthe cumtime column to see how much total time was spent inside each function. Note\\nthat if a function calls some other function, the clock does not stop running . cProfile\\nrecords the start and end time of each function call and uses that to produce the timing.\\nIn addition to the above command-line usage, cProfile can also be used programmat-\\nically to profile arbitrary blocks of code without having to run a new process. IPython\\nhas a convenient interface to this capability using the %prun command and the -p option\\nto %run. %prun takes the same “command line options” as cProfile but will profile an\\narbitrary Python statement instead of a while .py file:\\nIn [4]: %prun -l 7 -s cumulative run_experiment()\\n         4203 function calls in 0.643 seconds\\nOrdered by: cumulative time\\nList reduced from 32 to 7 due to restriction <7>\\nncalls  tottime  percall  cumtime  percall filename:lineno(function)\\n     1    0.000    0.000    0.643    0.643 <string>:1(<module>)\\n     1    0.001    0.001    0.643    0.643 cprof_example.py:4(run_experiment)\\n   100    0.003    0.000    0.583    0.006 linalg.py:702(eigvals)\\nSoftware Development Tools | 69\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"200    0.569    0.003    0.569    0.003 {numpy.linalg.lapack_lite.dgeev}\\n   100    0.058    0.001    0.058    0.001 {method 'randn'}\\n   100    0.003    0.000    0.005    0.000 linalg.py:162(_assertFinite)\\n   200    0.002    0.000    0.002    0.000 {method 'all' of 'numpy.ndarray' objects}\\nSimilarly, calling %run -p -s cumulative cprof_example.py has the same effect as the\\ncommand-line approach above, except you never have to leave IPython.\\nProfiling a Function Line-by-Line\\nIn some cases the information you obtain from %prun (or another cProfile-based profile\\nmethod) may not tell the whole story about a function’s execution time, or it may be\\nso complex that the results, aggregated by function name, are hard to interpret. For\\nthis case, there is a small library called line_profiler (obtainable via PyPI or one of the\\npackage management tools). It contains an IPython extension enabling a new magic\\nfunction %lprun that computes a line-by-line-profiling of one or more functions. You\\ncan enable this extension by modifying your IPython configuration (see the IPython\\ndocumentation or the section on configuration later in this chapter) to include the\\nfollowing line:\\n# A list of dotted module names of IPython extensions to load.\\nc.TerminalIPythonApp.extensions = ['line_profiler']\\nline_profiler can be used programmatically (see the full documentation), but it is\\nperhaps most powerful when used interactively in IPython. Suppose you had a module\\nprof_mod with the following code doing some NumPy array operations:\\nfrom numpy.random import randn\\ndef add_and_sum(x, y):\\n    added = x + y\\n    summed = added.sum(axis=1)\\n    return summed\\ndef call_function():\\n    x = randn(1000, 1000)\\n    y = randn(1000, 1000)\\n    return add_and_sum(x, y)\\nIf we wanted to understand the performance of the add_and_sum function, %prun gives\\nus the following:\\nIn [569]: %run prof_mod\\nIn [570]: x = randn(3000, 3000)\\nIn [571]: y = randn(3000, 3000)\\nIn [572]: %prun add_and_sum(x, y)\\n         4 function calls in 0.049 seconds\\n   Ordered by: internal time\\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\\n        1    0.036    0.036    0.046    0.046 prof_mod.py:3(add_and_sum)\\n70 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"1    0.009    0.009    0.009    0.009 {method 'sum' of 'numpy.ndarray' objects}\\n        1    0.003    0.003    0.049    0.049 <string>:1(<module>)\\n        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\\nThis is not especially enlightening. With the line_profiler IPython extension activa-\\nted, a new command %lprun is available. The only difference in usage is that we must\\ninstruct %lprun which function or functions we wish to profile. The general syntax is:\\n%lprun -f func1 -f func2 statement_to_profile\\nIn this case, we want to profile add_and_sum, so we run:\\nIn [573]: %lprun -f add_and_sum add_and_sum(x, y)\\nTimer unit: 1e-06 s\\nFile: book_scripts/prof_mod.py\\nFunction: add_and_sum at line 3\\nTotal time: 0.045936 s\\nLine #      Hits         Time  Per Hit   % Time  Line Contents\\n==============================================================\\n     3                                           def add_and_sum(x, y):\\n     4         1        36510  36510.0     79.5      added = x + y\\n     5         1         9425   9425.0     20.5      summed = added.sum(axis=1)\\n     6         1            1      1.0      0.0      return summed\\nYou’ll probably agree this is much easier to interpret. In this case we profiled the same\\nfunction we used in the statement. Looking at the module code above, we could call\\ncall_function and profile that as well as add_and_sum, thus getting a full picture of the\\nperformance of the code:\\nIn [574]: %lprun -f add_and_sum -f call_function call_function()\\nTimer unit: 1e-06 s\\nFile: book_scripts/prof_mod.py\\nFunction: add_and_sum at line 3\\nTotal time: 0.005526 s\\nLine #      Hits         Time  Per Hit   % Time  Line Contents\\n==============================================================\\n     3                                           def add_and_sum(x, y):\\n     4         1         4375   4375.0     79.2      added = x + y\\n     5         1         1149   1149.0     20.8      summed = added.sum(axis=1)\\n     6         1            2      2.0      0.0      return summed\\nFile: book_scripts/prof_mod.py\\nFunction: call_function at line 8\\nTotal time: 0.121016 s\\nLine #      Hits         Time  Per Hit   % Time  Line Contents\\n==============================================================\\n     8                                           def call_function():\\n     9         1        57169  57169.0     47.2      x = randn(1000, 1000)\\n    10         1        58304  58304.0     48.2      y = randn(1000, 1000)\\n    11         1         5543   5543.0      4.6      return add_and_sum(x, y)\\nAs a general rule of thumb, I tend to prefer %prun (cProfile) for “macro” profiling and\\n%lprun (line_profiler) for “micro” profiling. It’s worthwhile to have a good under-\\nstanding of both tools.\\nSoftware Development Tools | 71\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"The reason that you have to specify explicitly the names of the functions\\nyou want to profile with %lprun is that the overhead of “tracing” the\\nexecution time of each line is significant. Tracing functions that are not\\nof interest would potentially significantly alter the profile results.\\nIPython HTML Notebook\\nStarting in 2011, the IPython team, led by Brian Granger, built a web technology−based\\ninteractive computational document format that is commonly known as the IPython\\nNotebook. It has grown into a wonderful tool for interactive computing and an ideal\\nmedium for reproducible research and teaching. I’ve used it while writing most of the\\nexamples in the book; I encourage you to make use of it, too.\\nIt has a JSON-based .ipynb document format that enables easy sharing of code, output,\\nand figures. Recently in Python conferences, a popular approach for demonstrations\\nhas been to use the notebook and post the .ipynb files online afterward for everyone\\nto play with.\\nThe notebook application runs as a lightweight server process on the command line.\\nIt can be started by running:\\n$ ipython notebook --pylab=inline\\n[NotebookApp] Using existing profile dir: u'/home/wesm/.config/ipython/profile_default'\\n[NotebookApp] Serving notebooks from /home/wesm/book_scripts\\n[NotebookApp] The IPython Notebook is running at: http://127.0.0.1:8888/\\n[NotebookApp] Use Control-C to stop this server and shut down all kernels.\\nOn most platforms, your primary web browser will automatically open up to the note-\\nbook dashboard. In some cases you may have to navigate to the listed URL. From there,\\nyou can create a new notebook and start exploring.\\nSince you use the notebook inside a web browser, the server process can run anywhere.\\nYou can even securely connect to notebooks running on cloud service providers like\\nAmazon EC2. As of this writing, a new project NotebookCloud (http://notebookcloud\\n.appspot.com) makes it easy to launch notebooks on EC2.\\nTips for Productive Code Development Using IPython\\nWriting code in a way that makes it easy to develop, debug, and ultimately use inter-\\nactively may be a paradigm shift for many users. There are procedural details like code\\nreloading that may require some adjustment as well as coding style concerns.\\nAs such, most of this section is more of an art than a science and will require some\\nexperimentation on your part to determine a way to write your Python code that is\\neffective and productive for you. Ultimately you want to structure your code in a way\\nthat makes it easy to use iteratively and to be able to explore the results of running a\\nprogram or function as effortlessly as possible. I have found software designed with\\n72 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'IPython in mind to be easier to work with than code intended only to be run as as\\nstandalone command-line application. This becomes especially important when some-\\nthing goes wrong and you have to diagnose an error in code that you or someone else\\nmight have written months or years beforehand.\\nFigure 3-4. IPython Notebook\\nTips for Productive Code Development Using IPython | 73\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Reloading Module Dependencies\\nIn Python, when you type import some_lib, the code in some_lib is executed and all the\\nvariables, functions, and imports defined within are stored in the newly created\\nsome_lib module namespace. The next time you type import some_lib, you will get a\\nreference to the existing module namespace. The potential difficulty in interactive code\\ndevelopment in IPython comes when you, say, %run a script that depends on some other\\nmodule where you may have made changes. Suppose I had the following code in\\ntest_script.py:\\nimport some_lib\\nx = 5\\ny = [1, 2, 3, 4]\\nresult = some_lib.get_answer(x, y)\\nIf you were to execute %run test_script.py then modify some_lib.py, the next time you\\nexecute %run test_script.py you will still get the old version of some_lib because of\\nPython’s “load-once” module system. This behavior differs from some other data anal-\\nysis environments, like MATLAB, which automatically propagate code changes. 1 To\\ncope with this, you have a couple of options. The first way is to use Python's built-in \\nreload function, altering test_script.py to look like the following:\\nimport some_lib\\nreload(some_lib)\\nx = 5\\ny = [1, 2, 3, 4]\\nresult = some_lib.get_answer(x, y)\\nThis guarantees that you will get a fresh copy of some_lib every time you run\\ntest_script.py. Obviously, if the dependencies go deeper, it might be a bit tricky to be\\ninserting usages of reload all over the place. For this problem, IPython has a special \\ndreload function (not a magic function) for “deep” (recursive) reloading of modules. If\\nI were to run import some_lib then type dreload(some_lib), it will attempt to reload\\nsome_lib as well as all of its dependencies. This will not work in all cases, unfortunately,\\nbut when it does it beats having to restart IPython.\\nCode Design Tips\\nThere’s no simple recipe for this, but here are some high-level principles I have found\\neffective in my own work.\\n1. Since a module or package may be imported in many different places in a particular program, Python\\ncaches a module’s code the first time it is imported rather than executing the code in the module every\\ntime. Otherwise, modularity and good code organization could potentially cause inefficiency in an\\napplication.\\n74 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Keep relevant objects and data alive\\nIt’s not unusual to see a program written for the command line with a structure some-\\nwhat like the following trivial example:\\nfrom my_functions import g\\ndef f(x, y):\\n    return g(x + y)\\ndef main():\\n    x = 6\\n    y = 7.5\\n    result = x + y\\nif __name__ == '__main__':\\n    main()\\nDo you see what might be wrong with this program if we were to run it in IPython?\\nAfter it’s done, none of the results or objects defined in the main function willl be ac-\\ncessible in the IPython shell. A better way is to have whatever code is in main execute\\ndirectly in the module’s global namespace (or in the if __name__ == '__main__': block,\\nif you want the module to also be importable). That way, when you %run the code,\\nyou’ll be able to look at all of the variables defined in main. It’s less meaningful in this\\nsimple example, but in this book we’ll be looking at some complex data analysis prob-\\nlems involving large data sets that you will want to be able to play with in IPython.\\nFlat is better than nested\\nDeeply nested code makes me think about the many layers of an onion. When testing\\nor debugging a function, how many layers of the onion must you peel back in order to\\nreach the code of interest? The idea that “flat is better than nested” is a part of the Zen\\nof Python, and it applies generally to developing code for interactive use as well. Making\\nfunctions and classes as decoupled and modular as possible makes them easier to test\\n(if you are writing unit tests), debug, and use interactively.\\nOvercome a fear of longer files\\nIf you come from a Java (or another such language) background, you may have been\\ntold to keep files short. In many languages, this is sound advice; long length is usually\\na bad “code smell”, indicating refactoring or reorganization may be necessary. How-\\never, while developing code using IPython, working with 10 small, but interconnected\\nfiles (under, say, 100 lines each) is likely to cause you more headache in general than a\\nsingle large file or two or three longer files. Fewer files means fewer modules to reload\\nand less jumping between files while editing, too. I have found maintaining larger\\nmodules, each with high internal cohesion, to be much more useful and pythonic. After\\niterating toward a solution, it sometimes will make sense to refactor larger files into\\nsmaller ones.\\nTips for Productive Code Development Using IPython | 75\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Obviously, I don’t support taking this argument to the extreme, which would to be to\\nput all of your code in a single monstrous file. Finding a sensible and intuitive module\\nand package structure for a large codebase often takes a bit of work, but it is especially\\nimportant to get right in teams. Each module should be internally cohesive, and it\\nshould be as obvious as possible where to find functions and classes responsible for\\neach area of functionality.\\nAdvanced IPython Features\\nMaking Your Own Classes IPython-friendly\\nIPython makes every effort to display a console-friendly string representation of any\\nobject that you inspect. For many objects, like dicts, lists, and tuples, the built-in \\npprint module is used to do the nice formatting. In user-defined classes, however, you\\nhave to generate the desired string output yourself. Suppose we had the following sim-\\nple class:\\nclass Message:\\n    def __init__(self, msg):\\n        self.msg = msg\\nIf you wrote this, you would be disappointed to discover that the default output for\\nyour class isn’t very nice:\\nIn [576]: x = Message('I have a secret')\\nIn [577]: x\\nOut[577]: <__main__.Message instance at 0x60ebbd8>\\nIPython takes the string returned by the __repr__ magic method (by doing output =\\nrepr(obj)) and prints that to the console. Thus, we can add a simple __repr__ method\\nto the above class to get a more helpful output:\\nclass Message:\\n    def __init__(self, msg):\\n        self.msg = msg\\n    def __repr__(self):\\n        return 'Message: %s' % self.msg\\nIn [579]: x = Message('I have a secret')\\nIn [580]: x\\nOut[580]: Message: I have a secret\\n76 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Profiles and Configuration\\nMost aspects of the appearance (colors, prompt, spacing between lines, etc.) and be-\\nhavior of the IPython shell are configurable through an extensive configuration system.\\nHere are some of the things you can do via configuration:\\n• Change the color scheme\\n• Change how the input and output prompts look, or remove the blank line after\\nOut and before the next In prompt\\n• Change how the input and output prompts look\\n• Execute an arbitrary list of Python statements. These could be imports that you\\nuse all the time or anything else you want to happen each time you launch IPython\\n• Enable IPython extensions, like the %lprun magic in line_profiler\\n• Define your own magics or system aliases\\nAll of these configuration options are specified in a special ipython_config.py file which\\nwill be found in the ~/.config/ipython/ directory on UNIX-like systems and %HOME\\n%/.ipython/ directory on Windows. Where your home directory is depends on your\\nsystem. Configuration is performed based on a particular profile. When you start IPy-\\nthon normally, you load up, by default, the default profile , stored in the pro\\nfile_default directory. Thus, on my Linux OS the full path to my default IPython\\nconfiguration file is:\\n/home/wesm/.config/ipython/profile_default/ipython_config.py\\nI’ll spare you the gory details of what’s in this file. Fortunately it has comments de-\\nscribing what each configuration option is for, so I will leave it to the reader to tinker\\nand customize. One additional useful feature is that it’s possible to have multiple pro-\\nfiles. Suppose you wanted to have an alternate IPython configuration tailored for a\\nparticular application or project. Creating a new profile is as simple is typing something\\nlike\\nipython profile create secret_project\\nOnce you’ve done this, edit the config files in the newly-created pro\\nfile_secret_project directory then launch IPython like so\\n$ ipython --profile=secret_project\\nPython 2.7.2 |EPD 7.1-2 (64-bit)| (default, Jul  3 2011, 15:17:51)\\nType \"copyright\", \"credits\" or \"license\" for more information.\\nIPython 0.13 -- An enhanced Interactive Python.\\n?         -> Introduction and overview of IPython\\'s features.\\n%quickref -> Quick reference.\\nhelp      -> Python\\'s own help system.\\nobject?   -> Details about \\'object\\', use \\'object??\\' for extra details.\\nIPython profile: secret_project\\nAdvanced IPython Features | 77\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'In [1]:\\nAs always, the online IPython documentation is an excellent resource for more on\\nprofiles and configuration.\\nCredits\\nParts of this chapter were derived from the wonderful documentation put together by\\nthe IPython Development Team. I can’t thank them enough for all of their work build-\\ning this amazing set of tools.\\n78 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'CHAPTER 4\\nNumPy Basics: Arrays and Vectorized\\nComputation\\nNumPy, short for Numerical Python, is the fundamental package required for high\\nperformance scientific computing and data analysis. It is the foundation on which\\nnearly all of the higher-level tools in this book are built. Here are some of the things it\\nprovides:\\n• ndarray, a fast and space-efficient multidimensional array providing vectorized\\narithmetic operations and sophisticated broadcasting capabilities\\n• Standard mathematical functions for fast operations on entire arrays of data\\nwithout having to write loops\\n• Tools for reading / writing array data to disk and working with memory-mapped\\nfiles\\n• Linear algebra, random number generation, and Fourier transform capabilities\\n• Tools for integrating code written in C, C++, and Fortran\\nThe last bullet point is also one of the most important ones from an ecosystem point\\nof view. Because NumPy provides an easy-to-use C API, it is very easy to pass data to\\nexternal libraries written in a low-level language and also for external libraries to return\\ndata to Python as NumPy arrays. This feature has made Python a language of choice\\nfor wrapping legacy C/C++/Fortran codebases and giving them a dynamic and easy-\\nto-use interface.\\nWhile NumPy by itself does not provide very much high-level data analytical func-\\ntionality, having an understanding of NumPy arrays and array-oriented computing will\\nhelp you use tools like pandas much more effectively. If you’re new to Python and just\\nlooking to get your hands dirty working with data using pandas, feel free to give this\\nchapter a skim. For more on advanced NumPy features like broadcasting, see Chap-\\nter 12.\\n79\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'For most data analysis applications, the main areas of functionality I’ll focus on are:\\n• Fast vectorized array operations for data munging and cleaning, subsetting and\\nfiltering, transformation, and any other kinds of computations\\n• Common array algorithms like sorting, unique, and set operations\\n• Efficient descriptive statistics and aggregating/summarizing data\\n• Data alignment and relational data manipulations for merging and joining together\\nheterogeneous data sets\\n• Expressing conditional logic as array expressions instead of loops with if-elif-\\nelse branches\\n• Group-wise data manipulations (aggregation, transformation, function applica-\\ntion). Much more on this in Chapter 5\\nWhile NumPy provides the computational foundation for these operations, you will\\nlikely want to use pandas as your basis for most kinds of data analysis (especially for\\nstructured or tabular data) as it provides a rich, high-level interface making most com-\\nmon data tasks very concise and simple. pandas also provides some more domain-\\nspecific functionality like time series manipulation, which is not present in NumPy.\\nIn this chapter and throughout the book, I use the standard NumPy\\nconvention of always using import numpy as np . You are, of course,\\nwelcome to put from numpy import * in your code to avoid having to\\nwrite np., but I would caution you against making a habit of this.\\nThe NumPy ndarray: A Multidimensional Array Object\\nOne of the key features of NumPy is its N-dimensional array object, or ndarray, which\\nis a fast, flexible container for large data sets in Python. Arrays enable you to perform\\nmathematical operations on whole blocks of data using similar syntax to the equivalent\\noperations between scalar elements:\\nIn [8]: data\\nOut[8]: \\narray([[ 0.9526, -0.246 , -0.8856],\\n       [ 0.5639,  0.2379,  0.9104]])\\nIn [9]: data * 10                         In [10]: data + data                \\nOut[9]:                                   Out[10]:                            \\narray([[ 9.5256, -2.4601, -8.8565],       array([[ 1.9051, -0.492 , -1.7713], \\n       [ 5.6385,  2.3794,  9.104 ]])             [ 1.1277,  0.4759,  1.8208]])\\nAn ndarray is a generic multidimensional container for homogeneous data; that is, all\\nof the elements must be the same type. Every array has a shape, a tuple indicating the\\nsize of each dimension, and a dtype, an object describing the data type of the array:\\nIn [11]: data.shape\\nOut[11]: (2, 3)\\n80 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [12]: data.dtype\\nOut[12]: dtype('float64')\\nThis chapter will introduce you to the basics of using NumPy arrays, and should be\\nsufficient for following along with the rest of the book. While it’s not necessary to have\\na deep understanding of NumPy for many data analytical applications, becoming pro-\\nficient in array-oriented programming and thinking is a key step along the way to be-\\ncoming a scientific Python guru.\\nWhenever you see “array”, “NumPy array”, or “ndarray” in the text,\\nwith few exceptions they all refer to the same thing: the ndarray object.\\nCreating ndarrays\\nThe easiest way to create an array is to use the array function. This accepts any se-\\nquence-like object (including other arrays) and produces a new NumPy array contain-\\ning the passed data. For example, a list is a good candidate for conversion:\\nIn [13]: data1 = [6, 7.5, 8, 0, 1]\\nIn [14]: arr1 = np.array(data1)\\nIn [15]: arr1\\nOut[15]: array([ 6. ,  7.5,  8. ,  0. ,  1. ])\\nNested sequences, like a list of equal-length lists, will be converted into a multidimen-\\nsional array:\\nIn [16]: data2 = [[1, 2, 3, 4], [5, 6, 7, 8]]\\nIn [17]: arr2 = np.array(data2)\\nIn [18]: arr2\\nOut[18]: \\narray([[1, 2, 3, 4],\\n       [5, 6, 7, 8]])\\nIn [19]: arr2.ndim\\nOut[19]: 2\\nIn [20]: arr2.shape\\nOut[20]: (2, 4)\\nUnless explicitly specified (more on this later), np.array tries to infer a good data type\\nfor the array that it creates. The data type is stored in a special dtype object; for example,\\nin the above two examples we have:\\nIn [21]: arr1.dtype\\nOut[21]: dtype('float64')\\nThe NumPy ndarray: A Multidimensional Array Object | 81\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [22]: arr2.dtype\\nOut[22]: dtype('int64')\\nIn addition to np.array, there are a number of other functions for creating new arrays.\\nAs examples, zeros and ones create arrays of 0’s or 1’s, respectively, with a given length\\nor shape. empty creates an array without initializing its values to any particular value.\\nTo create a higher dimensional array with these methods, pass a tuple for the shape:\\nIn [23]: np.zeros(10)\\nOut[23]: array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])\\nIn [24]: np.zeros((3, 6))                    \\nOut[24]:                                                         \\narray([[ 0.,  0.,  0.,  0.,  0.,  0.],                \\n       [ 0.,  0.,  0.,  0.,  0.,  0.],                \\n       [ 0.,  0.,  0.,  0.,  0.,  0.]])              \\nIn [25]: np.empty((2, 3, 2))\\nOut[25]:\\narray([[[  4.94065646e-324,   4.94065646e-324],\\n        [  3.87491056e-297,   2.46845796e-130],\\n        [  4.94065646e-324,   4.94065646e-324]],\\n       [[  1.90723115e+083,   5.73293533e-053],\\n        [ -2.33568637e+124,  -6.70608105e-012],\\n        [  4.42786966e+160,   1.27100354e+025]]])\\nIt’s not safe to assume that np.empty will return an array of all zeros. In\\nmany cases, as previously shown, it will return uninitialized garbage\\nvalues.\\narange is an array-valued version of the built-in Python range function:\\nIn [26]: np.arange(15)\\nOut[26]: array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])\\nSee Table 4-1 for a short list of standard array creation functions. Since NumPy is\\nfocused on numerical computing, the data type, if not specified, will in many cases be\\nfloat64 (floating point).\\nTable 4-1. Array creation functions\\nFunction Description\\narray Convert input data (list, tuple, array, or other sequence type) to an ndarray either by\\ninferring a dtype or explicitly specifying a dtype. Copies the input data by default.\\nasarray Convert input to ndarray, but do not copy if the input is already an ndarray\\narange Like the built-in range but returns an ndarray instead of a list.\\nones, ones_like Produce an array of all 1’s with the given shape and dtype. ones_like takes another\\narray and produces a ones array of the same shape and dtype.\\nzeros, zeros_like Like ones and ones_like but producing arrays of 0’s instead\\n82 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Function Description\\nempty, empty_like Create new arrays by allocating new memory, but do not populate with any values like \\nones and zeros\\neye, identity Create a square N x N identity matrix (1’s on the diagonal and 0’s elsewhere)\\nData Types for ndarrays\\nThe data type or dtype is a special object containing the information the ndarray needs\\nto interpret a chunk of memory as a particular type of data:\\nIn [27]: arr1 = np.array([1, 2, 3], dtype=np.float64)\\nIn [28]: arr2 = np.array([1, 2, 3], dtype=np.int32)\\nIn [29]: arr1.dtype            In [30]: arr2.dtype    \\nOut[29]: dtype('float64')      Out[30]: dtype('int32')\\nDtypes are part of what make NumPy so powerful and flexible. In most cases they map\\ndirectly onto an underlying machine representation, which makes it easy to read and\\nwrite binary streams of data to disk and also to connect to code written in a low-level\\nlanguage like C or Fortran. The numerical dtypes are named the same way: a type name,\\nlike float or int, followed by a number indicating the number of bits per element. A\\nstandard double-precision floating point value (what’s used under the hood in Python’s\\nfloat object) takes up 8 bytes or 64 bits. Thus, this type is known in NumPy as\\nfloat64. See Table 4-2 for a full listing of NumPy’s supported data types.\\nDon’t worry about memorizing the NumPy dtypes, especially if you’re\\na new user. It’s often only necessary to care about the general kind of\\ndata you’re dealing with, whether floating point, complex, integer,\\nboolean, string, or general Python object. When you need more control\\nover how data are stored in memory and on disk, especially large data\\nsets, it is good to know that you have control over the storage type.\\nTable 4-2. NumPy data types\\nType Type Code Description\\nint8, uint8 i1, u1 Signed and unsigned 8-bit (1 byte) integer types\\nint16, uint16 i2, u2 Signed and unsigned 16-bit integer types\\nint32, uint32 i4, u4 Signed and unsigned 32-bit integer types\\nint64, uint64 i8, u8 Signed and unsigned 32-bit integer types\\nfloat16 f2 Half-precision floating point\\nfloat32 f4 or f Standard single-precision floating point. Compatible with C float\\nfloat64, float128 f8 or d Standard double-precision floating point. Compatible with C double\\nand Python float object\\nThe NumPy ndarray: A Multidimensional Array Object | 83\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Type Type Code Description\\nfloat128 f16 or g Extended-precision floating point\\ncomplex64, complex128,\\ncomplex256\\nc8, c16,\\nc32\\nComplex numbers represented by two 32, 64, or 128 floats, respectively\\nbool ? Boolean type storing True and False values\\nobject O Python object type\\nstring_ S Fixed-length string type (1 byte per character). For example, to create\\na string dtype with length 10, use 'S10'.\\nunicode_ U Fixed-length unicode type (number of bytes platform specific). Same\\nspecification semantics as string_ (e.g. 'U10').\\nYou can explicitly convert or cast an array from one dtype to another using ndarray’s \\nastype method:\\nIn [31]: arr = np.array([1, 2, 3, 4, 5])\\nIn [32]: arr.dtype\\nOut[32]: dtype('int64')\\nIn [33]: float_arr = arr.astype(np.float64)\\nIn [34]: float_arr.dtype\\nOut[34]: dtype('float64')\\nIn this example, integers were cast to floating point. If I cast some floating point num-\\nbers to be of integer dtype, the decimal part will be truncated:\\nIn [35]: arr = np.array([3.7, -1.2, -2.6, 0.5, 12.9, 10.1])\\nIn [36]: arr\\nOut[36]: array([  3.7,  -1.2,  -2.6,   0.5,  12.9,  10.1])\\nIn [37]: arr.astype(np.int32)\\nOut[37]: array([ 3, -1, -2,  0, 12, 10], dtype=int32)\\nShould you have an array of strings representing numbers, you can use astype to convert\\nthem to numeric form:\\nIn [38]: numeric_strings = np.array(['1.25', '-9.6', '42'], dtype=np.string_)\\nIn [39]: numeric_strings.astype(float)\\nOut[39]: array([  1.25,  -9.6 ,  42.  ])\\nIf casting were to fail for some reason (like a string that cannot be converted to\\nfloat64), a TypeError will be raised. See that I was a bit lazy and wrote float instead of\\nnp.float64; NumPy is smart enough to alias the Python types to the equivalent dtypes.\\nYou can also use another array’s dtype attribute:\\nIn [40]: int_array = np.arange(10)\\n84 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [41]: calibers = np.array([.22, .270, .357, .380, .44, .50], dtype=np.float64)\\nIn [42]: int_array.astype(calibers.dtype)\\nOut[42]: array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.])\\nThere are shorthand type code strings you can also use to refer to a dtype:\\nIn [43]: empty_uint32 = np.empty(8, dtype='u4')\\nIn [44]: empty_uint32\\nOut[44]: \\narray([       0,        0, 65904672,        0, 64856792,        0,\\n       39438163,        0], dtype=uint32)\\nCalling astype always creates a new array (a copy of the data), even if\\nthe new dtype is the same as the old dtype.\\nIt’s worth keeping in mind that floating point numbers, such as those\\nin float64 and float32 arrays, are only capable of approximating frac-\\ntional quantities. In complex computations, you may accrue some\\nfloating point error, making comparisons only valid up to a certain num-\\nber of decimal places.\\nOperations between Arrays and Scalars\\nArrays are important because they enable you to express batch operations on data\\nwithout writing any for loops. This is usually called vectorization. Any arithmetic op-\\nerations between equal-size arrays applies the operation elementwise:\\nIn [45]: arr = np.array([[1., 2., 3.], [4., 5., 6.]])\\nIn [46]: arr\\nOut[46]: \\narray([[ 1.,  2.,  3.],\\n       [ 4.,  5.,  6.]])\\nIn [47]: arr * arr                 In [48]: arr - arr      \\nOut[47]:                           Out[48]:                \\narray([[  1.,   4.,   9.],         array([[ 0.,  0.,  0.], \\n       [ 16.,  25.,  36.]])               [ 0.,  0.,  0.]])\\nArithmetic operations with scalars are as you would expect, propagating the value to\\neach element:\\nIn [49]: 1 / arr                            In [50]: arr ** 0.5                 \\nOut[49]:                                    Out[50]:                            \\narray([[ 1.    ,  0.5   ,  0.3333],         array([[ 1.    ,  1.4142,  1.7321], \\n       [ 0.25  ,  0.2   ,  0.1667]])               [ 2.    ,  2.2361,  2.4495]])\\nThe NumPy ndarray: A Multidimensional Array Object | 85\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Operations between differently sized arrays is called broadcasting and will be discussed\\nin more detail in Chapter 12. Having a deep understanding of broadcasting is not nec-\\nessary for most of this book.\\nBasic Indexing and Slicing\\nNumPy array indexing is a rich topic, as there are many ways you may want to select\\na subset of your data or individual elements. One-dimensional arrays are simple; on\\nthe surface they act similarly to Python lists:\\nIn [51]: arr = np.arange(10)\\nIn [52]: arr\\nOut[52]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\\nIn [53]: arr[5]\\nOut[53]: 5\\nIn [54]: arr[5:8]\\nOut[54]: array([5, 6, 7])\\nIn [55]: arr[5:8] = 12\\nIn [56]: arr\\nOut[56]: array([ 0,  1,  2,  3,  4, 12, 12, 12,  8,  9])\\nAs you can see, if you assign a scalar value to a slice, as in arr[5:8] = 12, the value is\\npropagated (or broadcasted henceforth) to the entire selection. An important first dis-\\ntinction from lists is that array slices are views on the original array. This means that\\nthe data is not copied, and any modifications to the view will be reflected in the source\\narray:\\nIn [57]: arr_slice = arr[5:8]\\nIn [58]: arr_slice[1] = 12345\\nIn [59]: arr\\nOut[59]: array([    0,     1,     2,     3,     4,    12, 12345,    12,     8,     9])\\nIn [60]: arr_slice[:] = 64\\nIn [61]: arr\\nOut[61]: array([ 0,  1,  2,  3,  4, 64, 64, 64,  8,  9])\\nIf you are new to NumPy, you might be surprised by this, especially if they have used\\nother array programming languages which copy data more zealously. As NumPy has\\nbeen designed with large data use cases in mind, you could imagine performance and\\nmemory problems if NumPy insisted on copying data left and right.\\n86 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'If you want a copy of a slice of an ndarray instead of a view, you will\\nneed to explicitly copy the array; for example arr[5:8].copy().\\nWith higher dimensional arrays, you have many more options. In a two-dimensional\\narray, the elements at each index are no longer scalars but rather one-dimensional\\narrays:\\nIn [62]: arr2d = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\\nIn [63]: arr2d[2]\\nOut[63]: array([7, 8, 9])\\nThus, individual elements can be accessed recursively. But that is a bit too much work,\\nso you can pass a comma-separated list of indices to select individual elements. So these\\nare equivalent:\\nIn [64]: arr2d[0][2]\\nOut[64]: 3\\nIn [65]: arr2d[0, 2]\\nOut[65]: 3\\nSee Figure 4-1 for an illustration of indexing on a 2D array.\\nFigure 4-1. Indexing elements in a NumPy array\\nIn multidimensional arrays, if you omit later indices, the returned object will be a lower-\\ndimensional ndarray consisting of all the data along the higher dimensions. So in the\\n2 × 2 × 3 array arr3d\\nIn [66]: arr3d = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\\nIn [67]: arr3d\\nOut[67]: \\narray([[[ 1,  2,  3],\\nThe NumPy ndarray: A Multidimensional Array Object | 87\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': '[ 4,  5,  6]],\\n       [[ 7,  8,  9],\\n        [10, 11, 12]]])\\narr3d[0] is a 2 × 3 array:\\nIn [68]: arr3d[0]\\nOut[68]: \\narray([[1, 2, 3],\\n       [4, 5, 6]])\\nBoth scalar values and arrays can be assigned to arr3d[0]:\\nIn [69]: old_values = arr3d[0].copy()\\nIn [70]: arr3d[0] = 42\\nIn [71]: arr3d\\nOut[71]: \\narray([[[42, 42, 42],\\n        [42, 42, 42]],\\n       [[ 7,  8,  9],\\n        [10, 11, 12]]])\\nIn [72]: arr3d[0] = old_values\\nIn [73]: arr3d\\nOut[73]: \\narray([[[ 1,  2,  3],\\n        [ 4,  5,  6]],\\n       [[ 7,  8,  9],\\n        [10, 11, 12]]])\\nSimilarly, arr3d[1, 0] gives you all of the values whose indices start with (1, 0), form-\\ning a 1-dimensional array:\\nIn [74]: arr3d[1, 0]\\nOut[74]: array([7, 8, 9])\\nNote that in all of these cases where subsections of the array have been selected, the\\nreturned arrays are views.\\nIndexing with slices\\nLike one-dimensional objects such as Python lists, ndarrays can be sliced using the\\nfamiliar syntax:\\nIn [75]: arr[1:6]\\nOut[75]: array([ 1,  2,  3,  4, 64])\\nHigher dimensional objects give you more options as you can slice one or more axes\\nand also mix integers. Consider the 2D array above, arr2d. Slicing this array is a bit\\ndifferent:\\nIn [76]: arr2d            In [77]: arr2d[:2]\\nOut[76]:                  Out[77]:          \\n88 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"array([[1, 2, 3],         array([[1, 2, 3], \\n       [4, 5, 6],                [4, 5, 6]])\\n       [7, 8, 9]])\\nAs you can see, it has sliced along axis 0, the first axis. A slice, therefore, selects a range\\nof elements along an axis. You can pass multiple slices just like you can pass multiple\\nindexes:\\nIn [78]: arr2d[:2, 1:]\\nOut[78]: \\narray([[2, 3],\\n       [5, 6]])\\nWhen slicing like this, you always obtain array views of the same number of dimensions.\\nBy mixing integer indexes and slices, you get lower dimensional slices:\\nIn [79]: arr2d[1, :2]         In [80]: arr2d[2, :1]\\nOut[79]: array([4, 5])        Out[80]: array([7])\\nSee Figure 4-2 for an illustration. Note that a colon by itself means to take the entire\\naxis, so you can slice only higher dimensional axes by doing:\\nIn [81]: arr2d[:, :1]\\nOut[81]: \\narray([[1],\\n       [4],\\n       [7]])\\nOf course, assigning to a slice expression assigns to the whole selection:\\nIn [82]: arr2d[:2, 1:] = 0\\nBoolean Indexing\\nLet’s consider an example where we have some data in an array and an array of names\\nwith duplicates. I’m going to use here the randn function in numpy.random to generate\\nsome random normally distributed data:\\nIn [83]: names = np.array(['Bob', 'Joe', 'Will', 'Bob', 'Will', 'Joe', 'Joe'])\\nIn [84]: data = randn(7, 4)\\nIn [85]: names\\nOut[85]: \\narray(['Bob', 'Joe', 'Will', 'Bob', 'Will', 'Joe', 'Joe'], \\n      dtype='|S4')\\nIn [86]: data\\nOut[86]: \\narray([[-0.048 ,  0.5433, -0.2349,  1.2792],\\n       [-0.268 ,  0.5465,  0.0939, -2.0445],\\n       [-0.047 , -2.026 ,  0.7719,  0.3103],\\n       [ 2.1452,  0.8799, -0.0523,  0.0672],\\n       [-1.0023, -0.1698,  1.1503,  1.7289],\\nThe NumPy ndarray: A Multidimensional Array Object | 89\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"[ 0.1913,  0.4544,  0.4519,  0.5535],\\n       [ 0.5994,  0.8174, -0.9297, -1.2564]])\\nFigure 4-2. Two-dimensional array slicing\\nSuppose each name corresponds to a row in the data array. If we wanted to select all\\nthe rows with corresponding name 'Bob'. Like arithmetic operations, comparisons\\n(such as ==) with arrays are also vectorized. Thus, comparing names with the string\\n'Bob' yields a boolean array:\\nIn [87]: names == 'Bob'\\nOut[87]: array([ True, False, False, True, False, False, False], dtype=bool)\\nThis boolean array can be passed when indexing the array:\\nIn [88]: data[names == 'Bob']\\nOut[88]: \\narray([[-0.048 ,  0.5433, -0.2349,  1.2792],\\n       [ 2.1452,  0.8799, -0.0523,  0.0672]])\\nThe boolean array must be of the same length as the axis it’s indexing. You can even\\nmix and match boolean arrays with slices or integers (or sequences of integers, more\\non this later):\\nIn [89]: data[names == 'Bob', 2:]\\nOut[89]: \\narray([[-0.2349,  1.2792],\\n90 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"[-0.0523,  0.0672]])\\nIn [90]: data[names == 'Bob', 3]\\nOut[90]: array([ 1.2792,  0.0672])\\nTo select everything but 'Bob', you can either use != or negate the condition using -:\\nIn [91]: names != 'Bob'\\nOut[91]: array([False, True, True, False, True, True, True], dtype=bool)\\nIn [92]: data[-(names == 'Bob')]\\nOut[92]: \\narray([[-0.268 ,  0.5465,  0.0939, -2.0445],\\n       [-0.047 , -2.026 ,  0.7719,  0.3103],\\n       [-1.0023, -0.1698,  1.1503,  1.7289],\\n       [ 0.1913,  0.4544,  0.4519,  0.5535],\\n       [ 0.5994,  0.8174, -0.9297, -1.2564]])\\nSelecting two of the three names to combine multiple boolean conditions, use boolean\\narithmetic operators like & (and) and | (or):\\nIn [93]: mask = (names == 'Bob') | (names == 'Will')\\nIn [94]: mask\\nOut[94]: array([True, False, True, True, True, False, False], dtype=bool)\\nIn [95]: data[mask]\\nOut[95]: \\narray([[-0.048 ,  0.5433, -0.2349,  1.2792],\\n       [-0.047 , -2.026 ,  0.7719,  0.3103],\\n       [ 2.1452,  0.8799, -0.0523,  0.0672],\\n       [-1.0023, -0.1698,  1.1503,  1.7289]])\\nSelecting data from an array by boolean indexing always creates a copy of the data,\\neven if the returned array is unchanged.\\nThe Python keywords and and or do not work with boolean arrays.\\nSetting values with boolean arrays works in a common-sense way. To set all of the\\nnegative values in data to 0 we need only do:\\nIn [96]: data[data < 0] = 0\\nIn [97]: data\\nOut[97]: \\narray([[ 0.    ,  0.5433,  0.    ,  1.2792],\\n       [ 0.    ,  0.5465,  0.0939,  0.    ],\\n       [ 0.    ,  0.    ,  0.7719,  0.3103],\\n       [ 2.1452,  0.8799,  0.    ,  0.0672],\\n       [ 0.    ,  0.    ,  1.1503,  1.7289],\\n       [ 0.1913,  0.4544,  0.4519,  0.5535],\\n       [ 0.5994,  0.8174,  0.    ,  0.    ]])\\nThe NumPy ndarray: A Multidimensional Array Object | 91\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Setting whole rows or columns using a 1D boolean array is also easy:\\nIn [98]: data[names != 'Joe'] = 7\\nIn [99]: data\\nOut[99]: \\narray([[ 7.    ,  7.    ,  7.    ,  7.    ],\\n       [ 0.    ,  0.5465,  0.0939,  0.    ],\\n       [ 7.    ,  7.    ,  7.    ,  7.    ],\\n       [ 7.    ,  7.    ,  7.    ,  7.    ],\\n       [ 7.    ,  7.    ,  7.    ,  7.    ],\\n       [ 0.1913,  0.4544,  0.4519,  0.5535],\\n       [ 0.5994,  0.8174,  0.    ,  0.    ]])\\nFancy Indexing\\nFancy indexing is a term adopted by NumPy to describe indexing using integer arrays.\\nSuppose we had a 8 × 4 array:\\nIn [100]: arr = np.empty((8, 4))\\nIn [101]: for i in range(8):\\n   .....:     arr[i] = i\\nIn [102]: arr\\nOut[102]: \\narray([[ 0.,  0.,  0.,  0.],\\n       [ 1.,  1.,  1.,  1.],\\n       [ 2.,  2.,  2.,  2.],\\n       [ 3.,  3.,  3.,  3.],\\n       [ 4.,  4.,  4.,  4.],\\n       [ 5.,  5.,  5.,  5.],\\n       [ 6.,  6.,  6.,  6.],\\n       [ 7.,  7.,  7.,  7.]])\\nTo select out a subset of the rows in a particular order, you can simply pass a list or\\nndarray of integers specifying the desired order:\\nIn [103]: arr[[4, 3, 0, 6]]\\nOut[103]: \\narray([[ 4.,  4.,  4.,  4.],\\n       [ 3.,  3.,  3.,  3.],\\n       [ 0.,  0.,  0.,  0.],\\n       [ 6.,  6.,  6.,  6.]])\\nHopefully this code did what you expected! Using negative indices select rows from\\nthe end:\\nIn [104]: arr[[-3, -5, -7]]\\nOut[104]: \\narray([[ 5.,  5.,  5.,  5.],\\n       [ 3.,  3.,  3.,  3.],\\n       [ 1.,  1.,  1.,  1.]])\\n92 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Passing multiple index arrays does something slightly different; it selects a 1D array of\\nelements corresponding to each tuple of indices:\\n# more on reshape in Chapter 12\\nIn [105]: arr = np.arange(32).reshape((8, 4))\\nIn [106]: arr\\nOut[106]: \\narray([[ 0,  1,  2,  3],\\n       [ 4,  5,  6,  7],\\n       [ 8,  9, 10, 11],\\n       [12, 13, 14, 15],\\n       [16, 17, 18, 19],\\n       [20, 21, 22, 23],\\n       [24, 25, 26, 27],\\n       [28, 29, 30, 31]])\\nIn [107]: arr[[1, 5, 7, 2], [0, 3, 1, 2]]\\nOut[107]: array([ 4, 23, 29, 10])\\nTake a moment to understand what just happened: the elements (1, 0), (5, 3), (7,\\n1), and (2, 2) were selected. The behavior of fancy indexing in this case is a bit different\\nfrom what some users might have expected (myself included), which is the rectangular\\nregion formed by selecting a subset of the matrix’s rows and columns. Here is one way\\nto get that:\\nIn [108]: arr[[1, 5, 7, 2]][:, [0, 3, 1, 2]]\\nOut[108]: \\narray([[ 4,  7,  5,  6],\\n       [20, 23, 21, 22],\\n       [28, 31, 29, 30],\\n       [ 8, 11,  9, 10]])\\nAnother way is to use the np.ix_ function, which converts two 1D integer arrays to an\\nindexer that selects the square region:\\nIn [109]: arr[np.ix_([1, 5, 7, 2], [0, 3, 1, 2])]\\nOut[109]: \\narray([[ 4,  7,  5,  6],\\n       [20, 23, 21, 22],\\n       [28, 31, 29, 30],\\n       [ 8, 11,  9, 10]])\\nKeep in mind that fancy indexing, unlike slicing, always copies the data into a new array.\\nTransposing Arrays and Swapping Axes\\nTransposing is a special form of reshaping which similarly returns a view on the un-\\nderlying data without copying anything. Arrays have the transpose method and also\\nthe special T attribute:\\nIn [110]: arr = np.arange(15).reshape((3, 5))\\nIn [111]: arr                        In [112]: arr.T      \\nThe NumPy ndarray: A Multidimensional Array Object | 93\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Out[111]:                            Out[112]:            \\narray([[ 0,  1,  2,  3,  4],         array([[ 0,  5, 10], \\n       [ 5,  6,  7,  8,  9],                [ 1,  6, 11], \\n       [10, 11, 12, 13, 14]])               [ 2,  7, 12], \\n                                            [ 3,  8, 13], \\n                                            [ 4,  9, 14]])\\nWhen doing matrix computations, you will do this very often, like for example com-\\nputing the inner matrix product XTX using np.dot:\\nIn [113]: arr = np.random.randn(6, 3)\\nIn [114]: np.dot(arr.T, arr)\\nOut[114]: \\narray([[ 2.584 ,  1.8753,  0.8888],\\n       [ 1.8753,  6.6636,  0.3884],\\n       [ 0.8888,  0.3884,  3.9781]])\\nFor higher dimensional arrays, transpose will accept a tuple of axis numbers to permute\\nthe axes (for extra mind bending):\\nIn [115]: arr = np.arange(16).reshape((2, 2, 4))\\nIn [116]: arr\\nOut[116]: \\narray([[[ 0,  1,  2,  3],\\n        [ 4,  5,  6,  7]],\\n       [[ 8,  9, 10, 11],\\n        [12, 13, 14, 15]]])\\nIn [117]: arr.transpose((1, 0, 2))\\nOut[117]: \\narray([[[ 0,  1,  2,  3],\\n        [ 8,  9, 10, 11]],\\n       [[ 4,  5,  6,  7],\\n        [12, 13, 14, 15]]])\\nSimple transposing with .T is just a special case of swapping axes. ndarray has the\\nmethod swapaxes which takes a pair of axis numbers:\\nIn [118]: arr                      In [119]: arr.swapaxes(1, 2)\\nOut[118]:                          Out[119]:                   \\narray([[[ 0,  1,  2,  3],          array([[[ 0,  4],           \\n        [ 4,  5,  6,  7]],                 [ 1,  5],           \\n                                           [ 2,  6],           \\n       [[ 8,  9, 10, 11],                  [ 3,  7]],          \\n        [12, 13, 14, 15]]])                                    \\n                                          [[ 8, 12],           \\n                                           [ 9, 13],           \\n                                           [10, 14],           \\n                                           [11, 15]]])\\nswapaxes similarly returns a view on the data without making a copy.\\n94 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Universal Functions: Fast Element-wise Array Functions\\nA universal function, or ufunc, is a function that performs elementwise operations on\\ndata in ndarrays. You can think of them as fast vectorized wrappers for simple functions\\nthat take one or more scalar values and produce one or more scalar results.\\nMany ufuncs are simple elementwise transformations, like sqrt or exp:\\nIn [120]: arr = np.arange(10)\\nIn [121]: np.sqrt(arr)\\nOut[121]: \\narray([ 0.    ,  1.    ,  1.4142,  1.7321,  2.    ,  2.2361,  2.4495,\\n        2.6458,  2.8284,  3.    ])\\nIn [122]: np.exp(arr)\\nOut[122]: \\narray([    1.    ,     2.7183,     7.3891,    20.0855,    54.5982,\\n         148.4132,   403.4288,  1096.6332,  2980.958 ,  8103.0839])\\nThese are referred to as unary ufuncs. Others, such as add or maximum, take 2 arrays\\n(thus, binary ufuncs) and return a single array as the result:\\nIn [123]: x = randn(8)\\nIn [124]: y = randn(8)\\nIn [125]: x\\nOut[125]: \\narray([ 0.0749,  0.0974,  0.2002, -0.2551,  0.4655,  0.9222,  0.446 ,\\n       -0.9337])\\nIn [126]: y\\nOut[126]: \\narray([ 0.267 , -1.1131, -0.3361,  0.6117, -1.2323,  0.4788,  0.4315,\\n       -0.7147])\\nIn [127]: np.maximum(x, y) # element-wise maximum\\nOut[127]: \\narray([ 0.267 ,  0.0974,  0.2002,  0.6117,  0.4655,  0.9222,  0.446 ,\\n       -0.7147])\\nWhile not common, a ufunc can return multiple arrays. modf is one example, a vector-\\nized version of the built-in Python divmod: it returns the fractional and integral parts of\\na floating point array:\\nIn [128]: arr = randn(7) * 5\\nIn [129]: np.modf(arr)\\nOut[129]: \\n(array([-0.6808,  0.0636, -0.386 ,  0.1393, -0.8806,  0.9363, -0.883 ]),\\n array([-2.,  4., -3.,  5., -3.,  3., -6.]))\\nUniversal Functions: Fast Element-wise Array Functions | 95\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'See Table 4-3 and Table 4-4 for a listing of available ufuncs.\\nTable 4-3. Unary ufuncs\\nFunction Description\\nabs, fabs Compute the absolute value element-wise for integer, floating point, or complex values.\\nUse fabs as a faster alternative for non-complex-valued data\\nsqrt Compute the square root of each element. Equivalent to arr ** 0.5\\nsquare Compute the square of each element. Equivalent to arr ** 2\\nexp Compute the exponent ex of each element\\nlog, log10, log2, log1p Natural logarithm (base e), log base 10, log base 2, and log(1 + x), respectively\\nsign Compute the sign of each element: 1 (positive), 0 (zero), or -1 (negative)\\nceil Compute the ceiling of each element, i.e. the smallest integer greater than or equal to\\neach element\\nfloor Compute the floor of each element, i.e. the largest integer less than or equal to each\\nelement\\nrint Round elements to the nearest integer, preserving the dtype\\nmodf Return fractional and integral parts of array as separate array\\nisnan Return boolean array indicating whether each value is NaN (Not a Number)\\nisfinite, isinf Return boolean array indicating whether each element is finite (non-inf, non-NaN) or\\ninfinite, respectively\\ncos, cosh, sin, sinh,\\ntan, tanh\\nRegular and hyperbolic trigonometric functions\\narccos, arccosh, arcsin,\\narcsinh, arctan, arctanh\\nInverse trigonometric functions\\nlogical_not Compute truth value of not x element-wise. Equivalent to -arr.\\nTable 4-4. Binary universal functions\\nFunction Description\\nadd Add corresponding elements in arrays\\nsubtract Subtract elements in second array from first array\\nmultiply Multiply array elements\\ndivide, floor_divide Divide or floor divide (truncating the remainder)\\npower Raise elements in first array to powers indicated in second array\\nmaximum, fmax Element-wise maximum. fmax ignores NaN\\nminimum, fmin Element-wise minimum. fmin ignores NaN\\nmod Element-wise modulus (remainder of division)\\ncopysign Copy sign of values in second argument to values in first argument\\n96 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Function Description\\ngreater, greater_equal,\\nless, less_equal, equal,\\nnot_equal\\nPerform element-wise comparison, yielding boolean array. Equivalent to infix operators\\n>, >=, <, <=, ==, !=\\nlogical_and,\\nlogical_or, logical_xor\\nCompute element-wise truth value of logical operation. Equivalent to infix operators &\\n|, ^\\nData Processing Using Arrays\\nUsing NumPy arrays enables you to express many kinds of data processing tasks as\\nconcise array expressions that might otherwise require writing loops. This practice of\\nreplacing explicit loops with array expressions is commonly referred to as vectoriza-\\ntion. In general, vectorized array operations will often be one or two (or more) orders\\nof magnitude faster than their pure Python equivalents, with the biggest impact in any\\nkind of numerical computations. Later, in Chapter 12, I will explain broadcasting, a\\npowerful method for vectorizing computations.\\nAs a simple example, suppose we wished to evaluate the function sqrt(x^2 + y^2)\\nacross a regular grid of values. The np.meshgrid function takes two 1D arrays and pro-\\nduces two 2D matrices corresponding to all pairs of (x, y) in the two arrays:\\nIn [130]: points = np.arange(-5, 5, 0.01) # 1000 equally spaced points\\nIn [131]: xs, ys = np.meshgrid(points, points)\\nIn [132]: ys\\nOut[132]: \\narray([[-5.  , -5.  , -5.  , ..., -5.  , -5.  , -5.  ],\\n       [-4.99, -4.99, -4.99, ..., -4.99, -4.99, -4.99],\\n       [-4.98, -4.98, -4.98, ..., -4.98, -4.98, -4.98],\\n       ..., \\n       [ 4.97,  4.97,  4.97, ...,  4.97,  4.97,  4.97],\\n       [ 4.98,  4.98,  4.98, ...,  4.98,  4.98,  4.98],\\n       [ 4.99,  4.99,  4.99, ...,  4.99,  4.99,  4.99]])\\nNow, evaluating the function is a simple matter of writing the same expression you\\nwould write with two points:\\nIn [134]: import matplotlib.pyplot as plt\\nIn [135]: z = np.sqrt(xs ** 2 + ys ** 2)\\nIn [136]: z\\nOut[136]: \\narray([[ 7.0711,  7.064 ,  7.0569, ...,  7.0499,  7.0569,  7.064 ],\\n       [ 7.064 ,  7.0569,  7.0499, ...,  7.0428,  7.0499,  7.0569],\\n       [ 7.0569,  7.0499,  7.0428, ...,  7.0357,  7.0428,  7.0499],\\n       ..., \\n       [ 7.0499,  7.0428,  7.0357, ...,  7.0286,  7.0357,  7.0428],\\n       [ 7.0569,  7.0499,  7.0428, ...,  7.0357,  7.0428,  7.0499],\\n       [ 7.064 ,  7.0569,  7.0499, ...,  7.0428,  7.0499,  7.0569]])\\nData Processing Using Arrays | 97\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'In [137]: plt.imshow(z, cmap=plt.cm.gray); plt.colorbar()\\nOut[137]: <matplotlib.colorbar.Colorbar instance at 0x4e46d40>\\nIn [138]: plt.title(\"Image plot of $\\\\sqrt{x^2 + y^2}$ for a grid of values\")\\nOut[138]: <matplotlib.text.Text at 0x4565790>\\nSee Figure 4-3. Here I used the matplotlib function imshow to create an image plot from\\na 2D array of function values.\\nFigure 4-3. Plot of function evaluated on grid\\nExpressing Conditional Logic as Array Operations\\nThe numpy.where function is a vectorized version of the ternary expression x if condi\\ntion else y. Suppose we had a boolean array and two arrays of values:\\nIn [140]: xarr = np.array([1.1, 1.2, 1.3, 1.4, 1.5])\\nIn [141]: yarr = np.array([2.1, 2.2, 2.3, 2.4, 2.5])\\nIn [142]: cond = np.array([True, False, True, True, False])\\nSuppose we wanted to take a value from xarr whenever the corresponding value in\\ncond is True otherwise take the value from yarr. A list comprehension doing this might\\nlook like:\\nIn [143]: result = [(x if c else y)\\n   .....:           for x, y, c in zip(xarr, yarr, cond)]\\nIn [144]: result\\nOut[144]: [1.1000000000000001, 2.2000000000000002, 1.3, 1.3999999999999999, 2.5]\\n98 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'This has multiple problems. First, it will not be very fast for large arrays (because all\\nthe work is being done in pure Python). Secondly, it will not work with multidimen-\\nsional arrays. With np.where you can write this very concisely:\\nIn [145]: result = np.where(cond, xarr, yarr)\\nIn [146]: result\\nOut[146]: array([ 1.1,  2.2,  1.3,  1.4,  2.5])\\nThe second and third arguments to np.where don’t need to be arrays; one or both of\\nthem can be scalars. A typical use of where in data analysis is to produce a new array of\\nvalues based on another array. Suppose you had a matrix of randomly generated data\\nand you wanted to replace all positive values with 2 and all negative values with -2.\\nThis is very easy to do with np.where:\\nIn [147]: arr = randn(4, 4)\\nIn [148]: arr\\nOut[148]: \\narray([[ 0.6372,  2.2043,  1.7904,  0.0752],\\n       [-1.5926, -1.1536,  0.4413,  0.3483],\\n       [-0.1798,  0.3299,  0.7827, -0.7585],\\n       [ 0.5857,  0.1619,  1.3583, -1.3865]])\\nIn [149]: np.where(arr > 0, 2, -2)\\nOut[149]: \\narray([[ 2,  2,  2,  2],\\n       [-2, -2,  2,  2],\\n       [-2,  2,  2, -2],\\n       [ 2,  2,  2, -2]])\\nIn [150]: np.where(arr > 0, 2, arr) # set only positive values to 2\\nOut[150]: \\narray([[ 2.    ,  2.    ,  2.    ,  2.    ],\\n       [-1.5926, -1.1536,  2.    ,  2.    ],\\n       [-0.1798,  2.    ,  2.    , -0.7585],\\n       [ 2.    ,  2.    ,  2.    , -1.3865]])\\nThe arrays passed to where can be more than just equal sizes array or scalers.\\nWith some cleverness you can use where to express more complicated logic; consider\\nthis example where I have two boolean arrays, cond1 and cond2, and wish to assign a\\ndifferent value for each of the 4 possible pairs of boolean values:\\nresult = []\\nfor i in range(n):\\n    if cond1[i] and cond2[i]:\\n        result.append(0)\\n    elif cond1[i]:\\n        result.append(1)\\n    elif cond2[i]:\\n        result.append(2)\\n    else:\\n        result.append(3)\\nData Processing Using Arrays | 99\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'While perhaps not immediately obvious, this for loop can be converted into a nested\\nwhere expression:\\nnp.where(cond1 & cond2, 0,\\n         np.where(cond1, 1,\\n                  np.where(cond2, 2, 3)))\\nIn this particular example, we can also take advantage of the fact that boolean values\\nare treated as 0 or 1 in calculations, so this could alternatively be expressed (though a\\nbit more cryptically) as an arithmetic operation:\\nresult = 1 * cond1 + 2 * cond2 + 3 * -(cond1 | cond2)\\nMathematical and Statistical Methods\\nA set of mathematical functions which compute statistics about an entire array or about\\nthe data along an axis are accessible as array methods. Aggregations (often called\\nreductions) like sum, mean, and standard deviation std can either be used by calling the\\narray instance method or using the top level NumPy function:\\nIn [151]: arr = np.random.randn(5, 4) # normally-distributed data\\nIn [152]: arr.mean()\\nOut[152]: 0.062814911084854597\\nIn [153]: np.mean(arr)\\nOut[153]: 0.062814911084854597\\nIn [154]: arr.sum()\\nOut[154]: 1.2562982216970919\\nFunctions like mean and sum take an optional axis argument which computes the statistic\\nover the given axis, resulting in an array with one fewer dimension:\\nIn [155]: arr.mean(axis=1)\\nOut[155]: array([-1.2833,  0.2844,  0.6574,  0.6743, -0.0187])\\nIn [156]: arr.sum(0)\\nOut[156]: array([-3.1003, -1.6189,  1.4044,  4.5712])\\nOther methods like cumsum and cumprod do not aggregate, instead producing an array\\nof the intermediate results:\\nIn [157]: arr = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8]])\\nIn [158]: arr.cumsum(0)        In [159]: arr.cumprod(1)\\nOut[158]:                      Out[159]:               \\narray([[ 0,  1,  2],           array([[  0,   0,   0], \\n       [ 3,  5,  7],                  [  3,  12,  60], \\n       [ 9, 12, 15]])                 [  6,  42, 336]])\\nSee Table 4-5 for a full listing. We’ll see many examples of these methods in action in\\nlater chapters.\\n100 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Table 4-5. Basic array statistical methods\\nMethod Description\\nsum Sum of all the elements in the array or along an axis. Zero-length arrays have sum 0.\\nmean Arithmetic mean. Zero-length arrays have NaN mean.\\nstd, var Standard deviation and variance, respectively, with optional degrees of freedom adjust-\\nment (default denominator n).\\nmin, max Minimum and maximum.\\nargmin, argmax Indices of minimum and maximum elements, respectively.\\ncumsum Cumulative sum of elements starting from 0\\ncumprod Cumulative product of elements starting from 1\\nMethods for Boolean Arrays\\nBoolean values are coerced to 1 (True) and 0 (False) in the above methods. Thus, sum\\nis often used as a means of counting True values in a boolean array:\\nIn [160]: arr = randn(100)\\nIn [161]: (arr > 0).sum() # Number of positive values\\nOut[161]: 44\\nThere are two additional methods, any and all, useful especially for boolean arrays. \\nany tests whether one or more values in an array is True, while all checks if every value\\nis True:\\nIn [162]: bools = np.array([False, False, True, False])\\nIn [163]: bools.any()\\nOut[163]: True\\nIn [164]: bools.all()\\nOut[164]: False\\nThese methods also work with non-boolean arrays, where non-zero elements evaluate\\nto True.\\nSorting\\nLike Python’s built-in list type, NumPy arrays can be sorted in-place using the sort\\nmethod:\\nIn [165]: arr = randn(8)\\nIn [166]: arr\\nOut[166]: \\narray([ 0.6903,  0.4678,  0.0968, -0.1349,  0.9879,  0.0185, -1.3147,\\n       -0.5425])\\nIn [167]: arr.sort()\\nData Processing Using Arrays | 101\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [168]: arr\\nOut[168]: \\narray([-1.3147, -0.5425, -0.1349,  0.0185,  0.0968,  0.4678,  0.6903,\\n        0.9879])\\nMultidimensional arrays can have each 1D section of values sorted in-place along an\\naxis by passing the axis number to sort:\\nIn [169]: arr = randn(5, 3)\\nIn [170]: arr\\nOut[170]: \\narray([[-0.7139, -1.6331, -0.4959],\\n       [ 0.8236, -1.3132, -0.1935],\\n       [-1.6748,  3.0336, -0.863 ],\\n       [-0.3161,  0.5362, -2.468 ],\\n       [ 0.9058,  1.1184, -1.0516]])\\nIn [171]: arr.sort(1)\\nIn [172]: arr\\nOut[172]: \\narray([[-1.6331, -0.7139, -0.4959],\\n       [-1.3132, -0.1935,  0.8236],\\n       [-1.6748, -0.863 ,  3.0336],\\n       [-2.468 , -0.3161,  0.5362],\\n       [-1.0516,  0.9058,  1.1184]])\\nThe top level method np.sort returns a sorted copy of an array instead of modifying\\nthe array in place. A quick-and-dirty way to compute the quantiles of an array is to sort\\nit and select the value at a particular rank:\\nIn [173]: large_arr = randn(1000)\\nIn [174]: large_arr.sort()\\nIn [175]: large_arr[int(0.05 * len(large_arr))] # 5% quantile\\nOut[175]: -1.5791023260896004\\nFor more details on using NumPy’s sorting methods, and more advanced techniques\\nlike indirect sorts, see Chapter 12. Several other kinds of data manipulations related to\\nsorting (for example, sorting a table of data by one or more columns) are also to be\\nfound in pandas.\\nUnique and Other Set Logic\\nNumPy has some basic set operations for one-dimensional ndarrays. Probably the most\\ncommonly used one is np.unique, which returns the sorted unique values in an array:\\nIn [176]: names = np.array(['Bob', 'Joe', 'Will', 'Bob', 'Will', 'Joe', 'Joe'])\\nIn [177]: np.unique(names)\\nOut[177]: \\n102 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"array(['Bob', 'Joe', 'Will'], \\n      dtype='|S4')\\nIn [178]: ints = np.array([3, 3, 3, 2, 2, 1, 1, 4, 4])\\nIn [179]: np.unique(ints)\\nOut[179]: array([1, 2, 3, 4])\\nContrast np.unique with the pure Python alternative:\\nIn [180]: sorted(set(names))\\nOut[180]: ['Bob', 'Joe', 'Will']\\nAnother function, np.in1d, tests membership of the values in one array in another,\\nreturning a boolean array:\\nIn [181]: values = np.array([6, 0, 0, 3, 2, 5, 6])\\nIn [182]: np.in1d(values, [2, 3, 6])\\nOut[182]: array([ True, False, False,  True,  True, False,  True], dtype=bool)\\nSee Table 4-6 for a listing of set functions in NumPy.\\nTable 4-6. Array set operations\\nMethod Description\\nunique(x) Compute the sorted, unique elements in x\\nintersect1d(x, y) Compute the sorted, common elements in x and y\\nunion1d(x, y) Compute the sorted union of elements\\nin1d(x, y) Compute a boolean array indicating whether each element of x is contained in y\\nsetdiff1d(x, y) Set difference, elements in x that are not in y\\nsetxor1d(x, y) Set symmetric differences; elements that are in either of the arrays, but not both\\nFile Input and Output with Arrays\\nNumPy is able to save and load data to and from disk either in text or binary format.\\nIn later chapters you will learn about tools in pandas for reading tabular data into\\nmemory.\\nStoring Arrays on Disk in Binary Format\\nnp.save and np.load are the two workhorse functions for efficiently saving and loading\\narray data on disk. Arrays are saved by default in an uncompressed raw binary format\\nwith file extension .npy.\\nIn [183]: arr = np.arange(10)\\nIn [184]: np.save('some_array', arr)\\nFile Input and Output with Arrays | 103\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"If the file path does not already end in .npy, the extension will be appended. The array\\non disk can then be loaded using np.load:\\nIn [185]: np.load('some_array.npy')\\nOut[185]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\\nYou save multiple arrays in a zip archive using np.savez and passing the arrays as key-\\nword arguments:\\nIn [186]: np.savez('array_archive.npz', a=arr, b=arr)\\nWhen loading an .npz file, you get back a dict-like object which loads the individual\\narrays lazily:\\nIn [187]: arch = np.load('array_archive.npz')\\nIn [188]: arch['b']\\nOut[188]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\\nSaving and Loading Text Files\\nLoading text from files is a fairly standard task. The landscape of file reading and writing\\nfunctions in Python can be a bit confusing for a newcomer, so I will focus mainly on\\nthe read_csv and read_table functions in pandas. It will at times be useful to load data\\ninto vanilla NumPy arrays using np.loadtxt or the more specialized np.genfromtxt.\\nThese functions have many options allowing you to specify different delimiters, con-\\nverter functions for certain columns, skipping rows, and other things. Take a simple\\ncase of a comma-separated file (CSV) like this:\\nIn [191]: !cat array_ex.txt\\n0.580052,0.186730,1.040717,1.134411\\n0.194163,-0.636917,-0.938659,0.124094\\n-0.126410,0.268607,-0.695724,0.047428\\n-1.484413,0.004176,-0.744203,0.005487\\n2.302869,0.200131,1.670238,-1.881090\\n-0.193230,1.047233,0.482803,0.960334\\nThis can be loaded into a 2D array like so:\\nIn [192]: arr = np.loadtxt('array_ex.txt', delimiter=',')\\nIn [193]: arr\\nOut[193]: \\narray([[ 0.5801,  0.1867,  1.0407,  1.1344],\\n       [ 0.1942, -0.6369, -0.9387,  0.1241],\\n       [-0.1264,  0.2686, -0.6957,  0.0474],\\n       [-1.4844,  0.0042, -0.7442,  0.0055],\\n       [ 2.3029,  0.2001,  1.6702, -1.8811],\\n       [-0.1932,  1.0472,  0.4828,  0.9603]])\\nnp.savetxt performs the inverse operation: writing an array to a delimited text file.\\ngenfromtxt is similar to loadtxt but is geared for structured arrays and missing data\\nhandling; see Chapter 12 for more on structured arrays.\\n104 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'For more on file reading and writing, especially tabular or spreadsheet-\\nlike data, see the later chapters involving pandas and DataFrame objects.\\nLinear Algebra\\nLinear algebra, like matrix multiplication, decompositions, determinants, and other\\nsquare matrix math, is an important part of any array library. Unlike some languages\\nlike MATLAB, multiplying two two-dimensional arrays with * is an element-wise\\nproduct instead of a matrix dot product. As such, there is a function dot, both an array\\nmethod, and a function in the numpy namespace, for matrix multiplication:\\nIn [194]: x = np.array([[1., 2., 3.], [4., 5., 6.]])\\nIn [195]: y = np.array([[6., 23.], [-1, 7], [8, 9]])\\nIn [196]: x                   In [197]: y          \\nOut[196]:                     Out[197]:            \\narray([[ 1.,  2.,  3.],       array([[  6.,  23.], \\n       [ 4.,  5.,  6.]])             [ -1.,   7.], \\n                                     [  8.,   9.]])\\n                                                   \\nIn [198]: x.dot(y)  # equivalently np.dot(x, y)\\nOut[198]: \\narray([[  28.,   64.],\\n       [  67.,  181.]])\\nA matrix product between a 2D array and a suitably sized 1D array results in a 1D array:\\nIn [199]: np.dot(x, np.ones(3))\\nOut[199]: array([  6.,  15.])\\nnumpy.linalg has a standard set of matrix decompositions and things like inverse and\\ndeterminant. These are implemented under the hood using the same industry-standard\\nFortran libraries used in other languages like MATLAB and R, such as like BLAS, LA-\\nPACK, or possibly (depending on your NumPy build) the Intel MKL:\\nIn [201]: from numpy.linalg import inv, qr\\nIn [202]: X = randn(5, 5)\\nIn [203]: mat = X.T.dot(X)\\nIn [204]: inv(mat)\\nOut[204]: \\narray([[ 3.0361, -0.1808, -0.6878, -2.8285, -1.1911],\\n       [-0.1808,  0.5035,  0.1215,  0.6702,  0.0956],\\n       [-0.6878,  0.1215,  0.2904,  0.8081,  0.3049],\\n       [-2.8285,  0.6702,  0.8081,  3.4152,  1.1557],\\n       [-1.1911,  0.0956,  0.3049,  1.1557,  0.6051]])\\nIn [205]: mat.dot(inv(mat))\\nLinear Algebra | 105\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Out[205]: \\narray([[ 1.,  0.,  0.,  0., -0.],\\n       [ 0.,  1., -0.,  0.,  0.],\\n       [ 0., -0.,  1.,  0.,  0.],\\n       [ 0., -0., -0.,  1., -0.],\\n       [ 0.,  0.,  0.,  0.,  1.]])\\nIn [206]: q, r = qr(mat)\\nIn [207]: r\\nOut[207]: \\narray([[ -6.9271,   7.389 ,   6.1227,  -7.1163,  -4.9215],\\n       [  0.    ,  -3.9735,  -0.8671,   2.9747,  -5.7402],\\n       [  0.    ,   0.    , -10.2681,   1.8909,   1.6079],\\n       [  0.    ,   0.    ,   0.    ,  -1.2996,   3.3577],\\n       [  0.    ,   0.    ,   0.    ,   0.    ,   0.5571]])\\nSee Table 4-7 for a list of some of the most commonly-used linear algebra functions.\\nThe scientific Python community is hopeful that there may be a matrix\\nmultiplication infix operator implemented someday, providing syntac-\\ntically nicer alternative to using np.dot. But for now this is the way.\\nTable 4-7. Commonly-used numpy.linalg functions\\nFunction Description\\ndiag Return the diagonal (or off-diagonal) elements of a square matrix as a 1D array, or convert a 1D array into a square\\nmatrix with zeros on the off-diagonal\\ndot Matrix multiplication\\ntrace Compute the sum of the diagonal elements\\ndet Compute the matrix determinant\\neig Compute the eigenvalues and eigenvectors of a square matrix\\ninv Compute the inverse of a square matrix\\npinv Compute the Moore-Penrose pseudo-inverse inverse of a square matrix\\nqr Compute the QR decomposition\\nsvd Compute the singular value decomposition (SVD)\\nsolve Solve the linear system Ax = b for x, where A is a square matrix\\nlstsq Compute the least-squares solution to y = Xb\\nRandom Number Generation\\nThe numpy.random module supplements the built-in Python random with functions for\\nefficiently generating whole arrays of sample values from many kinds of probability\\n106 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'distributions. For example, you can get a 4 by 4 array of samples from the standard\\nnormal distribution using normal:\\nIn [208]: samples = np.random.normal(size=(4, 4))\\nIn [209]: samples\\nOut[209]: \\narray([[ 0.1241,  0.3026,  0.5238,  0.0009],\\n       [ 1.3438, -0.7135, -0.8312, -2.3702],\\n       [-1.8608, -0.8608,  0.5601, -1.2659],\\n       [ 0.1198, -1.0635,  0.3329, -2.3594]])\\nPython’s built-in random module, by contrast, only samples one value at a time. As you\\ncan see from this benchmark, numpy.random is well over an order of magnitude faster\\nfor generating very large samples:\\nIn [210]: from random import normalvariate\\nIn [211]: N = 1000000\\nIn [212]: %timeit samples = [normalvariate(0, 1) for _ in xrange(N)]\\n1 loops, best of 3: 1.33 s per loop\\nIn [213]: %timeit np.random.normal(size=N)\\n10 loops, best of 3: 57.7 ms per loop\\nSee table Table 4-8 for a partial list of functions available in numpy.random. I’ll give some\\nexamples of leveraging these functions’ ability to generate large arrays of samples all at\\nonce in the next section.\\nTable 4-8. Partial list of numpy.random functions\\nFunction Description\\nseed Seed the random number generator\\npermutation Return a random permutation of a sequence, or return a permuted range\\nshuffle Randomly permute a sequence in place\\nrand Draw samples from a uniform distribution\\nrandint Draw random integers from a given low-to-high range\\nrandn Draw samples from a normal distribution with mean 0 and standard deviation 1 (MATLAB-like interface)\\nbinomial Draw samples a binomial distribution\\nnormal Draw samples from a normal (Gaussian) distribution\\nbeta Draw samples from a beta distribution\\nchisquare Draw samples from a chi-square distribution\\ngamma Draw samples from a gamma distribution\\nuniform Draw samples from a uniform [0, 1) distribution\\nRandom Number Generation | 107\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Example: Random Walks\\nAn illustrative application of utilizing array operations is in the simulation of random\\nwalks. Let’s first consider a simple random walk starting at 0 with steps of 1 and -1\\noccurring with equal probability. A pure Python way to implement a single random\\nwalk with 1,000 steps using the built-in random module:\\nimport random\\nposition = 0\\nwalk = [position]\\nsteps = 1000\\nfor i in xrange(steps):\\n    step = 1 if random.randint(0, 1) else -1\\n    position += step\\n    walk.append(position)\\nSee Figure 4-4 for an example plot of the first 100 values on one of these random walks.\\nFigure 4-4. A simple random walk\\nYou might make the observation that walk is simply the cumulative sum of the random\\nsteps and could be evaluated as an array expression. Thus, I use the np.random module\\nto draw 1,000 coin flips at once, set these to 1 and -1, and compute the cumulative sum:\\nIn [215]: nsteps = 1000\\nIn [216]: draws = np.random.randint(0, 2, size=nsteps)\\nIn [217]: steps = np.where(draws > 0, 1, -1)\\nIn [218]: walk = steps.cumsum()\\n108 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'From this we can begin to extract statistics like the minimum and maximum value along\\nthe walk’s trajectory:\\nIn [219]: walk.min()        In [220]: walk.max()\\nOut[219]: -3                Out[220]: 31\\nA more complicated statistic is the first crossing time , the step at which the random\\nwalk reaches a particular value. Here we might want to know how long it took the\\nrandom walk to get at least 10 steps away from the origin 0 in either direction.\\nnp.abs(walk) >= 10 gives us a boolean array indicating where the walk has reached or\\nexceeded 10, but we want the index of the first 10 or -10. Turns out this can be com-\\nputed using argmax, which returns the first index of the maximum value in the boolean\\narray (True is the maximum value):\\nIn [221]: (np.abs(walk) >= 10).argmax()\\nOut[221]: 37\\nNote that using argmax here is not always efficient because it always makes a full scan\\nof the array. In this special case once a True is observed we know it to be the maximum\\nvalue.\\nSimulating Many Random Walks at Once\\nIf your goal was to simulate many random walks, say 5,000 of them, you can generate\\nall of the random walks with minor modifications to the above code. The numpy.ran\\ndom functions if passed a 2-tuple will generate a 2D array of draws, and we can compute\\nthe cumulative sum across the rows to compute all 5,000 random walks in one shot:\\nIn [222]: nwalks = 5000\\nIn [223]: nsteps = 1000\\nIn [224]: draws = np.random.randint(0, 2, size=(nwalks, nsteps)) # 0 or 1\\nIn [225]: steps = np.where(draws > 0, 1, -1)\\nIn [226]: walks = steps.cumsum(1)\\nIn [227]: walks\\nOut[227]: \\narray([[  1,   0,   1, ...,   8,   7,   8],\\n       [  1,   0,  -1, ...,  34,  33,  32],\\n       [  1,   0,  -1, ...,   4,   5,   4],\\n       ..., \\n       [  1,   2,   1, ...,  24,  25,  26],\\n       [  1,   2,   3, ...,  14,  13,  14],\\n       [ -1,  -2,  -3, ..., -24, -23, -22]])\\nNow, we can compute the maximum and minimum values obtained over all of the\\nwalks:\\nIn [228]: walks.max()        In [229]: walks.min()\\nOut[228]: 138                Out[229]: -133\\nExample: Random Walks | 109\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Out of these walks, let’s compute the minimum crossing time to 30 or -30. This is\\nslightly tricky because not all 5,000 of them reach 30. We can check this using the \\nany method:\\nIn [230]: hits30 = (np.abs(walks) >= 30).any(1)\\nIn [231]: hits30\\nOut[231]: array([False, True, False, ..., False, True, False], dtype=bool)\\nIn [232]: hits30.sum() # Number that hit 30 or -30\\nOut[232]: 3410\\nWe can use this boolean array to select out the rows of walks that actually cross the\\nabsolute 30 level and call argmax across axis 1 to get the crossing times:\\nIn [233]: crossing_times = (np.abs(walks[hits30]) >= 30).argmax(1)\\nIn [234]: crossing_times.mean()\\nOut[234]: 498.88973607038122\\nFeel free to experiment with other distributions for the steps other than equal sized\\ncoin flips. You need only use a different random number generation function, like \\nnormal to generate normally distributed steps with some mean and standard deviation:\\nIn [235]: steps = np.random.normal(loc=0, scale=0.25,\\n   .....:                          size=(nwalks, nsteps))\\n110 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'CHAPTER 5\\nGetting Started with pandas\\npandas will be the primary library of interest throughout much of the rest of the book.\\nIt contains high-level data structures and manipulation tools designed to make data\\nanalysis fast and easy in Python. pandas is built on top of NumPy and makes it easy to\\nuse in NumPy-centric applications.\\nAs a bit of background, I started building pandas in early 2008 during my tenure at\\nAQR, a quantitative investment management firm. At the time, I had a distinct set of\\nrequirements that were not well-addressed by any single tool at my disposal:\\n• Data structures with labeled axes supporting automatic or explicit data alignment.\\nThis prevents common errors resulting from misaligned data and working with\\ndifferently-indexed data coming from different sources.\\n• Integrated time series functionality.\\n• The same data structures handle both time series data and non-time series data.\\n• Arithmetic operations and reductions (like summing across an axis) would pass\\non the metadata (axis labels).\\n• Flexible handling of missing data.\\n• Merge and other relational operations found in popular database databases (SQL-\\nbased, for example).\\nI wanted to be able to do all of these things in one place, preferably in a language well-\\nsuited to general purpose software development. Python was a good candidate lan-\\nguage for this, but at that time there was not an integrated set of data structures and\\ntools providing this functionality.\\nOver the last four years, pandas has matured into a quite large library capable of solving\\na much broader set of data handling problems than I ever anticipated, but it has ex-\\npanded in its scope without compromising the simplicity and ease-of-use that I desired\\nfrom the very beginning. I hope that after reading this book, you will find it to be just\\nas much of an indispensable tool as I do.\\nThroughout the rest of the book, I use the following import conventions for pandas:\\n111\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [1]: from pandas import Series, DataFrame\\nIn [2]: import pandas as pd\\nThus, whenever you see pd. in code, it’s referring to pandas. Series and DataFrame are\\nused so much that I find it easier to import them into the local namespace.\\nIntroduction to pandas Data Structures\\nTo get started with pandas, you will need to get comfortable with its two workhorse\\ndata structures: Series and DataFrame. While they are not a universal solution for every\\nproblem, they provide a solid, easy-to-use basis for most applications.\\nSeries\\nA Series is a one-dimensional array-like object containing an array of data (of any\\nNumPy data type) and an associated array of data labels, called its index. The simplest\\nSeries is formed from only an array of data:\\nIn [4]: obj = Series([4, 7, -5, 3])\\nIn [5]: obj\\nOut[5]: \\n0    4\\n1    7\\n2   -5\\n3    3\\nThe string representation of a Series displayed interactively shows the index on the left\\nand the values on the right. Since we did not specify an index for the data, a default\\none consisting of the integers 0 through N - 1 (where N is the length of the data) is\\ncreated. You can get the array representation and index object of the Series via its values\\nand index attributes, respectively:\\nIn [6]: obj.values\\nOut[6]: array([ 4,  7, -5,  3])\\nIn [7]: obj.index\\nOut[7]: Int64Index([0, 1, 2, 3])\\nOften it will be desirable to create a Series with an index identifying each data point:\\nIn [8]: obj2 = Series([4, 7, -5, 3], index=['d', 'b', 'a', 'c'])\\nIn [9]: obj2\\nOut[9]: \\nd    4\\nb    7\\na   -5\\nc    3\\n112 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [10]: obj2.index\\nOut[10]: Index([d, b, a, c], dtype=object)\\nCompared with a regular NumPy array, you can use values in the index when selecting\\nsingle values or a set of values:\\nIn [11]: obj2['a']\\nOut[11]: -5\\nIn [12]: obj2['d'] = 6\\nIn [13]: obj2[['c', 'a', 'd']]\\nOut[13]: \\nc    3\\na   -5\\nd    6\\nNumPy array operations, such as filtering with a boolean array, scalar multiplication,\\nor applying math functions, will preserve the index-value link:\\nIn [14]: obj2\\nOut[14]: \\nd    6\\nb    7\\na   -5\\nc    3\\nIn [15]: obj2[obj2 > 0]      In [16]: obj2 * 2      In [17]: np.exp(obj2)\\nOut[15]:                     Out[16]:               Out[17]:             \\nd    6                       d    12                d     403.428793     \\nb    7                       b    14                b    1096.633158     \\nc    3                       a   -10                a       0.006738     \\n                             c     6                c      20.085537\\nAnother way to think about a Series is as a fixed-length, ordered dict, as it is a mapping\\nof index values to data values. It can be substituted into many functions that expect a\\ndict:\\nIn [18]: 'b' in obj2\\nOut[18]: True\\nIn [19]: 'e' in obj2\\nOut[19]: False\\nShould you have data contained in a Python dict, you can create a Series from it by\\npassing the dict:\\nIn [20]: sdata = {'Ohio': 35000, 'Texas': 71000, 'Oregon': 16000, 'Utah': 5000}\\nIn [21]: obj3 = Series(sdata)\\nIn [22]: obj3\\nOut[22]: \\nOhio      35000\\nOregon    16000\\nIntroduction to pandas Data Structures | 113\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Texas     71000\\nUtah       5000\\nWhen only passing a dict, the index in the resulting Series will have the dict’s keys in\\nsorted order.\\nIn [23]: states = ['California', 'Ohio', 'Oregon', 'Texas']\\nIn [24]: obj4 = Series(sdata, index=states)\\nIn [25]: obj4\\nOut[25]: \\nCalifornia      NaN\\nOhio          35000\\nOregon        16000\\nTexas         71000\\nIn this case, 3 values found in sdata were placed in the appropriate locations, but since\\nno value for 'California' was found, it appears as NaN (not a number) which is con-\\nsidered in pandas to mark missing or NA values. I will use the terms “missing” or “NA”\\nto refer to missing data. The isnull and notnull functions in pandas should be used to\\ndetect missing data:\\nIn [26]: pd.isnull(obj4)      In [27]: pd.notnull(obj4)\\nOut[26]:                      Out[27]:                 \\nCalifornia     True           California    False      \\nOhio          False           Ohio           True      \\nOregon        False           Oregon         True      \\nTexas         False           Texas          True\\nSeries also has these as instance methods:\\nIn [28]: obj4.isnull()\\nOut[28]: \\nCalifornia     True\\nOhio          False\\nOregon        False\\nTexas         False\\nI discuss working with missing data in more detail later in this chapter.\\nA critical Series feature for many applications is that it automatically aligns differently-\\nindexed data in arithmetic operations:\\nIn [29]: obj3          In [30]: obj4      \\nOut[29]:               Out[30]:           \\nOhio      35000        California      NaN\\nOregon    16000        Ohio          35000\\nTexas     71000        Oregon        16000\\nUtah       5000        Texas         71000\\n                                          \\nIn [31]: obj3 + obj4\\nOut[31]: \\nCalifornia       NaN\\nOhio           70000\\nOregon         32000\\n114 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Texas         142000\\nUtah             NaN\\nData alignment features are addressed as a separate topic.\\nBoth the Series object itself and its index have a name attribute, which integrates with\\nother key areas of pandas functionality:\\nIn [32]: obj4.name = 'population'\\nIn [33]: obj4.index.name = 'state'\\nIn [34]: obj4\\nOut[34]: \\nstate\\nCalifornia      NaN\\nOhio          35000\\nOregon        16000\\nTexas         71000\\nName: population\\nA Series’s index can be altered in place by assignment:\\nIn [35]: obj.index = ['Bob', 'Steve', 'Jeff', 'Ryan']\\nIn [36]: obj\\nOut[36]: \\nBob      4\\nSteve    7\\nJeff    -5\\nRyan     3\\nDataFrame\\nA DataFrame represents a tabular, spreadsheet-like data structure containing an or-\\ndered collection of columns, each of which can be a different value type (numeric,\\nstring, boolean, etc.). The DataFrame has both a row and column index; it can be\\nthought of as a dict of Series (one for all sharing the same index). Compared with other\\nsuch DataFrame-like structures you may have used before (like R’s data.frame), row-\\noriented and column-oriented operations in DataFrame are treated roughly symmet-\\nrically. Under the hood, the data is stored as one or more two-dimensional blocks rather\\nthan a list, dict, or some other collection of one-dimensional arrays. The exact details\\nof DataFrame’s internals are far outside the scope of this book.\\nWhile DataFrame stores the data internally in a two-dimensional for-\\nmat, you can easily represent much higher-dimensional data in a tabular\\nformat using hierarchical indexing, a subject of a later section and a key\\ningredient in many of the more advanced data-handling features in pan-\\ndas.\\nIntroduction to pandas Data Structures | 115\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"There are numerous ways to construct a DataFrame, though one of the most common\\nis from a dict of equal-length lists or NumPy arrays\\ndata = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'],\\n        'year': [2000, 2001, 2002, 2001, 2002],\\n        'pop': [1.5, 1.7, 3.6, 2.4, 2.9]}\\nframe = DataFrame(data)\\nThe resulting DataFrame will have its index assigned automatically as with Series, and\\nthe columns are placed in sorted order:\\nIn [38]: frame\\nOut[38]: \\n   pop   state  year\\n0  1.5    Ohio  2000\\n1  1.7    Ohio  2001\\n2  3.6    Ohio  2002\\n3  2.4  Nevada  2001\\n4  2.9  Nevada  2002\\nIf you specify a sequence of columns, the DataFrame’s columns will be exactly what\\nyou pass:\\nIn [39]: DataFrame(data, columns=['year', 'state', 'pop'])\\nOut[39]: \\n   year   state  pop\\n0  2000    Ohio  1.5\\n1  2001    Ohio  1.7\\n2  2002    Ohio  3.6\\n3  2001  Nevada  2.4\\n4  2002  Nevada  2.9\\nAs with Series, if you pass a column that isn’t contained in data, it will appear with NA\\nvalues in the result:\\nIn [40]: frame2 = DataFrame(data, columns=['year', 'state', 'pop', 'debt'],\\n   ....:                    index=['one', 'two', 'three', 'four', 'five'])\\nIn [41]: frame2\\nOut[41]: \\n       year   state  pop  debt\\none    2000    Ohio  1.5   NaN\\ntwo    2001    Ohio  1.7   NaN\\nthree  2002    Ohio  3.6   NaN\\nfour   2001  Nevada  2.4   NaN\\nfive   2002  Nevada  2.9   NaN\\nIn [42]: frame2.columns\\nOut[42]: Index([year, state, pop, debt], dtype=object)\\nA column in a DataFrame can be retrieved as a Series either by dict-like notation or by\\nattribute:\\nIn [43]: frame2['state']        In [44]: frame2.year\\nOut[43]:                        Out[44]:            \\none        Ohio                 one      2000       \\n116 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"two        Ohio                 two      2001       \\nthree      Ohio                 three    2002       \\nfour     Nevada                 four     2001       \\nfive     Nevada                 five     2002       \\nName: state                     Name: year\\nNote that the returned Series have the same index as the DataFrame, and their name\\nattribute has been appropriately set.\\nRows can also be retrieved by position or name by a couple of methods, such as the\\nix indexing field (much more on this later):\\nIn [45]: frame2.ix['three']\\nOut[45]: \\nyear     2002\\nstate    Ohio\\npop       3.6\\ndebt      NaN\\nName: three\\nColumns can be modified by assignment. For example, the empty 'debt' column could\\nbe assigned a scalar value or an array of values:\\nIn [46]: frame2['debt'] = 16.5\\nIn [47]: frame2\\nOut[47]: \\n       year   state  pop  debt\\none    2000    Ohio  1.5  16.5\\ntwo    2001    Ohio  1.7  16.5\\nthree  2002    Ohio  3.6  16.5\\nfour   2001  Nevada  2.4  16.5\\nfive   2002  Nevada  2.9  16.5\\nIn [48]: frame2['debt'] = np.arange(5.)\\nIn [49]: frame2\\nOut[49]: \\n       year   state  pop  debt\\none    2000    Ohio  1.5     0\\ntwo    2001    Ohio  1.7     1\\nthree  2002    Ohio  3.6     2\\nfour   2001  Nevada  2.4     3\\nfive   2002  Nevada  2.9     4\\nWhen assigning lists or arrays to a column, the value’s length must match the length\\nof the DataFrame. If you assign a Series, it will be instead conformed exactly to the\\nDataFrame’s index, inserting missing values in any holes:\\nIn [50]: val = Series([-1.2, -1.5, -1.7], index=['two', 'four', 'five'])\\nIn [51]: frame2['debt'] = val\\nIn [52]: frame2\\nOut[52]: \\n       year   state  pop  debt\\nIntroduction to pandas Data Structures | 117\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"one    2000    Ohio  1.5   NaN\\ntwo    2001    Ohio  1.7  -1.2\\nthree  2002    Ohio  3.6   NaN\\nfour   2001  Nevada  2.4  -1.5\\nfive   2002  Nevada  2.9  -1.7\\nAssigning a column that doesn’t exist will create a new column. The del keyword will\\ndelete columns as with a dict:\\nIn [53]: frame2['eastern'] = frame2.state == 'Ohio'\\nIn [54]: frame2\\nOut[54]: \\n       year   state  pop  debt eastern\\none    2000    Ohio  1.5   NaN    True\\ntwo    2001    Ohio  1.7  -1.2    True\\nthree  2002    Ohio  3.6   NaN    True\\nfour   2001  Nevada  2.4  -1.5   False\\nfive   2002  Nevada  2.9  -1.7   False\\nIn [55]: del frame2['eastern']\\nIn [56]: frame2.columns\\nOut[56]: Index([year, state, pop, debt], dtype=object)\\nThe column returned when indexing a DataFrame is a view on the un-\\nderlying data, not a copy. Thus, any in-place modifications to the Series\\nwill be reflected in the DataFrame. The column can be explicitly copied\\nusing the Series’s copy method.\\nAnother common form of data is a nested dict of dicts format:\\nIn [57]: pop = {'Nevada': {2001: 2.4, 2002: 2.9},\\n   ....:        'Ohio': {2000: 1.5, 2001: 1.7, 2002: 3.6}}\\nIf passed to DataFrame, it will interpret the outer dict keys as the columns and the inner\\nkeys as the row indices:\\nIn [58]: frame3 = DataFrame(pop)\\nIn [59]: frame3\\nOut[59]: \\n      Nevada  Ohio\\n2000     NaN   1.5\\n2001     2.4   1.7\\n2002     2.9   3.6\\nOf course you can always transpose the result:\\nIn [60]: frame3.T\\nOut[60]: \\n        2000  2001  2002\\nNevada   NaN   2.4   2.9\\nOhio     1.5   1.7   3.6\\n118 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"The keys in the inner dicts are unioned and sorted to form the index in the result. This\\nisn’t true if an explicit index is specified:\\nIn [61]: DataFrame(pop, index=[2001, 2002, 2003])\\nOut[61]: \\n      Nevada  Ohio\\n2001     2.4   1.7\\n2002     2.9   3.6\\n2003     NaN   NaN\\nDicts of Series are treated much in the same way:\\nIn [62]: pdata = {'Ohio': frame3['Ohio'][:-1],\\n   ....:          'Nevada': frame3['Nevada'][:2]}\\nIn [63]: DataFrame(pdata)\\nOut[63]: \\n      Nevada  Ohio\\n2000     NaN   1.5\\n2001     2.4   1.7\\nFor a complete list of things you can pass the DataFrame constructor, see Table 5-1.\\nIf a DataFrame’s index and columns have their name attributes set, these will also be\\ndisplayed:\\nIn [64]: frame3.index.name = 'year'; frame3.columns.name = 'state'\\nIn [65]: frame3\\nOut[65]: \\nstate  Nevada  Ohio\\nyear               \\n2000      NaN   1.5\\n2001      2.4   1.7\\n2002      2.9   3.6\\nLike Series, the values attribute returns the data contained in the DataFrame as a 2D\\nndarray:\\nIn [66]: frame3.values\\nOut[66]: \\narray([[ nan,  1.5],\\n       [ 2.4,  1.7],\\n       [ 2.9,  3.6]])\\nIf the DataFrame’s columns are different dtypes, the dtype of the values array will be\\nchosen to accomodate all of the columns:\\nIn [67]: frame2.values\\nOut[67]: \\narray([[2000, Ohio, 1.5, nan],\\n       [2001, Ohio, 1.7, -1.2],\\n       [2002, Ohio, 3.6, nan],\\n       [2001, Nevada, 2.4, -1.5],\\n       [2002, Nevada, 2.9, -1.7]], dtype=object)\\nIntroduction to pandas Data Structures | 119\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Table 5-1. Possible data inputs to DataFrame constructor\\nType Notes\\n2D ndarray A matrix of data, passing optional row and column labels\\ndict of arrays, lists, or tuples Each sequence becomes a column in the DataFrame. All sequences must be the same length.\\nNumPy structured/record array Treated as the “dict of arrays” case\\ndict of Series Each value becomes a column. Indexes from each Series are unioned together to form the\\nresult’s row index if no explicit index is passed.\\ndict of dicts Each inner dict becomes a column. Keys are unioned to form the row index as in the “dict of\\nSeries” case.\\nlist of dicts or Series Each item becomes a row in the DataFrame. Union of dict keys or Series indexes become the\\nDataFrame’s column labels\\nList of lists or tuples Treated as the “2D ndarray” case\\nAnother DataFrame The DataFrame’s indexes are used unless different ones are passed\\nNumPy MaskedArray\\nLike the “2D ndarray” case except masked values become NA/missing in the DataFrame result\\nIndex Objects\\npandas’s \\nIndex objects are responsible for holding the axis labels and other metadata\\n(like the axis name or names). Any array or other sequence of labels used when con-\\nstructing a Series or DataFrame is internally converted to an Index:\\nIn [68]: obj = Series(range(3), index=[\\'a\\', \\'b\\', \\'c\\'])\\nIn [69]: index = obj.index\\nIn [70]: index\\nOut[70]: Index([a, b, c], dtype=object)\\nIn [71]: index[1:]\\nOut[71]: Index([b, c], dtype=object)\\nIndex objects are immutable and thus can’t be modified by the user:\\nIn [72]: index[1] = \\'d\\'\\n---------------------------------------------------------------------------\\nException                                 Traceback (most recent call last)\\n<ipython-input-72-676fdeb26a68> in <module>()\\n----> 1 index[1] = \\'d\\'\\n/Users/wesm/code/pandas/pandas/core/index.pyc in __setitem__(self, key, value)\\n    302     def __setitem__(self, key, value):\\n    303         \"\"\"Disable the setting of values.\"\"\"\\n--> 304         raise Exception(str(self.__class__) + \\' object is immutable\\')\\n    305 \\n    306     def __getitem__(self, key):\\nException: <class \\'pandas.core.index.Index\\'> object is immutable\\n120 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Immutability is important so that Index objects can be safely shared among data\\nstructures:\\nIn [73]: index = pd.Index(np.arange(3))\\nIn [74]: obj2 = Series([1.5, -2.5, 0], index=index)\\nIn [75]: obj2.index is index\\nOut[75]: True\\nTable 5-2 has a list of built-in Index classes in the library. With some development\\neffort, Index can even be subclassed to implement specialized axis indexing function-\\nality.\\nMany users will not need to know much about Index objects, but they’re\\nnonetheless an important part of pandas’s data model.\\nTable 5-2. Main Index objects in pandas\\nClass Description\\nIndex The most general Index object, representing axis labels in a NumPy array of Python objects.\\nInt64Index Specialized Index for integer values.\\nMultiIndex “Hierarchical” index object representing multiple levels of indexing on a single axis. Can be thought of\\nas similar to an array of tuples.\\nDatetimeIndex Stores nanosecond timestamps (represented using NumPy’s datetime64 dtype).\\nPeriodIndex Specialized Index for Period data (timespans).\\nIn addition to being array-like, an Index also functions as a fixed-size set:\\nIn [76]: frame3\\nOut[76]: \\nstate  Nevada  Ohio\\nyear               \\n2000      NaN   1.5\\n2001      2.4   1.7\\n2002      2.9   3.6\\nIn [77]: 'Ohio' in frame3.columns\\nOut[77]: True\\nIn [78]: 2003 in frame3.index\\nOut[78]: False\\nEach Index has a number of methods and properties for set logic and answering other\\ncommon questions about the data it contains. These are summarized in Table 5-3.\\nIntroduction to pandas Data Structures | 121\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Table 5-3. Index methods and properties\\nMethod Description\\nappend Concatenate with additional Index objects, producing a new Index\\ndiff Compute set difference as an Index\\nintersection Compute set intersection\\nunion Compute set union\\nisin Compute boolean array indicating whether each value is contained in the passed collection\\ndelete Compute new Index with element at index i deleted\\ndrop Compute new index by deleting passed values\\ninsert Compute new Index by inserting element at index i\\nis_monotonic Returns True if each element is greater than or equal to the previous element\\nis_unique Returns True if the Index has no duplicate values\\nunique Compute the array of unique values in the Index\\nEssential Functionality\\nIn this section, I’ll walk you through the fundamental mechanics of interacting with\\nthe data contained in a Series or DataFrame. Upcoming chapters will delve more deeply\\ninto data analysis and manipulation topics using pandas. This book is not intended to\\nserve as exhaustive documentation for the pandas library; I instead focus on the most\\nimportant features, leaving the less common (that is, more esoteric) things for you to\\nexplore on your own.\\nReindexing\\nA critical method on pandas objects is reindex, which means to create a new object\\nwith the data conformed to a new index. Consider a simple example from above:\\nIn [79]: obj = Series([4.5, 7.2, -5.3, 3.6], index=['d', 'b', 'a', 'c'])\\nIn [80]: obj\\nOut[80]: \\nd    4.5\\nb    7.2\\na   -5.3\\nc    3.6\\nCalling reindex on this Series rearranges the data according to the new index, intro-\\nducing missing values if any index values were not already present:\\nIn [81]: obj2 = obj.reindex(['a', 'b', 'c', 'd', 'e'])\\nIn [82]: obj2\\nOut[82]: \\na   -5.3\\n122 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"b    7.2\\nc    3.6\\nd    4.5\\ne    NaN\\nIn [83]: obj.reindex(['a', 'b', 'c', 'd', 'e'], fill_value=0)\\nOut[83]: \\na   -5.3\\nb    7.2\\nc    3.6\\nd    4.5\\ne    0.0\\nFor ordered data like time series, it may be desirable to do some interpolation or filling\\nof values when reindexing. The method option allows us to do this, using a method such\\nas ffill which forward fills the values:\\nIn [84]: obj3 = Series(['blue', 'purple', 'yellow'], index=[0, 2, 4])\\nIn [85]: obj3.reindex(range(6), method='ffill')\\nOut[85]: \\n0      blue\\n1      blue\\n2    purple\\n3    purple\\n4    yellow\\n5    yellow\\nTable 5-4 lists available method options. At this time, interpolation more sophisticated\\nthan forward- and backfilling would need to be applied after the fact.\\nTable 5-4. reindex method (interpolation) options\\nArgument Description\\nffill or pad Fill (or carry) values forward\\nbfill or backfill Fill (or carry) values backward\\nWith DataFrame, reindex can alter either the (row) index, columns, or both. When\\npassed just a sequence, the rows are reindexed in the result:\\nIn [86]: frame = DataFrame(np.arange(9).reshape((3, 3)), index=['a', 'c', 'd'],\\n   ....:                   columns=['Ohio', 'Texas', 'California'])\\nIn [87]: frame\\nOut[87]: \\n   Ohio  Texas  California\\na     0      1           2\\nc     3      4           5\\nd     6      7           8\\nIn [88]: frame2 = frame.reindex(['a', 'b', 'c', 'd'])\\nIn [89]: frame2\\nOut[89]: \\nEssential Functionality | 123\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Ohio  Texas  California\\na     0      1           2\\nb   NaN    NaN         NaN\\nc     3      4           5\\nd     6      7           8\\nThe columns can be reindexed using the columns keyword:\\nIn [90]: states = ['Texas', 'Utah', 'California']\\nIn [91]: frame.reindex(columns=states)\\nOut[91]: \\n   Texas  Utah  California\\na      1   NaN           2\\nc      4   NaN           5\\nd      7   NaN           8\\nBoth can be reindexed in one shot, though interpolation will only apply row-wise (axis\\n0):\\nIn [92]: frame.reindex(index=['a', 'b', 'c', 'd'], method='ffill',\\n   ....:               columns=states)\\nOut[92]: \\n   Texas  Utah  California\\na      1   NaN           2\\nb      1   NaN           2\\nc      4   NaN           5\\nd      7   NaN           8\\nAs you’ll see soon, reindexing can be done more succinctly by label-indexing with ix:\\nIn [93]: frame.ix[['a', 'b', 'c', 'd'], states]\\nOut[93]: \\n   Texas  Utah  California\\na      1   NaN           2\\nb    NaN   NaN         NaN\\nc      4   NaN           5\\nd      7   NaN           8\\nTable 5-5. reindex function arguments\\nArgument Description\\nindex New sequence to use as index. Can be Index instance or any other sequence-like Python data structure. An\\nIndex will be used exactly as is without any copying\\nmethod Interpolation (fill) method, see Table 5-4 for options.\\nfill_value Substitute value to use when introducing missing data by reindexing\\nlimit When forward- or backfilling, maximum size gap to fill\\nlevel Match simple Index on level of MultiIndex, otherwise select subset of\\ncopy Do not copy underlying data if new index is equivalent to old index. True by default (i.e. always copy data).\\n124 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Dropping entries from an axis\\nDropping one or more entries from an axis is easy if you have an index array or list\\nwithout those entries. As that can require a bit of munging and set logic, the drop\\nmethod will return a new object with the indicated value or values deleted from an axis:\\nIn [94]: obj = Series(np.arange(5.), index=['a', 'b', 'c', 'd', 'e'])\\nIn [95]: new_obj = obj.drop('c')\\nIn [96]: new_obj\\nOut[96]: \\na    0\\nb    1\\nd    3\\ne    4\\nIn [97]: obj.drop(['d', 'c'])\\nOut[97]: \\na    0\\nb    1\\ne    4\\nWith DataFrame, index values can be deleted from either axis:\\nIn [98]: data = DataFrame(np.arange(16).reshape((4, 4)),\\n   ....:                  index=['Ohio', 'Colorado', 'Utah', 'New York'],\\n   ....:                  columns=['one', 'two', 'three', 'four'])\\nIn [99]: data.drop(['Colorado', 'Ohio'])\\nOut[99]: \\n          one  two  three  four\\nUtah        8    9     10    11\\nNew York   12   13     14    15\\nIn [100]: data.drop('two', axis=1)      In [101]: data.drop(['two', 'four'], axis=1)\\nOut[100]:                               Out[101]:                                   \\n          one  three  four                        one  three                        \\nOhio        0      2     3              Ohio        0      2                        \\nColorado    4      6     7              Colorado    4      6                        \\nUtah        8     10    11              Utah        8     10                        \\nNew York   12     14    15              New York   12     14\\nIndexing, selection, and filtering\\nSeries indexing (obj[...]) works analogously to NumPy array indexing, except you can\\nuse the Series’s index values instead of only integers. Here are some examples this:\\nIn [102]: obj = Series(np.arange(4.), index=['a', 'b', 'c', 'd'])\\nIn [103]: obj['b']          In [104]: obj[1]\\nOut[103]: 1.0               Out[104]: 1.0   \\n                                            \\nIn [105]: obj[2:4]          In [106]: obj[['b', 'a', 'd']]\\nOut[105]:                   Out[106]:                     \\nEssential Functionality | 125\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"c    2                      b    1                        \\nd    3                      a    0                        \\n                            d    3                        \\n                                                          \\nIn [107]: obj[[1, 3]]       In [108]: obj[obj < 2]\\nOut[107]:                   Out[108]:             \\nb    1                      a    0                \\nd    3                      b    1\\nSlicing with labels behaves differently than normal Python slicing in that the endpoint\\nis inclusive:\\nIn [109]: obj['b':'c']\\nOut[109]: \\nb    1\\nc    2\\nSetting using these methods works just as you would expect:\\nIn [110]: obj['b':'c'] = 5\\nIn [111]: obj\\nOut[111]: \\na    0\\nb    5\\nc    5\\nd    3\\nAs you’ve seen above, indexing into a DataFrame is for retrieving one or more columns\\neither with a single value or sequence:\\nIn [112]: data = DataFrame(np.arange(16).reshape((4, 4)),\\n   .....:                  index=['Ohio', 'Colorado', 'Utah', 'New York'],\\n   .....:                  columns=['one', 'two', 'three', 'four'])\\nIn [113]: data\\nOut[113]: \\n          one  two  three  four\\nOhio        0    1      2     3\\nColorado    4    5      6     7\\nUtah        8    9     10    11\\nNew York   12   13     14    15\\nIn [114]: data['two']        In [115]: data[['three', 'one']]\\nOut[114]:                    Out[115]:                       \\nOhio         1                         three  one            \\nColorado     5               Ohio          2    0            \\nUtah         9               Colorado      6    4            \\nNew York    13               Utah         10    8            \\nName: two                    New York     14   12\\nIndexing like this has a few special cases. First selecting rows by slicing or a boolean\\narray:\\nIn [116]: data[:2]                     In [117]: data[data['three'] > 5]\\nOut[116]:                              Out[117]:                        \\n          one  two  three  four                  one  two  three  four  \\n126 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Ohio        0    1      2     3        Colorado    4    5      6     7  \\nColorado    4    5      6     7        Utah        8    9     10    11  \\n                                       New York   12   13     14    15\\nThis might seem inconsistent to some readers, but this syntax arose out of practicality\\nand nothing more. Another use case is in indexing with a boolean DataFrame, such as\\none produced by a scalar comparison:\\nIn [118]: data < 5\\nOut[118]: \\n            one    two  three   four\\nOhio       True   True   True   True\\nColorado   True  False  False  False\\nUtah      False  False  False  False\\nNew York  False  False  False  False\\nIn [119]: data[data < 5] = 0\\nIn [120]: data\\nOut[120]: \\n          one  two  three  four\\nOhio        0    0      0     0\\nColorado    0    5      6     7\\nUtah        8    9     10    11\\nNew York   12   13     14    15\\nThis is intended to make DataFrame syntactically more like an ndarray in this case.\\nFor DataFrame label-indexing on the rows, I introduce the special indexing field ix. It\\nenables you to select a subset of the rows and columns from a DataFrame with NumPy-\\nlike notation plus axis labels. As I mentioned earlier, this is also a less verbose way to\\ndo reindexing:\\nIn [121]: data.ix['Colorado', ['two', 'three']]\\nOut[121]: \\ntwo      5\\nthree    6\\nName: Colorado\\nIn [122]: data.ix[['Colorado', 'Utah'], [3, 0, 1]]\\nOut[122]: \\n          four  one  two\\nColorado     7    0    5\\nUtah        11    8    9\\nIn [123]: data.ix[2]        In [124]: data.ix[:'Utah', 'two']\\nOut[123]:                   Out[124]:                        \\none       8                 Ohio        0                    \\ntwo       9                 Colorado    5                    \\nthree    10                 Utah        9                    \\nfour     11                 Name: two                        \\nName: Utah                                                   \\n                                                             \\nIn [125]: data.ix[data.three > 5, :3]\\nOut[125]: \\nEssential Functionality | 127\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"one  two  three\\nColorado    0    5      6\\nUtah        8    9     10\\nNew York   12   13     14\\nSo there are many ways to select and rearrange the data contained in a pandas object.\\nFor DataFrame, there is a short summary of many of them in Table 5-6. You have a\\nnumber of additional options when working with hierarchical indexes as you’ll later\\nsee.\\nWhen designing pandas, I felt that having to type frame[:, col] to select\\na column was too verbose (and error-prone), since column selection is\\none of the most common operations. Thus I made the design trade-off\\nto push all of the rich label-indexing into ix.\\nTable 5-6. Indexing options with DataFrame\\nType Notes\\nobj[val] Select single column or sequence of columns from the DataFrame. Special case con-\\nveniences: boolean array (filter rows), slice (slice rows), or boolean DataFrame (set\\nvalues based on some criterion).\\nobj.ix[val] Selects single row of subset of rows from the DataFrame.\\nobj.ix[:, val] Selects single column of subset of columns.\\nobj.ix[val1, val2] Select both rows and columns.\\nreindex method Conform one or more axes to new indexes.\\nxs method Select single row or column as a Series by label.\\nicol, irow methods Select single column or row, respectively, as a Series by integer location.\\nget_value, set_value methods Select single value by row and column label.\\nArithmetic and data alignment\\nOne of the most important pandas features is the behavior of arithmetic between ob-\\njects with different indexes. When adding together objects, if any index pairs are not\\nthe same, the respective index in the result will be the union of the index pairs. Let’s\\nlook at a simple example:\\nIn [126]: s1 = Series([7.3, -2.5, 3.4, 1.5], index=['a', 'c', 'd', 'e'])\\nIn [127]: s2 = Series([-2.1, 3.6, -1.5, 4, 3.1], index=['a', 'c', 'e', 'f', 'g'])\\nIn [128]: s1        In [129]: s2\\nOut[128]:           Out[129]:   \\na    7.3            a   -2.1    \\nc   -2.5            c    3.6    \\nd    3.4            e   -1.5    \\n128 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"e    1.5            f    4.0    \\n                    g    3.1\\nAdding these together yields:\\nIn [130]: s1 + s2\\nOut[130]: \\na    5.2\\nc    1.1\\nd    NaN\\ne    0.0\\nf    NaN\\ng    NaN\\nThe internal data alignment introduces NA values in the indices that don’t overlap.\\nMissing values propagate in arithmetic computations.\\nIn the case of DataFrame, alignment is performed on both the rows and the columns:\\nIn [131]: df1 = DataFrame(np.arange(9.).reshape((3, 3)), columns=list('bcd'),\\n   .....:                 index=['Ohio', 'Texas', 'Colorado'])\\nIn [132]: df2 = DataFrame(np.arange(12.).reshape((4, 3)), columns=list('bde'),\\n   .....:                 index=['Utah', 'Ohio', 'Texas', 'Oregon'])\\nIn [133]: df1            In [134]: df2    \\nOut[133]:                Out[134]:        \\n          b  c  d                b   d   e\\nOhio      0  1  2        Utah    0   1   2\\nTexas     3  4  5        Ohio    3   4   5\\nColorado  6  7  8        Texas   6   7   8\\n                         Oregon  9  10  11\\nAdding these together returns a DataFrame whose index and columns are the unions\\nof the ones in each DataFrame:\\nIn [135]: df1 + df2\\nOut[135]: \\n           b   c   d   e\\nColorado NaN NaN NaN NaN\\nOhio       3 NaN   6 NaN\\nOregon   NaN NaN NaN NaN\\nTexas      9 NaN  12 NaN\\nUtah     NaN NaN NaN NaN\\nArithmetic methods with fill values\\nIn arithmetic operations between differently-indexed objects, you might want to fill\\nwith a special value, like 0, when an axis label is found in one object but not the other:\\nIn [136]: df1 = DataFrame(np.arange(12.).reshape((3, 4)), columns=list('abcd'))\\nIn [137]: df2 = DataFrame(np.arange(20.).reshape((4, 5)), columns=list('abcde'))\\nIn [138]: df1          In [139]: df2        \\nOut[138]:              Out[139]:            \\n   a  b   c   d            a   b   c   d   e\\nEssential Functionality | 129\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': '0  0  1   2   3        0   0   1   2   3   4\\n1  4  5   6   7        1   5   6   7   8   9\\n2  8  9  10  11        2  10  11  12  13  14\\n                       3  15  16  17  18  19\\nAdding these together results in NA values in the locations that don’t overlap:\\nIn [140]: df1 + df2\\nOut[140]: \\n    a   b   c   d   e\\n0   0   2   4   6 NaN\\n1   9  11  13  15 NaN\\n2  18  20  22  24 NaN\\n3 NaN NaN NaN NaN NaN\\nUsing the add method on df1, I pass df2 and an argument to fill_value:\\nIn [141]: df1.add(df2, fill_value=0)\\nOut[141]: \\n    a   b   c   d   e\\n0   0   2   4   6   4\\n1   9  11  13  15   9\\n2  18  20  22  24  14\\n3  15  16  17  18  19\\nRelatedly, when reindexing a Series or DataFrame, you can also specify a different fill\\nvalue:\\nIn [142]: df1.reindex(columns=df2.columns, fill_value=0)\\nOut[142]: \\n   a  b   c   d  e\\n0  0  1   2   3  0\\n1  4  5   6   7  0\\n2  8  9  10  11  0\\nTable 5-7. Flexible arithmetic methods\\nMethod Description\\nadd Method for addition (+)\\nsub Method for subtraction (-)\\ndiv Method for division (/)\\nmul Method for multiplication (*)\\nOperations between DataFrame and Series\\nAs with NumPy arrays, arithmetic between DataFrame and Series is well-defined. First,\\nas a motivating example, consider the difference between a 2D array and one of its rows:\\nIn [143]: arr = np.arange(12.).reshape((3, 4))\\nIn [144]: arr\\nOut[144]: \\narray([[  0.,   1.,   2.,   3.],\\n       [  4.,   5.,   6.,   7.],\\n130 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"[  8.,   9.,  10.,  11.]])\\nIn [145]: arr[0]\\nOut[145]: array([ 0.,  1.,  2.,  3.])\\nIn [146]: arr - arr[0]\\nOut[146]: \\narray([[ 0.,  0.,  0.,  0.],\\n       [ 4.,  4.,  4.,  4.],\\n       [ 8.,  8.,  8.,  8.]])\\nThis is referred to as broadcasting and is explained in more detail in Chapter 12. Op-\\nerations between a DataFrame and a Series are similar:\\nIn [147]: frame = DataFrame(np.arange(12.).reshape((4, 3)), columns=list('bde'),\\n   .....:                   index=['Utah', 'Ohio', 'Texas', 'Oregon'])\\nIn [148]: series = frame.ix[0]\\nIn [149]: frame          In [150]: series\\nOut[149]:                Out[150]:       \\n        b   d   e        b    0          \\nUtah    0   1   2        d    1          \\nOhio    3   4   5        e    2          \\nTexas   6   7   8        Name: Utah      \\nOregon  9  10  11\\nBy default, arithmetic between DataFrame and Series matches the index of the Series\\non the DataFrame's columns, broadcasting down the rows:\\nIn [151]: frame - series\\nOut[151]: \\n        b  d  e\\nUtah    0  0  0\\nOhio    3  3  3\\nTexas   6  6  6\\nOregon  9  9  9\\nIf an index value is not found in either the DataFrame’s columns or the Series’s index,\\nthe objects will be reindexed to form the union:\\nIn [152]: series2 = Series(range(3), index=['b', 'e', 'f'])\\nIn [153]: frame + series2\\nOut[153]: \\n        b   d   e   f\\nUtah    0 NaN   3 NaN\\nOhio    3 NaN   6 NaN\\nTexas   6 NaN   9 NaN\\nOregon  9 NaN  12 NaN\\nIf you want to instead broadcast over the columns, matching on the rows, you have to\\nuse one of the arithmetic methods. For example:\\nIn [154]: series3 = frame['d']\\nIn [155]: frame      In [156]: series3\\nEssential Functionality | 131\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Out[155]:            Out[156]:        \\n        b   d   e    Utah       1     \\nUtah    0   1   2    Ohio       4     \\nOhio    3   4   5    Texas      7     \\nTexas   6   7   8    Oregon    10     \\nOregon  9  10  11    Name: d          \\n                                      \\nIn [157]: frame.sub(series3, axis=0)\\nOut[157]: \\n        b  d  e\\nUtah   -1  0  1\\nOhio   -1  0  1\\nTexas  -1  0  1\\nOregon -1  0  1\\nThe axis number that you pass is the axis to match on. In this case we mean to match\\non the DataFrame’s row index and broadcast across.\\nFunction application and mapping\\nNumPy ufuncs (element-wise array methods) work fine with pandas objects:\\nIn [158]: frame = DataFrame(np.random.randn(4, 3), columns=list('bde'),\\n   .....:                   index=['Utah', 'Ohio', 'Texas', 'Oregon'])\\nIn [159]: frame                           In [160]: np.abs(frame)             \\nOut[159]:                                 Out[160]:                           \\n               b         d         e                     b         d         e\\nUtah   -0.204708  0.478943 -0.519439      Utah    0.204708  0.478943  0.519439\\nOhio   -0.555730  1.965781  1.393406      Ohio    0.555730  1.965781  1.393406\\nTexas   0.092908  0.281746  0.769023      Texas   0.092908  0.281746  0.769023\\nOregon  1.246435  1.007189 -1.296221      Oregon  1.246435  1.007189  1.296221\\nAnother frequent operation is applying a function on 1D arrays to each column or row.\\nDataFrame’s apply method does exactly this:\\nIn [161]: f = lambda x: x.max() - x.min()\\nIn [162]: frame.apply(f)        In [163]: frame.apply(f, axis=1)\\nOut[162]:                       Out[163]:                       \\nb    1.802165                   Utah      0.998382              \\nd    1.684034                   Ohio      2.521511              \\ne    2.689627                   Texas     0.676115              \\n                                Oregon    2.542656\\nMany of the most common array statistics (like sum and mean) are DataFrame methods,\\nso using apply is not necessary.\\nThe function passed to apply need not return a scalar value, it can also return a Series\\nwith multiple values:\\nIn [164]: def f(x):\\n   .....:     return Series([x.min(), x.max()], index=['min', 'max'])\\nIn [165]: frame.apply(f)\\n132 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Out[165]: \\n            b         d         e\\nmin -0.555730  0.281746 -1.296221\\nmax  1.246435  1.965781  1.393406\\nElement-wise Python functions can be used, too. Suppose you wanted to compute a\\nformatted string from each floating point value in frame. You can do this with applymap:\\nIn [166]: format = lambda x: '%.2f' % x\\nIn [167]: frame.applymap(format)\\nOut[167]: \\n            b     d      e\\nUtah    -0.20  0.48  -0.52\\nOhio    -0.56  1.97   1.39\\nTexas    0.09  0.28   0.77\\nOregon   1.25  1.01  -1.30\\nThe reason for the name applymap is that Series has a map method for applying an ele-\\nment-wise function:\\nIn [168]: frame['e'].map(format)\\nOut[168]: \\nUtah      -0.52\\nOhio       1.39\\nTexas      0.77\\nOregon    -1.30\\nName: e\\nSorting and ranking\\nSorting a data set by some criterion is another important built-in operation. To sort\\nlexicographically by row or column index, use the sort_index method, which returns\\na new, sorted object:\\nIn [169]: obj = Series(range(4), index=['d', 'a', 'b', 'c'])\\nIn [170]: obj.sort_index()\\nOut[170]: \\na    1\\nb    2\\nc    3\\nd    0\\nWith a DataFrame, you can sort by index on either axis:\\nIn [171]: frame = DataFrame(np.arange(8).reshape((2, 4)), index=['three', 'one'],\\n   .....:                   columns=['d', 'a', 'b', 'c'])\\nIn [172]: frame.sort_index()        In [173]: frame.sort_index(axis=1)\\nOut[172]:                           Out[173]:                         \\n       d  a  b  c                          a  b  c  d                 \\none    4  5  6  7                   three  1  2  3  0                 \\nthree  0  1  2  3                   one    5  6  7  4\\nEssential Functionality | 133\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"The data is sorted in ascending order by default, but can be sorted in descending order,\\ntoo:\\nIn [174]: frame.sort_index(axis=1, ascending=False)\\nOut[174]: \\n       d  c  b  a\\nthree  0  3  2  1\\none    4  7  6  5\\nTo sort a Series by its values, use its order method:\\nIn [175]: obj = Series([4, 7, -3, 2])\\nIn [176]: obj.order()\\nOut[176]: \\n2   -3\\n3    2\\n0    4\\n1    7\\nAny missing values are sorted to the end of the Series by default:\\nIn [177]: obj = Series([4, np.nan, 7, np.nan, -3, 2])\\nIn [178]: obj.order()\\nOut[178]: \\n4    -3\\n5     2\\n0     4\\n2     7\\n1   NaN\\n3   NaN\\nOn DataFrame, you may want to sort by the values in one or more columns. To do so,\\npass one or more column names to the by option:\\nIn [179]: frame = DataFrame({'b': [4, 7, -3, 2], 'a': [0, 1, 0, 1]})\\nIn [180]: frame        In [181]: frame.sort_index(by='b')\\nOut[180]:              Out[181]:                         \\n   a  b                   a  b                           \\n0  0  4                2  0 -3                           \\n1  1  7                3  1  2                           \\n2  0 -3                0  0  4                           \\n3  1  2                1  1  7\\nTo sort by multiple columns, pass a list of names:\\nIn [182]: frame.sort_index(by=['a', 'b'])\\nOut[182]: \\n   a  b\\n2  0 -3\\n0  0  4\\n3  1  2\\n1  1  7\\n134 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Ranking is closely related to sorting, assigning ranks from one through the number of\\nvalid data points in an array. It is similar to the indirect sort indices produced by \\nnumpy.argsort, except that ties are broken according to a rule. The rank methods for\\nSeries and DataFrame are the place to look; by default rank breaks ties by assigning\\neach group the mean rank:\\nIn [183]: obj = Series([7, -5, 7, 4, 2, 0, 4])\\nIn [184]: obj.rank()\\nOut[184]: \\n0    6.5\\n1    1.0\\n2    6.5\\n3    4.5\\n4    3.0\\n5    2.0\\n6    4.5\\nRanks can also be assigned according to the order they’re observed in the data:\\nIn [185]: obj.rank(method='first')\\nOut[185]: \\n0    6\\n1    1\\n2    7\\n3    4\\n4    3\\n5    2\\n6    5\\nNaturally, you can rank in descending order, too:\\nIn [186]: obj.rank(ascending=False, method='max')\\nOut[186]: \\n0    2\\n1    7\\n2    2\\n3    4\\n4    5\\n5    6\\n6    4\\nSee Table 5-8 for a list of tie-breaking methods available. DataFrame can compute ranks\\nover the rows or the columns:\\nIn [187]: frame = DataFrame({'b': [4.3, 7, -3, 2], 'a': [0, 1, 0, 1],\\n   .....:                    'c': [-2, 5, 8, -2.5]})\\nIn [188]: frame        In [189]: frame.rank(axis=1)\\nOut[188]:              Out[189]:                   \\n   a    b    c            a  b  c                  \\n0  0  4.3 -2.0         0  2  3  1                  \\n1  1  7.0  5.0         1  1  3  2                  \\n2  0 -3.0  8.0         2  2  1  3                  \\n3  1  2.0 -2.5         3  2  3  1\\nEssential Functionality | 135\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Table 5-8. Tie-breaking methods with rank\\nMethod Description\\n'average' Default: assign the average rank to each entry in the equal group.\\n'min' Use the minimum rank for the whole group.\\n'max' Use the maximum rank for the whole group.\\n'first' Assign ranks in the order the values appear in the data.\\nAxis indexes with duplicate values\\nUp until now all of the examples I’ve showed you have had unique axis labels (index\\nvalues). While many pandas functions (like reindex) require that the labels be unique,\\nit’s not mandatory. Let’s consider a small Series with duplicate indices:\\nIn [190]: obj = Series(range(5), index=['a', 'a', 'b', 'b', 'c'])\\nIn [191]: obj\\nOut[191]: \\na    0\\na    1\\nb    2\\nb    3\\nc    4\\nThe index’s is_unique property can tell you whether its values are unique or not:\\nIn [192]: obj.index.is_unique\\nOut[192]: False\\nData selection is one of the main things that behaves differently with duplicates. In-\\ndexing a value with multiple entries returns a Series while single entries return a scalar\\nvalue:\\nIn [193]: obj['a']    In [194]: obj['c']\\nOut[193]:             Out[194]: 4       \\na    0                                  \\na    1\\nThe same logic extends to indexing rows in a DataFrame:\\nIn [195]: df = DataFrame(np.random.randn(4, 3), index=['a', 'a', 'b', 'b'])\\nIn [196]: df\\nOut[196]: \\n          0         1         2\\na  0.274992  0.228913  1.352917\\na  0.886429 -2.001637 -0.371843\\nb  1.669025 -0.438570 -0.539741\\nb  0.476985  3.248944 -1.021228\\nIn [197]: df.ix['b']\\nOut[197]: \\n          0         1         2\\n136 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"b  1.669025 -0.438570 -0.539741\\nb  0.476985  3.248944 -1.021228\\nSummarizing and Computing Descriptive Statistics\\npandas objects are equipped with a set of common mathematical and statistical meth-\\nods. Most of these fall into the category of reductions or summary statistics, methods\\nthat extract a single value (like the sum or mean) from a Series or a Series of values from\\nthe rows or columns of a DataFrame. Compared with the equivalent methods of vanilla\\nNumPy arrays, they are all built from the ground up to exclude missing data. Consider\\na small DataFrame:\\nIn [198]: df = DataFrame([[1.4, np.nan], [7.1, -4.5],\\n   .....:                 [np.nan, np.nan], [0.75, -1.3]],\\n   .....:                index=['a', 'b', 'c', 'd'],\\n   .....:                columns=['one', 'two'])\\nIn [199]: df\\nOut[199]: \\n    one  two\\na  1.40  NaN\\nb  7.10 -4.5\\nc   NaN  NaN\\nd  0.75 -1.3\\nCalling DataFrame’s sum method returns a Series containing column sums:\\nIn [200]: df.sum()\\nOut[200]: \\none    9.25\\ntwo   -5.80\\nPassing axis=1 sums over the rows instead:\\nIn [201]: df.sum(axis=1)\\nOut[201]: \\na    1.40\\nb    2.60\\nc     NaN\\nd   -0.55\\nNA values are excluded unless the entire slice (row or column in this case) is NA. This\\ncan be disabled using the skipna option:\\nIn [202]: df.mean(axis=1, skipna=False)\\nOut[202]: \\na      NaN\\nb    1.300\\nc      NaN\\nd   -0.275\\nSee Table 5-9 for a list of common options for each reduction method options.\\nSummarizing and Computing Descriptive Statistics | 137\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Table 5-9. Options for reduction methods\\nMethod Description\\naxis Axis to reduce over. 0 for DataFrame’s rows and 1 for columns.\\nskipna Exclude missing values, True by default.\\nlevel Reduce grouped by level if the axis is hierarchically-indexed (MultiIndex).\\nSome methods, like idxmin and idxmax, return indirect statistics like the index value\\nwhere the minimum or maximum values are attained:\\nIn [203]: df.idxmax()\\nOut[203]: \\none    b\\ntwo    d\\nOther methods are accumulations:\\nIn [204]: df.cumsum()\\nOut[204]: \\n    one  two\\na  1.40  NaN\\nb  8.50 -4.5\\nc   NaN  NaN\\nd  9.25 -5.8\\nAnother type of method is neither a reduction nor an accumulation. describe is one\\nsuch example, producing multiple summary statistics in one shot:\\nIn [205]: df.describe()\\nOut[205]: \\n            one       two\\ncount  3.000000  2.000000\\nmean   3.083333 -2.900000\\nstd    3.493685  2.262742\\nmin    0.750000 -4.500000\\n25%    1.075000 -3.700000\\n50%    1.400000 -2.900000\\n75%    4.250000 -2.100000\\nmax    7.100000 -1.300000\\nOn non-numeric data, describe produces alternate summary statistics:\\nIn [206]: obj = Series(['a', 'a', 'b', 'c'] * 4)\\nIn [207]: obj.describe()\\nOut[207]: \\ncount     16\\nunique     3\\ntop        a\\nfreq       8\\nSee Table 5-10 for a full list of summary statistics and related methods.\\n138 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Table 5-10. Descriptive and summary statistics\\nMethod Description\\ncount Number of non-NA values\\ndescribe Compute set of summary statistics for Series or each DataFrame column\\nmin, max Compute minimum and maximum values\\nargmin, argmax Compute index locations (integers) at which minimum or maximum value obtained, respectively\\nidxmin, idxmax Compute index values at which minimum or maximum value obtained, respectively\\nquantile Compute sample quantile ranging from 0 to 1\\nsum Sum of values\\nmean Mean of values\\nmedian Arithmetic median (50% quantile) of values\\nmad Mean absolute deviation from mean value\\nvar Sample variance of values\\nstd Sample standard deviation of values\\nskew Sample skewness (3rd moment) of values\\nkurt Sample kurtosis (4th moment) of values\\ncumsum Cumulative sum of values\\ncummin, cummax Cumulative minimum or maximum of values, respectively\\ncumprod Cumulative product of values\\ndiff Compute 1st arithmetic difference (useful for time series)\\npct_change Compute percent changes\\nCorrelation and Covariance\\nSome summary statistics, like correlation and covariance, are computed from pairs of\\narguments. Let’s consider some DataFrames of stock prices and volumes obtained from\\nYahoo! Finance:\\nimport pandas.io.data as web\\nall_data = {}\\nfor ticker in ['AAPL', 'IBM', 'MSFT', 'GOOG']:\\n    all_data[ticker] = web.get_data_yahoo(ticker, '1/1/2000', '1/1/2010')\\nprice = DataFrame({tic: data['Adj Close']\\n                   for tic, data in all_data.iteritems()})\\nvolume = DataFrame({tic: data['Volume']\\n                    for tic, data in all_data.iteritems()})\\nI now compute percent changes of the prices:\\nIn [209]: returns = price.pct_change()\\nIn [210]: returns.tail()\\nSummarizing and Computing Descriptive Statistics | 139\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Out[210]: \\n                AAPL      GOOG       IBM      MSFT\\nDate                                              \\n2009-12-24  0.034339  0.011117  0.004420  0.002747\\n2009-12-28  0.012294  0.007098  0.013282  0.005479\\n2009-12-29 -0.011861 -0.005571 -0.003474  0.006812\\n2009-12-30  0.012147  0.005376  0.005468 -0.013532\\n2009-12-31 -0.004300 -0.004416 -0.012609 -0.015432\\nThe corr method of Series computes the correlation of the overlapping, non-NA,\\naligned-by-index values in two Series. Relatedly, cov computes the covariance:\\nIn [211]: returns.MSFT.corr(returns.IBM)\\nOut[211]: 0.49609291822168838\\nIn [212]: returns.MSFT.cov(returns.IBM)\\nOut[212]: 0.00021600332437329015\\nDataFrame’s corr and cov methods, on the other hand, return a full correlation or\\ncovariance matrix as a DataFrame, respectively:\\nIn [213]: returns.corr()\\nOut[213]: \\n          AAPL      GOOG       IBM      MSFT\\nAAPL  1.000000  0.470660  0.410648  0.424550\\nGOOG  0.470660  1.000000  0.390692  0.443334\\nIBM   0.410648  0.390692  1.000000  0.496093\\nMSFT  0.424550  0.443334  0.496093  1.000000\\nIn [214]: returns.cov()\\nOut[214]: \\n          AAPL      GOOG       IBM      MSFT\\nAAPL  0.001028  0.000303  0.000252  0.000309\\nGOOG  0.000303  0.000580  0.000142  0.000205\\nIBM   0.000252  0.000142  0.000367  0.000216\\nMSFT  0.000309  0.000205  0.000216  0.000516\\nUsing DataFrame’s corrwith method, you can compute pairwise correlations between\\na DataFrame’s columns or rows with another Series or DataFrame. Passing a Series\\nreturns a Series with the correlation value computed for each column:\\nIn [215]: returns.corrwith(returns.IBM)\\nOut[215]: \\nAAPL    0.410648\\nGOOG    0.390692\\nIBM     1.000000\\nMSFT    0.496093\\nPassing a DataFrame computes the correlations of matching column names. Here I\\ncompute correlations of percent changes with volume:\\nIn [216]: returns.corrwith(volume)\\nOut[216]: \\nAAPL   -0.057461\\nGOOG    0.062644\\n140 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"IBM    -0.007900\\nMSFT   -0.014175\\nPassing axis=1 does things row-wise instead. In all cases, the data points are aligned by\\nlabel before computing the correlation.\\nUnique Values, Value Counts, and Membership\\nAnother class of related methods extracts information about the values contained in a\\none-dimensional Series. To illustrate these, consider this example:\\nIn [217]: obj = Series(['c', 'a', 'd', 'a', 'a', 'b', 'b', 'c', 'c'])\\nThe first function is unique, which gives you an array of the unique values in a Series:\\nIn [218]: uniques = obj.unique()\\nIn [219]: uniques\\nOut[219]: array([c, a, d, b], dtype=object)\\nThe unique values are not necessarily returned in sorted order, but could be sorted after\\nthe fact if needed ( uniques.sort()). Relatedly, value_counts computes a Series con-\\ntaining value frequencies:\\nIn [220]: obj.value_counts()\\nOut[220]: \\nc    3\\na    3\\nb    2\\nd    1\\nThe Series is sorted by value in descending order as a convenience. value_counts is also\\navailable as a top-level pandas method that can be used with any array or sequence:\\nIn [221]: pd.value_counts(obj.values, sort=False)\\nOut[221]: \\na    3\\nb    2\\nc    3\\nd    1\\nLastly, isin is responsible for vectorized set membership and can be very useful in\\nfiltering a data set down to a subset of values in a Series or column in a DataFrame:\\nIn [222]: mask = obj.isin(['b', 'c'])\\nIn [223]: mask        In [224]: obj[mask]\\nOut[223]:             Out[224]:          \\n0     True            0    c             \\n1    False            5    b             \\n2    False            6    b             \\n3    False            7    c             \\n4    False            8    c             \\n5     True                               \\n6     True                               \\nSummarizing and Computing Descriptive Statistics | 141\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"7     True                               \\n8     True\\nSee Table 5-11 for a reference on these methods.\\nTable 5-11. Unique, value counts, and binning methods\\nMethod Description\\nisin Compute boolean array indicating whether each Series value is contained in the passed sequence of values.\\nunique Compute array of unique values in a Series, returned in the order observed.\\nvalue_counts Return a Series containing unique values as its index and frequencies as its values, ordered count in\\ndescending order.\\nIn some cases, you may want to compute a histogram on multiple related columns in\\na DataFrame. Here’s an example:\\nIn [225]: data = DataFrame({'Qu1': [1, 3, 4, 3, 4],\\n   .....:                   'Qu2': [2, 3, 1, 2, 3],\\n   .....:                   'Qu3': [1, 5, 2, 4, 4]})\\nIn [226]: data\\nOut[226]: \\n   Qu1  Qu2  Qu3\\n0    1    2    1\\n1    3    3    5\\n2    4    1    2\\n3    3    2    4\\n4    4    3    4\\nPassing pandas.value_counts to this DataFrame’s apply function gives:\\nIn [227]: result = data.apply(pd.value_counts).fillna(0)\\nIn [228]: result\\nOut[228]: \\n   Qu1  Qu2  Qu3\\n1    1    1    1\\n2    0    2    1\\n3    2    2    0\\n4    2    0    2\\n5    0    0    1\\nHandling Missing Data\\nMissing data is common in most data analysis applications. One of the goals in de-\\nsigning pandas was to make working with missing data as painless as possible. For\\nexample, all of the descriptive statistics on pandas objects exclude missing data as\\nyou’ve seen earlier in the chapter.\\n142 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"pandas uses the floating point value NaN (Not a Number) to represent missing data in\\nboth floating as well as in non-floating point arrays. It is just used as a sentinel that can\\nbe easily detected:\\nIn [229]: string_data = Series(['aardvark', 'artichoke', np.nan, 'avocado'])\\nIn [230]: string_data        In [231]: string_data.isnull()\\nOut[230]:                    Out[231]:                     \\n0     aardvark               0    False                    \\n1    artichoke               1    False                    \\n2          NaN               2     True                    \\n3      avocado               3    False\\nThe built-in Python None value is also treated as NA in object arrays:\\nIn [232]: string_data[0] = None\\nIn [233]: string_data.isnull()\\nOut[233]: \\n0     True\\n1    False\\n2     True\\n3    False\\nI do not claim that pandas’s NA representation is optimal, but it is simple and reason-\\nably consistent. It’s the best solution, with good all-around performance characteristics\\nand a simple API, that I could concoct in the absence of a true NA data type or bit\\npattern in NumPy’s data types. Ongoing development work in NumPy may change this\\nin the future.\\nTable 5-12. NA handling methods\\nArgument Description\\ndropna Filter axis labels based on whether values for each label have missing data, with varying thresholds for how much\\nmissing data to tolerate.\\nfillna Fill in missing data with some value or using an interpolation method such as 'ffill' or 'bfill'.\\nisnull Return like-type object containing boolean values indicating which values are missing / NA.\\nnotnull Negation of isnull.\\nFiltering Out Missing Data\\nYou have a number of options for filtering out missing data. While doing it by hand is\\nalways an option, dropna can be very helpful. On a Series, it returns the Series with only\\nthe non-null data and index values:\\nIn [234]: from numpy import nan as NA\\nIn [235]: data = Series([1, NA, 3.5, NA, 7])\\nIn [236]: data.dropna()\\nOut[236]: \\nHandling Missing Data | 143\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"0    1.0\\n2    3.5\\n4    7.0\\nNaturally, you could have computed this yourself by boolean indexing:\\nIn [237]: data[data.notnull()]\\nOut[237]: \\n0    1.0\\n2    3.5\\n4    7.0\\nWith DataFrame objects, these are a bit more complex. You may want to drop rows\\nor columns which are all NA or just those containing any NAs. dropna by default drops\\nany row containing a missing value:\\nIn [238]: data = DataFrame([[1., 6.5, 3.], [1., NA, NA],\\n   .....:                   [NA, NA, NA], [NA, 6.5, 3.]])\\nIn [239]: cleaned = data.dropna()\\nIn [240]: data        In [241]: cleaned\\nOut[240]:             Out[241]:        \\n    0    1   2           0    1  2     \\n0   1  6.5   3        0  1  6.5  3     \\n1   1  NaN NaN                         \\n2 NaN  NaN NaN                         \\n3 NaN  6.5   3\\nPassing how='all' will only drop rows that are all NA:\\nIn [242]: data.dropna(how='all')\\nOut[242]: \\n    0    1   2\\n0   1  6.5   3\\n1   1  NaN NaN\\n3 NaN  6.5   3\\nDropping columns in the same way is only a matter of passing axis=1:\\nIn [243]: data[4] = NA\\nIn [244]: data            In [245]: data.dropna(axis=1, how='all')\\nOut[244]:                 Out[245]:                               \\n    0    1   2   4            0    1   2                          \\n0   1  6.5   3 NaN        0   1  6.5   3                          \\n1   1  NaN NaN NaN        1   1  NaN NaN                          \\n2 NaN  NaN NaN NaN        2 NaN  NaN NaN                          \\n3 NaN  6.5   3 NaN        3 NaN  6.5   3\\nA related way to filter out DataFrame rows tends to concern time series data. Suppose\\nyou want to keep only rows containing a certain number of observations. You can\\nindicate this with the thresh argument:\\nIn [246]: df = DataFrame(np.random.randn(7, 3))\\nIn [247]: df.ix[:4, 1] = NA; df.ix[:2, 2] = NA\\n144 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'In [248]: df                           In [249]: df.dropna(thresh=3)  \\nOut[248]:                              Out[249]:                      \\n          0         1         2                  0         1         2\\n0 -0.577087       NaN       NaN        5  0.332883 -2.359419 -0.199543\\n1  0.523772       NaN       NaN        6 -1.541996 -0.970736 -1.307030\\n2 -0.713544       NaN       NaN                                       \\n3 -1.860761       NaN  0.560145                                       \\n4 -1.265934       NaN -1.063512                                       \\n5  0.332883 -2.359419 -0.199543                                       \\n6 -1.541996 -0.970736 -1.307030\\nFilling in Missing Data\\nRather than filtering out missing data (and potentially discarding other data along with\\nit), you may want to fill in the “holes” in any number of ways. For most purposes, the \\nfillna method is the workhorse function to use. Calling fillna with a constant replaces\\nmissing values with that value:\\nIn [250]: df.fillna(0)\\nOut[250]: \\n          0         1         2\\n0 -0.577087  0.000000  0.000000\\n1  0.523772  0.000000  0.000000\\n2 -0.713544  0.000000  0.000000\\n3 -1.860761  0.000000  0.560145\\n4 -1.265934  0.000000 -1.063512\\n5  0.332883 -2.359419 -0.199543\\n6 -1.541996 -0.970736 -1.307030\\nCalling fillna with a dict you can use a different fill value for each column:\\nIn [251]: df.fillna({1: 0.5, 3: -1})\\nOut[251]: \\n          0         1         2\\n0 -0.577087  0.500000       NaN\\n1  0.523772  0.500000       NaN\\n2 -0.713544  0.500000       NaN\\n3 -1.860761  0.500000  0.560145\\n4 -1.265934  0.500000 -1.063512\\n5  0.332883 -2.359419 -0.199543\\n6 -1.541996 -0.970736 -1.307030\\nfillna returns a new object, but you can modify the existing object in place:\\n# always returns a reference to the filled object\\nIn [252]: _ = df.fillna(0, inplace=True)\\nIn [253]: df\\nOut[253]: \\n          0         1         2\\n0 -0.577087  0.000000  0.000000\\n1  0.523772  0.000000  0.000000\\n2 -0.713544  0.000000  0.000000\\n3 -1.860761  0.000000  0.560145\\nHandling Missing Data | 145\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"4 -1.265934  0.000000 -1.063512\\n5  0.332883 -2.359419 -0.199543\\n6 -1.541996 -0.970736 -1.307030\\nThe same interpolation methods available for reindexing can be used with fillna:\\nIn [254]: df = DataFrame(np.random.randn(6, 3))\\nIn [255]: df.ix[2:, 1] = NA; df.ix[4:, 2] = NA\\nIn [256]: df\\nOut[256]: \\n          0         1         2\\n0  0.286350  0.377984 -0.753887\\n1  0.331286  1.349742  0.069877\\n2  0.246674       NaN  1.004812\\n3  1.327195       NaN -1.549106\\n4  0.022185       NaN       NaN\\n5  0.862580       NaN       NaN\\nIn [257]: df.fillna(method='ffill')      In [258]: df.fillna(method='ffill', limit=2)\\nOut[257]:                                Out[258]:                                   \\n          0         1         2                    0         1         2             \\n0  0.286350  0.377984 -0.753887          0  0.286350  0.377984 -0.753887             \\n1  0.331286  1.349742  0.069877          1  0.331286  1.349742  0.069877             \\n2  0.246674  1.349742  1.004812          2  0.246674  1.349742  1.004812             \\n3  1.327195  1.349742 -1.549106          3  1.327195  1.349742 -1.549106             \\n4  0.022185  1.349742 -1.549106          4  0.022185       NaN -1.549106             \\n5  0.862580  1.349742 -1.549106          5  0.862580       NaN -1.549106\\nWith fillna you can do lots of other things with a little creativity. For example, you\\nmight pass the mean or median value of a Series:\\nIn [259]: data = Series([1., NA, 3.5, NA, 7])\\nIn [260]: data.fillna(data.mean())\\nOut[260]: \\n0    1.000000\\n1    3.833333\\n2    3.500000\\n3    3.833333\\n4    7.000000\\nSee Table 5-13 for a reference on fillna.\\nTable 5-13. fillna function arguments\\nArgument Description\\nvalue Scalar value or dict-like object to use to fill missing values\\nmethod Interpolation, by default 'ffill' if function called with no other arguments\\naxis Axis to fill on, default axis=0\\ninplace Modify the calling object without producing a copy\\nlimit For forward and backward filling, maximum number of consecutive periods to fill\\n146 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Hierarchical Indexing\\nHierarchical indexing is an important feature of pandas enabling you to have multiple\\n(two or more) index levels on an axis. Somewhat abstractly, it provides a way for you\\nto work with higher dimensional data in a lower dimensional form. Let’s start with a\\nsimple example; create a Series with a list of lists or arrays as the index:\\nIn [261]: data = Series(np.random.randn(10),\\n   .....:               index=[['a', 'a', 'a', 'b', 'b', 'b', 'c', 'c', 'd', 'd'],\\n   .....:                      [1, 2, 3, 1, 2, 3, 1, 2, 2, 3]])\\nIn [262]: data\\nOut[262]: \\na  1    0.670216\\n   2    0.852965\\n   3   -0.955869\\nb  1   -0.023493\\n   2   -2.304234\\n   3   -0.652469\\nc  1   -1.218302\\n   2   -1.332610\\nd  2    1.074623\\n   3    0.723642\\nWhat you’re seeing is a prettified view of a Series with a MultiIndex as its index. The\\n“gaps” in the index display mean “use the label directly above”:\\nIn [263]: data.index\\nOut[263]: \\nMultiIndex\\n[('a', 1) ('a', 2) ('a', 3) ('b', 1) ('b', 2) ('b', 3) ('c', 1)\\n ('c', 2) ('d', 2) ('d', 3)]\\nWith a hierarchically-indexed object, so-called partial indexing is possible, enabling\\nyou to concisely select subsets of the data:\\nIn [264]: data['b']\\nOut[264]: \\n1   -0.023493\\n2   -2.304234\\n3   -0.652469\\nIn [265]: data['b':'c']        In [266]: data.ix[['b', 'd']]\\nOut[265]:                      Out[266]:                    \\nb  1   -0.023493               b  1   -0.023493             \\n   2   -2.304234                  2   -2.304234             \\n   3   -0.652469                  3   -0.652469             \\nc  1   -1.218302               d  2    1.074623             \\n   2   -1.332610                  3    0.723642\\nSelection is even possible in some cases from an “inner” level:\\nIn [267]: data[:, 2]\\nOut[267]: \\na    0.852965\\nHierarchical Indexing | 147\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"b   -2.304234\\nc   -1.332610\\nd    1.074623\\nHierarchical indexing plays a critical role in reshaping data and group-based operations\\nlike forming a pivot table. For example, this data could be rearranged into a DataFrame\\nusing its unstack method:\\nIn [268]: data.unstack()\\nOut[268]: \\n          1         2         3\\na  0.670216  0.852965 -0.955869\\nb -0.023493 -2.304234 -0.652469\\nc -1.218302 -1.332610       NaN\\nd       NaN  1.074623  0.723642\\nThe inverse operation of unstack is stack:\\nIn [269]: data.unstack().stack()\\nOut[269]: \\na  1    0.670216\\n   2    0.852965\\n   3   -0.955869\\nb  1   -0.023493\\n   2   -2.304234\\n   3   -0.652469\\nc  1   -1.218302\\n   2   -1.332610\\nd  2    1.074623\\n   3    0.723642\\nstack and unstack will be explored in more detail in Chapter 7.\\nWith a DataFrame, either axis can have a hierarchical index:\\nIn [270]: frame = DataFrame(np.arange(12).reshape((4, 3)),\\n   .....:                   index=[['a', 'a', 'b', 'b'], [1, 2, 1, 2]],\\n   .....:                   columns=[['Ohio', 'Ohio', 'Colorado'],\\n   .....:                            ['Green', 'Red', 'Green']])\\nIn [271]: frame\\nOut[271]: \\n      Ohio       Colorado\\n     Green  Red     Green\\na 1      0    1         2\\n  2      3    4         5\\nb 1      6    7         8\\n  2      9   10        11\\nThe hierarchical levels can have names (as strings or any Python objects). If so, these\\nwill show up in the console output (don’t confuse the index names with the axis labels!):\\nIn [272]: frame.index.names = ['key1', 'key2']\\nIn [273]: frame.columns.names = ['state', 'color']\\nIn [274]: frame\\n148 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Out[274]: \\nstate       Ohio       Colorado\\ncolor      Green  Red     Green\\nkey1 key2                      \\na    1         0    1         2\\n     2         3    4         5\\nb    1         6    7         8\\n     2         9   10        11\\nWith partial column indexing you can similarly select groups of columns:\\nIn [275]: frame['Ohio']\\nOut[275]: \\ncolor      Green  Red\\nkey1 key2            \\na    1         0    1\\n     2         3    4\\nb    1         6    7\\n     2         9   10\\nA MultiIndex can be created by itself and then reused; the columns in the above Data-\\nFrame with level names could be created like this:\\nMultiIndex.from_arrays([['Ohio', 'Ohio', 'Colorado'], ['Green', 'Red', 'Green']],\\n                       names=['state', 'color'])\\nReordering and Sorting Levels\\nAt times you will need to rearrange the order of the levels on an axis or sort the data\\nby the values in one specific level. The swaplevel takes two level numbers or names and\\nreturns a new object with the levels interchanged (but the data is otherwise unaltered):\\nIn [276]: frame.swaplevel('key1', 'key2')\\nOut[276]: \\nstate       Ohio       Colorado\\ncolor      Green  Red     Green\\nkey2 key1                      \\n1    a         0    1         2\\n2    a         3    4         5\\n1    b         6    7         8\\n2    b         9   10        11\\nsortlevel, on the other hand, sorts the data (stably) using only the values in a single\\nlevel. When swapping levels, it’s not uncommon to also use sortlevel so that the result\\nis lexicographically sorted:\\nIn [277]: frame.sortlevel(1)           In [278]: frame.swaplevel(0, 1).sortlevel(0)\\nOut[277]:                              Out[278]:                                   \\nstate       Ohio       Colorado        state       Ohio       Colorado             \\ncolor      Green  Red     Green        color      Green  Red     Green             \\nkey1 key2                              key2 key1                                   \\na    1         0    1         2        1    a         0    1         2             \\nb    1         6    7         8             b         6    7         8             \\na    2         3    4         5        2    a         3    4         5             \\nb    2         9   10        11             b         9   10        11\\nHierarchical Indexing | 149\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Data selection performance is much better on hierarchically indexed\\nobjects if the index is lexicographically sorted starting with the outer-\\nmost level, that is, the result of calling sortlevel(0) or sort_index().\\nSummary Statistics by Level\\nMany descriptive and summary statistics on DataFrame and Series have a level option\\nin which you can specify the level you want to sum by on a particular axis. Consider\\nthe above DataFrame; we can sum by level on either the rows or columns like so:\\nIn [279]: frame.sum(level='key2')\\nOut[279]: \\nstate   Ohio       Colorado\\ncolor  Green  Red     Green\\nkey2                       \\n1          6    8        10\\n2         12   14        16\\nIn [280]: frame.sum(level='color', axis=1)\\nOut[280]: \\ncolor      Green  Red\\nkey1 key2            \\na    1         2    1\\n     2         8    4\\nb    1        14    7\\n     2        20   10\\nUnder the hood, this utilizes pandas’s groupby machinery which will be discussed in\\nmore detail later in the book.\\nUsing a DataFrame’s Columns\\nIt’s not unusual to want to use one or more columns from a DataFrame as the row\\nindex; alternatively, you may wish to move the row index into the DataFrame’s col-\\numns. Here’s an example DataFrame:\\nIn [281]: frame = DataFrame({'a': range(7), 'b': range(7, 0, -1),\\n   .....:                    'c': ['one', 'one', 'one', 'two', 'two', 'two', 'two'],\\n   .....:                    'd': [0, 1, 2, 0, 1, 2, 3]})\\nIn [282]: frame\\nOut[282]: \\n   a  b    c  d\\n0  0  7  one  0\\n1  1  6  one  1\\n2  2  5  one  2\\n3  3  4  two  0\\n4  4  3  two  1\\n5  5  2  two  2\\n6  6  1  two  3\\n150 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"DataFrame’s set_index function will create a new DataFrame using one or more of its\\ncolumns as the index:\\nIn [283]: frame2 = frame.set_index(['c', 'd'])\\nIn [284]: frame2\\nOut[284]: \\n       a  b\\nc   d      \\none 0  0  7\\n    1  1  6\\n    2  2  5\\ntwo 0  3  4\\n    1  4  3\\n    2  5  2\\n    3  6  1\\nBy default the columns are removed from the DataFrame, though you can leave them in:\\nIn [285]: frame.set_index(['c', 'd'], drop=False)\\nOut[285]: \\n       a  b    c  d\\nc   d              \\none 0  0  7  one  0\\n    1  1  6  one  1\\n    2  2  5  one  2\\ntwo 0  3  4  two  0\\n    1  4  3  two  1\\n    2  5  2  two  2\\n    3  6  1  two  3\\nreset_index, on the other hand, does the opposite of set_index; the hierarchical index\\nlevels are are moved into the columns:\\nIn [286]: frame2.reset_index()\\nOut[286]: \\n     c  d  a  b\\n0  one  0  0  7\\n1  one  1  1  6\\n2  one  2  2  5\\n3  two  0  3  4\\n4  two  1  4  3\\n5  two  2  5  2\\n6  two  3  6  1\\nOther pandas Topics\\nHere are some additional topics that may be of use to you in your data travels.\\nInteger Indexing\\nWorking with pandas objects indexed by integers is something that often trips up new\\nusers due to some differences with indexing semantics on built-in Python data\\nOther pandas Topics | 151\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"structures like lists and tuples. For example, you would not expect the following code\\nto generate an error:\\nser = Series(np.arange(3.))\\nser[-1]\\nIn this case, pandas could “fall back” on integer indexing, but there’s not a safe and\\ngeneral way (that I know of) to do this without introducing subtle bugs. Here we have\\nan index containing 0, 1, 2, but inferring what the user wants (label-based indexing or\\nposition-based) is difficult::\\nIn [288]: ser\\nOut[288]: \\n0    0\\n1    1\\n2    2\\nOn the other hand, with a non-integer index, there is no potential for ambiguity:\\nIn [289]: ser2 = Series(np.arange(3.), index=['a', 'b', 'c'])\\nIn [290]: ser2[-1]\\nOut[290]: 2.0\\nTo keep things consistent, if you have an axis index containing indexers, data selection\\nwith integers will always be label-oriented. This includes slicing with ix, too:\\nIn [291]: ser.ix[:1]\\nOut[291]: \\n0    0\\n1    1\\nIn cases where you need reliable position-based indexing regardless of the index type,\\nyou can use the iget_value method from Series and irow and icol methods from Da-\\ntaFrame:\\nIn [292]: ser3 = Series(range(3), index=[-5, 1, 3])\\nIn [293]: ser3.iget_value(2)\\nOut[293]: 2\\nIn [294]: frame = DataFrame(np.arange(6).reshape(3, 2)), index=[2, 0, 1])\\nIn [295]: frame.irow(0)\\nOut[295]: \\n0    0\\n1    1\\nName: 2\\nPanel Data\\nWhile not a major topic of this book, pandas has a Panel data structure, which you can\\nthink of as a three-dimensional analogue of DataFrame. Much of the development focus\\nof pandas has been in tabular data manipulations as these are easier to reason about,\\n152 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"and hierarchical indexing makes using truly N-dimensional arrays unnecessary in a lot\\nof cases.\\nTo create a Panel, you can use a dict of DataFrame objects or a three-dimensional\\nndarray:\\nimport pandas.io.data as web\\npdata = pd.Panel(dict((stk, web.get_data_yahoo(stk, '1/1/2009', '6/1/2012'))\\n                       for stk in ['AAPL', 'GOOG', 'MSFT', 'DELL']))\\nEach item (the analogue of columns in a DataFrame) in the Panel is a DataFrame:\\nIn [297]: pdata\\nOut[297]: \\n<class 'pandas.core.panel.Panel'>\\nDimensions: 4 (items) x 861 (major) x 6 (minor)\\nItems: AAPL to MSFT\\nMajor axis: 2009-01-02 00:00:00 to 2012-06-01 00:00:00\\nMinor axis: Open to Adj Close\\nIn [298]: pdata = pdata.swapaxes('items', 'minor')\\nIn [299]: pdata['Adj Close']\\nOut[299]: \\n<class 'pandas.core.frame.DataFrame'>\\nDatetimeIndex: 861 entries, 2009-01-02 00:00:00 to 2012-06-01 00:00:00\\nData columns:\\nAAPL    861  non-null values\\nDELL    861  non-null values\\nGOOG    861  non-null values\\nMSFT    861  non-null values\\ndtypes: float64(4)\\nix-based label indexing generalizes to three dimensions, so we can select all data at a\\nparticular date or a range of dates like so:\\nIn [300]: pdata.ix[:, '6/1/2012', :]\\nOut[300]: \\n        Open    High     Low   Close    Volume  Adj Close\\nAAPL  569.16  572.65  560.52  560.99  18606700     560.99\\nDELL   12.15   12.30   12.05   12.07  19396700      12.07\\nGOOG  571.79  572.65  568.35  570.98   3057900     570.98\\nMSFT   28.76   28.96   28.44   28.45  56634300      28.45\\nIn [301]: pdata.ix['Adj Close', '5/22/2012':, :]\\nOut[301]: \\n              AAPL   DELL    GOOG   MSFT\\nDate                                    \\n2012-05-22  556.97  15.08  600.80  29.76\\n2012-05-23  570.56  12.49  609.46  29.11\\n2012-05-24  565.32  12.45  603.66  29.07\\n2012-05-25  562.29  12.46  591.53  29.06\\n2012-05-29  572.27  12.66  594.34  29.56\\n2012-05-30  579.17  12.56  588.23  29.34\\nOther pandas Topics | 153\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"2012-05-31  577.73  12.33  580.86  29.19\\n2012-06-01  560.99  12.07  570.98  28.45\\nAn alternate way to represent panel data, especially for fitting statistical models, is in\\n“stacked” DataFrame form:\\nIn [302]: stacked = pdata.ix[:, '5/30/2012':, :].to_frame()\\nIn [303]: stacked\\nOut[303]: \\n                    Open    High     Low   Close    Volume  Adj Close\\nmajor      minor                                                     \\n2012-05-30 AAPL   569.20  579.99  566.56  579.17  18908200     579.17\\n           DELL    12.59   12.70   12.46   12.56  19787800      12.56\\n           GOOG   588.16  591.90  583.53  588.23   1906700     588.23\\n           MSFT    29.35   29.48   29.12   29.34  41585500      29.34\\n2012-05-31 AAPL   580.74  581.50  571.46  577.73  17559800     577.73\\n           DELL    12.53   12.54   12.33   12.33  19955500      12.33\\n           GOOG   588.72  590.00  579.00  580.86   2968300     580.86\\n           MSFT    29.30   29.42   28.94   29.19  39134000      29.19\\n2012-06-01 AAPL   569.16  572.65  560.52  560.99  18606700     560.99\\n           DELL    12.15   12.30   12.05   12.07  19396700      12.07\\n           GOOG   571.79  572.65  568.35  570.98   3057900     570.98\\n           MSFT    28.76   28.96   28.44   28.45  56634300      28.45\\nDataFrame has a related to_panel method, the inverse of to_frame:\\nIn [304]: stacked.to_panel()\\nOut[304]: \\n<class 'pandas.core.panel.Panel'>\\nDimensions: 6 (items) x 3 (major) x 4 (minor)\\nItems: Open to Adj Close\\nMajor axis: 2012-05-30 00:00:00 to 2012-06-01 00:00:00\\nMinor axis: AAPL to MSFT\\n154 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"CHAPTER 6\\nData Loading, Storage, and File\\nFormats\\nThe tools in this book are of little use if you can’t easily import and export data in\\nPython. I’m going to be focused on input and output with pandas objects, though there\\nare of course numerous tools in other libraries to aid in this process. NumPy, for ex-\\nample, features low-level but extremely fast binary data loading and storage, including\\nsupport for memory-mapped array. See Chapter 12 for more on those.\\nInput and output typically falls into a few main categories: reading text files and other\\nmore efficient on-disk formats, loading data from databases, and interacting with net-\\nwork sources like web APIs.\\nReading and Writing Data in Text Format\\nPython has become a beloved language for text and file munging due to its simple syntax\\nfor interacting with files, intuitive data structures, and convenient features like tuple\\npacking and unpacking.\\npandas features a number of functions for reading tabular data as a DataFrame object.\\nTable 6-1 has a summary of all of them, though read_csv and read_table are likely the\\nones you’ll use the most.\\nTable 6-1. Parsing functions in pandas\\nFunction Description\\nread_csv Load delimited data from a file, URL, or file-like object. Use comma as default delimiter\\nread_table Load delimited data from a file, URL, or file-like object. Use tab ('\\\\t') as default delimiter\\nread_fwf Read data in fixed-width column format (that is, no delimiters)\\nread_clipboard Version of read_table that reads data from the clipboard. Useful for converting tables from web pages\\n155\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"I’ll give an overview of the mechanics of these functions, which are meant to convert\\ntext data into a DataFrame. The options for these functions fall into a few categories:\\n• Indexing: can treat one or more columns as the returned DataFrame, and whether\\nto get column names from the file, the user, or not at all.\\n• Type inference and data conversion: this includes the user-defined value conver-\\nsions and custom list of missing value markers.\\n• Datetime parsing: includes combining capability, including combining date and\\ntime information spread over multiple columns into a single column in the result.\\n• Iterating: support for iterating over chunks of very large files.\\n• Unclean data issues: skipping rows or a footer, comments, or other minor things\\nlike numeric data with thousands separated by commas.\\nType inference is one of the more important features of these functions; that means you\\ndon’t have to specify which columns are numeric, integer, boolean, or string. Handling\\ndates and other custom types requires a bit more effort, though. Let’s start with a small\\ncomma-separated (CSV) text file:\\nIn [846]: !cat ch06/ex1.csv\\na,b,c,d,message\\n1,2,3,4,hello\\n5,6,7,8,world\\n9,10,11,12,foo\\nSince this is comma-delimited, we can use read_csv to read it into a DataFrame:\\nIn [847]: df = pd.read_csv('ch06/ex1.csv')\\nIn [848]: df\\nOut[848]: \\n   a   b   c   d message\\n0  1   2   3   4   hello\\n1  5   6   7   8   world\\n2  9  10  11  12     foo\\nWe could also have used read_table and specifying the delimiter:\\nIn [849]: pd.read_table('ch06/ex1.csv', sep=',')\\nOut[849]: \\n   a   b   c   d message\\n0  1   2   3   4   hello\\n1  5   6   7   8   world\\n2  9  10  11  12     foo\\nHere I used the Unix cat shell command to print the raw contents of\\nthe file to the screen. If you’re on Windows, you can use type instead\\nof cat to achieve the same effect.\\n156 | Chapter 6: \\u2002Data Loading, Storage, and File Formats\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"A file will not always have a header row. Consider this file:\\nIn [850]: !cat ch06/ex2.csv\\n1,2,3,4,hello\\n5,6,7,8,world\\n9,10,11,12,foo\\nTo read this in, you have a couple of options. You can allow pandas to assign default\\ncolumn names, or you can specify names yourself:\\nIn [851]: pd.read_csv('ch06/ex2.csv', header=None)\\nOut[851]: \\n   X.1  X.2  X.3  X.4    X.5\\n0    1    2    3    4  hello\\n1    5    6    7    8  world\\n2    9   10   11   12    foo\\nIn [852]: pd.read_csv('ch06/ex2.csv', names=['a', 'b', 'c', 'd', 'message'])\\nOut[852]: \\n   a   b   c   d message\\n0  1   2   3   4   hello\\n1  5   6   7   8   world\\n2  9  10  11  12     foo\\nSuppose you wanted the message column to be the index of the returned DataFrame.\\nYou can either indicate you want the column at index 4 or named 'message' using the\\nindex_col argument:\\nIn [853]: names = ['a', 'b', 'c', 'd', 'message']\\nIn [854]: pd.read_csv('ch06/ex2.csv', names=names, index_col='message')\\nOut[854]: \\n         a   b   c   d\\nmessage               \\nhello    1   2   3   4\\nworld    5   6   7   8\\nfoo      9  10  11  12\\nIn the event that you want to form a hierarchical index from multiple columns, just\\npass a list of column numbers or names:\\nIn [855]: !cat ch06/csv_mindex.csv\\nkey1,key2,value1,value2\\none,a,1,2\\none,b,3,4\\none,c,5,6\\none,d,7,8\\ntwo,a,9,10\\ntwo,b,11,12\\ntwo,c,13,14\\ntwo,d,15,16\\nIn [856]: parsed = pd.read_csv('ch06/csv_mindex.csv', index_col=['key1', 'key2'])\\nIn [857]: parsed\\nOut[857]: \\nReading and Writing Data in Text Format | 157\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"value1  value2\\nkey1 key2                \\none  a          1       2\\n     b          3       4\\n     c          5       6\\n     d          7       8\\ntwo  a          9      10\\n     b         11      12\\n     c         13      14\\n     d         15      16\\nIn some cases, a table might not have a fixed delimiter, using whitespace or some other\\npattern to separate fields. In these cases, you can pass a regular expression as a delimiter\\nfor read_table. Consider a text file that looks like this:\\nIn [858]: list(open('ch06/ex3.txt'))\\nOut[858]: \\n['            A         B         C\\\\n',\\n 'aaa -0.264438 -1.026059 -0.619500\\\\n',\\n 'bbb  0.927272  0.302904 -0.032399\\\\n',\\n 'ccc -0.264273 -0.386314 -0.217601\\\\n',\\n 'ddd -0.871858 -0.348382  1.100491\\\\n']\\nWhile you could do some munging by hand, in this case fields are separated by a variable\\namount of whitespace. This can be expressed by the regular expression \\\\s+, so we have\\nthen:\\nIn [859]: result = pd.read_table('ch06/ex3.txt', sep='\\\\s+')\\nIn [860]: result\\nOut[860]: \\n            A         B         C\\naaa -0.264438 -1.026059 -0.619500\\nbbb  0.927272  0.302904 -0.032399\\nccc -0.264273 -0.386314 -0.217601\\nddd -0.871858 -0.348382  1.100491\\nBecause there was one fewer column name than the number of data rows, read_table\\ninfers that the first column should be the DataFrame’s index in this special case.\\nThe parser functions have many additional arguments to help you handle the wide\\nvariety of exception file formats that occur (see Table 6-2). For example, you can skip\\nthe first, third, and fourth rows of a file with skiprows:\\nIn [861]: !cat ch06/ex4.csv\\n# hey!\\na,b,c,d,message\\n# just wanted to make things more difficult for you\\n# who reads CSV files with computers, anyway?\\n1,2,3,4,hello\\n5,6,7,8,world\\n9,10,11,12,foo\\nIn [862]: pd.read_csv('ch06/ex4.csv', skiprows=[0, 2, 3])\\nOut[862]: \\n   a   b   c   d message\\n158 | Chapter 6: \\u2002Data Loading, Storage, and File Formats\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"0  1   2   3   4   hello\\n1  5   6   7   8   world\\n2  9  10  11  12     foo\\nHandling missing values is an important and frequently nuanced part of the file parsing\\nprocess. Missing data is usually either not present (empty string) or marked by some \\nsentinel value. By default, pandas uses a set of commonly occurring sentinels, such as\\nNA, -1.#IND, and NULL:\\nIn [863]: !cat ch06/ex5.csv\\nsomething,a,b,c,d,message\\none,1,2,3,4,NA\\ntwo,5,6,,8,world\\nthree,9,10,11,12,foo\\nIn [864]: result = pd.read_csv('ch06/ex5.csv')\\nIn [865]: result\\nOut[865]: \\n  something  a   b   c   d message\\n0       one  1   2   3   4     NaN\\n1       two  5   6 NaN   8   world\\n2     three  9  10  11  12     foo\\nIn [866]: pd.isnull(result)\\nOut[866]: \\n  something      a      b      c      d message\\n0     False  False  False  False  False    True\\n1     False  False  False   True  False   False\\n2     False  False  False  False  False   False\\nThe na_values option can take either a list or set of strings to consider missing values:\\nIn [867]: result = pd.read_csv('ch06/ex5.csv', na_values=['NULL'])\\nIn [868]: result\\nOut[868]: \\n  something  a   b   c   d message\\n0       one  1   2   3   4     NaN\\n1       two  5   6 NaN   8   world\\n2     three  9  10  11  12     foo\\nDifferent NA sentinels can be specified for each column in a dict:\\nIn [869]: sentinels = {'message': ['foo', 'NA'], 'something': ['two']}\\nIn [870]: pd.read_csv('ch06/ex5.csv', na_values=sentinels)\\nOut[870]: \\n  something  a   b   c   d message\\n0       one  1   2   3   4     NaN\\n1       NaN  5   6 NaN   8   world\\n2     three  9  10  11  12     NaN\\nReading and Writing Data in Text Format | 159\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Table 6-2. read_csv /read_table function arguments\\nArgument Description\\npath String indicating filesystem location, URL, or file-like object\\nsep or delimiter Character sequence or regular expression to use to split fields in each row\\nheader Row number to use as column names. Defaults to 0 (first row), but should be None if there is no header\\nrow\\nindex_col Column numbers or names to use as the row index in the result. Can be a single name/number or a list\\nof them for a hierarchical index\\nnames List of column names for result, combine with header=None\\nskiprows Number of rows at beginning of file to ignore or list of row numbers (starting from 0) to skip\\nna_values Sequence of values to replace with NA\\ncomment Character or characters to split comments off the end of lines\\nparse_dates Attempt to parse data to datetime; False by default. If True, will attempt to parse all columns. Otherwise\\ncan specify a list of column numbers or name to parse. If element of list is tuple or list, will combine\\nmultiple columns together and parse to date (for example if date/time split across two columns)\\nkeep_date_col If joining columns to parse date, drop the joined columns. Default True\\nconverters Dict containing column number of name mapping to functions. For example {'foo': f} would apply\\nthe function f to all values in the 'foo' column\\ndayfirst When parsing potentially ambiguous dates, treat as international format (e.g. 7/6/2012 -> June 7,\\n2012). Default False\\ndate_parser Function to use to parse dates\\nnrows Number of rows to read from beginning of file\\niterator Return a TextParser object for reading file piecemeal\\nchunksize For iteration, size of file chunks\\nskip_footer Number of lines to ignore at end of file\\nverbose Print various parser output information, like the number of missing values placed in non-numeric\\ncolumns\\nencoding Text encoding for unicode. For example 'utf-8' for UTF-8 encoded text\\nsqueeze If the parsed data only contains one column return a Series\\nthousands Separator for thousands, e.g. ',' or '.'\\nReading Text Files in Pieces\\nWhen processing very large files or figuring out the right set of arguments to correctly\\nprocess a large file, you may only want to read in a small piece of a file or iterate through\\nsmaller chunks of the file.\\nIn [871]: result = pd.read_csv('ch06/ex6.csv')\\nIn [872]: result\\nOut[872]: \\n160 | Chapter 6: \\u2002Data Loading, Storage, and File Formats\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 10000 entries, 0 to 9999\\nData columns:\\none      10000  non-null values\\ntwo      10000  non-null values\\nthree    10000  non-null values\\nfour     10000  non-null values\\nkey      10000  non-null values\\ndtypes: float64(4), object(1)\\nIf you want to only read out a small number of rows (avoiding reading the entire file),\\nspecify that with nrows:\\nIn [873]: pd.read_csv('ch06/ex6.csv', nrows=5)\\nOut[873]: \\n        one       two     three      four key\\n0  0.467976 -0.038649 -0.295344 -1.824726   L\\n1 -0.358893  1.404453  0.704965 -0.200638   B\\n2 -0.501840  0.659254 -0.421691 -0.057688   G\\n3  0.204886  1.074134  1.388361 -0.982404   R\\n4  0.354628 -0.133116  0.283763 -0.837063   Q\\nTo read out a file in pieces, specify a chunksize as a number of rows:\\nIn [874]: chunker = pd.read_csv('ch06/ex6.csv', chunksize=1000)\\nIn [875]: chunker\\nOut[875]: <pandas.io.parsers.TextParser at 0x8398150>\\nThe TextParser object returned by read_csv allows you to iterate over the parts of the\\nfile according to the chunksize. For example, we can iterate over ex6.csv, aggregating\\nthe value counts in the 'key' column like so:\\nchunker = pd.read_csv('ch06/ex6.csv', chunksize=1000)\\ntot = Series([])\\nfor piece in chunker:\\n    tot = tot.add(piece['key'].value_counts(), fill_value=0)\\ntot = tot.order(ascending=False)\\nWe have then:\\nIn [877]: tot[:10]\\nOut[877]: \\nE    368\\nX    364\\nL    346\\nO    343\\nQ    340\\nM    338\\nJ    337\\nF    335\\nK    334\\nH    330\\nReading and Writing Data in Text Format | 161\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"TextParser is also equipped with a get_chunk method which enables you to read pieces\\nof an arbitrary size.\\nWriting Data Out to Text Format\\nData can also be exported to delimited format. Let’s consider one of the CSV files read\\nabove:\\nIn [878]: data = pd.read_csv('ch06/ex5.csv')\\nIn [879]: data\\nOut[879]: \\n  something  a   b   c   d message\\n0       one  1   2   3   4     NaN\\n1       two  5   6 NaN   8   world\\n2     three  9  10  11  12     foo\\nUsing DataFrame’s to_csv method, we can write the data out to a comma-separated file:\\nIn [880]: data.to_csv('ch06/out.csv')\\nIn [881]: !cat ch06/out.csv\\n,something,a,b,c,d,message\\n0,one,1,2,3.0,4,\\n1,two,5,6,,8,world\\n2,three,9,10,11.0,12,foo\\nOther delimiters can be used, of course (writing to sys.stdout so it just prints the text\\nresult):\\nIn [882]: data.to_csv(sys.stdout, sep='|')\\n|something|a|b|c|d|message\\n0|one|1|2|3.0|4|\\n1|two|5|6||8|world\\n2|three|9|10|11.0|12|foo\\nMissing values appear as empty strings in the output. You might want to denote them\\nby some other sentinel value:\\nIn [883]: data.to_csv(sys.stdout, na_rep='NULL')\\n,something,a,b,c,d,message\\n0,one,1,2,3.0,4,NULL\\n1,two,5,6,NULL,8,world\\n2,three,9,10,11.0,12,foo\\nWith no other options specified, both the row and column labels are written. Both of\\nthese can be disabled:\\nIn [884]: data.to_csv(sys.stdout, index=False, header=False)\\none,1,2,3.0,4,\\ntwo,5,6,,8,world\\nthree,9,10,11.0,12,foo\\nYou can also write only a subset of the columns, and in an order of your choosing:\\n162 | Chapter 6: \\u2002Data Loading, Storage, and File Formats\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'In [885]: data.to_csv(sys.stdout, index=False, cols=[\\'a\\', \\'b\\', \\'c\\'])\\na,b,c\\n1,2,3.0\\n5,6,\\n9,10,11.0\\nSeries also has a to_csv method:\\nIn [886]: dates = pd.date_range(\\'1/1/2000\\', periods=7)\\nIn [887]: ts = Series(np.arange(7), index=dates)\\nIn [888]: ts.to_csv(\\'ch06/tseries.csv\\')\\nIn [889]: !cat ch06/tseries.csv\\n2000-01-01 00:00:00,0\\n2000-01-02 00:00:00,1\\n2000-01-03 00:00:00,2\\n2000-01-04 00:00:00,3\\n2000-01-05 00:00:00,4\\n2000-01-06 00:00:00,5\\n2000-01-07 00:00:00,6\\nWith a bit of wrangling (no header, first column as index), you can read a CSV version\\nof a Series with read_csv, but there is also a from_csv convenience method that makes\\nit a bit simpler:\\nIn [890]: Series.from_csv(\\'ch06/tseries.csv\\', parse_dates=True)\\nOut[890]: \\n2000-01-01    0\\n2000-01-02    1\\n2000-01-03    2\\n2000-01-04    3\\n2000-01-05    4\\n2000-01-06    5\\n2000-01-07    6\\nSee the docstrings for to_csv and from_csv in IPython for more information.\\nManually Working with Delimited Formats\\nMost forms of tabular data can be loaded from disk using functions like pan\\ndas.read_table. In some cases, however, some manual processing may be necessary.\\nIt’s not uncommon to receive a file with one or more malformed lines that trip up \\nread_table. To illustrate the basic tools, consider a small CSV file:\\nIn [891]: !cat ch06/ex7.csv\\n\"a\",\"b\",\"c\"\\n\"1\",\"2\",\"3\"\\n\"1\",\"2\",\"3\",\"4\"\\nFor any file with a single-character delimiter, you can use Python’s built-in csv module.\\nTo use it, pass any open file or file-like object to csv.reader:\\nReading and Writing Data in Text Format | 163\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'import csv\\nf = open(\\'ch06/ex7.csv\\')\\nreader = csv.reader(f)\\nIterating through the reader like a file yields tuples of values in each like with any quote\\ncharacters removed:\\nIn [893]: for line in reader:\\n   .....:     print line\\n[\\'a\\', \\'b\\', \\'c\\']\\n[\\'1\\', \\'2\\', \\'3\\']\\n[\\'1\\', \\'2\\', \\'3\\', \\'4\\']\\nFrom there, it’s up to you to do the wrangling necessary to put the data in the form\\nthat you need it. For example:\\nIn [894]: lines = list(csv.reader(open(\\'ch06/ex7.csv\\')))\\nIn [895]: header, values = lines[0], lines[1:]\\nIn [896]: data_dict = {h: v for h, v in zip(header, zip(*values))}\\nIn [897]: data_dict\\nOut[897]: {\\'a\\': (\\'1\\', \\'1\\'), \\'b\\': (\\'2\\', \\'2\\'), \\'c\\': (\\'3\\', \\'3\\')}\\nCSV files come in many different flavors. Defining a new format with a different de-\\nlimiter, string quoting convention, or line terminator is done by defining a simple sub-\\nclass of csv.Dialect:\\nclass my_dialect(csv.Dialect):\\n    lineterminator = \\'\\\\n\\'\\n    delimiter = \\';\\'\\n    quotechar = \\'\"\\'\\nreader = csv.reader(f, dialect=my_dialect)\\nIndividual CSV dialect parameters can also be given as keywords to csv.reader without\\nhaving to define a subclass:\\nreader = csv.reader(f, delimiter=\\'|\\')\\nThe possible options (attributes of csv.Dialect) and what they do can be found in\\nTable 6-3.\\nTable 6-3. CSV dialect options\\nArgument Description\\ndelimiter One-character string to separate fields. Defaults to \\',\\'.\\nlineterminator Line terminator for writing, defaults to \\'\\\\r\\\\n\\'. Reader ignores this and recognizes\\ncross-platform line terminators.\\nquotechar Quote character for fields with special characters (like a delimiter). Default is \\'\"\\'.\\nquoting Quoting convention. Options include csv.QUOTE_ALL (quote all fields),\\ncsv.QUOTE_MINIMAL (only fields with special characters like the delimiter),\\n164 | Chapter 6: \\u2002Data Loading, Storage, and File Formats\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Argument Description\\ncsv.QUOTE_NONNUMERIC, and csv.QUOTE_NON (no quoting). See Python’s\\ndocumentation for full details. Defaults to QUOTE_MINIMAL.\\nskipinitialspace Ignore whitespace after each delimiter. Default False.\\ndoublequote How to handle quoting character inside a field. If True, it is doubled. See online\\ndocumentation for full detail and behavior.\\nescapechar String to escape the delimiter if quoting is set to csv.QUOTE_NONE. Disabled by\\ndefault\\nFor files with more complicated or fixed multicharacter delimiters, you\\nwill not be able to use the csv module. In those cases, you’ll have to do\\nthe line splitting and other cleanup using string’s split method or the\\nregular expression method re.split.\\nTo write delimited files manually, you can use csv.writer. It accepts an open, writable\\nfile object and the same dialect and format options as csv.reader:\\nwith open(\\'mydata.csv\\', \\'w\\') as f:\\n    writer = csv.writer(f, dialect=my_dialect)\\n    writer.writerow((\\'one\\', \\'two\\', \\'three\\'))\\n    writer.writerow((\\'1\\', \\'2\\', \\'3\\'))\\n    writer.writerow((\\'4\\', \\'5\\', \\'6\\'))\\n    writer.writerow((\\'7\\', \\'8\\', \\'9\\'))\\nJSON Data\\nJSON (short for JavaScript Object Notation) has become one of the standard formats\\nfor sending data by HTTP request between web browsers and other applications. It is\\na much more flexible data format than a tabular text form like CSV. Here is an example:\\nobj = \"\"\"\\n{\"name\": \"Wes\",\\n \"places_lived\": [\"United States\", \"Spain\", \"Germany\"],\\n \"pet\": null,\\n \"siblings\": [{\"name\": \"Scott\", \"age\": 25, \"pet\": \"Zuko\"},\\n              {\"name\": \"Katie\", \"age\": 33, \"pet\": \"Cisco\"}]\\n}\\n\"\"\"\\nJSON is very nearly valid Python code with the exception of its null value null and\\nsome other nuances (such as disallowing trailing commas at the end of lists). The basic\\ntypes are objects (dicts), arrays (lists), strings, numbers, booleans, and nulls. All of the\\nkeys in an object must be strings. There are several Python libraries for reading and\\nwriting JSON data. I’ll use json here as it is built into the Python standard library. To\\nconvert a JSON string to Python form, use json.loads:\\nIn [899]: import json\\nReading and Writing Data in Text Format | 165\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [900]: result = json.loads(obj)\\nIn [901]: result\\nOut[901]: \\n{u'name': u'Wes',\\n u'pet': None,\\n u'places_lived': [u'United States', u'Spain', u'Germany'],\\n u'siblings': [{u'age': 25, u'name': u'Scott', u'pet': u'Zuko'},\\n  {u'age': 33, u'name': u'Katie', u'pet': u'Cisco'}]}\\njson.dumps on the other hand converts a Python object back to JSON:\\nIn [902]: asjson = json.dumps(result)\\nHow you convert a JSON object or list of objects to a DataFrame or some other data\\nstructure for analysis will be up to you. Conveniently, you can pass a list of JSON objects\\nto the DataFrame constructor and select a subset of the data fields:\\nIn [903]: siblings = DataFrame(result['siblings'], columns=['name', 'age'])\\nIn [904]: siblings\\nOut[904]: \\n    name  age\\n0  Scott   25\\n1  Katie   33\\nFor an extended example of reading and manipulating JSON data (including nested\\nrecords), see the USDA Food Database example in the next chapter.\\nAn effort is underway to add fast native JSON export ( to_json) and\\ndecoding (from_json) to pandas. This was not ready at the time of writ-\\ning.\\nXML and HTML: Web Scraping\\nPython has many libraries for reading and writing data in the ubiquitous HTML and\\nXML formats. lxml (http://lxml.de) is one that has consistently strong performance in\\nparsing very large files. lxml has multiple programmer interfaces; first I’ll show using \\nlxml.html for HTML, then parse some XML using lxml.objectify.\\nMany websites make data available in HTML tables for viewing in a browser, but not\\ndownloadable as an easily machine-readable format like JSON, HTML, or XML. I no-\\nticed that this was the case with Yahoo! Finance’s stock options data. If you aren’t\\nfamiliar with this data; options are derivative contracts giving you the right to buy\\n(call option) or sell ( put option) a company’s stock at some particular price (the\\nstrike) between now and some fixed point in the future (the expiry). People trade both\\ncall and put options across many strikes and expiries; this data can all be found together\\nin tables on Yahoo! Finance.\\n166 | Chapter 6: \\u2002Data Loading, Storage, and File Formats\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"To get started, find the URL you want to extract data from, open it with urllib2 and\\nparse the stream with lxml like so:\\nfrom lxml.html import parse\\nfrom urllib2 import urlopen\\nparsed = parse(urlopen('http://finance.yahoo.com/q/op?s=AAPL+Options'))\\ndoc = parsed.getroot()\\nUsing this object, you can extract all HTML tags of a particular type, such as table tags\\ncontaining the data of interest. As a simple motivating example, suppose you wanted\\nto get a list of every URL linked to in the document; links are a tags in HTML. Using\\nthe document root’s findall method along with an XPath (a means of expressing\\n“queries” on the document):\\nIn [906]: links = doc.findall('.//a')\\nIn [907]: links[15:20]\\nOut[907]: \\n[<Element a at 0x6c488f0>,\\n <Element a at 0x6c48950>,\\n <Element a at 0x6c489b0>,\\n <Element a at 0x6c48a10>,\\n <Element a at 0x6c48a70>]\\nBut these are objects representing HTML elements; to get the URL and link text you\\nhave to use each element’s get method (for the URL) and text_content method (for\\nthe display text):\\nIn [908]: lnk = links[28]\\nIn [909]: lnk\\nOut[909]: <Element a at 0x6c48dd0>\\nIn [910]: lnk.get('href')\\nOut[910]: 'http://biz.yahoo.com/special.html'\\nIn [911]: lnk.text_content()\\nOut[911]: 'Special Editions'\\nThus, getting a list of all URLs in the document is a matter of writing this list compre-\\nhension:\\nIn [912]: urls = [lnk.get('href') for lnk in doc.findall('.//a')]\\nIn [913]: urls[-10:]\\nOut[913]: \\n['http://info.yahoo.com/privacy/us/yahoo/finance/details.html',\\n 'http://info.yahoo.com/relevantads/',\\n 'http://docs.yahoo.com/info/terms/',\\n 'http://docs.yahoo.com/info/copyright/copyright.html',\\n 'http://help.yahoo.com/l/us/yahoo/finance/forms_index.html',\\n 'http://help.yahoo.com/l/us/yahoo/finance/quotes/fitadelay.html',\\n 'http://help.yahoo.com/l/us/yahoo/finance/quotes/fitadelay.html',\\nReading and Writing Data in Text Format | 167\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"'http://www.capitaliq.com',\\n 'http://www.csidata.com',\\n 'http://www.morningstar.com/']\\nNow, finding the right tables in the document can be a matter of trial and error; some\\nwebsites make it easier by giving a table of interest an id attribute. I determined that\\nthese were the two tables containing the call data and put data, respectively:\\ntables = doc.findall('.//table')\\ncalls = tables[9]\\nputs = tables[13]\\nEach table has a header row followed by each of the data rows:\\nIn [915]: rows = calls.findall('.//tr')\\nFor the header as well as the data rows, we want to extract the text from each cell; in\\nthe case of the header these are th cells and td cells for the data:\\ndef _unpack(row, kind='td'):\\n    elts = row.findall('.//%s' % kind)\\n    return [val.text_content() for val in elts]\\nThus, we obtain:\\nIn [917]: _unpack(rows[0], kind='th')\\nOut[917]: ['Strike', 'Symbol', 'Last', 'Chg', 'Bid', 'Ask', 'Vol', 'Open Int']\\nIn [918]: _unpack(rows[1], kind='td')\\nOut[918]: \\n['295.00',\\n 'AAPL120818C00295000',\\n '310.40',\\n ' 0.00',\\n '289.80',\\n '290.80',\\n '1',\\n '169']\\nNow, it’s a matter of combining all of these steps together to convert this data into a\\nDataFrame. Since the numerical data is still in string format, we want to convert some,\\nbut perhaps not all of the columns to floating point format. You could do this by hand,\\nbut, luckily, pandas has a class TextParser that is used internally in the read_csv and\\nother parsing functions to do the appropriate automatic type conversion:\\nfrom pandas.io.parsers import TextParser\\ndef parse_options_data(table):\\n    rows = table.findall('.//tr')\\n    header = _unpack(rows[0], kind='th')\\n    data = [_unpack(r) for r in rows[1:]]\\n    return TextParser(data, names=header).get_chunk()\\nFinally, we invoke this parsing function on the lxml table objects and get DataFrame\\nresults:\\n168 | Chapter 6: \\u2002Data Loading, Storage, and File Formats\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'In [920]: call_data = parse_options_data(calls)\\nIn [921]: put_data = parse_options_data(puts)\\nIn [922]: call_data[:10]\\nOut[922]: \\n   Strike               Symbol    Last  Chg     Bid     Ask  Vol Open Int\\n0     295  AAPL120818C00295000  310.40  0.0  289.80  290.80    1      169\\n1     300  AAPL120818C00300000  277.10  1.7  284.80  285.60    2      478\\n2     305  AAPL120818C00305000  300.97  0.0  279.80  280.80   10      316\\n3     310  AAPL120818C00310000  267.05  0.0  274.80  275.65    6      239\\n4     315  AAPL120818C00315000  296.54  0.0  269.80  270.80   22       88\\n5     320  AAPL120818C00320000  291.63  0.0  264.80  265.80   96      173\\n6     325  AAPL120818C00325000  261.34  0.0  259.80  260.80  N/A      108\\n7     330  AAPL120818C00330000  230.25  0.0  254.80  255.80  N/A       21\\n8     335  AAPL120818C00335000  266.03  0.0  249.80  250.65    4       46\\n9     340  AAPL120818C00340000  272.58  0.0  244.80  245.80    4       30\\nParsing XML with lxml.objectify\\nXML (extensible markup language) is another common structured data format sup-\\nporting hierarchical, nested data with metadata. The files that generate the book you\\nare reading actually form a series of large XML documents.\\nAbove, I showed the lxml library and its lxml.html interface. Here I show an alternate\\ninterface that’s convenient for XML data, lxml.objectify.\\nThe New York Metropolitan Transportation Authority (MTA) publishes a number of\\ndata series about its bus and train services ( http://www.mta.info/developers/download\\n.html). Here we’ll look at the performance data which is contained in a set of XML files.\\nEach train or bus service has a different file (like Performance_MNR.xml for the Metro-\\nNorth Railroad) containing monthly data as a series of XML records that look like this:\\n<INDICATOR>\\n  <INDICATOR_SEQ>373889</INDICATOR_SEQ>\\n  <PARENT_SEQ></PARENT_SEQ>\\n  <AGENCY_NAME>Metro-North Railroad</AGENCY_NAME>\\n  <INDICATOR_NAME>Escalator Availability</INDICATOR_NAME>\\n  <DESCRIPTION>Percent of the time that escalators are operational\\n  systemwide. The availability rate is based on physical observations performed\\n  the morning of regular business days only. This is a new indicator the agency\\n  began reporting in 2009.</DESCRIPTION>\\n  <PERIOD_YEAR>2011</PERIOD_YEAR>\\n  <PERIOD_MONTH>12</PERIOD_MONTH>\\n  <CATEGORY>Service Indicators</CATEGORY>\\n  <FREQUENCY>M</FREQUENCY>\\n  <DESIRED_CHANGE>U</DESIRED_CHANGE>\\n  <INDICATOR_UNIT>%</INDICATOR_UNIT>\\n  <DECIMAL_PLACES>1</DECIMAL_PLACES>\\n  <YTD_TARGET>97.00</YTD_TARGET>\\n  <YTD_ACTUAL></YTD_ACTUAL>\\n  <MONTHLY_TARGET>97.00</MONTHLY_TARGET>\\n  <MONTHLY_ACTUAL></MONTHLY_ACTUAL>\\n</INDICATOR>\\nReading and Writing Data in Text Format | 169\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Using lxml.objectify, we parse the file and get a reference to the root node of the XML\\nfile with getroot:\\nfrom lxml import objectify\\npath = \\'Performance_MNR.xml\\'\\nparsed = objectify.parse(open(path))\\nroot = parsed.getroot()\\nroot.INDICATOR return a generator yielding each <INDICATOR> XML element. For each\\nrecord, we can populate a dict of tag names (like YTD_ACTUAL) to data values (excluding\\na few tags):\\ndata = []\\nskip_fields = [\\'PARENT_SEQ\\', \\'INDICATOR_SEQ\\',\\n               \\'DESIRED_CHANGE\\', \\'DECIMAL_PLACES\\']\\nfor elt in root.INDICATOR:\\n    el_data = {}\\n    for child in elt.getchildren():\\n        if child.tag in skip_fields:\\n            continue\\n        el_data[child.tag] = child.pyval\\n    data.append(el_data)\\nLastly, convert this list of dicts into a DataFrame:\\nIn [927]: perf = DataFrame(data)\\nIn [928]: perf\\nOut[928]: \\nEmpty DataFrame\\nColumns: array([], dtype=int64)\\nIndex: array([], dtype=int64)\\nXML data can get much more complicated than this example. Each tag can have met-\\nadata, too. Consider an HTML link tag which is also valid XML:\\nfrom StringIO import StringIO\\ntag = \\'<a href=\"http://www.google.com\">Google</a>\\'\\nroot = objectify.parse(StringIO(tag)).getroot()\\nYou can now access any of the fields (like href) in the tag or the link text:\\nIn [930]: root\\nOut[930]: <Element a at 0x88bd4b0>\\nIn [931]: root.get(\\'href\\')\\nOut[931]: \\'http://www.google.com\\'\\nIn [932]: root.text\\nOut[932]: \\'Google\\'\\n170 | Chapter 6: \\u2002Data Loading, Storage, and File Formats\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Binary Data Formats\\nOne of the easiest ways to store data efficiently in binary format is using Python’s built-\\nin pickle serialization. Conveniently, pandas objects all have a save method which\\nwrites the data to disk as a pickle:\\nIn [933]: frame = pd.read_csv('ch06/ex1.csv')\\nIn [934]: frame\\nOut[934]: \\n   a   b   c   d message\\n0  1   2   3   4   hello\\n1  5   6   7   8   world\\n2  9  10  11  12     foo\\nIn [935]: frame.save('ch06/frame_pickle')\\nYou read the data back into Python with pandas.load, another pickle convenience\\nfunction:\\nIn [936]: pd.load('ch06/frame_pickle')\\nOut[936]: \\n   a   b   c   d message\\n0  1   2   3   4   hello\\n1  5   6   7   8   world\\n2  9  10  11  12     foo\\npickle is only recommended as a short-term storage format. The prob-\\nlem is that it is hard to guarantee that the format will be stable over time;\\nan object pickled today may not unpickle with a later version of a library.\\nI have made every effort to ensure that this does not occur with pandas,\\nbut at some point in the future it may be necessary to “break” the pickle\\nformat.\\nUsing HDF5 Format\\nThere are a number of tools that facilitate efficiently reading and writing large amounts\\nof scientific data in binary format on disk. A popular industry-grade library for this is\\nHDF5, which is a C library with interfaces in many other languages like Java, Python,\\nand MATLAB. The “HDF” in HDF5 stands for hierarchical data format. Each HDF5\\nfile contains an internal file system-like node structure enabling you to store multiple\\ndatasets and supporting metadata. Compared with simpler formats, HDF5 supports\\non-the-fly compression with a variety of compressors, enabling data with repeated pat-\\nterns to be stored more efficiently. For very large datasets that don’t fit into memory,\\nHDF5 is a good choice as you can efficiently read and write small sections of much\\nlarger arrays.\\nThere are not one but two interfaces to the HDF5 library in Python, PyTables and h5py,\\neach of which takes a different approach to the problem. h5py provides a direct, but\\nhigh-level interface to the HDF5 API, while PyTables abstracts many of the details of\\nBinary Data Formats | 171\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"HDF5 to provide multiple flexible data containers, table indexing, querying capability,\\nand some support for out-of-core computations.\\npandas has a minimal dict-like HDFStore class, which uses PyTables to store pandas\\nobjects:\\nIn [937]: store = pd.HDFStore('mydata.h5')\\nIn [938]: store['obj1'] = frame\\nIn [939]: store['obj1_col'] = frame['a']\\nIn [940]: store\\nOut[940]: \\n<class 'pandas.io.pytables.HDFStore'>\\nFile path: mydata.h5\\nobj1         DataFrame\\nobj1_col     Series\\nObjects contained in the HDF5 file can be retrieved in a dict-like fashion:\\nIn [941]: store['obj1']\\nOut[941]: \\n   a   b   c   d message\\n0  1   2   3   4   hello\\n1  5   6   7   8   world\\n2  9  10  11  12     foo\\nIf you work with huge quantities of data, I would encourage you to explore PyTables\\nand h5py to see how they can suit your needs. Since many data analysis problems are\\nIO-bound (rather than CPU-bound), using a tool like HDF5 can massively accelerate\\nyour applications.\\nHDF5 is not a database. It is best suited for write-once, read-many da-\\ntasets. While data can be added to a file at any time, if multiple writers\\ndo so simultaneously, the file can become corrupted.\\nReading Microsoft Excel Files\\npandas also supports reading tabular data stored in Excel 2003 (and higher) files using\\nthe ExcelFile class. Interally ExcelFile uses the xlrd and openpyxl packages, so you\\nmay have to install them first. To use ExcelFile, create an instance by passing a path\\nto an xls or xlsx file:\\nxls_file = pd.ExcelFile('data.xls')\\nData stored in a sheet can then be read into DataFrame using parse:\\ntable = xls_file.parse('Sheet1')\\n172 | Chapter 6: \\u2002Data Loading, Storage, and File Formats\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Interacting with HTML and Web APIs\\nMany websites have public APIs providing data feeds via JSON or some other format.\\nThere are a number of ways to access these APIs from Python; one easy-to-use method\\nthat I recommend is the requests package ( http://docs.python-requests.org). To search\\nfor the words “python pandas” on Twitter, we can make an HTTP GET request like so:\\nIn [944]: import requests\\nIn [945]: url = \\'http://search.twitter.com/search.json?q=python%20pandas\\'\\nIn [946]: resp = requests.get(url)\\nIn [947]: resp\\nOut[947]: <Response [200]>\\nThe Response object’s text attribute contains the content of the GET query. Many web\\nAPIs will return a JSON string that must be loaded into a Python object:\\nIn [948]: import json\\nIn [949]: data = json.loads(resp.text)\\nIn [950]: data.keys()\\nOut[950]: \\n[u\\'next_page\\',\\n u\\'completed_in\\',\\n u\\'max_id_str\\',\\n u\\'since_id_str\\',\\n u\\'refresh_url\\',\\n u\\'results\\',\\n u\\'since_id\\',\\n u\\'results_per_page\\',\\n u\\'query\\',\\n u\\'max_id\\',\\n u\\'page\\']\\nThe results field in the response contains a list of tweets, each of which is represented\\nas a Python dict that looks like:\\n{u\\'created_at\\': u\\'Mon, 25 Jun 2012 17:50:33 +0000\\',\\n u\\'from_user\\': u\\'wesmckinn\\',\\n u\\'from_user_id\\': 115494880,\\n u\\'from_user_id_str\\': u\\'115494880\\',\\n u\\'from_user_name\\': u\\'Wes McKinney\\',\\n u\\'geo\\': None,\\n u\\'id\\': 217313849177686018,\\n u\\'id_str\\': u\\'217313849177686018\\',\\n u\\'iso_language_code\\': u\\'pt\\',\\n u\\'metadata\\': {u\\'result_type\\': u\\'recent\\'},\\n u\\'source\\': u\\'<a href=\"http://twitter.com/\">web</a>\\',\\n u\\'text\\': u\\'Lunchtime pandas-fu http://t.co/SI70xZZQ #pydata\\',\\n u\\'to_user\\': None,\\n u\\'to_user_id\\': 0,\\nInteracting with HTML and Web APIs | 173\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'u\\'to_user_id_str\\': u\\'0\\',\\n u\\'to_user_name\\': None}\\nWe can then make a list of the tweet fields of interest then pass the results list to Da-\\ntaFrame:\\nIn [951]: tweet_fields = [\\'created_at\\', \\'from_user\\', \\'id\\', \\'text\\']\\nIn [952]: tweets = DataFrame(data[\\'results\\'], columns=tweet_fields)\\nIn [953]: tweets\\nOut[953]: \\n<class \\'pandas.core.frame.DataFrame\\'>\\nInt64Index: 15 entries, 0 to 14\\nData columns:\\ncreated_at    15  non-null values\\nfrom_user     15  non-null values\\nid            15  non-null values\\ntext          15  non-null values\\ndtypes: int64(1), object(3)\\nEach row in the DataFrame now has the extracted data from each tweet:\\nIn [121]: tweets.ix[7]\\nOut[121]:\\ncreated_at                  Thu, 23 Jul 2012 09:54:00 +0000\\nfrom_user                                           deblike\\nid                                       227419585803059201\\ntext          pandas: powerful Python data analysis toolkit\\nName: 7\\nWith a bit of elbow grease, you can create some higher-level interfaces to common web\\nAPIs that return DataFrame objects for easy analysis.\\nInteracting with Databases\\nIn many applications data rarely comes from text files, that being a fairly inefficient\\nway to store large amounts of data. SQL-based relational databases (such as SQL Server,\\nPostgreSQL, and MySQL) are in wide use, and many alternative non-SQL (so-called\\nNoSQL) databases have become quite popular. The choice of database is usually de-\\npendent on the performance, data integrity, and scalability needs of an application.\\nLoading data from SQL into a DataFrame is fairly straightforward, and pandas has\\nsome functions to simplify the process. As an example, I’ll use an in-memory SQLite\\ndatabase using Python’s built-in sqlite3 driver:\\nimport sqlite3\\nquery = \"\"\"\\nCREATE TABLE test\\n(a VARCHAR(20), b VARCHAR(20),\\n c REAL,        d INTEGER\\n);\"\"\"\\n174 | Chapter 6: \\u2002Data Loading, Storage, and File Formats\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'con = sqlite3.connect(\\':memory:\\')\\ncon.execute(query)\\ncon.commit()\\nThen, insert a few rows of data:\\ndata = [(\\'Atlanta\\', \\'Georgia\\', 1.25, 6),\\n        (\\'Tallahassee\\', \\'Florida\\', 2.6, 3),\\n        (\\'Sacramento\\', \\'California\\', 1.7, 5)]\\nstmt = \"INSERT INTO test VALUES(?, ?, ?, ?)\"\\ncon.executemany(stmt, data)\\ncon.commit()\\nMost Python SQL drivers (PyODBC, psycopg2, MySQLdb, pymssql, etc.) return a list\\nof tuples when selecting data from a table:\\nIn [956]: cursor = con.execute(\\'select * from test\\')\\nIn [957]: rows = cursor.fetchall()\\nIn [958]: rows\\nOut[958]: \\n[(u\\'Atlanta\\', u\\'Georgia\\', 1.25, 6),\\n (u\\'Tallahassee\\', u\\'Florida\\', 2.6, 3),\\n (u\\'Sacramento\\', u\\'California\\', 1.7, 5)]\\nYou can pass the list of tuples to the DataFrame constructor, but you also need the\\ncolumn names, contained in the cursor’s description attribute:\\nIn [959]: cursor.description\\nOut[959]: \\n((\\'a\\', None, None, None, None, None, None),\\n (\\'b\\', None, None, None, None, None, None),\\n (\\'c\\', None, None, None, None, None, None),\\n (\\'d\\', None, None, None, None, None, None))\\nIn [960]: DataFrame(rows, columns=zip(*cursor.description)[0])\\nOut[960]: \\n             a           b     c  d\\n0      Atlanta     Georgia  1.25  6\\n1  Tallahassee     Florida  2.60  3\\n2   Sacramento  California  1.70  5\\nThis is quite a bit of munging that you’d rather not repeat each time you query the\\ndatabase. pandas has a read_frame function in its pandas.io.sql module that simplifies\\nthe process. Just pass the select statement and the connection object:\\nIn [961]: import pandas.io.sql as sql\\nIn [962]: sql.read_frame(\\'select * from test\\', con)\\nOut[962]: \\n             a           b     c  d\\n0      Atlanta     Georgia  1.25  6\\n1  Tallahassee     Florida  2.60  3\\n2   Sacramento  California  1.70  5\\nInteracting with Databases | 175\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Storing and Loading Data in MongoDB\\nNoSQL databases take many different forms. Some are simple dict-like key-value stores\\nlike BerkeleyDB or Tokyo Cabinet, while others are document-based, with a dict-like\\nobject being the basic unit of storage. I've chosen MongoDB ( http://mongodb.org) for\\nmy example. I started a MongoDB instance locally on my machine, and connect to it\\non the default port using pymongo, the official driver for MongoDB:\\nimport pymongo\\ncon = pymongo.Connection('localhost', port=27017)\\nDocuments stored in MongoDB are found in collections inside databases. Each running\\ninstance of the MongoDB server can have multiple databases, and each database can\\nhave multiple collections. Suppose I wanted to store the Twitter API data from earlier\\nin the chapter. First, I can access the (currently empty) tweets collection:\\ntweets = con.db.tweets\\nThen, I load the list of tweets and write each of them to the collection using\\ntweets.save (which writes the Python dict to MongoDB):\\nimport requests, json\\nurl = 'http://search.twitter.com/search.json?q=python%20pandas'\\ndata = json.loads(requests.get(url).text)\\nfor tweet in data['results']:\\n    tweets.save(tweet)\\nNow, if I wanted to get all of my tweets (if any) from the collection, I can query the\\ncollection with the following syntax:\\ncursor = tweets.find({'from_user': 'wesmckinn'})\\nThe cursor returned is an iterator that yields each document as a dict. As above I can\\nconvert this into a DataFrame, optionally extracting a subset of the data fields in each\\ntweet:\\ntweet_fields = ['created_at', 'from_user', 'id', 'text']\\nresult = DataFrame(list(cursor), columns=tweet_fields)\\n176 | Chapter 6: \\u2002Data Loading, Storage, and File Formats\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'CHAPTER 7\\nData Wrangling: Clean, Transform,\\nMerge, Reshape\\nMuch of the programming work in data analysis and modeling is spent on data prep-\\naration: loading, cleaning, transforming, and rearranging. Sometimes the way that data\\nis stored in files or databases is not the way you need it for a data processing application.\\nMany people choose to do ad hoc processing of data from one form to another using\\na general purpose programming, like Python, Perl, R, or Java, or UNIX text processing\\ntools like sed or awk. Fortunately, pandas along with the Python standard library pro-\\nvide you with a high-level, flexible, and high-performance set of core manipulations\\nand algorithms to enable you to wrangle data into the right form without much trouble.\\nIf you identify a type of data manipulation that isn’t anywhere in this book or elsewhere\\nin the pandas library, feel free to suggest it on the mailing list or GitHub site. Indeed,\\nmuch of the design and implementation of pandas has been driven by the needs of real\\nworld applications.\\nCombining and Merging Data Sets\\nData contained in pandas objects can be combined together in a number of built-in\\nways:\\n• pandas.merge connects rows in DataFrames based on one or more keys. This will\\nbe familiar to users of SQL or other relational databases, as it implements database\\njoin operations.\\n• pandas.concat glues or stacks together objects along an axis.\\n• combine_first instance method enables splicing together overlapping data to fill\\nin missing values in one object with values from another.\\nI will address each of these and give a number of examples. They’ll be utilized in ex-\\namples throughout the rest of the book.\\n177\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Database-style DataFrame Merges\\nMerge or join operations combine data sets by linking rows using one or more keys.\\nThese operations are central to relational databases. The merge function in pandas is\\nthe main entry point for using these algorithms on your data.\\nLet’s start with a simple example:\\nIn [15]: df1 = DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'a', 'b'],\\n   ....:                  'data1': range(7)})\\nIn [16]: df2 = DataFrame({'key': ['a', 'b', 'd'],\\n   ....:                  'data2': range(3)})\\nIn [17]: df1        In [18]: df2\\nOut[17]:            Out[18]:\\n   data1 key           data2 key\\n0      0   b        0      0   a\\n1      1   b        1      1   b\\n2      2   a        2      2   d\\n3      3   c\\n4      4   a\\n5      5   a\\n6      6   b\\nThis is an example of a many-to-one merge situation; the data in df1 has multiple rows\\nlabeled a and b, whereas df2 has only one row for each value in the key column. Calling\\nmerge with these objects we obtain:\\nIn [19]: pd.merge(df1, df2)\\nOut[19]:\\n   data1 key  data2\\n0      2   a      0\\n1      4   a      0\\n2      5   a      0\\n3      0   b      1\\n4      1   b      1\\n5      6   b      1\\nNote that I didn’t specify which column to join on. If not specified, merge uses the\\noverlapping column names as the keys. It’s a good practice to specify explicitly, though:\\nIn [20]: pd.merge(df1, df2, on='key')\\nOut[20]:\\n   data1 key  data2\\n0      2   a      0\\n1      4   a      0\\n2      5   a      0\\n3      0   b      1\\n4      1   b      1\\n5      6   b      1\\nIf the column names are different in each object, you can specify them separately:\\nIn [21]: df3 = DataFrame({'lkey': ['b', 'b', 'a', 'c', 'a', 'a', 'b'],\\n   ....:                  'data1': range(7)})\\n178 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [22]: df4 = DataFrame({'rkey': ['a', 'b', 'd'],\\n   ....:                  'data2': range(3)})\\nIn [23]: pd.merge(df3, df4, left_on='lkey', right_on='rkey')\\nOut[23]:\\n   data1 lkey  data2 rkey\\n0      2    a      0    a\\n1      4    a      0    a\\n2      5    a      0    a\\n3      0    b      1    b\\n4      1    b      1    b\\n5      6    b      1    b\\nYou probably noticed that the 'c' and 'd' values and associated data are missing from\\nthe result. By default merge does an 'inner' join; the keys in the result are the intersec-\\ntion. Other possible options are 'left', 'right', and 'outer'. The outer join takes the\\nunion of the keys, combining the effect of applying both left and right joins:\\nIn [24]: pd.merge(df1, df2, how='outer')\\nOut[24]:\\n   data1 key  data2\\n0      2   a      0\\n1      4   a      0\\n2      5   a      0\\n3      0   b      1\\n4      1   b      1\\n5      6   b      1\\n6      3   c    NaN\\n7    NaN   d      2\\nMany-to-many merges have well-defined though not necessarily intuitive behavior.\\nHere’s an example:\\nIn [25]: df1 = DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'b'],\\n   ....:                  'data1': range(6)})\\nIn [26]: df2 = DataFrame({'key': ['a', 'b', 'a', 'b', 'd'],\\n   ....:                  'data2': range(5)})\\nIn [27]: df1        In [28]: df2\\nOut[27]:            Out[28]:\\n   data1 key           data2 key\\n0      0   b        0      0   a\\n1      1   b        1      1   b\\n2      2   a        2      2   a\\n3      3   c        3      3   b\\n4      4   a        4      4   d\\n5      5   b\\nIn [29]: pd.merge(df1, df2, on='key', how='left')\\nOut[29]:\\n    data1 key  data2\\n0       2   a      0\\n1       2   a      2\\nCombining and Merging Data Sets | 179\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"2       4   a      0\\n3       4   a      2\\n4       0   b      1\\n5       0   b      3\\n6       1   b      1\\n7       1   b      3\\n8       5   b      1\\n9       5   b      3\\n10      3   c    NaN\\nMany-to-many joins form the Cartesian product of the rows. Since there were 3 'b'\\nrows in the left DataFrame and 2 in the right one, there are 6 'b' rows in the result.\\nThe join method only affects the distinct key values appearing in the result:\\nIn [30]: pd.merge(df1, df2, how='inner')\\nOut[30]:\\n   data1 key  data2\\n0      2   a      0\\n1      2   a      2\\n2      4   a      0\\n3      4   a      2\\n4      0   b      1\\n5      0   b      3\\n6      1   b      1\\n7      1   b      3\\n8      5   b      1\\n9      5   b      3\\nTo merge with multiple keys, pass a list of column names:\\nIn [31]: left = DataFrame({'key1': ['foo', 'foo', 'bar'],\\n   ....:                   'key2': ['one', 'two', 'one'],\\n   ....:                   'lval': [1, 2, 3]})\\nIn [32]: right = DataFrame({'key1': ['foo', 'foo', 'bar', 'bar'],\\n   ....:                    'key2': ['one', 'one', 'one', 'two'],\\n   ....:                    'rval': [4, 5, 6, 7]})\\nIn [33]: pd.merge(left, right, on=['key1', 'key2'], how='outer')\\nOut[33]:\\n  key1 key2  lval  rval\\n0  bar  one     3     6\\n1  bar  two   NaN     7\\n2  foo  one     1     4\\n3  foo  one     1     5\\n4  foo  two     2   NaN\\nTo determine which key combinations will appear in the result depending on the choice\\nof merge method, think of the multiple keys as forming an array of tuples to be used\\nas a single join key (even though it’s not actually implemented that way).\\nWhen joining columns-on-columns, the indexes on the passed Data-\\nFrame objects are discarded.\\n180 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"A last issue to consider in merge operations is the treatment of overlapping column\\nnames. While you can address the overlap manually (see the later section on renaming\\naxis labels), merge has a suffixes option for specifying strings to append to overlapping\\nnames in the left and right DataFrame objects:\\nIn [34]: pd.merge(left, right, on='key1')\\nOut[34]:\\n  key1 key2_x  lval key2_y  rval\\n0  bar    one     3    one     6\\n1  bar    one     3    two     7\\n2  foo    one     1    one     4\\n3  foo    one     1    one     5\\n4  foo    two     2    one     4\\n5  foo    two     2    one     5\\nIn [35]: pd.merge(left, right, on='key1', suffixes=('_left', '_right'))\\nOut[35]:\\n  key1 key2_left  lval key2_right  rval\\n0  bar       one     3        one     6\\n1  bar       one     3        two     7\\n2  foo       one     1        one     4\\n3  foo       one     1        one     5\\n4  foo       two     2        one     4\\n5  foo       two     2        one     5\\nSee Table 7-1 for an argument reference on merge. Joining on index is the subject of the\\nnext section.\\nTable 7-1. merge function arguments\\nArgument Description\\nleft DataFrame to be merged on the left side\\nright DataFrame to be merged on the right side\\nhow One of 'inner', 'outer', 'left' or 'right'. 'inner' by default\\non Column names to join on. Must be found in both DataFrame objects. If not specified and no other join keys\\ngiven, will use the intersection of the column names in left and right as the join keys\\nleft_on Columns in left DataFrame to use as join keys\\nright_on Analogous to left_on for left DataFrame\\nleft_index Use row index in left as its join key (or keys, if a MultiIndex)\\nright_index Analogous to left_index\\nsort Sort merged data lexicographically by join keys; True by default. Disable to get better performance in some\\ncases on large datasets\\nsuffixes Tuple of string values to append to column names in case of overlap; defaults to ('_x', '_y'). For\\nexample, if 'data' in both DataFrame objects, would appear as 'data_x' and 'data_y' in result\\ncopy If False, avoid copying data into resulting data structure in some exceptional cases. By default always copies\\nCombining and Merging Data Sets | 181\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Merging on Index\\nIn some cases, the merge key or keys in a DataFrame will be found in its index. In this\\ncase, you can pass left_index=True or right_index=True (or both) to indicate that the\\nindex should be used as the merge key:\\nIn [36]: left1 = DataFrame({'key': ['a', 'b', 'a', 'a', 'b', 'c'],\\n   ....:                   'value': range(6)})\\nIn [37]: right1 = DataFrame({'group_val': [3.5, 7]}, index=['a', 'b'])\\nIn [38]: left1        In [39]: right1\\nOut[38]:              Out[39]:\\n  key  value             group_val\\n0   a      0          a        3.5\\n1   b      1          b        7.0\\n2   a      2\\n3   a      3\\n4   b      4\\n5   c      5\\nIn [40]: pd.merge(left1, right1, left_on='key', right_index=True)\\nOut[40]:\\n  key  value  group_val\\n0   a      0        3.5\\n2   a      2        3.5\\n3   a      3        3.5\\n1   b      1        7.0\\n4   b      4        7.0\\nSince the default merge method is to intersect the join keys, you can instead form the\\nunion of them with an outer join:\\nIn [41]: pd.merge(left1, right1, left_on='key', right_index=True, how='outer')\\nOut[41]:\\n  key  value  group_val\\n0   a      0        3.5\\n2   a      2        3.5\\n3   a      3        3.5\\n1   b      1        7.0\\n4   b      4        7.0\\n5   c      5        NaN\\nWith hierarchically-indexed data, things are a bit more complicated:\\nIn [42]: lefth = DataFrame({'key1': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'],\\n   ....:                    'key2': [2000, 2001, 2002, 2001, 2002],\\n   ....:                    'data': np.arange(5.)})\\nIn [43]: righth = DataFrame(np.arange(12).reshape((6, 2)),\\n   ....:                    index=[['Nevada', 'Nevada', 'Ohio', 'Ohio', 'Ohio', 'Ohio'],\\n   ....:                           [2001, 2000, 2000, 2000, 2001, 2002]],\\n   ....:                    columns=['event1', 'event2'])\\nIn [44]: lefth               In [45]: righth\\nOut[44]:                     Out[45]:\\n182 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"data    key1  key2                     event1  event2\\n0     0    Ohio  2000        Nevada 2001       0       1\\n1     1    Ohio  2001               2000       2       3\\n2     2    Ohio  2002        Ohio   2000       4       5\\n3     3  Nevada  2001               2000       6       7\\n4     4  Nevada  2002               2001       8       9\\n                                    2002      10      11\\nIn this case, you have to indicate multiple columns to merge on as a list (pay attention\\nto the handling of duplicate index values):\\nIn [46]: pd.merge(lefth, righth, left_on=['key1', 'key2'], right_index=True)\\nOut[46]:\\n   data    key1  key2  event1  event2\\n3     3  Nevada  2001       0       1\\n0     0    Ohio  2000       4       5\\n0     0    Ohio  2000       6       7\\n1     1    Ohio  2001       8       9\\n2     2    Ohio  2002      10      11\\nIn [47]: pd.merge(lefth, righth, left_on=['key1', 'key2'],\\n   ....:          right_index=True, how='outer')\\nOut[47]:\\n   data    key1  key2  event1  event2\\n4   NaN  Nevada  2000       2       3\\n3     3  Nevada  2001       0       1\\n4     4  Nevada  2002     NaN     NaN\\n0     0    Ohio  2000       4       5\\n0     0    Ohio  2000       6       7\\n1     1    Ohio  2001       8       9\\n2     2    Ohio  2002      10      11\\nUsing the indexes of both sides of the merge is also not an issue:\\nIn [48]: left2 = DataFrame([[1., 2.], [3., 4.], [5., 6.]], index=['a', 'c', 'e'],\\n   ....:                  columns=['Ohio', 'Nevada'])\\nIn [49]: right2 = DataFrame([[7., 8.], [9., 10.], [11., 12.], [13, 14]],\\n   ....:                    index=['b', 'c', 'd', 'e'], columns=['Missouri', 'Alabama'])\\nIn [50]: left2         In [51]: right2\\nOut[50]:               Out[51]:\\n   Ohio  Nevada           Missouri  Alabama\\na     1       2        b         7        8\\nc     3       4        c         9       10\\ne     5       6        d        11       12\\n                       e        13       14\\nIn [52]: pd.merge(left2, right2, how='outer', left_index=True, right_index=True)\\nOut[52]:\\n   Ohio  Nevada  Missouri  Alabama\\na     1       2       NaN      NaN\\nb   NaN     NaN         7        8\\nc     3       4         9       10\\nd   NaN     NaN        11       12\\ne     5       6        13       14\\nCombining and Merging Data Sets | 183\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"DataFrame has a more convenient join instance for merging by index. It can also be\\nused to combine together many DataFrame objects having the same or similar indexes\\nbut non-overlapping columns. In the prior example, we could have written:\\nIn [53]: left2.join(right2, how='outer')\\nOut[53]:\\n   Ohio  Nevada  Missouri  Alabama\\na     1       2       NaN      NaN\\nb   NaN     NaN         7        8\\nc     3       4         9       10\\nd   NaN     NaN        11       12\\ne     5       6        13       14\\nIn part for legacy reasons (much earlier versions of pandas), DataFrame’s join method\\nperforms a left join on the join keys. It also supports joining the index of the passed\\nDataFrame on one of the columns of the calling DataFrame:\\nIn [54]: left1.join(right1, on='key')\\nOut[54]:\\n  key  value  group_val\\n0   a      0        3.5\\n1   b      1        7.0\\n2   a      2        3.5\\n3   a      3        3.5\\n4   b      4        7.0\\n5   c      5        NaN\\nLastly, for simple index-on-index merges, you can pass a list of DataFrames to join as\\nan alternative to using the more general concat function described below:\\nIn [55]: another = DataFrame([[7., 8.], [9., 10.], [11., 12.], [16., 17.]],\\n   ....:                     index=['a', 'c', 'e', 'f'], columns=['New York', 'Oregon'])\\nIn [56]: left2.join([right2, another])\\nOut[56]:\\n   Ohio  Nevada  Missouri  Alabama  New York  Oregon\\na     1       2       NaN      NaN         7       8\\nc     3       4         9       10         9      10\\ne     5       6        13       14        11      12\\nIn [57]: left2.join([right2, another], how='outer')\\nOut[57]:\\n   Ohio  Nevada  Missouri  Alabama  New York  Oregon\\na     1       2       NaN      NaN         7       8\\nb   NaN     NaN         7        8       NaN     NaN\\nc     3       4         9       10         9      10\\nd   NaN     NaN        11       12       NaN     NaN\\ne     5       6        13       14        11      12\\nf   NaN     NaN       NaN      NaN        16      17\\n184 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Concatenating Along an Axis\\nAnother kind of data combination operation is alternatively referred to as concatena-\\ntion, binding, or stacking. NumPy has a concatenate function for doing this with raw\\nNumPy arrays:\\nIn [58]: arr = np.arange(12).reshape((3, 4))\\nIn [59]: arr\\nOut[59]:\\narray([[ 0,  1,  2,  3],\\n       [ 4,  5,  6,  7],\\n       [ 8,  9, 10, 11]])\\nIn [60]: np.concatenate([arr, arr], axis=1)\\nOut[60]:\\narray([[ 0,  1,  2,  3,  0,  1,  2,  3],\\n       [ 4,  5,  6,  7,  4,  5,  6,  7],\\n       [ 8,  9, 10, 11,  8,  9, 10, 11]])\\nIn the context of pandas objects such as Series and DataFrame, having labeled axes\\nenable you to further generalize array concatenation. In particular, you have a number\\nof additional things to think about:\\n• If the objects are indexed differently on the other axes, should the collection of\\naxes be unioned or intersected?\\n• Do the groups need to be identifiable in the resulting object?\\n• Does the concatenation axis matter at all?\\nThe concat function in pandas provides a consistent way to address each of these con-\\ncerns. I’ll give a number of examples to illustrate how it works. Suppose we have three\\nSeries with no index overlap:\\nIn [61]: s1 = Series([0, 1], index=['a', 'b'])\\nIn [62]: s2 = Series([2, 3, 4], index=['c', 'd', 'e'])\\nIn [63]: s3 = Series([5, 6], index=['f', 'g'])\\nCalling concat with these object in a list glues together the values and indexes:\\nIn [64]: pd.concat([s1, s2, s3])\\nOut[64]:\\na    0\\nb    1\\nc    2\\nd    3\\ne    4\\nf    5\\ng    6\\nCombining and Merging Data Sets | 185\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"By default concat works along axis=0, producing another Series. If you pass axis=1, the\\nresult will instead be a DataFrame (axis=1 is the columns):\\nIn [65]: pd.concat([s1, s2, s3], axis=1)\\nOut[65]:\\n    0   1   2\\na   0 NaN NaN\\nb   1 NaN NaN\\nc NaN   2 NaN\\nd NaN   3 NaN\\ne NaN   4 NaN\\nf NaN NaN   5\\ng NaN NaN   6\\nIn this case there is no overlap on the other axis, which as you can see is the sorted\\nunion (the 'outer' join) of the indexes. You can instead intersect them by passing\\njoin='inner':\\nIn [66]: s4 = pd.concat([s1 * 5, s3])\\nIn [67]: pd.concat([s1, s4], axis=1)      In [68]: pd.concat([s1, s4], axis=1, join='inner')\\nOut[67]:                                  Out[68]:\\n    0  1                                     0  1\\na   0  0                                  a  0  0\\nb   1  5                                  b  1  5\\nf NaN  5\\ng NaN  6\\nYou can even specify the axes to be used on the other axes with join_axes:\\nIn [69]: pd.concat([s1, s4], axis=1, join_axes=[['a', 'c', 'b', 'e']])\\nOut[69]:\\n    0   1\\na   0   0\\nc NaN NaN\\nb   1   5\\ne NaN NaN\\nOne issue is that the concatenated pieces are not identifiable in the result. Suppose\\ninstead you wanted to create a hierarchical index on the concatenation axis. To do this,\\nuse the keys argument:\\nIn [70]: result = pd.concat([s1, s1, s3], keys=['one', 'two', 'three'])\\nIn [71]: result\\nOut[71]:\\none    a    0\\n       b    1\\ntwo    a    0\\n       b    1\\nthree  f    5\\n       g    6\\n# Much more on the unstack function later\\nIn [72]: result.unstack()\\nOut[72]:\\n186 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"a   b   f   g\\none     0   1 NaN NaN\\ntwo     0   1 NaN NaN\\nthree NaN NaN   5   6\\nIn the case of combining Series along axis=1, the keys become the DataFrame column\\nheaders:\\nIn [73]: pd.concat([s1, s2, s3], axis=1, keys=['one', 'two', 'three'])\\nOut[73]:\\n   one  two  three\\na    0  NaN    NaN\\nb    1  NaN    NaN\\nc  NaN    2    NaN\\nd  NaN    3    NaN\\ne  NaN    4    NaN\\nf  NaN  NaN      5\\ng  NaN  NaN      6\\nThe same logic extends to DataFrame objects:\\nIn [74]: df1 = DataFrame(np.arange(6).reshape(3, 2), index=['a', 'b', 'c'],\\n   ....:                 columns=['one', 'two'])\\nIn [75]: df2 = DataFrame(5 + np.arange(4).reshape(2, 2), index=['a', 'c'],\\n   ....:                 columns=['three', 'four'])\\nIn [76]: pd.concat([df1, df2], axis=1, keys=['level1', 'level2'])\\nOut[76]:\\n   level1       level2\\n      one  two   three  four\\na       0    1       5     6\\nb       2    3     NaN   NaN\\nc       4    5       7     8\\nIf you pass a dict of objects instead of a list, the dict’s keys will be used for the keys\\noption:\\nIn [77]: pd.concat({'level1': df1, 'level2': df2}, axis=1)\\nOut[77]:\\n   level1       level2\\n      one  two   three  four\\na       0    1       5     6\\nb       2    3     NaN   NaN\\nc       4    5       7     8\\nThere are a couple of additional arguments governing how the hierarchical index is\\ncreated (see Table 7-2):\\nIn [78]: pd.concat([df1, df2], axis=1, keys=['level1', 'level2'],\\n   ....:           names=['upper', 'lower'])\\nOut[78]:\\nupper  level1       level2\\nlower     one  two   three  four\\na           0    1       5     6\\nb           2    3     NaN   NaN\\nc           4    5       7     8\\nCombining and Merging Data Sets | 187\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"A last consideration concerns DataFrames in which the row index is not meaningful in\\nthe context of the analysis:\\nIn [79]: df1 = DataFrame(np.random.randn(3, 4), columns=['a', 'b', 'c', 'd'])\\nIn [80]: df2 = DataFrame(np.random.randn(2, 3), columns=['b', 'd', 'a'])\\nIn [81]: df1                                     In [82]: df2\\nOut[81]:                                         Out[82]:\\n          a         b         c         d                  b         d         a\\n0 -0.204708  0.478943 -0.519439 -0.555730        0  0.274992  0.228913  1.352917\\n1  1.965781  1.393406  0.092908  0.281746        1  0.886429 -2.001637 -0.371843\\n2  0.769023  1.246435  1.007189 -1.296221\\nIn this case, you can pass ignore_index=True:\\nIn [83]: pd.concat([df1, df2], ignore_index=True)\\nOut[83]:\\n          a         b         c         d\\n0 -0.204708  0.478943 -0.519439 -0.555730\\n1  1.965781  1.393406  0.092908  0.281746\\n2  0.769023  1.246435  1.007189 -1.296221\\n3  1.352917  0.274992       NaN  0.228913\\n4 -0.371843  0.886429       NaN -2.001637\\nTable 7-2. concat function arguments\\nArgument Description\\nobjs List or dict of pandas objects to be concatenated. The only required argument\\naxis Axis to concatenate along; defaults to 0\\njoin One of 'inner', 'outer', defaulting to 'outer'; whether to intersection (inner) or union\\n(outer) together indexes along the other axes\\njoin_axes Specific indexes to use for the other n-1 axes instead of performing union/intersection logic\\nkeys Values to associate with objects being concatenated, forming a hierarchical index along the\\nconcatenation axis. Can either be a list or array of arbitrary values, an array of tuples, or a list of\\narrays (if multiple level arrays passed in levels)\\nlevels Specific indexes to use as hierarchical index level or levels if keys passed\\nnames Names for created hierarchical levels if keys and / or levels passed\\nverify_integrity Check new axis in concatenated object for duplicates and raise exception if so. By default\\n(False) allows duplicates\\nignore_index Do not preserve indexes along concatenation axis, instead producing a new\\nrange(total_length) index\\nCombining Data with Overlap\\nAnother data combination situation can’t be expressed as either a merge or concate-\\nnation operation. You may have two datasets whose indexes overlap in full or part. As\\na motivating example, consider NumPy’s where function, which expressed a vectorized\\nif-else:\\n188 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [84]: a = Series([np.nan, 2.5, np.nan, 3.5, 4.5, np.nan],\\n   ....:            index=['f', 'e', 'd', 'c', 'b', 'a'])\\nIn [85]: b = Series(np.arange(len(a), dtype=np.float64),\\n   ....:            index=['f', 'e', 'd', 'c', 'b', 'a'])\\nIn [86]: b[-1] = np.nan\\nIn [87]: a        In [88]: b        In [89]: np.where(pd.isnull(a), b, a)\\nOut[87]:          Out[88]:          Out[89]:\\nf    NaN          f     0           f    0.0\\ne    2.5          e     1           e    2.5\\nd    NaN          d     2           d    2.0\\nc    3.5          c     3           c    3.5\\nb    4.5          b     4           b    4.5\\na    NaN          a   NaN           a    NaN\\nSeries has a combine_first method, which performs the equivalent of this operation\\nplus data alignment:\\nIn [90]: b[:-2].combine_first(a[2:])\\nOut[90]:\\na    NaN\\nb    4.5\\nc    3.0\\nd    2.0\\ne    1.0\\nf    0.0\\nWith DataFrames, combine_first naturally does the same thing column by column, so\\nyou can think of it as “patching” missing data in the calling object with data from the\\nobject you pass:\\nIn [91]: df1 = DataFrame({'a': [1., np.nan, 5., np.nan],\\n   ....:                  'b': [np.nan, 2., np.nan, 6.],\\n   ....:                  'c': range(2, 18, 4)})\\nIn [92]: df2 = DataFrame({'a': [5., 4., np.nan, 3., 7.],\\n   ....:                  'b': [np.nan, 3., 4., 6., 8.]})\\nIn [93]: df1.combine_first(df2)\\nOut[93]:\\n   a   b   c\\n0  1 NaN   2\\n1  4   2   6\\n2  5   4  10\\n3  3   6  14\\n4  7   8 NaN\\nReshaping and Pivoting\\nThere are a number of fundamental operations for rearranging tabular data. These are\\nalternatingly referred to as reshape or pivot operations.\\nReshaping and Pivoting | 189\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Reshaping with Hierarchical Indexing\\nHierarchical indexing provides a consistent way to rearrange data in a DataFrame.\\nThere are two primary actions:\\n• stack: this “rotates” or pivots from the columns in the data to the rows\\n• unstack: this pivots from the rows into the columns\\nI’ll illustrate these operations through a series of examples. Consider a small DataFrame\\nwith string arrays as row and column indexes:\\nIn [94]: data = DataFrame(np.arange(6).reshape((2, 3)),\\n   ....:                  index=pd.Index(['Ohio', 'Colorado'], name='state'),\\n   ....:                  columns=pd.Index(['one', 'two', 'three'], name='number'))\\nIn [95]: data\\nOut[95]:\\nnumber    one  two  three\\nstate\\nOhio        0    1      2\\nColorado    3    4      5\\nUsing the stack method on this data pivots the columns into the rows, producing a\\nSeries:\\nIn [96]: result = data.stack()\\nIn [97]: result\\nOut[97]:\\nstate     number\\nOhio      one       0\\n          two       1\\n          three     2\\nColorado  one       3\\n          two       4\\n          three     5\\nFrom a hierarchically-indexed Series, you can rearrange the data back into a DataFrame\\nwith unstack:\\nIn [98]: result.unstack()\\nOut[98]:\\nnumber    one  two  three\\nstate\\nOhio        0    1      2\\nColorado    3    4      5\\nBy default the innermost level is unstacked (same with stack). You can unstack a dif-\\nferent level by passing a level number or name:\\nIn [99]: result.unstack(0)        In [100]: result.unstack('state')\\nOut[99]:                          Out[100]:\\nstate   Ohio  Colorado            state   Ohio  Colorado\\nnumber                            number\\none        0         3            one        0         3\\n190 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"two        1         4            two        1         4\\nthree      2         5            three      2         5\\nUnstacking might introduce missing data if all of the values in the level aren’t found in\\neach of the subgroups:\\nIn [101]: s1 = Series([0, 1, 2, 3], index=['a', 'b', 'c', 'd'])\\nIn [102]: s2 = Series([4, 5, 6], index=['c', 'd', 'e'])\\nIn [103]: data2 = pd.concat([s1, s2], keys=['one', 'two'])\\nIn [104]: data2.unstack()\\nOut[104]:\\n      a   b  c  d   e\\none   0   1  2  3 NaN\\ntwo NaN NaN  4  5   6\\nStacking filters out missing data by default, so the operation is easily invertible:\\nIn [105]: data2.unstack().stack()      In [106]: data2.unstack().stack(dropna=False)\\nOut[105]:                              Out[106]:\\none  a    0                            one  a     0\\n     b    1                                 b     1\\n     c    2                                 c     2\\n     d    3                                 d     3\\ntwo  c    4                                 e   NaN\\n     d    5                            two  a   NaN\\n     e    6                                 b   NaN\\n                                            c     4\\n                                            d     5\\n                                            e     6\\nWhen unstacking in a DataFrame, the level unstacked becomes the lowest level in the\\nresult:\\nIn [107]: df = DataFrame({'left': result, 'right': result + 5},\\n   .....:                columns=pd.Index(['left', 'right'], name='side'))\\nIn [108]: df\\nOut[108]:\\nside             left  right\\nstate    number\\nOhio     one        0      5\\n         two        1      6\\n         three      2      7\\nColorado one        3      8\\n         two        4      9\\n         three      5     10\\nIn [109]: df.unstack('state')                In [110]: df.unstack('state').stack('side')\\nOut[109]:                                    Out[110]:\\nside    left            right                state         Ohio  Colorado\\nstate   Ohio  Colorado   Ohio  Colorado      number side\\nnumber                                       one    left      0         3\\none        0         3      5         8             right     5         8\\ntwo        1         4      6         9      two    left      1         4\\nReshaping and Pivoting | 191\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"three      2         5      7        10             right     6         9\\n                                             three  left      2         5\\n                                                    right     7        10\\nPivoting “long” to “wide” Format\\nA common way to store multiple time series in databases and CSV is in so-called long\\nor stacked format:\\nIn [116]: ldata[:10]\\nOut[116]:\\n                 date     item     value\\n0 1959-03-31 00:00:00  realgdp  2710.349\\n1 1959-03-31 00:00:00     infl     0.000\\n2 1959-03-31 00:00:00    unemp     5.800\\n3 1959-06-30 00:00:00  realgdp  2778.801\\n4 1959-06-30 00:00:00     infl     2.340\\n5 1959-06-30 00:00:00    unemp     5.100\\n6 1959-09-30 00:00:00  realgdp  2775.488\\n7 1959-09-30 00:00:00     infl     2.740\\n8 1959-09-30 00:00:00    unemp     5.300\\n9 1959-12-31 00:00:00  realgdp  2785.204\\nData is frequently stored this way in relational databases like MySQL as a fixed schema\\n(column names and data types) allows the number of distinct values in the item column\\nto increase or decrease as data is added or deleted in the table. In the above example\\ndate and item would usually be the primary keys (in relational database parlance),\\noffering both relational integrity and easier joins and programmatic queries in many\\ncases. The downside, of course, is that the data may not be easy to work with in long\\nformat; you might prefer to have a DataFrame containing one column per distinct\\nitem value indexed by timestamps in the date column. DataFrame’s pivot method per-\\nforms exactly this transformation:\\nIn [117]: pivoted = ldata.pivot('date', 'item', 'value')\\nIn [118]: pivoted.head()\\nOut[118]:\\nitem        infl   realgdp  unemp\\ndate\\n1959-03-31  0.00  2710.349    5.8\\n1959-06-30  2.34  2778.801    5.1\\n1959-09-30  2.74  2775.488    5.3\\n1959-12-31  0.27  2785.204    5.6\\n1960-03-31  2.31  2847.699    5.2\\nThe first two values passed are the columns to be used as the row and column index,\\nand finally an optional value column to fill the DataFrame. Suppose you had two value\\ncolumns that you wanted to reshape simultaneously:\\nIn [119]: ldata['value2'] = np.random.randn(len(ldata))\\nIn [120]: ldata[:10]\\nOut[120]:\\n192 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"date     item     value    value2\\n0 1959-03-31 00:00:00  realgdp  2710.349  1.669025\\n1 1959-03-31 00:00:00     infl     0.000 -0.438570\\n2 1959-03-31 00:00:00    unemp     5.800 -0.539741\\n3 1959-06-30 00:00:00  realgdp  2778.801  0.476985\\n4 1959-06-30 00:00:00     infl     2.340  3.248944\\n5 1959-06-30 00:00:00    unemp     5.100 -1.021228\\n6 1959-09-30 00:00:00  realgdp  2775.488 -0.577087\\n7 1959-09-30 00:00:00     infl     2.740  0.124121\\n8 1959-09-30 00:00:00    unemp     5.300  0.302614\\n9 1959-12-31 00:00:00  realgdp  2785.204  0.523772\\nBy omitting the last argument, you obtain a DataFrame with hierarchical columns:\\nIn [121]: pivoted = ldata.pivot('date', 'item')\\nIn [122]: pivoted[:5]\\nOut[122]:\\n            value                     value2\\nitem         infl   realgdp  unemp      infl   realgdp     unemp\\ndate\\n1959-03-31   0.00  2710.349    5.8 -0.438570  1.669025 -0.539741\\n1959-06-30   2.34  2778.801    5.1  3.248944  0.476985 -1.021228\\n1959-09-30   2.74  2775.488    5.3  0.124121 -0.577087  0.302614\\n1959-12-31   0.27  2785.204    5.6  0.000940  0.523772  1.343810\\n1960-03-31   2.31  2847.699    5.2 -0.831154 -0.713544 -2.370232\\nIn [123]: pivoted['value'][:5]\\nOut[123]:\\nitem        infl   realgdp  unemp\\ndate\\n1959-03-31  0.00  2710.349    5.8\\n1959-06-30  2.34  2778.801    5.1\\n1959-09-30  2.74  2775.488    5.3\\n1959-12-31  0.27  2785.204    5.6\\n1960-03-31  2.31  2847.699    5.2\\nNote that pivot is just a shortcut for creating a hierarchical index using set_index and\\nreshaping with unstack:\\nIn [124]: unstacked = ldata.set_index(['date', 'item']).unstack('item')\\nIn [125]: unstacked[:7]\\nOut[125]:\\n            value                     value2\\nitem         infl   realgdp  unemp      infl   realgdp     unemp\\ndate\\n1959-03-31   0.00  2710.349    5.8 -0.438570  1.669025 -0.539741\\n1959-06-30   2.34  2778.801    5.1  3.248944  0.476985 -1.021228\\n1959-09-30   2.74  2775.488    5.3  0.124121 -0.577087  0.302614\\n1959-12-31   0.27  2785.204    5.6  0.000940  0.523772  1.343810\\n1960-03-31   2.31  2847.699    5.2 -0.831154 -0.713544 -2.370232\\n1960-06-30   0.14  2834.390    5.2 -0.860757 -1.860761  0.560145\\n1960-09-30   2.70  2839.022    5.6  0.119827 -1.265934 -1.063512\\nReshaping and Pivoting | 193\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Data Transformation\\nSo far in this chapter we’ve been concerned with rearranging data. Filtering, cleaning,\\nand other tranformations are another class of important operations.\\nRemoving Duplicates\\nDuplicate rows may be found in a DataFrame for any number of reasons. Here is an\\nexample:\\nIn [126]: data = DataFrame({'k1': ['one'] * 3 + ['two'] * 4,\\n   .....:                   'k2': [1, 1, 2, 3, 3, 4, 4]})\\nIn [127]: data\\nOut[127]:\\n    k1  k2\\n0  one   1\\n1  one   1\\n2  one   2\\n3  two   3\\n4  two   3\\n5  two   4\\n6  two   4\\nThe DataFrame method duplicated returns a boolean Series indicating whether each\\nrow is a duplicate or not:\\nIn [128]: data.duplicated()\\nOut[128]:\\n0    False\\n1     True\\n2    False\\n3    False\\n4     True\\n5    False\\n6     True\\nRelatedly, drop_duplicates returns a DataFrame where the duplicated array is True:\\nIn [129]: data.drop_duplicates()\\nOut[129]:\\n    k1  k2\\n0  one   1\\n2  one   2\\n3  two   3\\n5  two   4\\nBoth of these methods by default consider all of the columns; alternatively you can\\nspecify any subset of them to detect duplicates. Suppose we had an additional column\\nof values and wanted to filter duplicates only based on the 'k1' column:\\nIn [130]: data['v1'] = range(7)\\nIn [131]: data.drop_duplicates(['k1'])\\n194 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Out[131]:\\n    k1  k2  v1\\n0  one   1   0\\n3  two   3   3\\nduplicated and drop_duplicates by default keep the first observed value combination.\\nPassing take_last=True will return the last one:\\nIn [132]: data.drop_duplicates(['k1', 'k2'], take_last=True)\\nOut[132]:\\n    k1  k2  v1\\n1  one   1   1\\n2  one   2   2\\n4  two   3   4\\n6  two   4   6\\nTransforming Data Using a Function or Mapping\\nFor many data sets, you may wish to perform some transformation based on the values\\nin an array, Series, or column in a DataFrame. Consider the following hypothetical data\\ncollected about some kinds of meat:\\nIn [133]: data = DataFrame({'food': ['bacon', 'pulled pork', 'bacon', 'Pastrami',\\n   .....:                            'corned beef', 'Bacon', 'pastrami', 'honey ham',\\n   .....:                            'nova lox'],\\n   .....:                   'ounces': [4, 3, 12, 6, 7.5, 8, 3, 5, 6]})\\nIn [134]: data\\nOut[134]:\\n          food  ounces\\n0        bacon     4.0\\n1  pulled pork     3.0\\n2        bacon    12.0\\n3     Pastrami     6.0\\n4  corned beef     7.5\\n5        Bacon     8.0\\n6     pastrami     3.0\\n7    honey ham     5.0\\n8     nova lox     6.0\\nSuppose you wanted to add a column indicating the type of animal that each food came\\nfrom. Let’s write down a mapping of each distinct meat type to the kind of animal:\\nmeat_to_animal = {\\n  'bacon': 'pig',\\n  'pulled pork': 'pig',\\n  'pastrami': 'cow',\\n  'corned beef': 'cow',\\n  'honey ham': 'pig',\\n  'nova lox': 'salmon'\\n}\\nData Transformation | 195\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"The map method on a Series accepts a function or dict-like object containing a mapping,\\nbut here we have a small problem in that some of the meats above are capitalized and\\nothers are not. Thus, we also need to convert each value to lower case:\\nIn [136]: data['animal'] = data['food'].map(str.lower).map(meat_to_animal)\\nIn [137]: data\\nOut[137]:\\n          food  ounces  animal\\n0        bacon     4.0     pig\\n1  pulled pork     3.0     pig\\n2        bacon    12.0     pig\\n3     Pastrami     6.0     cow\\n4  corned beef     7.5     cow\\n5        Bacon     8.0     pig\\n6     pastrami     3.0     cow\\n7    honey ham     5.0     pig\\n8     nova lox     6.0  salmon\\nWe could also have passed a function that does all the work:\\nIn [138]: data['food'].map(lambda x: meat_to_animal[x.lower()])\\nOut[138]:\\n0       pig\\n1       pig\\n2       pig\\n3       cow\\n4       cow\\n5       pig\\n6       cow\\n7       pig\\n8    salmon\\nName: food\\nUsing map is a convenient way to perform element-wise transformations and other data\\ncleaning-related operations.\\nReplacing Values\\nFilling in missing data with the fillna method can be thought of as a special case of\\nmore general value replacement. While map, as you’ve seen above, can be used to modify\\na subset of values in an object, replace provides a simpler and more flexible way to do\\nso. Let’s consider this Series:\\nIn [139]: data = Series([1., -999., 2., -999., -1000., 3.])\\nIn [140]: data\\nOut[140]:\\n0       1\\n1    -999\\n2       2\\n3    -999\\n4   -1000\\n5       3\\n196 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"The -999 values might be sentinel values for missing data. To replace these with NA\\nvalues that pandas understands, we can use replace, producing a new Series:\\nIn [141]: data.replace(-999, np.nan)\\nOut[141]:\\n0       1\\n1     NaN\\n2       2\\n3     NaN\\n4   -1000\\n5       3\\nIf you want to replace multiple values at once, you instead pass a list then the substitute\\nvalue:\\nIn [142]: data.replace([-999, -1000], np.nan)\\nOut[142]:\\n0     1\\n1   NaN\\n2     2\\n3   NaN\\n4   NaN\\n5     3\\nTo use a different replacement for each value, pass a list of substitutes:\\nIn [143]: data.replace([-999, -1000], [np.nan, 0])\\nOut[143]:\\n0     1\\n1   NaN\\n2     2\\n3   NaN\\n4     0\\n5     3\\nThe argument passed can also be a dict:\\nIn [144]: data.replace({-999: np.nan, -1000: 0})\\nOut[144]:\\n0     1\\n1   NaN\\n2     2\\n3   NaN\\n4     0\\n5     3\\nRenaming Axis Indexes\\nLike values in a Series, axis labels can be similarly transformed by a function or mapping\\nof some form to produce new, differently labeled objects. The axes can also be modified\\nin place without creating a new data structure. Here’s a simple example:\\nIn [145]: data = DataFrame(np.arange(12).reshape((3, 4)),\\n   .....:                  index=['Ohio', 'Colorado', 'New York'],\\n   .....:                  columns=['one', 'two', 'three', 'four'])\\nData Transformation | 197\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Like a Series, the axis indexes have a map method:\\nIn [146]: data.index.map(str.upper)\\nOut[146]: array([OHIO, COLORADO, NEW YORK], dtype=object)\\nYou can assign to index, modifying the DataFrame in place:\\nIn [147]: data.index = data.index.map(str.upper)\\nIn [148]: data\\nOut[148]:\\n          one  two  three  four\\nOHIO        0    1      2     3\\nCOLORADO    4    5      6     7\\nNEW YORK    8    9     10    11\\nIf you want to create a transformed version of a data set without modifying the original,\\na useful method is rename:\\nIn [149]: data.rename(index=str.title, columns=str.upper)\\nOut[149]:\\n          ONE  TWO  THREE  FOUR\\nOhio        0    1      2     3\\nColorado    4    5      6     7\\nNew York    8    9     10    11\\nNotably, rename can be used in conjunction with a dict-like object providing new values\\nfor a subset of the axis labels:\\nIn [150]: data.rename(index={'OHIO': 'INDIANA'},\\n   .....:             columns={'three': 'peekaboo'})\\nOut[150]:\\n          one  two  peekaboo  four\\nINDIANA     0    1         2     3\\nCOLORADO    4    5         6     7\\nNEW YORK    8    9        10    11\\nrename saves having to copy the DataFrame manually and assign to its index and col\\numns attributes. Should you wish to modify a data set in place, pass inplace=True:\\n# Always returns a reference to a DataFrame\\nIn [151]: _ = data.rename(index={'OHIO': 'INDIANA'}, inplace=True)\\nIn [152]: data\\nOut[152]:\\n          one  two  three  four\\nINDIANA     0    1      2     3\\nCOLORADO    4    5      6     7\\nNEW YORK    8    9     10    11\\n198 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Discretization and Binning\\nContinuous data is often discretized or otherwised separated into “bins” for analysis.\\nSuppose you have data about a group of people in a study, and you want to group them\\ninto discrete age buckets:\\nIn [153]: ages = [20, 22, 25, 27, 21, 23, 37, 31, 61, 45, 41, 32]\\nLet’s divide these into bins of 18 to 25, 26 to 35, 35 to 60, and finally 60 and older. To\\ndo so, you have to use cut, a function in pandas:\\nIn [154]: bins = [18, 25, 35, 60, 100]\\nIn [155]: cats = pd.cut(ages, bins)\\nIn [156]: cats\\nOut[156]:\\nCategorical:\\narray([(18, 25], (18, 25], (18, 25], (25, 35], (18, 25], (18, 25],\\n       (35, 60], (25, 35], (60, 100], (35, 60], (35, 60], (25, 35]], dtype=object)\\nLevels (4): Index([(18, 25], (25, 35], (35, 60], (60, 100]], dtype=object)\\nThe object pandas returns is a special Categorical object. You can treat it like an array\\nof strings indicating the bin name; internally it contains a levels array indicating the\\ndistinct category names along with a labeling for the ages data in the labels attribute:\\nIn [157]: cats.labels\\nOut[157]: array([0, 0, 0, 1, 0, 0, 2, 1, 3, 2, 2, 1])\\nIn [158]: cats.levels\\nOut[158]: Index([(18, 25], (25, 35], (35, 60], (60, 100]], dtype=object)\\nIn [159]: pd.value_counts(cats)\\nOut[159]:\\n(18, 25]     5\\n(35, 60]     3\\n(25, 35]     3\\n(60, 100]    1\\nConsistent with mathematical notation for intervals, a parenthesis means that the side\\nis open while the square bracket means it is closed (inclusive). Which side is closed can\\nbe changed by passing right=False:\\nIn [160]: pd.cut(ages, [18, 26, 36, 61, 100], right=False)\\nOut[160]:\\nCategorical:\\narray([[18, 26), [18, 26), [18, 26), [26, 36), [18, 26), [18, 26),\\n       [36, 61), [26, 36), [61, 100), [36, 61), [36, 61), [26, 36)], dtype=object)\\nLevels (4): Index([[18, 26), [26, 36), [36, 61), [61, 100)], dtype=object)\\nYou can also pass your own bin names by passing a list or array to the labels option:\\nIn [161]: group_names = ['Youth', 'YoungAdult', 'MiddleAged', 'Senior']\\nIn [162]: pd.cut(ages, bins, labels=group_names)\\nOut[162]:\\nData Transformation | 199\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Categorical:\\narray([Youth, Youth, Youth, YoungAdult, Youth, Youth, MiddleAged,\\n       YoungAdult, Senior, MiddleAged, MiddleAged, YoungAdult], dtype=object)\\nLevels (4): Index([Youth, YoungAdult, MiddleAged, Senior], dtype=object)\\nIf you pass cut a integer number of bins instead of explicit bin edges, it will compute\\nequal-length bins based on the minimum and maximum values in the data. Consider\\nthe case of some uniformly distributed data chopped into fourths:\\nIn [163]: data = np.random.rand(20)\\nIn [164]: pd.cut(data, 4, precision=2)\\nOut[164]:\\nCategorical:\\narray([(0.45, 0.67], (0.23, 0.45], (0.0037, 0.23], (0.45, 0.67],\\n       (0.67, 0.9], (0.45, 0.67], (0.67, 0.9], (0.23, 0.45], (0.23, 0.45],\\n       (0.67, 0.9], (0.67, 0.9], (0.67, 0.9], (0.23, 0.45], (0.23, 0.45],\\n       (0.23, 0.45], (0.67, 0.9], (0.0037, 0.23], (0.0037, 0.23],\\n       (0.23, 0.45], (0.23, 0.45]], dtype=object)\\nLevels (4): Index([(0.0037, 0.23], (0.23, 0.45], (0.45, 0.67],\\n                   (0.67, 0.9]], dtype=object)\\nA closely related function, qcut, bins the data based on sample quantiles. Depending\\non the distribution of the data, using cut will not usually result in each bin having the\\nsame number of data points. Since qcut uses sample quantiles instead, by definition\\nyou will obtain roughly equal-size bins:\\nIn [165]: data = np.random.randn(1000) # Normally distributed\\nIn [166]: cats = pd.qcut(data, 4) # Cut into quartiles\\nIn [167]: cats\\nOut[167]:\\nCategorical:\\narray([(-0.022, 0.641], [-3.745, -0.635], (0.641, 3.26], ...,\\n       (-0.635, -0.022], (0.641, 3.26], (-0.635, -0.022]], dtype=object)\\nLevels (4): Index([[-3.745, -0.635], (-0.635, -0.022], (-0.022, 0.641],\\n                   (0.641, 3.26]], dtype=object)\\nIn [168]: pd.value_counts(cats)\\nOut[168]:\\n[-3.745, -0.635]    250\\n(0.641, 3.26]       250\\n(-0.635, -0.022]    250\\n(-0.022, 0.641]     250\\nSimilar to cut you can pass your own quantiles (numbers between 0 and 1, inclusive):\\nIn [169]: pd.qcut(data, [0, 0.1, 0.5, 0.9, 1.])\\nOut[169]:\\nCategorical:\\narray([(-0.022, 1.302], (-1.266, -0.022], (-0.022, 1.302], ...,\\n       (-1.266, -0.022], (-0.022, 1.302], (-1.266, -0.022]], dtype=object)\\nLevels (4): Index([[-3.745, -1.266], (-1.266, -0.022], (-0.022, 1.302],\\n                   (1.302, 3.26]], dtype=object)\\n200 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'We’ll return to cut and qcut later in the chapter on aggregation and group operations,\\nas these discretization functions are especially useful for quantile and group analysis.\\nDetecting and Filtering Outliers\\nFiltering or transforming outliers is largely a matter of applying array operations. Con-\\nsider a DataFrame with some normally distributed data:\\nIn [170]: np.random.seed(12345)\\nIn [171]: data = DataFrame(np.random.randn(1000, 4))\\nIn [172]: data.describe()\\nOut[172]:\\n                 0            1            2            3\\ncount  1000.000000  1000.000000  1000.000000  1000.000000\\nmean     -0.067684     0.067924     0.025598    -0.002298\\nstd       0.998035     0.992106     1.006835     0.996794\\nmin      -3.428254    -3.548824    -3.184377    -3.745356\\n25%      -0.774890    -0.591841    -0.641675    -0.644144\\n50%      -0.116401     0.101143     0.002073    -0.013611\\n75%       0.616366     0.780282     0.680391     0.654328\\nmax       3.366626     2.653656     3.260383     3.927528\\nSuppose you wanted to find values in one of the columns exceeding three in magnitude:\\nIn [173]: col = data[3]\\nIn [174]: col[np.abs(col) > 3]\\nOut[174]:\\n97     3.927528\\n305   -3.399312\\n400   -3.745356\\nName: 3\\nTo select all rows having a value exceeding 3 or -3, you can use the any method on a\\nboolean DataFrame:\\nIn [175]: data[(np.abs(data) > 3).any(1)]\\nOut[175]:\\n            0         1         2         3\\n5   -0.539741  0.476985  3.248944 -1.021228\\n97  -0.774363  0.552936  0.106061  3.927528\\n102 -0.655054 -0.565230  3.176873  0.959533\\n305 -2.315555  0.457246 -0.025907 -3.399312\\n324  0.050188  1.951312  3.260383  0.963301\\n400  0.146326  0.508391 -0.196713 -3.745356\\n499 -0.293333 -0.242459 -3.056990  1.918403\\n523 -3.428254 -0.296336 -0.439938 -0.867165\\n586  0.275144  1.179227 -3.184377  1.369891\\n808 -0.362528 -3.548824  1.553205 -2.186301\\n900  3.366626 -2.372214  0.851010  1.332846\\nValues can just as easily be set based on these criteria. Here is code to cap values outside\\nthe interval -3 to 3:\\nData Transformation | 201\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'In [176]: data[np.abs(data) > 3] = np.sign(data) * 3\\nIn [177]: data.describe()\\nOut[177]:\\n                 0            1            2            3\\ncount  1000.000000  1000.000000  1000.000000  1000.000000\\nmean     -0.067623     0.068473     0.025153    -0.002081\\nstd       0.995485     0.990253     1.003977     0.989736\\nmin      -3.000000    -3.000000    -3.000000    -3.000000\\n25%      -0.774890    -0.591841    -0.641675    -0.644144\\n50%      -0.116401     0.101143     0.002073    -0.013611\\n75%       0.616366     0.780282     0.680391     0.654328\\nmax       3.000000     2.653656     3.000000     3.000000\\nThe ufunc np.sign returns an array of 1 and -1 depending on the sign of the values.\\nPermutation and Random Sampling\\nPermuting (randomly reordering) a Series or the rows in a DataFrame is easy to do using\\nthe numpy.random.permutation function. Calling permutation with the length of the axis\\nyou want to permute produces an array of integers indicating the new ordering:\\nIn [178]: df = DataFrame(np.arange(5 * 4).reshape(5, 4))\\nIn [179]: sampler = np.random.permutation(5)\\nIn [180]: sampler\\nOut[180]: array([1, 0, 2, 3, 4])\\nThat array can then be used in ix-based indexing or the take function:\\nIn [181]: df             In [182]: df.take(sampler)\\nOut[181]:                Out[182]:\\n    0   1   2   3            0   1   2   3\\n0   0   1   2   3        1   4   5   6   7\\n1   4   5   6   7        0   0   1   2   3\\n2   8   9  10  11        2   8   9  10  11\\n3  12  13  14  15        3  12  13  14  15\\n4  16  17  18  19        4  16  17  18  19\\nTo select a random subset without replacement, one way is to slice off the first k ele-\\nments of the array returned by permutation, where k is the desired subset size. There\\nare much more efficient sampling-without-replacement algorithms, but this is an easy\\nstrategy that uses readily available tools:\\nIn [183]: df.take(np.random.permutation(len(df))[:3])\\nOut[183]:\\n    0   1   2   3\\n1   4   5   6   7\\n3  12  13  14  15\\n4  16  17  18  19\\nTo generate a sample with replacement, the fastest way is to use np.random.randint to\\ndraw random integers:\\n202 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [184]: bag = np.array([5, 7, -1, 6, 4])\\nIn [185]: sampler = np.random.randint(0, len(bag), size=10)\\nIn [186]: sampler\\nOut[186]: array([4, 4, 2, 2, 2, 0, 3, 0, 4, 1])\\nIn [187]: draws = bag.take(sampler)\\nIn [188]: draws\\nOut[188]: array([ 4,  4, -1, -1, -1,  5,  6,  5,  4,  7])\\nComputing Indicator/Dummy Variables\\nAnother type of transformation for statistical modeling or machine learning applica-\\ntions is converting a categorical variable into a “dummy” or “indicator” matrix. If a\\ncolumn in a DataFrame has k distinct values, you would derive a matrix or DataFrame\\ncontaining k columns containing all 1’s and 0’s. pandas has a get_dummies function for\\ndoing this, though devising one yourself is not difficult. Let’s return to an earlier ex-\\nample DataFrame:\\nIn [189]: df = DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'b'],\\n   .....:                 'data1': range(6)})\\nIn [190]: pd.get_dummies(df['key'])\\nOut[190]:\\n   a  b  c\\n0  0  1  0\\n1  0  1  0\\n2  1  0  0\\n3  0  0  1\\n4  1  0  0\\n5  0  1  0\\nIn some cases, you may want to add a prefix to the columns in the indicator DataFrame,\\nwhich can then be merged with the other data. get_dummies has a prefix argument for\\ndoing just this:\\nIn [191]: dummies = pd.get_dummies(df['key'], prefix='key')\\nIn [192]: df_with_dummy = df[['data1']].join(dummies)\\nIn [193]: df_with_dummy\\nOut[193]:\\n   data1  key_a  key_b  key_c\\n0      0      0      1      0\\n1      1      0      1      0\\n2      2      1      0      0\\n3      3      0      0      1\\n4      4      1      0      0\\n5      5      0      1      0\\nData Transformation | 203\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"If a row in a DataFrame belongs to multiple categories, things are a bit more compli-\\ncated. Let’s return to the MovieLens 1M dataset from earlier in the book:\\nIn [194]: mnames = ['movie_id', 'title', 'genres']\\nIn [195]: movies = pd.read_table('ch07/movies.dat', sep='::', header=None,\\n   .....:                         names=mnames)\\nIn [196]: movies[:10]\\nOut[196]:\\n   movie_id                               title                        genres\\n0         1                    Toy Story (1995)   Animation|Children's|Comedy\\n1         2                      Jumanji (1995)  Adventure|Children's|Fantasy\\n2         3             Grumpier Old Men (1995)                Comedy|Romance\\n3         4            Waiting to Exhale (1995)                  Comedy|Drama\\n4         5  Father of the Bride Part II (1995)                        Comedy\\n5         6                         Heat (1995)         Action|Crime|Thriller\\n6         7                      Sabrina (1995)                Comedy|Romance\\n7         8                 Tom and Huck (1995)          Adventure|Children's\\n8         9                 Sudden Death (1995)                        Action\\n9        10                    GoldenEye (1995)     Action|Adventure|Thriller\\nAdding indicator variables for each genre requires a little bit of wrangling. First, we\\nextract the list of unique genres in the dataset (using a nice set.union trick):\\nIn [197]: genre_iter = (set(x.split('|')) for x in movies.genres)\\nIn [198]: genres = sorted(set.union(*genre_iter))\\nNow, one way to construct the indicator DataFrame is to start with a DataFrame of all\\nzeros:\\nIn [199]: dummies = DataFrame(np.zeros((len(movies), len(genres))), columns=genres)\\nNow, iterate through each movie and set entries in each row of dummies to 1:\\nIn [200]: for i, gen in enumerate(movies.genres):\\n   .....:     dummies.ix[i, gen.split('|')] = 1\\nThen, as above, you can combine this with movies:\\nIn [201]: movies_windic = movies.join(dummies.add_prefix('Genre_'))\\nIn [202]: movies_windic.ix[0]\\nOut[202]:\\nmovie_id                                       1\\ntitle                           Toy Story (1995)\\ngenres               Animation|Children's|Comedy\\nGenre_Action                                   0\\nGenre_Adventure                                0\\nGenre_Animation                                1\\nGenre_Children's                               1\\nGenre_Comedy                                   1\\nGenre_Crime                                    0\\nGenre_Documentary                              0\\nGenre_Drama                                    0\\nGenre_Fantasy                                  0\\n204 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Genre_Film-Noir                                0\\nGenre_Horror                                   0\\nGenre_Musical                                  0\\nGenre_Mystery                                  0\\nGenre_Romance                                  0\\nGenre_Sci-Fi                                   0\\nGenre_Thriller                                 0\\nGenre_War                                      0\\nGenre_Western                                  0\\nName: 0\\nFor much larger data, this method of constructing indicator variables\\nwith multiple membership is not especially speedy. A lower-level func-\\ntion leveraging the internals of the DataFrame could certainly be writ-\\nten.\\nA useful recipe for statistical applications is to combine get_dummies with a discretiza-\\ntion function like cut:\\nIn [204]: values = np.random.rand(10)\\nIn [205]: values\\nOut[205]:\\narray([ 0.9296,  0.3164,  0.1839,  0.2046,  0.5677,  0.5955,  0.9645,\\n        0.6532,  0.7489,  0.6536])\\nIn [206]: bins = [0, 0.2, 0.4, 0.6, 0.8, 1]\\nIn [207]: pd.get_dummies(pd.cut(values, bins))\\nOut[207]:\\n   (0, 0.2]  (0.2, 0.4]  (0.4, 0.6]  (0.6, 0.8]  (0.8, 1]\\n0         0           0           0           0         1\\n1         0           1           0           0         0\\n2         1           0           0           0         0\\n3         0           1           0           0         0\\n4         0           0           1           0         0\\n5         0           0           1           0         0\\n6         0           0           0           0         1\\n7         0           0           0           1         0\\n8         0           0           0           1         0\\n9         0           0           0           1         0\\nString Manipulation\\nPython has long been a popular data munging language in part due to its ease-of-use\\nfor string and text processing. Most text operations are made simple with the string\\nobject’s built-in methods. For more complex pattern matching and text manipulations,\\nregular expressions may be needed. pandas adds to the mix by enabling you to apply\\nstring and regular expressions concisely on whole arrays of data, additionally handling\\nthe annoyance of missing data.\\nString Manipulation | 205\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"String Object Methods\\nIn many string munging and scripting applications, built-in string methods are suffi-\\ncient. As an example, a comma-separated string can be broken into pieces with split:\\nIn [208]: val = 'a,b,  guido'\\nIn [209]: val.split(',')\\nOut[209]: ['a', 'b', '  guido']\\nsplit is often combined with strip to trim whitespace (including newlines):\\nIn [210]: pieces = [x.strip() for x in val.split(',')]\\nIn [211]: pieces\\nOut[211]: ['a', 'b', 'guido']\\nThese substrings could be concatenated together with a two-colon delimiter using ad-\\ndition:\\nIn [212]: first, second, third = pieces\\nIn [213]: first + '::' + second + '::' + third\\nOut[213]: 'a::b::guido'\\nBut, this isn’t a practical generic method. A faster and more Pythonic way is to pass a\\nlist or tuple to the join method on the string '::':\\nIn [214]: '::'.join(pieces)\\nOut[214]: 'a::b::guido'\\nOther methods are concerned with locating substrings. Using Python’s in keyword is\\nthe best way to detect a substring, though index and find can also be used:\\nIn [215]: 'guido' in val\\nOut[215]: True\\nIn [216]: val.index(',')        In [217]: val.find(':')\\nOut[216]: 1                     Out[217]: -1\\nNote the difference between find and index is that index raises an exception if the string\\nisn’t found (versus returning -1):\\nIn [218]: val.index(':')\\n---------------------------------------------------------------------------\\nValueError                                Traceback (most recent call last)\\n<ipython-input-218-280f8b2856ce> in <module>()\\n----> 1 val.index(':')\\nValueError: substring not found\\nRelatedly, count returns the number of occurrences of a particular substring:\\nIn [219]: val.count(',')\\nOut[219]: 2\\nreplace will substitute occurrences of one pattern for another. This is commonly used\\nto delete patterns, too, by passing an empty string:\\n206 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [220]: val.replace(',', '::')        In [221]: val.replace(',', '')\\nOut[220]: 'a::b::  guido'               Out[221]: 'ab  guido'\\nRegular expressions can also be used with many of these operations as you’ll see below.\\nTable 7-3. Python built-in string methods\\nArgument Description\\ncount Return the number of non-overlapping occurrences of substring in the string.\\nendswith, startswith Returns True if string ends with suffix (starts with prefix).\\njoin Use string as delimiter for concatenating a sequence of other strings.\\nindex Return position of first character in substring if found in the string. Raises ValueEr\\nror if not found.\\nfind Return position of first character of first occurrence of substring in the string. Like\\nindex, but returns -1 if not found.\\nrfind Return position of first character of last occurrence of substring in the string. Returns -1\\nif not found.\\nreplace Replace occurrences of string with another string.\\nstrip, rstrip, lstrip Trim whitespace, including newlines; equivalent to x.strip() (and rstrip,\\nlstrip, respectively) for each element.\\nsplit Break string into list of substrings using passed delimiter.\\nlower, upper Convert alphabet characters to lowercase or uppercase, respectively.\\nljust, rjust Left justify or right justify, respectively. Pad opposite side of string with spaces (or some\\nother fill character) to return a string with a minimum width.\\nRegular expressions\\nRegular expressions provide a flexible way to search or match string patterns in text. A\\nsingle expression, commonly called a regex, is a string formed according to the regular\\nexpression language. Python’s built-in re module is responsible for applying regular\\nexpressions to strings; I’ll give a number of examples of its use here.\\nThe art of writing regular expressions could be a chapter of its own and\\nthus is outside the book’s scope. There are many excellent tutorials and\\nreferences on the internet, such as Zed Shaw’s Learn Regex The Hard\\nWay (http://regex.learncodethehardway.org/book/).\\nThe re module functions fall into three categories: pattern matching, substitution, and\\nsplitting. Naturally these are all related; a regex describes a pattern to locate in the text,\\nwhich can then be used for many purposes. Let’s look at a simple example: suppose I\\nwanted to split a string with a variable number of whitespace characters (tabs, spaces,\\nand newlines). The regex describing one or more whitespace characters is \\\\s+:\\nString Manipulation | 207\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'In [222]: import re\\nIn [223]: text = \"foo    bar\\\\t baz  \\\\tqux\"\\nIn [224]: re.split(\\'\\\\s+\\', text)\\nOut[224]: [\\'foo\\', \\'bar\\', \\'baz\\', \\'qux\\']\\nWhen you call re.split(\\'\\\\s+\\', text), the regular expression is first compiled, then its\\nsplit method is called on the passed text. You can compile the regex yourself with \\nre.compile, forming a reusable regex object:\\nIn [225]: regex = re.compile(\\'\\\\s+\\')\\nIn [226]: regex.split(text)\\nOut[226]: [\\'foo\\', \\'bar\\', \\'baz\\', \\'qux\\']\\nIf, instead, you wanted to get a list of all patterns matching the regex, you can use the \\nfindall method:\\nIn [227]: regex.findall(text)\\nOut[227]: [\\'    \\', \\'\\\\t \\', \\'  \\\\t\\']\\nTo avoid unwanted escaping with \\\\ in a regular expression, use raw\\nstring literals like r\\'C:\\\\x\\' instead of the equivalent \\'C:\\\\\\\\x\\'.\\nCreating a regex object with re.compile is highly recommended if you intend to apply\\nthe same expression to many strings; doing so will save CPU cycles.\\nmatch and search are closely related to findall. While findall returns all matches in a\\nstring, search returns only the first match. More rigidly, match only matches at the\\nbeginning of the string. As a less trivial example, let’s consider a block of text and a\\nregular expression capable of identifying most email addresses:\\ntext = \"\"\"Dave dave@google.com\\nSteve steve@gmail.com\\nRob rob@gmail.com\\nRyan ryan@yahoo.com\\n\"\"\"\\npattern = r\\'[A-Z0-9._%+-]+@[A-Z0-9.-]+\\\\.[A-Z]{2,4}\\'\\n# re.IGNORECASE makes the regex case-insensitive\\nregex = re.compile(pattern, flags=re.IGNORECASE)\\nUsing findall on the text produces a list of the e-mail addresses:\\nIn [229]: regex.findall(text)\\nOut[229]: [\\'dave@google.com\\', \\'steve@gmail.com\\', \\'rob@gmail.com\\', \\'ryan@yahoo.com\\']\\nsearch returns a special match object for the first email address in the text. For the\\nabove regex, the match object can only tell us the start and end position of the pattern\\nin the string:\\n208 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [230]: m = regex.search(text)\\nIn [231]: m\\nOut[231]: <_sre.SRE_Match at 0x10a05de00>\\nIn [232]: text[m.start():m.end()]\\nOut[232]: 'dave@google.com'\\nregex.match returns None, as it only will match if the pattern occurs at the start of the\\nstring:\\nIn [233]: print regex.match(text)\\nNone\\nRelatedly, sub will return a new string with occurrences of the pattern replaced by the\\na new string:\\nIn [234]: print regex.sub('REDACTED', text)\\nDave REDACTED\\nSteve REDACTED\\nRob REDACTED\\nRyan REDACTED\\nSuppose you wanted to find email addresses and simultaneously segment each address\\ninto its 3 components: username, domain name, and domain suffix. To do this, put\\nparentheses around the parts of the pattern to segment:\\nIn [235]: pattern = r'([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\\\\.([A-Z]{2,4})'\\nIn [236]: regex = re.compile(pattern, flags=re.IGNORECASE)\\nA match object produced by this modified regex returns a tuple of the pattern compo-\\nnents with its groups method:\\nIn [237]: m = regex.match('wesm@bright.net')\\nIn [238]: m.groups()\\nOut[238]: ('wesm', 'bright', 'net')\\nfindall returns a list of tuples when the pattern has groups:\\nIn [239]: regex.findall(text)\\nOut[239]:\\n[('dave', 'google', 'com'),\\n ('steve', 'gmail', 'com'),\\n ('rob', 'gmail', 'com'),\\n ('ryan', 'yahoo', 'com')]\\nsub also has access to groups in each match using special symbols like \\\\1, \\\\2, etc.:\\nIn [240]: print regex.sub(r'Username: \\\\1, Domain: \\\\2, Suffix: \\\\3', text)\\nDave Username: dave, Domain: google, Suffix: com\\nSteve Username: steve, Domain: gmail, Suffix: com\\nRob Username: rob, Domain: gmail, Suffix: com\\nRyan Username: ryan, Domain: yahoo, Suffix: com\\nString Manipulation | 209\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'There is much more to regular expressions in Python, most of which is outside the\\nbook’s scope. To give you a flavor, one variation on the above email regex gives names\\nto the match groups:\\nregex = re.compile(r\"\"\"\\n    (?P<username>[A-Z0-9._%+-]+)\\n    @\\n    (?P<domain>[A-Z0-9.-]+)\\n    \\\\.\\n    (?P<suffix>[A-Z]{2,4})\"\"\", flags=re.IGNORECASE|re.VERBOSE)\\nThe match object produced by such a regex can produce a handy dict with the specified\\ngroup names:\\nIn [242]: m = regex.match(\\'wesm@bright.net\\')\\nIn [243]: m.groupdict()\\nOut[243]: {\\'domain\\': \\'bright\\', \\'suffix\\': \\'net\\', \\'username\\': \\'wesm\\'}\\nTable 7-4. Regular expression methods\\nArgument Description\\nfindall, finditer Return all non-overlapping matching patterns in a string. findall returns a list of all\\npatterns while finditer returns them one by one from an iterator.\\nmatch Match pattern at start of string and optionally segment pattern components into groups.\\nIf the pattern matches, returns a match object, otherwise None.\\nsearch Scan string for match to pattern; returning a match object if so. Unlike match, the match\\ncan be anywhere in the string as opposed to only at the beginning.\\nsplit Break string into pieces at each occurrence of pattern.\\nsub, subn Replace all (sub) or first n occurrences (subn) of pattern in string with replacement\\nexpression. Use symbols \\\\1, \\\\2, ... to refer to match group elements in the re-\\nplacement string.\\nVectorized string functions in pandas\\nCleaning up a messy data set for analysis often requires a lot of string munging and\\nregularization. To complicate matters, a column containing strings will sometimes have\\nmissing data:\\nIn [244]: data = {\\'Dave\\': \\'dave@google.com\\', \\'Steve\\': \\'steve@gmail.com\\',\\n   .....:         \\'Rob\\': \\'rob@gmail.com\\', \\'Wes\\': np.nan}\\nIn [245]: data = Series(data)\\nIn [246]: data                  In [247]: data.isnull()\\nOut[246]:                       Out[247]:\\nDave     dave@google.com        Dave     False\\nRob        rob@gmail.com        Rob      False\\nSteve    steve@gmail.com        Steve    False\\nWes                  NaN        Wes       True\\n210 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"String and regular expression methods can be applied (passing a lambda or other func-\\ntion) to each value using data.map, but it will fail on the NA. To cope with this, Series\\nhas concise methods for string operations that skip NA values. These are accessed\\nthrough Series’s str attribute; for example, we could check whether each email address\\nhas 'gmail' in it with str.contains:\\nIn [248]: data.str.contains('gmail')\\nOut[248]:\\nDave     False\\nRob       True\\nSteve     True\\nWes        NaN\\nRegular expressions can be used, too, along with any re options like IGNORECASE:\\nIn [249]: pattern\\nOut[249]: '([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\\\\\\\\.([A-Z]{2,4})'\\nIn [250]: data.str.findall(pattern, flags=re.IGNORECASE)\\nOut[250]:\\nDave     [('dave', 'google', 'com')]\\nRob        [('rob', 'gmail', 'com')]\\nSteve    [('steve', 'gmail', 'com')]\\nWes                              NaN\\nThere are a couple of ways to do vectorized element retrieval. Either use str.get or\\nindex into the str attribute:\\nIn [251]: matches = data.str.match(pattern, flags=re.IGNORECASE)\\nIn [252]: matches\\nOut[252]:\\nDave     ('dave', 'google', 'com')\\nRob        ('rob', 'gmail', 'com')\\nSteve    ('steve', 'gmail', 'com')\\nWes                            NaN\\nIn [253]: matches.str.get(1)      In [254]: matches.str[0]\\nOut[253]:                         Out[254]:\\nDave     google                   Dave      dave\\nRob       gmail                   Rob        rob\\nSteve     gmail                   Steve    steve\\nWes         NaN                   Wes        NaN\\nYou can similarly slice strings using this syntax:\\nIn [255]: data.str[:5]\\nOut[255]:\\nDave     dave@\\nRob      rob@g\\nSteve    steve\\nWes        NaN\\nString Manipulation | 211\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Table 7-5. Vectorized string methods\\nMethod Description\\ncat Concatenate strings element-wise with optional delimiter\\ncontains Return boolean array if each string contains pattern/regex\\ncount Count occurrences of pattern\\nendswith, startswith Equivalent to x.endswith(pattern) or x.startswith(pattern) for each el-\\nement.\\nfindall Compute list of all occurrences of pattern/regex for each string\\nget Index into each element (retrieve i-th element)\\njoin Join strings in each element of the Series with passed separator\\nlen Compute length of each string\\nlower, upper Convert cases; equivalent to x.lower() or x.upper() for each element.\\nmatch Use re.match with the passed regular expression on each element, returning matched\\ngroups as list.\\npad Add whitespace to left, right, or both sides of strings\\ncenter Equivalent to pad(side=\\'both\\')\\nrepeat Duplicate values; for example s.str.repeat(3) equivalent to x * 3 for each string.\\nreplace Replace occurrences of pattern/regex with some other string\\nslice Slice each string in the Series.\\nsplit Split strings on delimiter or regular expression\\nstrip, rstrip, lstrip Trim whitespace, including newlines; equivalent to x.strip() (and rstrip,\\nlstrip, respectively) for each element.\\nExample: USDA Food Database\\nThe US Department of Agriculture makes available a database of food nutrient infor-\\nmation. Ashley Williams, an English hacker, has made available a version of this da-\\ntabase in JSON format ( http://ashleyw.co.uk/project/food-nutrient-database). The re-\\ncords look like this:\\n{\\n  \"id\": 21441,\\n  \"description\": \"KENTUCKY FRIED CHICKEN, Fried Chicken, EXTRA CRISPY,\\nWing, meat and skin with breading\",\\n  \"tags\": [\"KFC\"],\\n  \"manufacturer\": \"Kentucky Fried Chicken\",\\n  \"group\": \"Fast Foods\",\\n  \"portions\": [\\n    {\\n      \"amount\": 1,\\n      \"unit\": \"wing, with skin\",\\n      \"grams\": 68.0\\n    },\\n212 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': '...\\n  ],\\n  \"nutrients\": [\\n    {\\n      \"value\": 20.8,\\n      \"units\": \"g\",\\n      \"description\": \"Protein\",\\n      \"group\": \"Composition\"\\n    },\\n    ...\\n  ]\\n}\\nEach food has a number of identifying attributes along with two lists of nutrients and\\nportion sizes. Having the data in this form is not particularly amenable for analysis, so\\nwe need to do some work to wrangle the data into a better form.\\nAfter downloading and extracting the data from the link above, you can load it into\\nPython with any JSON library of your choosing. I’ll use the built-in Python json mod-\\nule:\\nIn [256]: import json\\nIn [257]: db = json.load(open(\\'ch07/foods-2011-10-03.json\\'))\\nIn [258]: len(db)\\nOut[258]: 6636\\nEach entry in db is a dict containing all the data for a single food. The \\'nutrients\\' field\\nis a list of dicts, one for each nutrient:\\nIn [259]: db[0].keys()        In [260]: db[0][\\'nutrients\\'][0]\\nOut[259]:                     Out[260]:\\n[u\\'portions\\',                 {u\\'description\\': u\\'Protein\\',\\n u\\'description\\',               u\\'group\\': u\\'Composition\\',\\n u\\'tags\\',                      u\\'units\\': u\\'g\\',\\n u\\'nutrients\\',                 u\\'value\\': 25.18}\\n u\\'group\\',\\n u\\'id\\',\\n u\\'manufacturer\\']\\nIn [261]: nutrients = DataFrame(db[0][\\'nutrients\\'])\\nIn [262]: nutrients[:7]\\nOut[262]:\\n                   description        group units    value\\n0                      Protein  Composition     g    25.18\\n1            Total lipid (fat)  Composition     g    29.20\\n2  Carbohydrate, by difference  Composition     g     3.06\\n3                          Ash        Other     g     3.28\\n4                       Energy       Energy  kcal   376.00\\n5                        Water  Composition     g    39.28\\n6                       Energy       Energy    kJ  1573.00\\nExample: USDA Food Database | 213\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"When converting a list of dicts to a DataFrame, we can specify a list of fields to extract.\\nWe’ll take the food names, group, id, and manufacturer:\\nIn [263]: info_keys = ['description', 'group', 'id', 'manufacturer']\\nIn [264]: info = DataFrame(db, columns=info_keys)\\nIn [265]: info[:5]\\nOut[265]:\\n                          description                   group    id manufacturer\\n0                     Cheese, caraway  Dairy and Egg Products  1008\\n1                     Cheese, cheddar  Dairy and Egg Products  1009\\n2                        Cheese, edam  Dairy and Egg Products  1018\\n3                        Cheese, feta  Dairy and Egg Products  1019\\n4  Cheese, mozzarella, part skim milk  Dairy and Egg Products  1028\\nIn [266]: info\\nOut[266]:\\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 6636 entries, 0 to 6635\\nData columns:\\ndescription     6636  non-null values\\ngroup           6636  non-null values\\nid              6636  non-null values\\nmanufacturer    5195  non-null values\\ndtypes: int64(1), object(3)\\nYou can see the distribution of food groups with value_counts:\\nIn [267]: pd.value_counts(info.group)[:10]\\nOut[267]:\\nVegetables and Vegetable Products    812\\nBeef Products                        618\\nBaked Products                       496\\nBreakfast Cereals                    403\\nLegumes and Legume Products          365\\nFast Foods                           365\\nLamb, Veal, and Game Products        345\\nSweets                               341\\nPork Products                        328\\nFruits and Fruit Juices              328\\nNow, to do some analysis on all of the nutrient data, it’s easiest to assemble the nutrients\\nfor each food into a single large table. To do so, we need to take several steps. First, I’ll\\nconvert each list of food nutrients to a DataFrame, add a column for the food id, and\\nappend the DataFrame to a list. Then, these can be concatenated together with concat:\\nnutrients = []\\nfor rec in db:\\n    fnuts = DataFrame(rec['nutrients'])\\n    fnuts['id'] = rec['id']\\n    nutrients.append(fnuts)\\nnutrients = pd.concat(nutrients, ignore_index=True)\\n214 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"If all goes well, nutrients should look like this:\\nIn [269]: nutrients\\nOut[269]:\\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 389355 entries, 0 to 389354\\nData columns:\\ndescription    389355  non-null values\\ngroup          389355  non-null values\\nunits          389355  non-null values\\nvalue          389355  non-null values\\nid             389355  non-null values\\ndtypes: float64(1), int64(1), object(3)\\nI noticed that, for whatever reason, there are duplicates in this DataFrame, so it makes\\nthings easier to drop them:\\nIn [270]: nutrients.duplicated().sum()\\nOut[270]: 14179\\nIn [271]: nutrients = nutrients.drop_duplicates()\\nSince 'group' and 'description' is in both DataFrame objects, we can rename them to\\nmake it clear what is what:\\nIn [272]: col_mapping = {'description' : 'food',\\n   .....:                'group'       : 'fgroup'}\\nIn [273]: info = info.rename(columns=col_mapping, copy=False)\\nIn [274]: info\\nOut[274]:\\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 6636 entries, 0 to 6635\\nData columns:\\nfood            6636  non-null values\\nfgroup          6636  non-null values\\nid              6636  non-null values\\nmanufacturer    5195  non-null values\\ndtypes: int64(1), object(3)\\nIn [275]: col_mapping = {'description' : 'nutrient',\\n   .....:                'group' : 'nutgroup'}\\nIn [276]: nutrients = nutrients.rename(columns=col_mapping, copy=False)\\nIn [277]: nutrients\\nOut[277]:\\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 375176 entries, 0 to 389354\\nData columns:\\nnutrient    375176  non-null values\\nnutgroup    375176  non-null values\\nunits       375176  non-null values\\nvalue       375176  non-null values\\nExample: USDA Food Database | 215\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"id          375176  non-null values\\ndtypes: float64(1), int64(1), object(3)\\nWith all of this done, we’re ready to merge info with nutrients:\\nIn [278]: ndata = pd.merge(nutrients, info, on='id', how='outer')\\nIn [279]: ndata\\nOut[279]:\\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 375176 entries, 0 to 375175\\nData columns:\\nnutrient        375176  non-null values\\nnutgroup        375176  non-null values\\nunits           375176  non-null values\\nvalue           375176  non-null values\\nid              375176  non-null values\\nfood            375176  non-null values\\nfgroup          375176  non-null values\\nmanufacturer    293054  non-null values\\ndtypes: float64(1), int64(1), object(6)\\nIn [280]: ndata.ix[30000]\\nOut[280]:\\nnutrient                       Folic acid\\nnutgroup                         Vitamins\\nunits                                 mcg\\nvalue                                   0\\nid                                   5658\\nfood            Ostrich, top loin, cooked\\nfgroup                   Poultry Products\\nmanufacturer\\nName: 30000\\nThe tools that you need to slice and dice, aggregate, and visualize this dataset will be\\nexplored in detail in the next two chapters, so after you get a handle on those methods\\nyou might return to this dataset. For example, we could a plot of median values by food\\ngroup and nutrient type (see Figure 7-1):\\nIn [281]: result = ndata.groupby(['nutrient', 'fgroup'])['value'].quantile(0.5)\\nIn [282]: result['Zinc, Zn'].order().plot(kind='barh')\\nWith a little cleverness, you can find which food is most dense in each nutrient:\\nby_nutrient = ndata.groupby(['nutgroup', 'nutrient'])\\nget_maximum = lambda x: x.xs(x.value.idxmax())\\nget_minimum = lambda x: x.xs(x.value.idxmin())\\nmax_foods = by_nutrient.apply(get_maximum)[['value', 'food']]\\n# make the food a little smaller\\nmax_foods.food = max_foods.food.str[:50]\\n216 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"The resulting DataFrame is a bit too large to display in the book; here is just the 'Amino\\nAcids' nutrient group:\\nIn [284]: max_foods.ix['Amino Acids']['food']\\nOut[284]:\\nnutrient\\nAlanine                           Gelatins, dry powder, unsweetened\\nArginine                               Seeds, sesame flour, low-fat\\nAspartic acid                                   Soy protein isolate\\nCystine                Seeds, cottonseed flour, low fat (glandless)\\nGlutamic acid                                   Soy protein isolate\\nGlycine                           Gelatins, dry powder, unsweetened\\nHistidine                Whale, beluga, meat, dried (Alaska Native)\\nHydroxyproline    KENTUCKY FRIED CHICKEN, Fried Chicken, ORIGINAL R\\nIsoleucine        Soy protein isolate, PROTEIN TECHNOLOGIES INTERNA\\nLeucine           Soy protein isolate, PROTEIN TECHNOLOGIES INTERNA\\nLysine            Seal, bearded (Oogruk), meat, dried (Alaska Nativ\\nMethionine                    Fish, cod, Atlantic, dried and salted\\nPhenylalanine     Soy protein isolate, PROTEIN TECHNOLOGIES INTERNA\\nProline                           Gelatins, dry powder, unsweetened\\nSerine            Soy protein isolate, PROTEIN TECHNOLOGIES INTERNA\\nThreonine         Soy protein isolate, PROTEIN TECHNOLOGIES INTERNA\\nTryptophan         Sea lion, Steller, meat with fat (Alaska Native)\\nTyrosine          Soy protein isolate, PROTEIN TECHNOLOGIES INTERNA\\nValine            Soy protein isolate, PROTEIN TECHNOLOGIES INTERNA\\nName: food\\nFigure 7-1. Median Zinc values by nutrient group\\nExample: USDA Food Database | 217\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf', 'page_content': 'www.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'CHAPTER 8\\nPlotting and Visualization\\nMaking plots and static or interactive visualizations is one of the most important tasks\\nin data analysis. It may be a part of the exploratory process; for example, helping iden-\\ntify outliers, needed data transformations, or coming up with ideas for models. For\\nothers, building an interactive visualization for the web using a toolkit like d3.js (http:\\n//d3js.org/) may be the end goal. Python has many visualization tools (see the end of\\nthis chapter), but I’ll be mainly focused on matplotlib ( http://matplotlib.sourceforge\\n.net).\\nmatplotlib is a (primarily 2D) desktop plotting package designed for creating publica-\\ntion-quality plots. The project was started by John Hunter in 2002 to enable a MAT-\\nLAB-like plotting interface in Python. He, Fernando Pérez (of IPython), and others have\\ncollaborated for many years since then to make IPython combined with matplotlib a\\nvery functional and productive environment for scientific computing. When used in\\ntandem with a GUI toolkit (for example, within IPython), matplotlib has interactive\\nfeatures like zooming and panning. It supports many different GUI backends on all\\noperating systems and additionally can export graphics to all of the common vector\\nand raster graphics formats: PDF, SVG, JPG, PNG, BMP, GIF, etc. I have used it to\\nproduce almost all of the graphics outside of diagrams in this book.\\nmatplotlib has a number of add-on toolkits, such as mplot3d for 3D plots and basemap\\nfor mapping and projections. I will give an example using basemap to plot data on a map\\nand to read shapefiles at the end of the chapter.\\nTo follow along with the code examples in the chapter, make sure you have started\\nIPython in Pylab mode (ipython --pylab) or enabled GUI event loop integration with\\nthe %gui magic.\\nA Brief matplotlib API Primer\\nThere are several ways to interact with matplotlib. The most common is through pylab\\nmode in IPython by running ipython --pylab. This launches IPython configured to be\\nable to support the matplotlib GUI backend of your choice (Tk, wxPython, PyQt, Mac\\n219\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'OS X native, GTK). For most users, the default backend will be sufficient. Pylab mode\\nalso imports a large set of modules and functions into IPython to provide a more MAT-\\nLAB-like interface. You can test that everything is working by making a simple plot:\\nplot(np.arange(10))\\nIf everything is set up right, a new window should pop up with a line plot. You can\\nclose it by using the mouse or entering close(). Matplotlib API functions like plot and \\nclose are all in the matplotlib.pyplot module, which is typically imported by conven-\\ntion as:\\nimport matplotlib.pyplot as plt\\nWhile the pandas plotting functions described later deal with many of the mundane\\ndetails of making plots, should you wish to customize them beyond the function op-\\ntions provided you will need to learn a bit about the matplotlib API.\\nThere is not enough room in the book to give a comprehensive treatment\\nto the breadth and depth of functionality in matplotlib. It should be\\nenough to teach you the ropes to get up and running. The matplotlib\\ngallery and documentation are the best resource for becoming a plotting\\nguru and using advanced features.\\nFigures and Subplots\\nPlots in matplotlib reside within a Figure object. You can create a new figure with\\nplt.figure:\\nIn [13]: fig = plt.figure()\\nFigure 8-1. A more complex matplotlib financial plot\\n220 | Chapter 8: \\u2002Plotting and Visualization\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"If you are in pylab mode in IPython, a new empty window should pop up. plt.fig\\nure has a number of options, notably figsize will guarantee the figure has a certain size\\nand aspect ratio if saved to disk. Figures in matplotlib also support a numbering scheme\\n(for example, plt.figure(2)) that mimics MATLAB. You can get a reference to the\\nactive figure using plt.gcf().\\nYou can’t make a plot with a blank figure. You have to create one or more subplots\\nusing add_subplot:\\nIn [14]: ax1 = fig.add_subplot(2, 2, 1)\\nThis means that the figure should be 2 × 2, and we’re selecting the first of 4 subplots\\n(numbered from 1). If you create the next two subplots, you’ll end up with a figure that\\nlooks like Figure 8-2.\\nIn [15]: ax2 = fig.add_subplot(2, 2, 2)\\nIn [16]: ax3 = fig.add_subplot(2, 2, 3)\\nFigure 8-2. An empty matplotlib Figure with 3 subplots\\nWhen you issue a plotting command like plt.plot([1.5, 3.5, -2, 1.6]), matplotlib\\ndraws on the last figure and subplot used (creating one if necessary), thus hiding the\\nfigure and subplot creation. Thus, if we run the following command, you’ll get some-\\nthing like Figure 8-3:\\nIn [17]: from numpy.random import randn\\nIn [18]: plt.plot(randn(50).cumsum(), 'k--')\\nThe 'k--' is a style option instructing matplotlib to plot a black dashed line. The objects\\nreturned by fig.add_subplot above are AxesSubplot objects, on which you can directly\\nplot on the other empty subplots by calling each one’s instance methods, see Figure 8-4:\\nA Brief matplotlib API Primer | 221\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [19]: _ = ax1.hist(randn(100), bins=20, color='k', alpha=0.3)\\nIn [20]: ax2.scatter(np.arange(30), np.arange(30) + 3 * randn(30))\\nYou can find a comprehensive catalogue of plot types in the matplotlib documentation.\\nSince creating a figure with multiple subplots according to a particular layout is such\\na common task, there is a convenience method, plt.subplots, that creates a new figure\\nand returns a NumPy array containing the created subplot objects:\\nFigure 8-3. Figure after single plot\\nFigure 8-4. Figure after additional plots\\n222 | Chapter 8: \\u2002Plotting and Visualization\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [22]: fig, axes = plt.subplots(2, 3)\\nIn [23]: axes\\nOut[23]: \\narray([[Axes(0.125,0.536364;0.227941x0.363636),\\n        Axes(0.398529,0.536364;0.227941x0.363636),\\n        Axes(0.672059,0.536364;0.227941x0.363636)],\\n       [Axes(0.125,0.1;0.227941x0.363636),\\n        Axes(0.398529,0.1;0.227941x0.363636),\\n        Axes(0.672059,0.1;0.227941x0.363636)]], dtype=object)\\nThis is very useful as the axes array can be easily indexed like a two-dimensional array;\\nfor example, axes[0, 1]. You can also indicate that subplots should have the same X\\nor Y axis using sharex and sharey, respectively. This is especially useful when comparing\\ndata on the same scale; otherwise, matplotlib auto-scales plot limits independently. See\\nTable 8-1 for more on this method.\\nTable 8-1. pyplot.subplots options\\nArgument Description\\nnrows Number of rows of subplots\\nncols Number of columns of subplots\\nsharex All subplots should use the same X-axis ticks (adjusting the xlim will affect all subplots)\\nsharey All subplots should use the same Y-axis ticks (adjusting the ylim will affect all subplots)\\nsubplot_kw Dict of keywords for creating the\\n**fig_kw Additional keywords to subplots are used when creating the figure, such as plt.subplots(2, 2,\\nfigsize=(8, 6))\\nAdjusting the spacing around subplots\\nBy default matplotlib leaves a certain amount of padding around the outside of the\\nsubplots and spacing between subplots. This spacing is all specified relative to the\\nheight and width of the plot, so that if you resize the plot either programmatically or\\nmanually using the GUI window, the plot will dynamically adjust itself. The spacing\\ncan be most easily changed using the subplots_adjust Figure method, also available as\\na top-level function:\\nsubplots_adjust(left=None, bottom=None, right=None, top=None,\\n                wspace=None, hspace=None)\\nwspace and hspace controls the percent of the figure width and figure height, respec-\\ntively, to use as spacing between subplots. Here is a small example where I shrink the\\nspacing all the way to zero (see Figure 8-5):\\nfig, axes = plt.subplots(2, 2, sharex=True, sharey=True)\\nfor i in range(2):\\n    for j in range(2):\\n        axes[i, j].hist(randn(500), bins=50, color='k', alpha=0.5)\\nplt.subplots_adjust(wspace=0, hspace=0)\\nA Brief matplotlib API Primer | 223\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Figure 8-5. Figure with no inter-subplot spacing\\nYou may notice that the axis labels overlap. matplotlib doesn’t check whether the labels\\noverlap, so in a case like this you would need to fix the labels yourself by specifying\\nexplicit tick locations and tick labels. More on this in the coming sections.\\nColors, Markers, and Line Styles\\nMatplotlib’s main plot function accepts arrays of X and Y coordinates and optionally\\na string abbreviation indicating color and line style. For example, to plot x versus y with\\ngreen dashes, you would execute:\\nax.plot(x, y, 'g--')\\nThis way of specifying both color and linestyle in a string is provided as a convenience;\\nin practice if you were creating plots programmatically you might prefer not to have to\\nmunge strings together to create plots with the desired style. The same plot could also\\nhave been expressed more explicitly as:\\nax.plot(x, y, linestyle='--', color='g')\\nThere are a number of color abbreviations provided for commonly-used colors, but any\\ncolor on the spectrum can be used by specifying its RGB value (for example, '#CECE\\nCE'). You can see the full set of linestyles by looking at the docstring for plot.\\nLine plots can additionally have markers to highlight the actual data points. Since mat-\\nplotlib creates a continuous line plot, interpolating between points, it can occasionally\\nbe unclear where the points lie. The marker can be part of the style string, which must\\nhave color followed by marker type and line style (see Figure 8-6):\\nIn [28]: plt.plot(randn(30).cumsum(), 'ko--')\\n224 | Chapter 8: \\u2002Plotting and Visualization\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"This could also have been written more explicitly as:\\nplot(randn(30).cumsum(), color='k', linestyle='dashed', marker='o')\\nFor line plots, you will notice that subsequent points are linearly interpolated by de-\\nfault. This can be altered with the drawstyle option:\\nIn [30]: data = randn(30).cumsum()\\nIn [31]: plt.plot(data, 'k--', label='Default')\\nOut[31]: [<matplotlib.lines.Line2D at 0x461cdd0>]\\nIn [32]: plt.plot(data, 'k-', drawstyle='steps-post', label='steps-post')\\nOut[32]: [<matplotlib.lines.Line2D at 0x461f350>]\\nIn [33]: plt.legend(loc='best')\\nTicks, Labels, and Legends\\nFor most kinds of plot decorations, there are two main ways to do things: using the\\nprocedural pyplot interface (which will be very familiar to MATLAB users) and the\\nmore object-oriented native matplotlib API.\\nThe pyplot interface, designed for interactive use, consists of methods like xlim,\\nxticks, and xticklabels. These control the plot range, tick locations, and tick labels,\\nrespectively. They can be used in two ways:\\n• Called with no arguments returns the current parameter value. For example \\nplt.xlim() returns the current X axis plotting range\\nFigure 8-6. Line plot with markers example\\nA Brief matplotlib API Primer | 225\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"• Called with parameters sets the parameter value. So plt.xlim([0, 10]), sets the X\\naxis range to 0 to 10\\nAll such methods act on the active or most recently-created AxesSubplot. Each of them\\ncorresponds to two methods on the subplot object itself; in the case of xlim these are \\nax.get_xlim and ax.set_xlim. I prefer to use the subplot instance methods myself in\\nthe interest of being explicit (and especially when working with multiple subplots), but\\nyou can certainly use whichever you find more convenient.\\nSetting the title, axis labels, ticks, and ticklabels\\nTo illustrate customizing the axes, I’ll create a simple figure and plot of a random walk\\n(see Figure 8-8):\\nIn [34]: fig = plt.figure(); ax = fig.add_subplot(1, 1, 1)\\nIn [35]: ax.plot(randn(1000).cumsum())\\nTo change the X axis ticks, it’s easiest to use set_xticks and set_xticklabels. The\\nformer instructs matplotlib where to place the ticks along the data range; by default\\nthese locations will also be the labels. But we can set any other values as the labels using\\nset_xticklabels:\\nIn [36]: ticks = ax.set_xticks([0, 250, 500, 750, 1000])\\nIn [37]: labels = ax.set_xticklabels(['one', 'two', 'three', 'four', 'five'],\\n   ....:                             rotation=30, fontsize='small')\\nLastly, set_xlabel gives a name to the X axis and set_title the subplot title:\\nFigure 8-7. Line plot with different drawstyle options\\n226 | Chapter 8: \\u2002Plotting and Visualization\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [38]: ax.set_title('My first matplotlib plot')\\nOut[38]: <matplotlib.text.Text at 0x7f9190912850>\\nIn [39]: ax.set_xlabel('Stages')\\nSee Figure 8-9 for the resulting figure. Modifying the Y axis consists of the same process,\\nsubstituting y for x in the above.\\nFigure 8-9. Simple plot for illustrating xticks\\nFigure 8-8. Simple plot for illustrating xticks\\nA Brief matplotlib API Primer | 227\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Adding legends\\nLegends are another critical element for identifying plot elements. There are a couple\\nof ways to add one. The easiest is to pass the label argument when adding each piece\\nof the plot:\\nIn [40]: fig = plt.figure(); ax = fig.add_subplot(1, 1, 1)\\nIn [41]: ax.plot(randn(1000).cumsum(), 'k', label='one')\\nOut[41]: [<matplotlib.lines.Line2D at 0x4720a90>]\\nIn [42]: ax.plot(randn(1000).cumsum(), 'k--', label='two')\\nOut[42]: [<matplotlib.lines.Line2D at 0x4720f90>]\\nIn [43]: ax.plot(randn(1000).cumsum(), 'k.', label='three')\\nOut[43]: [<matplotlib.lines.Line2D at 0x4723550>]\\nOnce you’ve done this, you can either call ax.legend() or plt.legend() to automatically\\ncreate a legend:\\nIn [44]: ax.legend(loc='best')\\nSee Figure 8-10. The loc tells matplotlib where to place the plot. If you aren’t picky\\n'best' is a good option, as it will choose a location that is most out of the way. To\\nexclude one or more elements from the legend, pass no label or label='_nolegend_'.\\nAnnotations and Drawing on a Subplot\\nIn addition to the standard plot types, you may wish to draw your own plot annotations,\\nwhich could consist of text, arrows, or other shapes.\\nFigure 8-10. Simple plot with 3 lines and legend\\n228 | Chapter 8: \\u2002Plotting and Visualization\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Annotations and text can be added using the text, arrow, and annotate functions.\\ntext draws text at given coordinates (x, y) on the plot with optional custom styling:\\nax.text(x, y, 'Hello world!',\\n        family='monospace', fontsize=10)\\nAnnotations can draw both text and arrows arranged appropriately. As an example,\\nlet’s plot the closing S&P 500 index price since 2007 (obtained from Yahoo! Finance)\\nand annotate it with some of the important dates from the 2008-2009 financial crisis.\\nSee Figure 8-11 for the result:\\nfrom datetime import datetime\\nfig = plt.figure()\\nax = fig.add_subplot(1, 1, 1)\\ndata = pd.read_csv('ch08/spx.csv', index_col=0, parse_dates=True)\\nspx = data['SPX']\\nspx.plot(ax=ax, style='k-')\\ncrisis_data = [\\n    (datetime(2007, 10, 11), 'Peak of bull market'),\\n    (datetime(2008, 3, 12), 'Bear Stearns Fails'),\\n    (datetime(2008, 9, 15), 'Lehman Bankruptcy')\\n]\\nfor date, label in crisis_data:\\n    ax.annotate(label, xy=(date, spx.asof(date) + 50),\\n                xytext=(date, spx.asof(date) + 200),\\n                arrowprops=dict(facecolor='black'),\\n                horizontalalignment='left', verticalalignment='top')\\n# Zoom in on 2007-2010\\nax.set_xlim(['1/1/2007', '1/1/2011'])\\nax.set_ylim([600, 1800])\\nax.set_title('Important dates in 2008-2009 financial crisis')\\nSee the online matplotlib gallery for many more annotation examples to learn from.\\nDrawing shapes requires some more care. matplotlib has objects that represent many\\ncommon shapes, referred to as patches. Some of these, like Rectangle and Circle are\\nfound in matplotlib.pyplot, but the full set is located in matplotlib.patches.\\nTo add a shape to a plot, you create the patch object shp and add it to a subplot by\\ncalling ax.add_patch(shp) (see Figure 8-12):\\nfig = plt.figure()\\nax = fig.add_subplot(1, 1, 1)\\nrect = plt.Rectangle((0.2, 0.75), 0.4, 0.15, color='k', alpha=0.3)\\ncirc = plt.Circle((0.7, 0.2), 0.15, color='b', alpha=0.3)\\npgon = plt.Polygon([[0.15, 0.15], [0.35, 0.4], [0.2, 0.6]],\\n                   color='g', alpha=0.5)\\nA Brief matplotlib API Primer | 229\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'ax.add_patch(rect)\\nax.add_patch(circ)\\nax.add_patch(pgon)\\nFigure 8-11. Important dates in 2008-2009 financial crisis\\nFigure 8-12. Figure composed from 3 different patches\\nIf you look at the implementation of many familiar plot types, you will see that they\\nare assembled from patches.\\n230 | Chapter 8: \\u2002Plotting and Visualization\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Saving Plots to File\\nThe active figure can be saved to file using plt.savefig. This method is equivalent to\\nthe figure object’s savefig instance method. For example, to save an SVG version of a\\nfigure, you need only type:\\nplt.savefig('figpath.svg')\\nThe file type is inferred from the file extension. So if you used .pdf instead you would\\nget a PDF. There are a couple of important options that I use frequently for publishing\\ngraphics: dpi, which controls the dots-per-inch resolution, and bbox_inches, which can\\ntrim the whitespace around the actual figure. To get the same plot as a PNG above with\\nminimal whitespace around the plot and at 400 DPI, you would do:\\nplt.savefig('figpath.png', dpi=400, bbox_inches='tight')\\nsavefig doesn’t have to write to disk; it can also write to any file-like object, such as a\\nStringIO:\\nfrom io import StringIO\\nbuffer = StringIO()\\nplt.savefig(buffer)\\nplot_data = buffer.getvalue()\\nFor example, this is useful for serving dynamically-generated images over the web.\\nTable 8-2. Figure.savefig options\\nArgument Description\\nfname String containing a filepath or a Python file-like object. The figure format is inferred from the file\\nextension, e.g. .pdf for PDF or .png for PNG.\\ndpi The figure resolution in dots per inch; defaults to 100 out of the box but can be configured\\nfacecolor, edge\\ncolor\\nThe color of the figure background outside of the subplots. 'w' (white), by default\\nformat The explicit file format to use ('png', 'pdf', 'svg', 'ps', 'eps', ...)\\nbbox_inches The portion of the figure to save. If 'tight' is passed, will attempt to trim the empty space around\\nthe figure\\nmatplotlib Configuration\\nmatplotlib comes configured with color schemes and defaults that are geared primarily\\ntoward preparing figures for publication. Fortunately, nearly all of the default behavior\\ncan be customized via an extensive set of global parameters governing figure size, sub-\\nplot spacing, colors, font sizes, grid styles, and so on. There are two main ways to\\ninteract with the matplotlib configuration system. The first is programmatically from\\nPython using the rc method. For example, to set the global default figure size to be 10\\nx 10, you could enter:\\nplt.rc('figure', figsize=(10, 10))\\nA Brief matplotlib API Primer | 231\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"The first argument to rc is the component you wish to customize, such as 'figure',\\n'axes', 'xtick', 'ytick', 'grid', 'legend' or many others. After that can follow a\\nsequence of keyword arguments indicating the new parameters. An easy way to write\\ndown the options in your program is as a dict:\\nfont_options = {'family' : 'monospace',\\n                'weight' : 'bold',\\n                'size'   : 'small'}\\nplt.rc('font', **font_options)\\nFor more extensive customization and to see a list of all the options, matplotlib comes\\nwith a configuration file matplotlibrc in the matplotlib/mpl-data directory. If you cus-\\ntomize this file and place it in your home directory titled .matplotlibrc, it will be loaded\\neach time you use matplotlib.\\nPlotting Functions in pandas\\nAs you’ve seen, matplotlib is actually a fairly low-level tool. You assemble a plot from\\nits base components: the data display (the type of plot: line, bar, box, scatter, contour,\\netc.), legend, title, tick labels, and other annotations. Part of the reason for this is that\\nin many cases the data needed to make a complete plot is spread across many objects.\\nIn pandas we have row labels, column labels, and possibly grouping information. This\\nmeans that many kinds of fully-formed plots that would ordinarily require a lot of\\nmatplotlib code can be expressed in one or two concise statements. Therefore, pandas\\nhas an increasing number of high-level plotting methods for creating standard visual-\\nizations that take advantage of how data is organized in DataFrame objects.\\nAs of this writing, the plotting functionality in pandas is undergoing\\nquite a bit of work. As part of the 2012 Google Summer of Code pro-\\ngram, a student is working full time to add features and to make the\\ninterface more consistent and usable. Thus, it’s possible that this code\\nmay fall out-of-date faster than the other things in this book. The online\\npandas documentation will be the best resource in that event.\\nLine Plots\\nSeries and DataFrame each have a plot method for making many different plot types.\\nBy default, they make line plots (see Figure 8-13):\\nIn [55]: s = Series(np.random.randn(10).cumsum(), index=np.arange(0, 100, 10))\\nIn [56]: s.plot()\\nThe Series object’s index is passed to matplotlib for plotting on the X axis, though this\\ncan be disabled by passing use_index=False. The X axis ticks and limits can be adjusted\\nusing the xticks and xlim options, and Y axis respectively using yticks and ylim. See\\n232 | Chapter 8: \\u2002Plotting and Visualization\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Table 8-3 for a full listing of plot options. I’ll comment on a few more of them through-\\nout this section and leave the rest to you to explore.\\nMost of pandas’s plotting methods accept an optional ax parameter, which can be a\\nmatplotlib subplot object. This gives you more flexible placement of subplots in a grid\\nlayout. There will be more on this in the later section on the matplotlib API.\\nDataFrame’s plot method plots each of its columns as a different line on the same\\nsubplot, creating a legend automatically (see Figure 8-14):\\nIn [57]: df = DataFrame(np.random.randn(10, 4).cumsum(0),\\n   ....:                columns=['A', 'B', 'C', 'D'],\\n   ....:                index=np.arange(0, 100, 10))\\nIn [58]: df.plot()\\nAdditional keyword arguments to plot are passed through to the re-\\nspective matplotlib plotting function, so you can further customize\\nthese plots by learning more about the matplotlib API.\\nTable 8-3. Series.plot method arguments\\nArgument Description\\nlabel Label for plot legend\\nax matplotlib subplot object to plot on. If nothing passed, uses active matplotlib subplot\\nstyle Style string, like 'ko--', to be passed to matplotlib.\\nalpha The plot fill opacity (from 0 to 1)\\nFigure 8-13. Simple Series plot example\\nPlotting Functions in pandas | 233\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Argument Description\\nkind Can be 'line', 'bar', 'barh', 'kde'\\nlogy Use logarithmic scaling on the Y axis\\nuse_index Use the object index for tick labels\\nrot Rotation of tick labels (0 through 360)\\nxticks Values to use for X axis ticks\\nyticks Values to use for Y axis ticks\\nxlim X axis limits (e.g. [0, 10])\\nylim Y axis limits\\ngrid Display axis grid (on by default)\\nDataFrame has a number of options allowing some flexibility with how the columns\\nare handled; for example, whether to plot them all on the same subplot or to create\\nseparate subplots. See Table 8-4 for more on these.\\nTable 8-4. DataFrame-specific plot arguments\\nArgument Description\\nsubplots Plot each DataFrame column in a separate subplot\\nsharex If subplots=True, share the same X axis, linking ticks and limits\\nsharey If subplots=True, share the same Y axis\\nfigsize Size of figure to create as tuple\\nFigure 8-14. Simple DataFrame plot example\\n234 | Chapter 8: \\u2002Plotting and Visualization\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Argument Description\\ntitle Plot title as string\\nlegend Add a subplot legend (True by default)\\nsort_columns Plot columns in alphabetical order; by default uses existing column order\\nFor time series plotting, see Chapter 10.\\nBar Plots\\nMaking bar plots instead of line plots is a simple as passing kind='bar' (for vertical\\nbars) or kind='barh' (for horizontal bars). In this case, the Series or DataFrame index\\nwill be used as the X (bar) or Y (barh) ticks (see Figure 8-15):\\nIn [59]: fig, axes = plt.subplots(2, 1)\\nIn [60]: data = Series(np.random.rand(16), index=list('abcdefghijklmnop'))\\nIn [61]: data.plot(kind='bar', ax=axes[0], color='k', alpha=0.7)\\nOut[61]: <matplotlib.axes.AxesSubplot at 0x4ee7750>\\nIn [62]: data.plot(kind='barh', ax=axes[1], color='k', alpha=0.7)\\nFor more on the plt.subplots function and matplotlib axes and figures,\\nsee the later section in this chapter.\\nWith a DataFrame, bar plots group the values in each row together in a group in bars,\\nside by side, for each value. See Figure 8-16:\\nIn [63]: df = DataFrame(np.random.rand(6, 4),\\n   ....:                index=['one', 'two', 'three', 'four', 'five', 'six'],\\n   ....:                columns=pd.Index(['A', 'B', 'C', 'D'], name='Genus'))\\nIn [64]: df\\nOut[64]: \\nGenus         A         B         C         D\\none    0.301686  0.156333  0.371943  0.270731\\ntwo    0.750589  0.525587  0.689429  0.358974\\nthree  0.381504  0.667707  0.473772  0.632528\\nfour   0.942408  0.180186  0.708284  0.641783\\nfive   0.840278  0.909589  0.010041  0.653207\\nsix    0.062854  0.589813  0.811318  0.060217\\nIn [65]: df.plot(kind='bar')\\nPlotting Functions in pandas | 235\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Figure 8-15. Horizonal and vertical bar plot example\\nNote that the name “Genus” on the DataFrame’s columns is used to title the legend.\\nStacked bar plots are created from a DataFrame by passing stacked=True, resulting in\\nthe value in each row being stacked together (see Figure 8-17):\\nIn [67]: df.plot(kind='barh', stacked=True, alpha=0.5)\\nA useful recipe for bar plots (as seen in an earlier chapter) is to visualize\\na Series’s value frequency using value_counts: s.value_counts\\n().plot(kind='bar')\\nReturning to the tipping data set used earlier in the book, suppose we wanted to make\\na stacked bar plot showing the percentage of data points for each party size on each\\nday. I load the data using read_csv and make a cross-tabulation by day and party size:\\nIn [68]: tips = pd.read_csv('ch08/tips.csv')\\nIn [69]: party_counts = pd.crosstab(tips.day, tips.size)\\nIn [70]: party_counts\\nOut[70]: \\nsize  1   2   3   4  5  6\\nday                      \\nFri   1  16   1   1  0  0\\nSat   2  53  18  13  1  0\\nSun   0  39  15  18  3  1\\nThur  1  48   4   5  1  3\\n236 | Chapter 8: \\u2002Plotting and Visualization\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': '# Not many 1- and 6-person parties\\nIn [71]: party_counts = party_counts.ix[:, 2:5]\\nFigure 8-16. DataFrame bar plot example\\nFigure 8-17. DataFrame stacked bar plot example\\nThen, normalize so that each row sums to 1 (I have to cast to float to avoid integer\\ndivision issues on Python 2.7) and make the plot (see Figure 8-18):\\n# Normalize to sum to 1\\nIn [72]: party_pcts = party_counts.div(party_counts.sum(1).astype(float), axis=0)\\nPlotting Functions in pandas | 237\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [73]: party_pcts\\nOut[73]: \\nsize         2         3         4         5\\nday                                         \\nFri   0.888889  0.055556  0.055556  0.000000\\nSat   0.623529  0.211765  0.152941  0.011765\\nSun   0.520000  0.200000  0.240000  0.040000\\nThur  0.827586  0.068966  0.086207  0.017241\\nIn [74]: party_pcts.plot(kind='bar', stacked=True)\\nFigure 8-18. Fraction of parties by size on each day\\nSo you can see that party sizes appear to increase on the weekend in this data set.\\nHistograms and Density Plots\\nA histogram, with which you may be well-acquainted, is a kind of bar plot that gives a\\ndiscretized display of value frequency. The data points are split into discrete, evenly\\nspaced bins, and the number of data points in each bin is plotted. Using the tipping\\ndata from before, we can make a histogram of tip percentages of the total bill using the \\nhist method on the Series (see Figure 8-19):\\nIn [76]: tips['tip_pct'] = tips['tip'] / tips['total_bill']\\nIn [77]: tips['tip_pct'].hist(bins=50)\\n238 | Chapter 8: \\u2002Plotting and Visualization\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Figure 8-19. Histogram of tip percentages\\nA related plot type is a density plot, which is formed by computing an estimate of a\\ncontinuous probability distribution that might have generated the observed data. A\\nusual procedure is to approximate this distribution as a mixture of kernels, that is,\\nsimpler distributions like the normal (Gaussian) distribution. Thus, density plots are\\nalso known as KDE (kernel density estimate) plots. Using plot with kind='kde' makes\\na density plot using the standard mixture-of-normals KDE (see Figure 8-20):\\nIn [79]: tips['tip_pct'].plot(kind='kde')\\nThese two plot types are often plotted together; the histogram in normalized form (to\\ngive a binned density) with a kernel density estimate plotted on top. As an example,\\nconsider a bimodal distribution consisting of draws from two different standard normal\\ndistributions (see Figure 8-21):\\nIn [81]: comp1 = np.random.normal(0, 1, size=200)  # N(0, 1)\\nIn [82]: comp2 = np.random.normal(10, 2, size=200)  # N(10, 4)\\nIn [83]: values = Series(np.concatenate([comp1, comp2]))\\nIn [84]: values.hist(bins=100, alpha=0.3, color='k', normed=True)\\nOut[84]: <matplotlib.axes.AxesSubplot at 0x5cd2350>\\nIn [85]: values.plot(kind='kde', style='k--')\\nScatter Plots\\nScatter plots are a useful way of examining the relationship between two one-dimen-\\nsional data series. matplotlib has a scatter plotting method that is the workhorse of\\nPlotting Functions in pandas | 239\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"making these kinds of plots. To give an example, I load the macrodata dataset from the\\nstatsmodels project, select a few variables, then compute log differences:\\nIn [86]: macro = pd.read_csv('ch08/macrodata.csv')\\nIn [87]: data = macro[['cpi', 'm1', 'tbilrate', 'unemp']]\\nIn [88]: trans_data = np.log(data).diff().dropna()\\nFigure 8-20. Density plot of tip percentages\\nFigure 8-21. Normalized histogram of normal mixture with density estimate\\n240 | Chapter 8: \\u2002Plotting and Visualization\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [89]: trans_data[-5:]\\nOut[89]: \\n          cpi        m1  tbilrate     unemp\\n198 -0.007904  0.045361 -0.396881  0.105361\\n199 -0.021979  0.066753 -2.277267  0.139762\\n200  0.002340  0.010286  0.606136  0.160343\\n201  0.008419  0.037461 -0.200671  0.127339\\n202  0.008894  0.012202 -0.405465  0.042560\\nIt’s easy to plot a simple scatter plot using plt.scatter (see Figure 8-22):\\nIn [91]: plt.scatter(trans_data['m1'], trans_data['unemp'])\\nOut[91]: <matplotlib.collections.PathCollection at 0x43c31d0>\\nIn [92]: plt.title('Changes in log %s vs. log %s' % ('m1', 'unemp'))\\nFigure 8-22. A simple scatter plot\\nIn exploratory data analysis it’s helpful to be able to look at all the scatter plots among\\na group of variables; this is known as a pairs plot or scatter plot matrix. Making such a\\nplot from scratch is a bit of work, so pandas has a scatter_matrix function for creating\\none from a DataFrame. It also supports placing histograms or density plots of each\\nvariable along the diagonal. See Figure 8-23 for the resulting plot:\\nIn [93]: scatter_matrix(trans_data, diagonal='kde', color='k', alpha=0.3)\\nPlotting Maps: Visualizing Haiti Earthquake Crisis Data\\nUshahidi is a non-profit software company that enables crowdsourcing of information\\nrelated to natural disasters and geopolitical events via text message. Many of these data\\nsets are then published on their website for analysis and visualization. I downloaded\\nPlotting Maps: Visualizing Haiti Earthquake Crisis Data | 241\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"the data collected during the 2010 Haiti earthquake crisis and aftermath, and I’ll show\\nyou how I prepared the data for analysis and visualization using pandas and other tools\\nwe have looked at thus far. After downloading the CSV file from the above link, we can\\nload it into a DataFrame using read_csv:\\nIn [94]: data = pd.read_csv('ch08/Haiti.csv')\\nIn [95]: data\\nOut[95]: \\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 3593 entries, 0 to 3592\\nData columns:\\nSerial            3593  non-null values\\nINCIDENT TITLE    3593  non-null values\\nINCIDENT DATE     3593  non-null values\\nLOCATION          3593  non-null values\\nDESCRIPTION       3593  non-null values\\nCATEGORY          3587  non-null values\\nLATITUDE          3593  non-null values\\nLONGITUDE         3593  non-null values\\nAPPROVED          3593  non-null values\\nVERIFIED          3593  non-null values\\ndtypes: float64(2), int64(1), object(7)\\nIt’s easy now to tinker with this data set to see what kinds of things we might want to\\ndo with it. Each row represents a report sent from someone’s mobile phone indicating\\nan emergency or some other problem. Each has an associated timestamp and a location\\nas latitude and longitude:\\nIn [96]: data[['INCIDENT DATE', 'LATITUDE', 'LONGITUDE']][:10]\\nOut[96]: \\n      INCIDENT DATE   LATITUDE   LONGITUDE\\nFigure 8-23. Scatter plot matrix of statsmodels macro data\\n242 | Chapter 8: \\u2002Plotting and Visualization\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"0  05/07/2010 17:26  18.233333  -72.533333\\n1  28/06/2010 23:06  50.226029    5.729886\\n2  24/06/2010 16:21  22.278381  114.174287\\n3  20/06/2010 21:59  44.407062    8.933989\\n4  18/05/2010 16:26  18.571084  -72.334671\\n5  26/04/2010 13:14  18.593707  -72.310079\\n6  26/04/2010 14:19  18.482800  -73.638800\\n7  26/04/2010 14:27  18.415000  -73.195000\\n8  15/03/2010 10:58  18.517443  -72.236841\\n9  15/03/2010 11:00  18.547790  -72.410010\\nThe CATEGORY field contains a comma-separated list of codes indicating the type of\\nmessage:\\nIn [97]: data['CATEGORY'][:6]\\nOut[97]: \\n0          1. Urgences | Emergency, 3. Public Health, \\n1    1. Urgences | Emergency, 2. Urgences logistiques \\n2    2. Urgences logistiques | Vital Lines, 8. Autre |\\n3                            1. Urgences | Emergency, \\n4                            1. Urgences | Emergency, \\n5                       5e. Communication lines down, \\nName: CATEGORY\\nIf you notice above in the data summary, some of the categories are missing, so we\\nmight want to drop these data points. Additionally, calling describe shows that there\\nare some aberrant locations:\\nIn [98]: data.describe()\\nOut[98]: \\n            Serial     LATITUDE    LONGITUDE\\ncount  3593.000000  3593.000000  3593.000000\\nmean   2080.277484    18.611495   -72.322680\\nstd    1171.100360     0.738572     3.650776\\nmin       4.000000    18.041313   -74.452757\\n25%    1074.000000    18.524070   -72.417500\\n50%    2163.000000    18.539269   -72.335000\\n75%    3088.000000    18.561820   -72.293570\\nmax    4052.000000    50.226029   114.174287\\nCleaning the bad locations and removing the missing categories is now fairly simple:\\nIn [99]: data = data[(data.LATITUDE > 18) & (data.LATITUDE < 20) &\\n   ....:             (data.LONGITUDE > -75) & (data.LONGITUDE < -70)\\n   ....:             & data.CATEGORY.notnull()]\\nNow we might want to do some analysis or visualization of this data by category, but\\neach category field may have multiple categories. Additionally, each category is given\\nas a code plus an English and possibly also a French code name. Thus, a little bit of\\nwrangling is required to get the data into a more agreeable form. First, I wrote these\\ntwo functions to get a list of all the categories and to split each category into a code and\\nan English name:\\ndef to_cat_list(catstr):\\n    stripped = (x.strip() for x in catstr.split(','))\\nPlotting Maps: Visualizing Haiti Earthquake Crisis Data | 243\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"return [x for x in stripped if x]\\ndef get_all_categories(cat_series):\\n    cat_sets = (set(to_cat_list(x)) for x in cat_series)\\n    return sorted(set.union(*cat_sets))\\ndef get_english(cat):\\n    code, names = cat.split('.')\\n    if '|' in names:\\n        names = names.split(' | ')[1]\\n    return code, names.strip()\\nYou can test out that the get_english function does what you expect:\\nIn [101]: get_english('2. Urgences logistiques | Vital Lines')\\nOut[101]: ('2', 'Vital Lines')\\nNow, I make a dict mapping code to name because we’ll use the codes for analysis.\\nWe’ll use this later when adorning plots (note the use of a generator expression in lieu\\nof a list comprehension):\\nIn [102]: all_cats = get_all_categories(data.CATEGORY)\\n# Generator expression\\nIn [103]: english_mapping = dict(get_english(x) for x in all_cats)\\nIn [104]: english_mapping['2a']\\nOut[104]: 'Food Shortage'\\nIn [105]: english_mapping['6c']\\nOut[105]: 'Earthquake and aftershocks'\\nThere are many ways to go about augmenting the data set to be able to easily select\\nrecords by category. One way is to add indicator (or dummy) columns, one for each\\ncategory. To do that, first extract the unique category codes and construct a DataFrame\\nof zeros having those as its columns and the same index as data:\\ndef get_code(seq):\\n    return [x.split('.')[0] for x in seq if x]\\nall_codes = get_code(all_cats)\\ncode_index = pd.Index(np.unique(all_codes))\\ndummy_frame = DataFrame(np.zeros((len(data), len(code_index))),\\n                        index=data.index, columns=code_index)\\nIf all goes well, dummy_frame should look something like this:\\nIn [107]: dummy_frame.ix[:, :6]\\nOut[107]: \\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 3569 entries, 0 to 3592\\nData columns:\\n1     3569  non-null values\\n1a    3569  non-null values\\n1b    3569  non-null values\\n1c    3569  non-null values\\n244 | Chapter 8: \\u2002Plotting and Visualization\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"1d    3569  non-null values\\n2     3569  non-null values\\ndtypes: float64(6)\\nAs you recall, the trick is then to set the appropriate entries of each row to 1, lastly\\njoining this with data:\\nfor row, cat in zip(data.index, data.CATEGORY):\\n    codes = get_code(to_cat_list(cat))\\n    dummy_frame.ix[row, codes] = 1\\ndata = data.join(dummy_frame.add_prefix('category_'))\\ndata finally now has new columns like:\\nIn [109]: data.ix[:, 10:15]\\nOut[109]: \\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 3569 entries, 0 to 3592\\nData columns:\\ncategory_1     3569  non-null values\\ncategory_1a    3569  non-null values\\ncategory_1b    3569  non-null values\\ncategory_1c    3569  non-null values\\ncategory_1d    3569  non-null values\\ndtypes: float64(5)\\nLet’s make some plots! As this is spatial data, we’d like to plot the data by category on\\na map of Haiti. The basemap toolkit (http://matplotlib.github.com/basemap), an add-on\\nto matplotlib, enables plotting 2D data on maps in Python. basemap provides many\\ndifferent globe projections and a means for transforming projecting latitude and lon-\\ngitude coordinates on the globe onto a two-dimensional matplotlib plot. After some\\ntrial and error and using the above data as a guideline, I wrote this function which draws\\na simple black and white map of Haiti:\\nfrom mpl_toolkits.basemap import Basemap\\nimport matplotlib.pyplot as plt\\ndef basic_haiti_map(ax=None, lllat=17.25, urlat=20.25,\\n                    lllon=-75, urlon=-71):\\n    # create polar stereographic Basemap instance.\\n    m = Basemap(ax=ax, projection='stere',\\n                lon_0=(urlon + lllon) / 2,\\n                lat_0=(urlat + lllat) / 2,\\n                llcrnrlat=lllat, urcrnrlat=urlat,\\n                llcrnrlon=lllon, urcrnrlon=urlon,\\n                resolution='f')\\n    # draw coastlines, state and country boundaries, edge of map.\\n    m.drawcoastlines()\\n    m.drawstates()\\n    m.drawcountries()\\n    return m\\nThe idea, now, is that the returned Basemap object, knows how to transform coordinates\\nonto the canvas. I wrote the following code to plot the data observations for a number\\nPlotting Maps: Visualizing Haiti Earthquake Crisis Data | 245\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"of report categories. For each category, I filter down the data set to the coordinates\\nlabeled by that category, plot a Basemap on the appropriate subplot, transform the co-\\nordinates, then plot the points using the Basemap’s plot method:\\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 10))\\nfig.subplots_adjust(hspace=0.05, wspace=0.05)\\nto_plot = ['2a', '1', '3c', '7a']\\nlllat=17.25; urlat=20.25; lllon=-75; urlon=-71\\nfor code, ax in zip(to_plot, axes.flat):\\n    m = basic_haiti_map(ax, lllat=lllat, urlat=urlat,\\n                        lllon=lllon, urlon=urlon)\\n    cat_data = data[data['category_%s' % code] == 1]\\n    # compute map proj coordinates.\\n    x, y = m(cat_data.LONGITUDE, cat_data.LATITUDE)\\n    m.plot(x, y, 'k.', alpha=0.5)\\n    ax.set_title('%s: %s' % (code, english_mapping[code]))\\nThe resulting figure can be seen in Figure 8-24.\\nIt seems from the plot that most of the data is concentrated around the most populous\\ncity, Port-au-Prince. basemap allows you to overlap additional map data which comes\\nfrom what are called shapefiles. I first downloaded a shapefile with roads in Port-au-\\nPrince (see http://cegrp.cga.harvard.edu/haiti/?q=resources_data). The Basemap object\\nconveniently has a readshapefile method so that, after extracting the road data archive,\\nI added just the following lines to my code:\\nFigure 8-24. Haiti crisis data for 4 categories\\n246 | Chapter 8: \\u2002Plotting and Visualization\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"shapefile_path = 'ch08/PortAuPrince_Roads/PortAuPrince_Roads'\\nm.readshapefile(shapefile_path, 'roads')\\nAfter a little more trial and error with the latitude and longitude boundaries, I was able\\nto make Figure 8-25 for the “Food shortage” category.\\nPython Visualization Tool Ecosystem\\nAs is common with open source, there are a plethora of options for creating graphics\\nin Python (too many to list). In addition to open source, there are numerous commercial\\nlibraries with Python bindings.\\nIn this chapter and throughout the book, I have been primarily concerned with mat-\\nplotlib as it is the most widely used plotting tool in Python. While it’s an important\\npart of the scientific Python ecosystem, matplotlib has plenty of shortcomings when it\\ncomes to the creation and display of statistical graphics. MATLAB users will likely find\\nmatplotlib familiar, while R users (especially users of the excellent ggplot2 and trel\\nlis packages) may be somewhat disappointed (at least as of this writing). It is possible\\nto make beautiful plots for display on the web in matplotlib, but doing so often requires\\nsignificant effort as the library is designed for the printed page. Aesthetics aside, it is\\nsufficient for most needs. In pandas, I, along with the other developers, have sought to\\nbuild a convenient user interface that makes it easier to make most kinds of plots com-\\nmonplace in data analysis.\\nThere are a number of other visualization tools in wide use. I list a few of them here\\nand encourage you to explore the ecosystem.\\nFigure 8-25. Food shortage reports in Port-au-Prince during the Haiti earthquake crisis\\nPython Visualization Tool Ecosystem | 247\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Chaco\\nChaco (http://code.enthought.com/chaco/), developed by Enthought, is a plotting tool-\\nkit suitable both for static plotting and interactive visualizations. It is especially well-\\nsuited for expressing complex visualizations with data interrelationships. Compared\\nwith matplotlib, Chaco has much better support for interacting with plot elements and\\nrendering is very fast, making it a good choice for building interactive GUI applications.\\nFigure 8-26. A Chaco example plot\\nmayavi\\nThe mayavi project, developed by Prabhu Ramachandran, Gaël Varoquaux, and others,\\nis a 3D graphics toolkit built on the open source C++ graphics library VTK. mayavi,\\nlike matplotlib, integrates with IPython so that it is easy to use interactively. The plots\\ncan be panned, rotated, and zoomed using the mouse and keyboard. I used mayavi to\\nmake one of the illustrations of broadcasting in Chapter 12. While I don’t show any\\nmayavi-using code here, there is plenty of documentation and examples available on-\\nline. In many cases, I believe it is a good alternative to a technology like WebGL, though\\nthe graphics are harder to share in interactive form.\\nOther Packages\\nOf course, there are numerous other visualization libraries and applications available\\nin Python: PyQwt, Veusz, gnuplot-py, biggles, and others. I have seen PyQwt put to\\ngood use in GUI applications built using the Qt application framework using PyQt.\\nWhile many of these libraries continue to be under active development (some of them\\n248 | Chapter 8: \\u2002Plotting and Visualization\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'are part of much larger applications), I have noted in the last few years a general trend\\ntoward web-based technologies and away from desktop graphics. I’ll say a few more\\nwords about this in the next section.\\nThe Future of Visualization Tools?\\nVisualizations built on web technologies (that is, JavaScript-based) appear to be the\\ninevitable future. Doubtlessly you have used many different kinds of static or interactive\\nvisualizations built in Flash or JavaScript over the years. New toolkits (such as d3.js\\nand its numerous off-shoot projects) for building such displays are appearing all the\\ntime. In contrast, development in non web-based visualization has slowed significantly\\nin recent years. This holds true of Python as well as other data analysis and statistical\\ncomputing environments like R.\\nThe development challenge, then, will be in building tighter integration between data\\nanalysis and preparation tools, such as pandas, and the web browser. I am hopeful that\\nthis will become a fruitful point of collaboration between Python and non-Python users\\nas well.\\nPython Visualization Tool Ecosystem | 249\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf', 'page_content': 'www.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'CHAPTER 9\\nData Aggregation and Group\\nOperations\\nCategorizing a data set and applying a function to each group, whether an aggregation\\nor transformation, is often a critical component of a data analysis workflow. After\\nloading, merging, and preparing a data set, a familiar task is to compute group statistics\\nor possibly pivot tables for reporting or visualization purposes. pandas provides a flex-\\nible and high-performance groupby facility, enabling you to slice and dice, and sum-\\nmarize data sets in a natural way.\\nOne reason for the popularity of relational databases and SQL (which stands for\\n“structured query language”) is the ease with which data can be joined, filtered, trans-\\nformed, and aggregated. However, query languages like SQL are rather limited in the\\nkinds of group operations that can be performed. As you will see, with the expressive-\\nness and power of Python and pandas, we can perform much more complex grouped\\noperations by utilizing any function that accepts a pandas object or NumPy array. In\\nthis chapter, you will learn how to:\\n• Split a pandas object into pieces using one or more keys (in the form of functions,\\narrays, or DataFrame column names)\\n• Computing group summary statistics, like count, mean, or standard deviation, or\\na user-defined function\\n• Apply a varying set of functions to each column of a DataFrame\\n• Apply within-group transformations or other manipulations, like normalization,\\nlinear regression, rank, or subset selection\\n• Compute pivot tables and cross-tabulations\\n• Perform quantile analysis and other data-derived group analyses\\n251\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Aggregation of time series data, a special use case of groupby, is referred\\nto as resampling in this book and will receive separate treatment in\\nChapter 10.\\nGroupBy Mechanics\\nHadley Wickham, an author of many popular packages for the R programming lan-\\nguage, coined the term split-apply-combine for talking about group operations, and I\\nthink that’s a good description of the process. In the first stage of the process, data\\ncontained in a pandas object, whether a Series, DataFrame, or otherwise, is split into\\ngroups based on one or more keys that you provide. The splitting is performed on a\\nparticular axis of an object. For example, a DataFrame can be grouped on its rows\\n(axis=0) or its columns (axis=1). Once this is done, a function is applied to each group,\\nproducing a new value. Finally, the results of all those function applications are com-\\nbined into a result object. The form of the resulting object will usually depend on what’s\\nbeing done to the data. See Figure 9-1 for a mockup of a simple group aggregation.\\nFigure 9-1. Illustration of a group aggregation\\nEach grouping key can take many forms, and the keys do not have to be all of the same\\ntype:\\n• A list or array of values that is the same length as the axis being grouped\\n• A value indicating a column name in a DataFrame\\n252 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"• A dict or Series giving a correspondence between the values on the axis being\\ngrouped and the group names\\n• A function to be invoked on the axis index or the individual labels in the index\\nNote that the latter three methods are all just shortcuts for producing an array of values\\nto be used to split up the object. Don’t worry if this all seems very abstract. Throughout\\nthis chapter, I will give many examples of all of these methods. To get started, here is\\na very simple small tabular dataset as a DataFrame:\\nIn [13]: df = DataFrame({'key1' : ['a', 'a', 'b', 'b', 'a'],\\n   ....:                 'key2' : ['one', 'two', 'one', 'two', 'one'],\\n   ....:                 'data1' : np.random.randn(5),\\n   ....:                 'data2' : np.random.randn(5)})\\nIn [14]: df\\nOut[14]: \\n      data1     data2 key1 key2\\n0 -0.204708  1.393406    a  one\\n1  0.478943  0.092908    a  two\\n2 -0.519439  0.281746    b  one\\n3 -0.555730  0.769023    b  two\\n4  1.965781  1.246435    a  one\\nSuppose you wanted to compute the mean of the data1 column using the groups labels\\nfrom key1. There are a number of ways to do this. One is to access data1 and call\\ngroupby with the column (a Series) at key1:\\nIn [15]: grouped = df['data1'].groupby(df['key1'])\\nIn [16]: grouped\\nOut[16]: <pandas.core.groupby.SeriesGroupBy at 0x2d78b10>\\nThis grouped variable is now a GroupBy object. It has not actually computed anything\\nyet except for some intermediate data about the group key df['key1']. The idea is that\\nthis object has all of the information needed to then apply some operation to each of\\nthe groups. For example, to compute group means we can call the GroupBy’s mean\\nmethod:\\nIn [17]: grouped.mean()\\nOut[17]: \\nkey1\\na       0.746672\\nb      -0.537585\\nLater, I'll explain more about what’s going on when you call .mean(). The important\\nthing here is that the data (a Series) has been aggregated according to the group key,\\nproducing a new Series that is now indexed by the unique values in the key1 column.\\nThe result index has the name 'key1' because the DataFrame column df['key1'] did.\\nIf instead we had passed multiple arrays as a list, we get something different:\\nIn [18]: means = df['data1'].groupby([df['key1'], df['key2']]).mean()\\nGroupBy Mechanics | 253\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [19]: means\\nOut[19]: \\nkey1  key2\\na     one     0.880536\\n      two     0.478943\\nb     one    -0.519439\\n      two    -0.555730\\nIn this case, we grouped the data using two keys, and the resulting Series now has a\\nhierarchical index consisting of the unique pairs of keys observed:\\nIn [20]: means.unstack()\\nOut[20]: \\nkey2       one       two\\nkey1                    \\na     0.880536  0.478943\\nb    -0.519439 -0.555730\\nIn these examples, the group keys are all Series, though they could be any arrays of the\\nright length:\\nIn [21]: states = np.array(['Ohio', 'California', 'California', 'Ohio', 'Ohio'])\\nIn [22]: years = np.array([2005, 2005, 2006, 2005, 2006])\\nIn [23]: df['data1'].groupby([states, years]).mean()\\nOut[23]: \\nCalifornia  2005    0.478943\\n            2006   -0.519439\\nOhio        2005   -0.380219\\n            2006    1.965781\\nFrequently the grouping information to be found in the same DataFrame as the data\\nyou want to work on. In that case, you can pass column names (whether those are\\nstrings, numbers, or other Python objects) as the group keys:\\nIn [24]: df.groupby('key1').mean()\\nOut[24]: \\n         data1     data2\\nkey1                    \\na     0.746672  0.910916\\nb    -0.537585  0.525384\\nIn [25]: df.groupby(['key1', 'key2']).mean()\\nOut[25]: \\n              data1     data2\\nkey1 key2                    \\na    one   0.880536  1.319920\\n     two   0.478943  0.092908\\nb    one  -0.519439  0.281746\\n     two  -0.555730  0.769023\\nYou may have noticed in the first case df.groupby('key1').mean() that there is no\\nkey2 column in the result. Because df['key2'] is not numeric data, it is said to be a\\nnuisance column, which is therefore excluded from the result. By default, all of the\\n254 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"numeric columns are aggregated, though it is possible to filter down to a subset as you’ll\\nsee soon.\\nRegardless of the objective in using groupby, a generally useful GroupBy method is \\nsize which return a Series containing group sizes:\\nIn [26]: df.groupby(['key1', 'key2']).size()\\nOut[26]: \\nkey1  key2\\na     one     2\\n      two     1\\nb     one     1\\n      two     1\\nAs of this writing, any missing values in a group key will be excluded\\nfrom the result. It’s possible (and, in fact, quite likely), that by the time\\nyou are reading this there will be an option to include the NA group in\\nthe result.\\nIterating Over Groups\\nThe GroupBy object supports iteration, generating a sequence of 2-tuples containing\\nthe group name along with the chunk of data. Consider the following small example\\ndata set:\\nIn [27]: for name, group in df.groupby('key1'):\\n   ....:     print name\\n   ....:     print group\\n   ....:\\na\\n      data1     data2 key1 key2\\n0 -0.204708  1.393406    a  one\\n1  0.478943  0.092908    a  two\\n4  1.965781  1.246435    a  one\\nb\\n      data1     data2 key1 key2\\n2 -0.519439  0.281746    b  one\\n3 -0.555730  0.769023    b  two\\nIn the case of multiple keys, the first element in the tuple will be a tuple of key values:\\nIn [28]: for (k1, k2), group in df.groupby(['key1', 'key2']):\\n   ....:     print k1, k2\\n   ....:     print group\\n   ....:\\na one\\n      data1     data2 key1 key2\\n0 -0.204708  1.393406    a  one\\n4  1.965781  1.246435    a  one\\na two\\n      data1     data2 key1 key2\\n1  0.478943  0.092908    a  two\\nb one\\n      data1     data2 key1 key2\\nGroupBy Mechanics | 255\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"2 -0.519439  0.281746    b  one\\nb two\\n     data1     data2 key1 key2\\n3 -0.55573  0.769023    b  two\\nOf course, you can choose to do whatever you want with the pieces of data. A recipe\\nyou may find useful is computing a dict of the data pieces as a one-liner:\\nIn [29]: pieces = dict(list(df.groupby('key1')))\\nIn [30]: pieces['b']\\nOut[30]: \\n      data1     data2 key1 key2\\n2 -0.519439  0.281746    b  one\\n3 -0.555730  0.769023    b  two\\nBy default groupby groups on axis=0, but you can group on any of the other axes. For\\nexample, we could group the columns of our example df here by dtype like so:\\nIn [31]: df.dtypes\\nOut[31]: \\ndata1    float64\\ndata2    float64\\nkey1      object\\nkey2      object\\nIn [32]: grouped = df.groupby(df.dtypes, axis=1)\\nIn [33]: dict(list(grouped))\\nOut[33]: \\n{dtype('float64'):       data1     data2\\n0 -0.204708  1.393406\\n1  0.478943  0.092908\\n2 -0.519439  0.281746\\n3 -0.555730  0.769023\\n4  1.965781  1.246435,\\n dtype('object'):   key1 key2\\n0    a  one\\n1    a  two\\n2    b  one\\n3    b  two\\n4    a  one}\\nSelecting a Column or Subset of Columns\\nIndexing a GroupBy object created from a DataFrame with a column name or array of\\ncolumn names has the effect of selecting those columns for aggregation. This means that:\\ndf.groupby('key1')['data1']\\ndf.groupby('key1')[['data2']]\\nare syntactic sugar for:\\ndf['data1'].groupby(df['key1'])\\ndf[['data2']].groupby(df['key1'])\\n256 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Especially for large data sets, it may be desirable to aggregate only a few columns. For\\nexample, in the above data set, to compute means for just the data2 column and get\\nthe result as a DataFrame, we could write:\\nIn [34]: df.groupby(['key1', 'key2'])[['data2']].mean()\\nOut[34]: \\n              data2\\nkey1 key2          \\na    one   1.319920\\n     two   0.092908\\nb    one   0.281746\\n     two   0.769023\\nThe object returned by this indexing operation is a grouped DataFrame if a list or array\\nis passed and a grouped Series is just a single column name that is passed as a scalar:\\nIn [35]: s_grouped = df.groupby(['key1', 'key2'])['data2']\\nIn [36]: s_grouped\\nOut[36]: <pandas.core.groupby.SeriesGroupBy at 0x2e215d0>\\nIn [37]: s_grouped.mean()\\nOut[37]: \\nkey1  key2\\na     one     1.319920\\n      two     0.092908\\nb     one     0.281746\\n      two     0.769023\\nName: data2\\nGrouping with Dicts and Series\\nGrouping information may exist in a form other than an array. Let’s consider another\\nexample DataFrame:\\nIn [38]: people = DataFrame(np.random.randn(5, 5),\\n   ....:                    columns=['a', 'b', 'c', 'd', 'e'],\\n   ....:                    index=['Joe', 'Steve', 'Wes', 'Jim', 'Travis'])\\nIn [39]: people.ix[2:3, ['b', 'c']] = np.nan # Add a few NA values\\nIn [40]: people\\nOut[40]: \\n               a         b         c         d         e\\nJoe     1.007189 -1.296221  0.274992  0.228913  1.352917\\nSteve   0.886429 -2.001637 -0.371843  1.669025 -0.438570\\nWes    -0.539741       NaN       NaN -1.021228 -0.577087\\nJim     0.124121  0.302614  0.523772  0.000940  1.343810\\nTravis -0.713544 -0.831154 -2.370232 -1.860761 -0.860757\\nNow, suppose I have a group correspondence for the columns and want to sum together\\nthe columns by group:\\nIn [41]: mapping = {'a': 'red', 'b': 'red', 'c': 'blue',\\n   ....:            'd': 'blue', 'e': 'red', 'f' : 'orange'}\\nGroupBy Mechanics | 257\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Now, you could easily construct an array from this dict to pass to groupby, but instead\\nwe can just pass the dict:\\nIn [42]: by_column = people.groupby(mapping, axis=1)\\nIn [43]: by_column.sum()\\nOut[43]: \\n            blue       red\\nJoe     0.503905  1.063885\\nSteve   1.297183 -1.553778\\nWes    -1.021228 -1.116829\\nJim     0.524712  1.770545\\nTravis -4.230992 -2.405455\\nThe same functionality holds for Series, which can be viewed as a fixed size mapping.\\nWhen I used Series as group keys in the above examples, pandas does, in fact, inspect\\neach Series to ensure that its index is aligned with the axis it’s grouping:\\nIn [44]: map_series = Series(mapping)\\nIn [45]: map_series\\nOut[45]: \\na       red\\nb       red\\nc      blue\\nd      blue\\ne       red\\nf    orange\\nIn [46]: people.groupby(map_series, axis=1).count()\\nOut[46]: \\n        blue  red\\nJoe        2    3\\nSteve      2    3\\nWes        1    2\\nJim        2    3\\nTravis     2    3\\nGrouping with Functions\\nUsing Python functions in what can be fairly creative ways is a more abstract way of\\ndefining a group mapping compared with a dict or Series. Any function passed as a\\ngroup key will be called once per index value, with the return values being used as the\\ngroup names. More concretely, consider the example DataFrame from the previous\\nsection, which has people’s first names as index values. Suppose you wanted to group\\nby the length of the names; you could compute an array of string lengths, but instead\\nyou can just pass the len function:\\nIn [47]: people.groupby(len).sum()\\nOut[47]: \\n          a         b         c         d         e\\n3  0.591569 -0.993608  0.798764 -0.791374  2.119639\\n258 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"5  0.886429 -2.001637 -0.371843  1.669025 -0.438570\\n6 -0.713544 -0.831154 -2.370232 -1.860761 -0.860757\\nMixing functions with arrays, dicts, or Series is not a problem as everything gets con-\\nverted to arrays internally:\\nIn [48]: key_list = ['one', 'one', 'one', 'two', 'two']\\nIn [49]: people.groupby([len, key_list]).min()\\nOut[49]: \\n              a         b         c         d         e\\n3 one -0.539741 -1.296221  0.274992 -1.021228 -0.577087\\n  two  0.124121  0.302614  0.523772  0.000940  1.343810\\n5 one  0.886429 -2.001637 -0.371843  1.669025 -0.438570\\n6 two -0.713544 -0.831154 -2.370232 -1.860761 -0.860757\\nGrouping by Index Levels\\nA final convenience for hierarchically-indexed data sets is the ability to aggregate using\\none of the levels of an axis index. To do this, pass the level number or name using the \\nlevel keyword:\\nIn [50]: columns = pd.MultiIndex.from_arrays([['US', 'US', 'US', 'JP', 'JP'],\\n   ....:                                     [1, 3, 5, 1, 3]], names=['cty', 'tenor'])\\nIn [51]: hier_df = DataFrame(np.random.randn(4, 5), columns=columns)\\nIn [52]: hier_df\\nOut[52]: \\ncty          US                            JP          \\ntenor         1         3         5         1         3\\n0      0.560145 -1.265934  0.119827 -1.063512  0.332883\\n1     -2.359419 -0.199543 -1.541996 -0.970736 -1.307030\\n2      0.286350  0.377984 -0.753887  0.331286  1.349742\\n3      0.069877  0.246674 -0.011862  1.004812  1.327195\\nIn [53]: hier_df.groupby(level='cty', axis=1).count()\\nOut[53]: \\ncty  JP  US\\n0     2   3\\n1     2   3\\n2     2   3\\n3     2   3\\nData Aggregation\\nBy aggregation, I am generally referring to any data transformation that produces scalar\\nvalues from arrays. In the examples above I have used several of them, such as mean,\\ncount, min and sum. You may wonder what is going on when you invoke mean() on a\\nGroupBy object. Many common aggregations, such as those found in Table 9-1, have\\noptimized implementations that compute the statistics on the dataset in place. How-\\never, you are not limited to only this set of methods. You can use aggregations of your\\nData Aggregation | 259\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"own devising and additionally call any method that is also defined on the grouped\\nobject. For example, as you recall quantile computes sample quantiles of a Series or a\\nDataFrame’s columns 1:\\nIn [54]: df\\nOut[54]: \\n      data1     data2 key1 key2\\n0 -0.204708  1.393406    a  one\\n1  0.478943  0.092908    a  two\\n2 -0.519439  0.281746    b  one\\n3 -0.555730  0.769023    b  two\\n4  1.965781  1.246435    a  one\\nIn [55]: grouped = df.groupby('key1')\\nIn [56]: grouped['data1'].quantile(0.9)\\nOut[56]: \\nkey1\\na       1.668413\\nb      -0.523068\\nWhile quantile is not explicitly implemented for GroupBy, it is a Series method and\\nthus available for use. Internally, GroupBy efficiently slices up the Series, calls\\npiece.quantile(0.9) for each piece, then assembles those results together into the result\\nobject.\\nTo use your own aggregation functions, pass any function that aggregates an array to\\nthe aggregate or agg method:\\nIn [57]: def peak_to_peak(arr):\\n   ....:     return arr.max() - arr.min()\\nIn [58]: grouped.agg(peak_to_peak)\\nOut[58]: \\n         data1     data2\\nkey1                    \\na     2.170488  1.300498\\nb     0.036292  0.487276\\nYou’ll notice that some methods like describe also work, even though they are not\\naggregations, strictly speaking:\\nIn [59]: grouped.describe()\\nOut[59]: \\n               data1     data2\\nkey1                          \\na    count  3.000000  3.000000\\n     mean   0.746672  0.910916\\n     std    1.109736  0.712217\\n     min   -0.204708  0.092908\\n     25%    0.137118  0.669671\\n     50%    0.478943  1.246435\\n1. Note that quantile performs linear interpolation if there is no value at exactly the passed percentile.\\n260 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"75%    1.222362  1.319920\\n     max    1.965781  1.393406\\nb    count  2.000000  2.000000\\n     mean  -0.537585  0.525384\\n     std    0.025662  0.344556\\n     min   -0.555730  0.281746\\n     25%   -0.546657  0.403565\\n     50%   -0.537585  0.525384\\n     75%   -0.528512  0.647203\\n     max   -0.519439  0.769023\\nI will explain in more detail what has happened here in the next major section on group-\\nwise operations and transformations.\\nYou may notice that custom aggregation functions are much slower than\\nthe optimized functions found in Table 9-1. This is because there is\\nsignificant overhead (function calls, data rearrangement) in construct-\\ning the intermediate group data chunks.\\nTable 9-1. Optimized groupby methods\\nFunction name Description\\ncount Number of non-NA values in the group\\nsum Sum of non-NA values\\nmean Mean of non-NA values\\nmedian Arithmetic median of non-NA values\\nstd, var Unbiased (n - 1 denominator) standard deviation and variance\\nmin, max Minimum and maximum of non-NA values\\nprod Product of non-NA values\\nfirst, last First and last non-NA values\\nTo illustrate some more advanced aggregation features, I’ll use a less trivial dataset, a\\ndataset on restaurant tipping. I obtained it from the R reshape2 package; it was origi-\\nnally found in Bryant & Smith’s 1995 text on business statistics (and found in the book’s\\nGitHub repository). After loading it with read_csv, I add a tipping percentage column\\ntip_pct.\\nIn [60]: tips = pd.read_csv('ch08/tips.csv')\\n# Add tip percentage of total bill\\nIn [61]: tips['tip_pct'] = tips['tip'] / tips['total_bill']\\nIn [62]: tips[:6]\\nOut[62]: \\n   total_bill   tip     sex smoker  day    time  size   tip_pct\\n0       16.99  1.01  Female     No  Sun  Dinner     2  0.059447\\n1       10.34  1.66    Male     No  Sun  Dinner     3  0.160542\\nData Aggregation | 261\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"2       21.01  3.50    Male     No  Sun  Dinner     3  0.166587\\n3       23.68  3.31    Male     No  Sun  Dinner     2  0.139780\\n4       24.59  3.61  Female     No  Sun  Dinner     4  0.146808\\n5       25.29  4.71    Male     No  Sun  Dinner     4  0.186240\\nColumn-wise and Multiple Function Application\\nAs you’ve seen above, aggregating a Series or all of the columns of a DataFrame is a\\nmatter of using aggregate with the desired function or calling a method like mean or\\nstd. However, you may want to aggregate using a different function depending on the\\ncolumn or multiple functions at once. Fortunately, this is straightforward to do, which\\nI’ll illustrate through a number of examples. First, I’ll group the tips by sex and smoker:\\nIn [63]: grouped = tips.groupby(['sex', 'smoker'])\\nNote that for descriptive statistics like those in Table 9-1, you can pass the name of the\\nfunction as a string:\\nIn [64]: grouped_pct = grouped['tip_pct']\\nIn [65]: grouped_pct.agg('mean')\\nOut[65]: \\nsex     smoker\\nFemale  No        0.156921\\n        Yes       0.182150\\nMale    No        0.160669\\n        Yes       0.152771\\nName: tip_pct\\nIf you pass a list of functions or function names instead, you get back a DataFrame with\\ncolumn names taken from the functions:\\nIn [66]: grouped_pct.agg(['mean', 'std', peak_to_peak])\\nOut[66]: \\n                   mean       std  peak_to_peak\\nsex    smoker                                  \\nFemale No      0.156921  0.036421      0.195876\\n       Yes     0.182150  0.071595      0.360233\\nMale   No      0.160669  0.041849      0.220186\\n       Yes     0.152771  0.090588      0.674707\\nYou don’t need to accept the names that GroupBy gives to the columns; notably \\nlambda functions have the name '<lambda>' which make them hard to identify (you can\\nsee for yourself by looking at a function’s __name__ attribute). As such, if you pass a list\\nof (name, function) tuples, the first element of each tuple will be used as the DataFrame\\ncolumn names (you can think of a list of 2-tuples as an ordered mapping):\\nIn [67]: grouped_pct.agg([('foo', 'mean'), ('bar', np.std)])\\nOut[67]: \\n                    foo       bar\\nsex    smoker                    \\nFemale No      0.156921  0.036421\\n       Yes     0.182150  0.071595\\n262 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Male   No      0.160669  0.041849\\n       Yes     0.152771  0.090588\\nWith a DataFrame, you have more options as you can specify a list of functions to apply\\nto all of the columns or different functions per column. To start, suppose we wanted\\nto compute the same three statistics for the tip_pct and total_bill columns:\\nIn [68]: functions = ['count', 'mean', 'max']\\nIn [69]: result = grouped['tip_pct', 'total_bill'].agg(functions)\\nIn [70]: result\\nOut[70]: \\n               tip_pct                      total_bill                  \\n                 count      mean       max       count       mean    max\\nsex    smoker                                                           \\nFemale No           54  0.156921  0.252672          54  18.105185  35.83\\n       Yes          33  0.182150  0.416667          33  17.977879  44.30\\nMale   No           97  0.160669  0.291990          97  19.791237  48.33\\n       Yes          60  0.152771  0.710345          60  22.284500  50.81\\nAs you can see, the resulting DataFrame has hierarchical columns, the same as you\\nwould get aggregating each column separately and using concat to glue the results\\ntogether using the column names as the keys argument:\\nIn [71]: result['tip_pct']\\nOut[71]: \\n               count      mean       max\\nsex    smoker                           \\nFemale No         54  0.156921  0.252672\\n       Yes        33  0.182150  0.416667\\nMale   No         97  0.160669  0.291990\\n       Yes        60  0.152771  0.710345\\nAs above, a list of tuples with custom names can be passed:\\nIn [72]: ftuples = [('Durchschnitt', 'mean'), ('Abweichung', np.var)]\\nIn [73]: grouped['tip_pct', 'total_bill'].agg(ftuples)\\nOut[73]: \\n                    tip_pct                total_bill            \\n               Durchschnitt  Abweichung  Durchschnitt  Abweichung\\nsex    smoker                                                    \\nFemale No          0.156921    0.001327     18.105185   53.092422\\n       Yes         0.182150    0.005126     17.977879   84.451517\\nMale   No          0.160669    0.001751     19.791237   76.152961\\n       Yes         0.152771    0.008206     22.284500   98.244673\\nNow, suppose you wanted to apply potentially different functions to one or more of\\nthe columns. The trick is to pass a dict to agg that contains a mapping of column names\\nto any of the function specifications listed so far:\\nIn [74]: grouped.agg({'tip' : np.max, 'size' : 'sum'})\\nOut[74]: \\n               size   tip\\nsex    smoker            \\nData Aggregation | 263\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Female No       140   5.2\\n       Yes       74   6.5\\nMale   No       263   9.0\\n       Yes      150  10.0\\nIn [75]: grouped.agg({'tip_pct' : ['min', 'max', 'mean', 'std'],\\n   ....:              'size' : 'sum'})\\nOut[75]: \\n                tip_pct                                size\\n                    min       max      mean       std   sum\\nsex    smoker                                              \\nFemale No      0.056797  0.252672  0.156921  0.036421   140\\n       Yes     0.056433  0.416667  0.182150  0.071595    74\\nMale   No      0.071804  0.291990  0.160669  0.041849   263\\n       Yes     0.035638  0.710345  0.152771  0.090588   150\\nA DataFrame will have hierarchical columns only if multiple functions are applied to\\nat least one column.\\nReturning Aggregated Data in “unindexed” Form\\nIn all of the examples up until now, the aggregated data comes back with an index,\\npotentially hierarchical, composed from the unique group key combinations observed.\\nSince this isn’t always desirable, you can disable this behavior in most cases by passing\\nas_index=False to groupby:\\nIn [76]: tips.groupby(['sex', 'smoker'], as_index=False).mean()\\nOut[76]: \\n      sex smoker  total_bill       tip      size   tip_pct\\n0  Female     No   18.105185  2.773519  2.592593  0.156921\\n1  Female    Yes   17.977879  2.931515  2.242424  0.182150\\n2    Male     No   19.791237  3.113402  2.711340  0.160669\\n3    Male    Yes   22.284500  3.051167  2.500000  0.152771\\nOf course, it’s always possible to obtain the result in this format by calling\\nreset_index on the result.\\nUsing groupby in this way is generally less flexible; results with hier-\\narchical columns, for example, are not currently implemented as the\\nform of the result would have to be somewhat arbitrary.\\nGroup-wise Operations and Transformations\\nAggregation is only one kind of group operation. It is a special case in the more general\\nclass of data transformations; that is, it accepts functions that reduce a one-dimensional\\narray to a scalar value. In this section, I will introduce you to the transform and apply\\nmethods, which will enable you to do many other kinds of group operations.\\nSuppose, instead, we wanted to add a column to a DataFrame containing group means\\nfor each index. One way to do this is to aggregate, then merge:\\n264 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [77]: df\\nOut[77]: \\n      data1     data2 key1 key2\\n0 -0.204708  1.393406    a  one\\n1  0.478943  0.092908    a  two\\n2 -0.519439  0.281746    b  one\\n3 -0.555730  0.769023    b  two\\n4  1.965781  1.246435    a  one\\nIn [78]: k1_means = df.groupby('key1').mean().add_prefix('mean_')\\nIn [79]: k1_means\\nOut[79]: \\n      mean_data1  mean_data2\\nkey1                        \\na       0.746672    0.910916\\nb      -0.537585    0.525384\\nIn [80]: pd.merge(df, k1_means, left_on='key1', right_index=True)\\nOut[80]: \\n      data1     data2 key1 key2  mean_data1  mean_data2\\n0 -0.204708  1.393406    a  one    0.746672    0.910916\\n1  0.478943  0.092908    a  two    0.746672    0.910916\\n4  1.965781  1.246435    a  one    0.746672    0.910916\\n2 -0.519439  0.281746    b  one   -0.537585    0.525384\\n3 -0.555730  0.769023    b  two   -0.537585    0.525384\\nThis works, but is somewhat inflexible. You can think of the operation as transforming\\nthe two data columns using the np.mean function. Let’s look back at the people Data-\\nFrame from earlier in the chapter and use the transform method on GroupBy:\\nIn [81]: key = ['one', 'two', 'one', 'two', 'one']\\nIn [82]: people.groupby(key).mean()\\nOut[82]: \\n            a         b         c         d         e\\none -0.082032 -1.063687 -1.047620 -0.884358 -0.028309\\ntwo  0.505275 -0.849512  0.075965  0.834983  0.452620\\nIn [83]: people.groupby(key).transform(np.mean)\\nOut[83]: \\n               a         b         c         d         e\\nJoe    -0.082032 -1.063687 -1.047620 -0.884358 -0.028309\\nSteve   0.505275 -0.849512  0.075965  0.834983  0.452620\\nWes    -0.082032 -1.063687 -1.047620 -0.884358 -0.028309\\nJim     0.505275 -0.849512  0.075965  0.834983  0.452620\\nTravis -0.082032 -1.063687 -1.047620 -0.884358 -0.028309\\nAs you may guess, transform applies a function to each group, then places the results\\nin the appropriate locations. If each group produces a scalar value, it will be propagated\\n(broadcasted). Suppose instead you wanted to subtract the mean value from each\\ngroup. To do this, create a demeaning function and pass it to transform:\\nIn [84]: def demean(arr):\\n   ....:     return arr - arr.mean()\\nGroup-wise Operations and Transformations | 265\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [85]: demeaned = people.groupby(key).transform(demean)\\nIn [86]: demeaned\\nOut[86]: \\n               a         b         c         d         e\\nJoe     1.089221 -0.232534  1.322612  1.113271  1.381226\\nSteve   0.381154 -1.152125 -0.447807  0.834043 -0.891190\\nWes    -0.457709       NaN       NaN -0.136869 -0.548778\\nJim    -0.381154  1.152125  0.447807 -0.834043  0.891190\\nTravis -0.631512  0.232534 -1.322612 -0.976402 -0.832448\\nYou can check that demeaned now has zero group means:\\nIn [87]: demeaned.groupby(key).mean()\\nOut[87]: \\n     a  b  c  d  e\\none  0 -0  0  0  0\\ntwo -0  0  0  0  0\\nAs you’ll see in the next section, group demeaning can be achieved using apply also.\\nApply: General split-apply-combine\\nLike aggregate, transform is a more specialized function having rigid requirements: the\\npassed function must either produce a scalar value to be broadcasted (like np.mean) or\\na transformed array of the same size. The most general purpose GroupBy method is\\napply, which is the subject of the rest of this section. As in Figure 9-1, apply splits the\\nobject being manipulated into pieces, invokes the passed function on each piece, then\\nattempts to concatenate the pieces together.\\nReturning to the tipping data set above, suppose you wanted to select the top five\\ntip_pct values by group. First, it’s straightforward to write a function that selects the\\nrows with the largest values in a particular column:\\nIn [88]: def top(df, n=5, column='tip_pct'):\\n   ....:     return df.sort_index(by=column)[-n:]\\nIn [89]: top(tips, n=6)\\nOut[89]: \\n     total_bill   tip     sex smoker  day    time  size   tip_pct\\n109       14.31  4.00  Female    Yes  Sat  Dinner     2  0.279525\\n183       23.17  6.50    Male    Yes  Sun  Dinner     4  0.280535\\n232       11.61  3.39    Male     No  Sat  Dinner     2  0.291990\\n67         3.07  1.00  Female    Yes  Sat  Dinner     1  0.325733\\n178        9.60  4.00  Female    Yes  Sun  Dinner     2  0.416667\\n172        7.25  5.15    Male    Yes  Sun  Dinner     2  0.710345\\nNow, if we group by smoker, say, and call apply with this function, we get the following:\\nIn [90]: tips.groupby('smoker').apply(top)\\nOut[90]: \\n            total_bill   tip     sex smoker   day    time  size   tip_pct\\nsmoker                                                                   \\n266 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"No     88        24.71  5.85    Male     No  Thur   Lunch     2  0.236746\\n       185       20.69  5.00    Male     No   Sun  Dinner     5  0.241663\\n       51        10.29  2.60  Female     No   Sun  Dinner     2  0.252672\\n       149        7.51  2.00    Male     No  Thur   Lunch     2  0.266312\\n       232       11.61  3.39    Male     No   Sat  Dinner     2  0.291990\\nYes    109       14.31  4.00  Female    Yes   Sat  Dinner     2  0.279525\\n       183       23.17  6.50    Male    Yes   Sun  Dinner     4  0.280535\\n       67         3.07  1.00  Female    Yes   Sat  Dinner     1  0.325733\\n       178        9.60  4.00  Female    Yes   Sun  Dinner     2  0.416667\\n       172        7.25  5.15    Male    Yes   Sun  Dinner     2  0.710345\\nWhat has happened here? The top function is called on each piece of the DataFrame,\\nthen the results are glued together using pandas.concat, labeling the pieces with the\\ngroup names. The result therefore has a hierarchical index whose inner level contains\\nindex values from the original DataFrame.\\nIf you pass a function to apply that takes other arguments or keywords, you can pass\\nthese after the function:\\nIn [91]: tips.groupby(['smoker', 'day']).apply(top, n=1, column='total_bill')\\nOut[91]: \\n                 total_bill    tip     sex smoker   day    time  size   tip_pct\\nsmoker day                                                                     \\nNo     Fri  94        22.75   3.25  Female     No   Fri  Dinner     2  0.142857\\n       Sat  212       48.33   9.00    Male     No   Sat  Dinner     4  0.186220\\n       Sun  156       48.17   5.00    Male     No   Sun  Dinner     6  0.103799\\n       Thur 142       41.19   5.00    Male     No  Thur   Lunch     5  0.121389\\nYes    Fri  95        40.17   4.73    Male    Yes   Fri  Dinner     4  0.117750\\n       Sat  170       50.81  10.00    Male    Yes   Sat  Dinner     3  0.196812\\n       Sun  182       45.35   3.50    Male    Yes   Sun  Dinner     3  0.077178\\n       Thur 197       43.11   5.00  Female    Yes  Thur   Lunch     4  0.115982\\nBeyond these basic usage mechanics, getting the most out of apply is\\nlargely a matter of creativity. What occurs inside the function passed is\\nup to you; it only needs to return a pandas object or a scalar value. The\\nrest of this chapter will mainly consist of examples showing you how to\\nsolve various problems using groupby.\\nYou may recall above I called describe on a GroupBy object:\\nIn [92]: result = tips.groupby('smoker')['tip_pct'].describe()\\nIn [93]: result\\nOut[93]: \\nsmoker       \\nNo      count    151.000000\\n        mean       0.159328\\n        std        0.039910\\n        min        0.056797\\n        25%        0.136906\\n        50%        0.155625\\n        75%        0.185014\\n        max        0.291990\\nGroup-wise Operations and Transformations | 267\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Yes     count     93.000000\\n        mean       0.163196\\n        std        0.085119\\n        min        0.035638\\n        25%        0.106771\\n        50%        0.153846\\n        75%        0.195059\\n        max        0.710345\\nIn [94]: result.unstack('smoker')\\nOut[94]: \\nsmoker          No        Yes\\ncount   151.000000  93.000000\\nmean      0.159328   0.163196\\nstd       0.039910   0.085119\\nmin       0.056797   0.035638\\n25%       0.136906   0.106771\\n50%       0.155625   0.153846\\n75%       0.185014   0.195059\\nmax       0.291990   0.710345\\nInside GroupBy, when you invoke a method like describe, it is actually just a shortcut\\nfor:\\nf = lambda x: x.describe()\\ngrouped.apply(f)\\nSuppressing the group keys\\nIn the examples above, you see that the resulting object has a hierarchical index formed\\nfrom the group keys along with the indexes of each piece of the original object. This\\ncan be disabled by passing group_keys=False to groupby:\\nIn [95]: tips.groupby('smoker', group_keys=False).apply(top)\\nOut[95]: \\n     total_bill   tip     sex smoker   day    time  size   tip_pct\\n88        24.71  5.85    Male     No  Thur   Lunch     2  0.236746\\n185       20.69  5.00    Male     No   Sun  Dinner     5  0.241663\\n51        10.29  2.60  Female     No   Sun  Dinner     2  0.252672\\n149        7.51  2.00    Male     No  Thur   Lunch     2  0.266312\\n232       11.61  3.39    Male     No   Sat  Dinner     2  0.291990\\n109       14.31  4.00  Female    Yes   Sat  Dinner     2  0.279525\\n183       23.17  6.50    Male    Yes   Sun  Dinner     4  0.280535\\n67         3.07  1.00  Female    Yes   Sat  Dinner     1  0.325733\\n178        9.60  4.00  Female    Yes   Sun  Dinner     2  0.416667\\n172        7.25  5.15    Male    Yes   Sun  Dinner     2  0.710345\\nQuantile and Bucket Analysis\\nAs you may recall from Chapter 7, pandas has some tools, in particular cut and qcut,\\nfor slicing data up into buckets with bins of your choosing or by sample quantiles.\\nCombining these functions with groupby, it becomes very simple to perform bucket or\\n268 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"quantile analysis on a data set. Consider a simple random data set and an equal-length\\nbucket categorization using cut:\\nIn [96]: frame = DataFrame({'data1': np.random.randn(1000),\\n   ....:                    'data2': np.random.randn(1000)})\\nIn [97]: factor = pd.cut(frame.data1, 4)\\nIn [98]: factor[:10]\\nOut[98]: \\nCategorical: \\narray([(-1.23, 0.489], (-2.956, -1.23], (-1.23, 0.489], (0.489, 2.208],\\n       (-1.23, 0.489], (0.489, 2.208], (-1.23, 0.489], (-1.23, 0.489],\\n       (0.489, 2.208], (0.489, 2.208]], dtype=object)\\nLevels (4): Index([(-2.956, -1.23], (-1.23, 0.489], (0.489, 2.208],\\n                   (2.208, 3.928]], dtype=object)\\nThe Factor object returned by cut can be passed directly to groupby. So we could com-\\npute a set of statistics for the data2 column like so:\\nIn [99]: def get_stats(group):\\n   ....:     return {'min': group.min(), 'max': group.max(),\\n   ....:             'count': group.count(), 'mean': group.mean()}\\nIn [100]: grouped = frame.data2.groupby(factor)\\nIn [101]: grouped.apply(get_stats).unstack()\\nOut[101]: \\n                 count       max      mean       min\\ndata1                                               \\n(-1.23, 0.489]     598  3.260383 -0.002051 -2.989741\\n(-2.956, -1.23]     95  1.670835 -0.039521 -3.399312\\n(0.489, 2.208]     297  2.954439  0.081822 -3.745356\\n(2.208, 3.928]      10  1.765640  0.024750 -1.929776\\nThese were equal-length buckets; to compute equal-size buckets based on sample\\nquantiles, use qcut. I’ll pass labels=False to just get quantile numbers.\\n# Return quantile numbers\\nIn [102]: grouping = pd.qcut(frame.data1, 10, labels=False)\\nIn [103]: grouped = frame.data2.groupby(grouping)\\nIn [104]: grouped.apply(get_stats).unstack()\\nOut[104]: \\n   count       max      mean       min\\n0    100  1.670835 -0.049902 -3.399312\\n1    100  2.628441  0.030989 -1.950098\\n2    100  2.527939 -0.067179 -2.925113\\n3    100  3.260383  0.065713 -2.315555\\n4    100  2.074345 -0.111653 -2.047939\\n5    100  2.184810  0.052130 -2.989741\\n6    100  2.458842 -0.021489 -2.223506\\n7    100  2.954439 -0.026459 -3.056990\\n8    100  2.735527  0.103406 -3.745356\\n9    100  2.377020  0.220122 -2.064111\\nGroup-wise Operations and Transformations | 269\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Example: Filling Missing Values with Group-specific Values\\nWhen cleaning up missing data, in some cases you will filter out data observations\\nusing dropna, but in others you may want to impute (fill in) the NA values using a fixed\\nvalue or some value derived from the data. fillna is the right tool to use; for example\\nhere I fill in NA values with the mean:\\nIn [105]: s = Series(np.random.randn(6))\\nIn [106]: s[::2] = np.nan\\nIn [107]: s\\nOut[107]: \\n0         NaN\\n1   -0.125921\\n2         NaN\\n3   -0.884475\\n4         NaN\\n5    0.227290\\nIn [108]: s.fillna(s.mean())\\nOut[108]: \\n0   -0.261035\\n1   -0.125921\\n2   -0.261035\\n3   -0.884475\\n4   -0.261035\\n5    0.227290\\nSuppose you need the fill value to vary by group. As you may guess, you need only\\ngroup the data and use apply with a function that calls fillna on each data chunk. Here\\nis some sample data on some US states divided into eastern and western states:\\nIn [109]: states = ['Ohio', 'New York', 'Vermont', 'Florida',\\n   .....:           'Oregon', 'Nevada', 'California', 'Idaho']\\nIn [110]: group_key = ['East'] * 4 + ['West'] * 4\\nIn [111]: data = Series(np.random.randn(8), index=states)\\nIn [112]: data[['Vermont', 'Nevada', 'Idaho']] = np.nan\\nIn [113]: data\\nOut[113]: \\nOhio          0.922264\\nNew York     -2.153545\\nVermont            NaN\\nFlorida      -0.375842\\nOregon        0.329939\\nNevada             NaN\\nCalifornia    1.105913\\nIdaho              NaN\\nIn [114]: data.groupby(group_key).mean()\\nOut[114]: \\n270 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"East   -0.535707\\nWest    0.717926\\nWe can fill the NA values using the group means like so:\\nIn [115]: fill_mean = lambda g: g.fillna(g.mean())\\nIn [116]: data.groupby(group_key).apply(fill_mean)\\nOut[116]: \\nOhio          0.922264\\nNew York     -2.153545\\nVermont      -0.535707\\nFlorida      -0.375842\\nOregon        0.329939\\nNevada        0.717926\\nCalifornia    1.105913\\nIdaho         0.717926\\nIn another case, you might have pre-defined fill values in your code that vary by group.\\nSince the groups have a name attribute set internally, we can use that:\\nIn [117]: fill_values = {'East': 0.5, 'West': -1}\\nIn [118]: fill_func = lambda g: g.fillna(fill_values[g.name])\\nIn [119]: data.groupby(group_key).apply(fill_func)\\nOut[119]: \\nOhio          0.922264\\nNew York     -2.153545\\nVermont       0.500000\\nFlorida      -0.375842\\nOregon        0.329939\\nNevada       -1.000000\\nCalifornia    1.105913\\nIdaho        -1.000000\\nExample: Random Sampling and Permutation\\nSuppose you wanted to draw a random sample (with or without replacement) from a\\nlarge dataset for Monte Carlo simulation purposes or some other application. There\\nare a number of ways to perform the “draws”; some are much more efficient than others.\\nOne way is to select the first K elements of np.random.permutation(N), where N is the\\nsize of your complete dataset and K the desired sample size. As a more fun example,\\nhere’s a way to construct a deck of English-style playing cards:\\n# Hearts, Spades, Clubs, Diamonds\\nsuits = ['H', 'S', 'C', 'D']\\ncard_val = (range(1, 11) + [10] * 3) * 4\\nbase_names = ['A'] + range(2, 11) + ['J', 'K', 'Q']\\ncards = []\\nfor suit in ['H', 'S', 'C', 'D']:\\n    cards.extend(str(num) + suit for num in base_names)\\ndeck = Series(card_val, index=cards)\\nGroup-wise Operations and Transformations | 271\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'So now we have a Series of length 52 whose index contains card names and values are\\nthe ones used in blackjack and other games (to keep things simple, I just let the ace be\\n1):\\nIn [121]: deck[:13]\\nOut[121]: \\nAH      1\\n2H      2\\n3H      3\\n4H      4\\n5H      5\\n6H      6\\n7H      7\\n8H      8\\n9H      9\\n10H    10\\nJH     10\\nKH     10\\nQH     10\\nNow, based on what I said above, drawing a hand of 5 cards from the desk could be\\nwritten as:\\nIn [122]: def draw(deck, n=5):\\n   .....:     return deck.take(np.random.permutation(len(deck))[:n])\\nIn [123]: draw(deck)\\nOut[123]: \\nAD     1\\n8C     8\\n5H     5\\nKC    10\\n2C     2\\nSuppose you wanted two random cards from each suit. Because the suit is the last\\ncharacter of each card name, we can group based on this and use apply:\\nIn [124]: get_suit = lambda card: card[-1] # last letter is suit\\nIn [125]: deck.groupby(get_suit).apply(draw, n=2)\\nOut[125]: \\nC  2C     2\\n   3C     3\\nD  KD    10\\n   8D     8\\nH  KH    10\\n   3H     3\\nS  2S     2\\n   4S     4\\n# alternatively\\nIn [126]: deck.groupby(get_suit, group_keys=False).apply(draw, n=2)\\nOut[126]: \\nKC    10\\nJC    10\\nAD     1\\n272 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"5D     5\\n5H     5\\n6H     6\\n7S     7\\nKS    10\\nExample: Group Weighted Average and Correlation\\nUnder the split-apply-combine paradigm of groupby, operations between columns in a\\nDataFrame or two Series, such a group weighted average, become a routine affair. As\\nan example, take this dataset containing group keys, values, and some weights:\\nIn [127]: df = DataFrame({'category': ['a', 'a', 'a', 'a', 'b', 'b', 'b', 'b'],\\n   .....:                 'data': np.random.randn(8),\\n   .....:                 'weights': np.random.rand(8)})\\nIn [128]: df\\nOut[128]: \\n  category      data   weights\\n0        a  1.561587  0.957515\\n1        a  1.219984  0.347267\\n2        a -0.482239  0.581362\\n3        a  0.315667  0.217091\\n4        b -0.047852  0.894406\\n5        b -0.454145  0.918564\\n6        b -0.556774  0.277825\\n7        b  0.253321  0.955905\\nThe group weighted average by category would then be:\\nIn [129]: grouped = df.groupby('category')\\nIn [130]: get_wavg = lambda g: np.average(g['data'], weights=g['weights'])\\nIn [131]: grouped.apply(get_wavg)\\nOut[131]: \\ncategory\\na           0.811643\\nb          -0.122262\\nAs a less trivial example, consider a data set from Yahoo! Finance containing end of\\nday prices for a few stocks and the S&P 500 index (the SPX ticker):\\nIn [132]: close_px = pd.read_csv('ch09/stock_px.csv', parse_dates=True, index_col=0)\\nIn [133]: close_px\\nOut[133]: \\n<class 'pandas.core.frame.DataFrame'>\\nDatetimeIndex: 2214 entries, 2003-01-02 00:00:00 to 2011-10-14 00:00:00\\nData columns:\\nAAPL    2214  non-null values\\nMSFT    2214  non-null values\\nXOM     2214  non-null values\\nSPX     2214  non-null values\\ndtypes: float64(4)\\nGroup-wise Operations and Transformations | 273\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [134]: close_px[-4:]\\nOut[134]: \\n              AAPL   MSFT    XOM      SPX\\n2011-10-11  400.29  27.00  76.27  1195.54\\n2011-10-12  402.19  26.96  77.16  1207.25\\n2011-10-13  408.43  27.18  76.37  1203.66\\n2011-10-14  422.00  27.27  78.11  1224.58\\nOne task of interest might be to compute a DataFrame consisting of the yearly corre-\\nlations of daily returns (computed from percent changes) with SPX. Here is one way to\\ndo it:\\nIn [135]: rets = close_px.pct_change().dropna()\\nIn [136]: spx_corr = lambda x: x.corrwith(x['SPX'])\\nIn [137]: by_year = rets.groupby(lambda x: x.year)\\nIn [138]: by_year.apply(spx_corr)\\nOut[138]: \\n          AAPL      MSFT       XOM  SPX\\n2003  0.541124  0.745174  0.661265    1\\n2004  0.374283  0.588531  0.557742    1\\n2005  0.467540  0.562374  0.631010    1\\n2006  0.428267  0.406126  0.518514    1\\n2007  0.508118  0.658770  0.786264    1\\n2008  0.681434  0.804626  0.828303    1\\n2009  0.707103  0.654902  0.797921    1\\n2010  0.710105  0.730118  0.839057    1\\n2011  0.691931  0.800996  0.859975    1\\nThere is, of course, nothing to stop you from computing inter-column correlations:\\n# Annual correlation of Apple with Microsoft\\nIn [139]: by_year.apply(lambda g: g['AAPL'].corr(g['MSFT']))\\nOut[139]: \\n2003    0.480868\\n2004    0.259024\\n2005    0.300093\\n2006    0.161735\\n2007    0.417738\\n2008    0.611901\\n2009    0.432738\\n2010    0.571946\\n2011    0.581987\\nExample: Group-wise Linear Regression\\nIn the same vein as the previous example, you can use groupby to perform more complex\\ngroup-wise statistical analysis, as long as the function returns a pandas object or scalar\\nvalue. For example, I can define the following regress function (using the statsmo\\ndels econometrics library) which executes an ordinary least squares (OLS) regression\\non each chunk of data:\\n274 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"import statsmodels.api as sm\\ndef regress(data, yvar, xvars):\\n    Y = data[yvar]\\n    X = data[xvars]\\n    X['intercept'] = 1.\\n    result = sm.OLS(Y, X).fit()\\n    return result.params\\nNow, to run a yearly linear regression of AAPL on SPX returns, I execute:\\nIn [141]: by_year.apply(regress, 'AAPL', ['SPX'])\\nOut[141]: \\n           SPX  intercept\\n2003  1.195406   0.000710\\n2004  1.363463   0.004201\\n2005  1.766415   0.003246\\n2006  1.645496   0.000080\\n2007  1.198761   0.003438\\n2008  0.968016  -0.001110\\n2009  0.879103   0.002954\\n2010  1.052608   0.001261\\n2011  0.806605   0.001514\\nPivot Tables and Cross-Tabulation\\nA pivot table is a data summarization tool frequently found in spreadsheet programs\\nand other data analysis software. It aggregates a table of data by one or more keys,\\narranging the data in a rectangle with some of the group keys along the rows and some\\nalong the columns. Pivot tables in Python with pandas are made possible using the\\ngroupby facility described in this chapter combined with reshape operations utilizing\\nhierarchical indexing. DataFrame has a pivot_table method, and additionally there is\\na top-level pandas.pivot_table function. In addition to providing a convenience inter-\\nface to groupby, pivot_table also can add partial totals, also known as margins.\\nReturning to the tipping data set, suppose I wanted to compute a table of group means\\n(the default pivot_table aggregation type) arranged by sex and smoker on the rows:\\nIn [142]: tips.pivot_table(rows=['sex', 'smoker'])\\nOut[142]: \\n                   size       tip   tip_pct  total_bill\\nsex    smoker                                          \\nFemale No      2.592593  2.773519  0.156921   18.105185\\n       Yes     2.242424  2.931515  0.182150   17.977879\\nMale   No      2.711340  3.113402  0.160669   19.791237\\n       Yes     2.500000  3.051167  0.152771   22.284500\\nThis could have been easily produced using groupby. Now, suppose we want to aggre-\\ngate only tip_pct and size, and additionally group by day. I’ll put smoker in the table\\ncolumns and day in the rows:\\nIn [143]: tips.pivot_table(['tip_pct', 'size'], rows=['sex', 'day'],\\n   .....:                  cols='smoker')\\nOut[143]: \\nPivot Tables and Cross-Tabulation | 275\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"tip_pct                size          \\nsmoker             No       Yes        No       Yes\\nsex    day                                         \\nFemale Fri   0.165296  0.209129  2.500000  2.000000\\n       Sat   0.147993  0.163817  2.307692  2.200000\\n       Sun   0.165710  0.237075  3.071429  2.500000\\n       Thur  0.155971  0.163073  2.480000  2.428571\\nMale   Fri   0.138005  0.144730  2.000000  2.125000\\n       Sat   0.162132  0.139067  2.656250  2.629630\\n       Sun   0.158291  0.173964  2.883721  2.600000\\n       Thur  0.165706  0.164417  2.500000  2.300000\\nThis table could be augmented to include partial totals by passing margins=True. This\\nhas the effect of adding All row and column labels, with corresponding values being\\nthe group statistics for all the data within a single tier. In this below example, the All\\nvalues are means without taking into account smoker vs. non-smoker (the All columns)\\nor any of the two levels of grouping on the rows (the All row):\\nIn [144]: tips.pivot_table(['tip_pct', 'size'], rows=['sex', 'day'],\\n   .....:                  cols='smoker', margins=True)\\nOut[144]: \\n                 size                       tip_pct                    \\nsmoker             No       Yes       All        No       Yes       All\\nsex    day                                                             \\nFemale Fri   2.500000  2.000000  2.111111  0.165296  0.209129  0.199388\\n       Sat   2.307692  2.200000  2.250000  0.147993  0.163817  0.156470\\n       Sun   3.071429  2.500000  2.944444  0.165710  0.237075  0.181569\\n       Thur  2.480000  2.428571  2.468750  0.155971  0.163073  0.157525\\nMale   Fri   2.000000  2.125000  2.100000  0.138005  0.144730  0.143385\\n       Sat   2.656250  2.629630  2.644068  0.162132  0.139067  0.151577\\n       Sun   2.883721  2.600000  2.810345  0.158291  0.173964  0.162344\\n       Thur  2.500000  2.300000  2.433333  0.165706  0.164417  0.165276\\nAll          2.668874  2.408602  2.569672  0.159328  0.163196  0.160803\\nTo use a different aggregation function, pass it to aggfunc. For example, 'count' or\\nlen will give you a cross-tabulation (count or frequency) of group sizes:\\nIn [145]: tips.pivot_table('tip_pct', rows=['sex', 'smoker'], cols='day',\\n   .....:                  aggfunc=len, margins=True)\\nOut[145]: \\nday            Fri  Sat  Sun  Thur  All\\nsex    smoker                          \\nFemale No        2   13   14    25   54\\n       Yes       7   15    4     7   33\\nMale   No        2   32   43    20   97\\n       Yes       8   27   15    10   60\\nAll             19   87   76    62  244\\nIf some combinations are empty (or otherwise NA), you may wish to pass a fill_value:\\nIn [146]: tips.pivot_table('size', rows=['time', 'sex', 'smoker'],\\n   .....:                  cols='day', aggfunc='sum', fill_value=0)\\nOut[146]: \\nday                   Fri  Sat  Sun  Thur\\ntime   sex    smoker                     \\nDinner Female No        2   30   43     2\\n276 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Yes       8   33   10     0\\n       Male   No        4   85  124     0\\n              Yes      12   71   39     0\\nLunch  Female No        3    0    0    60\\n              Yes       6    0    0    17\\n       Male   No        0    0    0    50\\n              Yes       5    0    0    23\\nSee Table 9-2 for a summary of pivot_table methods.\\nTable 9-2. pivot_table options\\nFunction name Description\\nvalues Column name or names to aggregate. By default aggregates all numeric columns\\nrows Column names or other group keys to group on the rows of the resulting pivot table\\ncols Column names or other group keys to group on the columns of the resulting pivot table\\naggfunc Aggregation function or list of functions; 'mean' by default. Can be any function valid in a groupby context\\nfill_value Replace missing values in result table\\nmargins Add row/column subtotals and grand total, False by default\\nCross-Tabulations: Crosstab\\nA cross-tabulation (or crosstab for short) is a special case of a pivot table that computes\\ngroup frequencies. Here is a canonical example taken from the Wikipedia page on cross-\\ntabulation:\\nIn [150]: data\\nOut[150]: \\n   Sample  Gender    Handedness\\n0       1  Female  Right-handed\\n1       2    Male   Left-handed\\n2       3  Female  Right-handed\\n3       4    Male  Right-handed\\n4       5    Male   Left-handed\\n5       6    Male  Right-handed\\n6       7  Female  Right-handed\\n7       8  Female   Left-handed\\n8       9    Male  Right-handed\\n9      10  Female  Right-handed\\nAs part of some survey analysis, we might want to summarize this data by gender and\\nhandedness. You could use pivot_table to do this, but the pandas.crosstab function\\nis very convenient:\\nIn [151]: pd.crosstab(data.Gender, data.Handedness, margins=True)\\nOut[151]: \\nHandedness  Left-handed  Right-handed  All\\nGender                                    \\nFemale                1             4    5\\nMale                  2             3    5\\nAll                   3             7   10\\nPivot Tables and Cross-Tabulation | 277\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"The first two arguments to crosstab can each either be an array or Series or a list of\\narrays. As in the tips data:\\nIn [152]: pd.crosstab([tips.time, tips.day], tips.smoker, margins=True)\\nOut[152]: \\nsmoker        No  Yes  All\\ntime   day                \\nDinner Fri     3    9   12\\n       Sat    45   42   87\\n       Sun    57   19   76\\n       Thur    1    0    1\\nLunch  Fri     1    6    7\\n       Thur   44   17   61\\nAll          151   93  244\\nExample: 2012 Federal Election Commission Database\\nThe US Federal Election Commission publishes data on contributions to political cam-\\npaigns. This includes contributor names, occupation and employer, address, and con-\\ntribution amount. An interesting dataset is from the 2012 US presidential election\\n(http://www.fec.gov/disclosurep/PDownload.do). As of this writing (June 2012), the full\\ndataset for all states is a 150 megabyte CSV file P00000001-ALL.csv, which can be loaded\\nwith pandas.read_csv:\\nIn [13]: fec = pd.read_csv('ch09/P00000001-ALL.csv')\\nIn [14]: fec\\nOut[14]:\\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 1001731 entries, 0 to 1001730\\nData columns:\\ncmte_id              1001731  non-null values\\ncand_id              1001731  non-null values\\ncand_nm              1001731  non-null values\\ncontbr_nm            1001731  non-null values\\ncontbr_city          1001716  non-null values\\ncontbr_st            1001727  non-null values\\ncontbr_zip           1001620  non-null values\\ncontbr_employer      994314   non-null values\\ncontbr_occupation    994433   non-null values\\ncontb_receipt_amt    1001731  non-null values\\ncontb_receipt_dt     1001731  non-null values\\nreceipt_desc         14166    non-null values\\nmemo_cd              92482    non-null values\\nmemo_text            97770    non-null values\\nform_tp              1001731  non-null values\\nfile_num             1001731  non-null values\\ndtypes: float64(1), int64(1), object(14)\\nA sample record in the DataFrame looks like this:\\nIn [15]: fec.ix[123456]\\nOut[15]:\\ncmte_id                             C00431445\\n278 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'cand_id                             P80003338\\ncand_nm                         Obama, Barack\\ncontbr_nm                         ELLMAN, IRA\\ncontbr_city                             TEMPE\\ncontbr_st                                  AZ\\ncontbr_zip                          852816719\\ncontbr_employer      ARIZONA STATE UNIVERSITY\\ncontbr_occupation                   PROFESSOR\\ncontb_receipt_amt                          50\\ncontb_receipt_dt                    01-DEC-11\\nreceipt_desc                              NaN\\nmemo_cd                                   NaN\\nmemo_text                                 NaN\\nform_tp                                 SA17A\\nfile_num                               772372\\nName: 123456\\nYou can probably think of many ways to start slicing and dicing this data to extract\\ninformative statistics about donors and patterns in the campaign contributions. I’ll\\nspend the next several pages showing you a number of different analyses that apply\\ntechniques you have learned about so far.\\nYou can see that there are no political party affiliations in the data, so this would be\\nuseful to add. You can get a list of all the unique political candidates using unique (note\\nthat NumPy suppresses the quotes around the strings in the output):\\nIn [16]: unique_cands = fec.cand_nm.unique()\\nIn [17]: unique_cands\\nOut[17]:\\narray([Bachmann, Michelle, Romney, Mitt, Obama, Barack,\\n       Roemer, Charles E. \\'Buddy\\' III, Pawlenty, Timothy,\\n       Johnson, Gary Earl, Paul, Ron, Santorum, Rick, Cain, Herman,\\n       Gingrich, Newt, McCotter, Thaddeus G, Huntsman, Jon, Perry, Rick], dtype=object)\\nIn [18]: unique_cands[2]\\nOut[18]: \\'Obama, Barack\\'\\nAn easy way to indicate party affiliation is using a dict:2\\nparties = {\\'Bachmann, Michelle\\': \\'Republican\\',\\n           \\'Cain, Herman\\': \\'Republican\\',\\n           \\'Gingrich, Newt\\': \\'Republican\\',\\n           \\'Huntsman, Jon\\': \\'Republican\\',\\n           \\'Johnson, Gary Earl\\': \\'Republican\\',\\n           \\'McCotter, Thaddeus G\\': \\'Republican\\',\\n           \\'Obama, Barack\\': \\'Democrat\\',\\n           \\'Paul, Ron\\': \\'Republican\\',\\n           \\'Pawlenty, Timothy\\': \\'Republican\\',\\n           \\'Perry, Rick\\': \\'Republican\\',\\n           \"Roemer, Charles E. \\'Buddy\\' III\": \\'Republican\\',\\n2. This makes the simplifying assumption that Gary Johnson is a Republican even though he later became\\nthe Libertarian party candidate.\\nExample: 2012 Federal Election Commission Database | 279\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"'Romney, Mitt': 'Republican',\\n           'Santorum, Rick': 'Republican'}\\nNow, using this mapping and the map method on Series objects, you can compute an\\narray of political parties from the candidate names:\\nIn [20]: fec.cand_nm[123456:123461]\\nOut[20]:\\n123456    Obama, Barack\\n123457    Obama, Barack\\n123458    Obama, Barack\\n123459    Obama, Barack\\n123460    Obama, Barack\\nName: cand_nm\\nIn [21]: fec.cand_nm[123456:123461].map(parties)\\nOut[21]:\\n123456    Democrat\\n123457    Democrat\\n123458    Democrat\\n123459    Democrat\\n123460    Democrat\\nName: cand_nm\\n# Add it as a column\\nIn [22]: fec['party'] = fec.cand_nm.map(parties)\\nIn [23]: fec['party'].value_counts()\\nOut[23]:\\nDemocrat      593746\\nRepublican    407985\\nA couple of data preparation points. First, this data includes both contributions and\\nrefunds (negative contribution amount):\\nIn [24]: (fec.contb_receipt_amt > 0).value_counts()\\nOut[24]:\\nTrue     991475\\nFalse     10256\\nTo simplify the analysis, I’ll restrict the data set to positive contributions:\\nIn [25]: fec = fec[fec.contb_receipt_amt > 0]\\nSince Barack Obama and Mitt Romney are the main two candidates, I’ll also prepare\\na subset that just has contributions to their campaigns:\\nIn [26]: fec_mrbo = fec[fec.cand_nm.isin(['Obama, Barack', 'Romney, Mitt'])]\\nDonation Statistics by Occupation and Employer\\nDonations by occupation is another oft-studied statistic. For example, lawyers (attor-\\nneys) tend to donate more money to Democrats, while business executives tend to\\ndonate more to Republicans. You have no reason to believe me; you can see for yourself\\nin the data. First, the total number of donations by occupation is easy:\\n280 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [27]: fec.contbr_occupation.value_counts()[:10]\\nOut[27]:\\nRETIRED                                   233990\\nINFORMATION REQUESTED                      35107\\nATTORNEY                                   34286\\nHOMEMAKER                                  29931\\nPHYSICIAN                                  23432\\nINFORMATION REQUESTED PER BEST EFFORTS     21138\\nENGINEER                                   14334\\nTEACHER                                    13990\\nCONSULTANT                                 13273\\nPROFESSOR                                  12555\\nYou will notice by looking at the occupations that many refer to the same basic job\\ntype, or there are several variants of the same thing. Here is a code snippet illustrates a\\ntechnique for cleaning up a few of them by mapping from one occupation to another;\\nnote the “trick” of using dict.get to allow occupations with no mapping to “pass\\nthrough”:\\nocc_mapping = {\\n   'INFORMATION REQUESTED PER BEST EFFORTS' : 'NOT PROVIDED',\\n   'INFORMATION REQUESTED' : 'NOT PROVIDED',\\n   'INFORMATION REQUESTED (BEST EFFORTS)' : 'NOT PROVIDED',\\n   'C.E.O.': 'CEO'\\n}\\n# If no mapping provided, return x\\nf = lambda x: occ_mapping.get(x, x)\\nfec.contbr_occupation = fec.contbr_occupation.map(f)\\nI’ll also do the same thing for employers:\\nemp_mapping = {\\n   'INFORMATION REQUESTED PER BEST EFFORTS' : 'NOT PROVIDED',\\n   'INFORMATION REQUESTED' : 'NOT PROVIDED',\\n   'SELF' : 'SELF-EMPLOYED',\\n   'SELF EMPLOYED' : 'SELF-EMPLOYED',\\n}\\n# If no mapping provided, return x\\nf = lambda x: emp_mapping.get(x, x)\\nfec.contbr_employer = fec.contbr_employer.map(f)\\nNow, you can use pivot_table to aggregate the data by party and occupation, then\\nfilter down to the subset that donated at least $2 million overall:\\nIn [34]: by_occupation = fec.pivot_table('contb_receipt_amt',\\n   ....:                                 rows='contbr_occupation',\\n   ....:                                 cols='party', aggfunc='sum')\\nIn [35]: over_2mm = by_occupation[by_occupation.sum(1) > 2000000]\\nIn [36]: over_2mm\\nOut[36]:\\nparty                 Democrat       Republican\\ncontbr_occupation\\nExample: 2012 Federal Election Commission Database | 281\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"ATTORNEY           11141982.97   7477194.430000\\nCEO                 2074974.79   4211040.520000\\nCONSULTANT          2459912.71   2544725.450000\\nENGINEER             951525.55   1818373.700000\\nEXECUTIVE           1355161.05   4138850.090000\\nHOMEMAKER           4248875.80  13634275.780000\\nINVESTOR             884133.00   2431768.920000\\nLAWYER              3160478.87    391224.320000\\nMANAGER              762883.22   1444532.370000\\nNOT PROVIDED        4866973.96  20565473.010000\\nOWNER               1001567.36   2408286.920000\\nPHYSICIAN           3735124.94   3594320.240000\\nPRESIDENT           1878509.95   4720923.760000\\nPROFESSOR           2165071.08    296702.730000\\nREAL ESTATE          528902.09   1625902.250000\\nRETIRED            25305116.38  23561244.489999\\nSELF-EMPLOYED        672393.40   1640252.540000\\nIt can be easier to look at this data graphically as a bar plot ( 'barh' means horizontal\\nbar plot, see Figure 9-2):\\nIn [38]: over_2mm.plot(kind='barh')\\nFigure 9-2. Total donations by party for top occupations\\nYou might be interested in the top donor occupations or top companies donating to\\nObama and Romney. To do this, you can group by candidate name and use a variant\\nof the top method from earlier in the chapter:\\ndef get_top_amounts(group, key, n=5):\\n    totals = group.groupby(key)['contb_receipt_amt'].sum()\\n    # Order totals by key in descending order\\n    return totals.order(ascending=False)[-n:]\\n282 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Then aggregated by occupation and employer:\\nIn [40]: grouped = fec_mrbo.groupby('cand_nm')\\nIn [41]: grouped.apply(get_top_amounts, 'contbr_occupation', n=7)\\nOut[41]:\\ncand_nm        contbr_occupation\\nObama, Barack  RETIRED              25305116.38\\n               ATTORNEY             11141982.97\\n               NOT PROVIDED          4866973.96\\n               HOMEMAKER             4248875.80\\n               PHYSICIAN             3735124.94\\n               LAWYER                3160478.87\\n               CONSULTANT            2459912.71\\nRomney, Mitt   RETIRED              11508473.59\\n               NOT PROVIDED         11396894.84\\n               HOMEMAKER             8147446.22\\n               ATTORNEY              5364718.82\\n               PRESIDENT             2491244.89\\n               EXECUTIVE             2300947.03\\n               C.E.O.                1968386.11\\nName: contb_receipt_amt\\nIn [42]: grouped.apply(get_top_amounts, 'contbr_employer', n=10)\\nOut[42]:\\ncand_nm        contbr_employer\\nObama, Barack  RETIRED               22694358.85\\n               SELF-EMPLOYED         18626807.16\\n               NOT EMPLOYED           8586308.70\\n               NOT PROVIDED           5053480.37\\n               HOMEMAKER              2605408.54\\n               STUDENT                 318831.45\\n               VOLUNTEER               257104.00\\n               MICROSOFT               215585.36\\n               SIDLEY AUSTIN LLP       168254.00\\n               REFUSED                 149516.07\\nRomney, Mitt   NOT PROVIDED          12059527.24\\n               RETIRED               11506225.71\\n               HOMEMAKER              8147196.22\\n               SELF-EMPLOYED          7414115.22\\n               STUDENT                 496490.94\\n               CREDIT SUISSE           281150.00\\n               MORGAN STANLEY          267266.00\\n               GOLDMAN SACH & CO.      238250.00\\n               BARCLAYS CAPITAL        162750.00\\n               H.I.G. CAPITAL          139500.00\\nName: contb_receipt_amt\\nBucketing Donation Amounts\\nA useful way to analyze this data is to use the cut function to discretize the contributor\\namounts into buckets by contribution size:\\nIn [43]: bins = np.array([0, 1, 10, 100, 1000, 10000, 100000, 1000000, 10000000])\\nExample: 2012 Federal Election Commission Database | 283\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [44]: labels = pd.cut(fec_mrbo.contb_receipt_amt, bins)\\nIn [45]: labels\\nOut[45]:\\nCategorical:contb_receipt_amt\\narray([(10, 100], (100, 1000], (100, 1000], ..., (1, 10], (10, 100],\\n       (100, 1000]], dtype=object)\\nLevels (8): array([(0, 1], (1, 10], (10, 100], (100, 1000], (1000, 10000],\\n       (10000, 100000], (100000, 1000000], (1000000, 10000000]], dtype=object)\\nWe can then group the data for Obama and Romney by name and bin label to get a\\nhistogram by donation size:\\nIn [46]: grouped = fec_mrbo.groupby(['cand_nm', labels])\\nIn [47]: grouped.size().unstack(0)\\nOut[47]:\\ncand_nm              Obama, Barack  Romney, Mitt\\ncontb_receipt_amt\\n(0, 1]                         493            77\\n(1, 10]                      40070          3681\\n(10, 100]                   372280         31853\\n(100, 1000]                 153991         43357\\n(1000, 10000]                22284         26186\\n(10000, 100000]                  2             1\\n(100000, 1000000]                3           NaN\\n(1000000, 10000000]              4           NaN\\nThis data shows that Obama has received a significantly larger number of small don-\\nations than Romney. You can also sum the contribution amounts and normalize within\\nbuckets to visualize percentage of total donations of each size by candidate:\\nIn [48]: bucket_sums = grouped.contb_receipt_amt.sum().unstack(0)\\nIn [49]: bucket_sums\\nOut[49]:\\ncand_nm              Obama, Barack  Romney, Mitt\\ncontb_receipt_amt\\n(0, 1]                      318.24         77.00\\n(1, 10]                  337267.62      29819.66\\n(10, 100]              20288981.41    1987783.76\\n(100, 1000]            54798531.46   22363381.69\\n(1000, 10000]          51753705.67   63942145.42\\n(10000, 100000]           59100.00      12700.00\\n(100000, 1000000]       1490683.08           NaN\\n(1000000, 10000000]     7148839.76           NaN\\nIn [50]: normed_sums = bucket_sums.div(bucket_sums.sum(axis=1), axis=0)\\nIn [51]: normed_sums\\nOut[51]:\\ncand_nm              Obama, Barack  Romney, Mitt\\ncontb_receipt_amt\\n(0, 1]                    0.805182      0.194818\\n(1, 10]                   0.918767      0.081233\\n(10, 100]                 0.910769      0.089231\\n284 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"(100, 1000]               0.710176      0.289824\\n(1000, 10000]             0.447326      0.552674\\n(10000, 100000]           0.823120      0.176880\\n(100000, 1000000]         1.000000           NaN\\n(1000000, 10000000]       1.000000           NaN\\nIn [52]: normed_sums[:-2].plot(kind='barh', stacked=True)\\nI excluded the two largest bins as these are not donations by individuals. See Fig-\\nure 9-3 for the resulting figure.\\nFigure 9-3. Percentage of total donations received by candidates for each donation size\\nThere are of course many refinements and improvements of this analysis. For example,\\nyou could aggregate donations by donor name and zip code to adjust for donors who\\ngave many small amounts versus one or more large donations. I encourage you to\\ndownload it and explore it yourself.\\nDonation Statistics by State\\nAggregating the data by candidate and state is a routine affair:\\nIn [53]: grouped = fec_mrbo.groupby(['cand_nm', 'contbr_st'])\\nIn [54]: totals = grouped.contb_receipt_amt.sum().unstack(0).fillna(0)\\nIn [55]: totals = totals[totals.sum(1) > 100000]\\nIn [56]: totals[:10]\\nOut[56]:\\ncand_nm    Obama, Barack  Romney, Mitt\\ncontbr_st\\nExample: 2012 Federal Election Commission Database | 285\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"AK             281840.15      86204.24\\nAL             543123.48     527303.51\\nAR             359247.28     105556.00\\nAZ            1506476.98    1888436.23\\nCA           23824984.24   11237636.60\\nCO            2132429.49    1506714.12\\nCT            2068291.26    3499475.45\\nDC            4373538.80    1025137.50\\nDE             336669.14      82712.00\\nFL            7318178.58    8338458.81\\nIf you divide each row by the total contribution amount, you get the relative percentage\\nof total donations by state for each candidate:\\nIn [57]: percent = totals.div(totals.sum(1), axis=0)\\nIn [58]: percent[:10]\\nOut[58]:\\ncand_nm    Obama, Barack  Romney, Mitt\\ncontbr_st\\nAK              0.765778      0.234222\\nAL              0.507390      0.492610\\nAR              0.772902      0.227098\\nAZ              0.443745      0.556255\\nCA              0.679498      0.320502\\nCO              0.585970      0.414030\\nCT              0.371476      0.628524\\nDC              0.810113      0.189887\\nDE              0.802776      0.197224\\nFL              0.467417      0.532583\\nI thought it would be interesting to look at this data plotted on a map, using ideas from\\nChapter 8. After locating a shape file for the state boundaries (http://nationalatlas.gov/\\natlasftp.html?openChapters=chpbound) and learning a bit more about matplotlib and\\nits basemap toolkit (I was aided by a blog posting from Thomas Lecocq) 3, I ended up\\nwith the following code for plotting these relative percentages:\\nfrom mpl_toolkits.basemap import Basemap, cm\\nimport numpy as np\\nfrom matplotlib import rcParams\\nfrom matplotlib.collections import LineCollection\\nimport matplotlib.pyplot as plt\\nfrom shapelib import ShapeFile\\nimport dbflib\\nobama = percent['Obama, Barack']\\nfig = plt.figure(figsize=(12, 12))\\nax = fig.add_axes([0.1,0.1,0.8,0.8])\\nlllat = 21; urlat = 53; lllon = -118; urlon = -62\\n3. http://www.geophysique.be/2011/01/27/matplotlib-basemap-tutorial-07-shapefiles-unleached/\\n286 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"m = Basemap(ax=ax, projection='stere',\\n            lon_0=(urlon + lllon) / 2, lat_0=(urlat + lllat) / 2,\\n            llcrnrlat=lllat, urcrnrlat=urlat, llcrnrlon=lllon,\\n            urcrnrlon=urlon, resolution='l')\\nm.drawcoastlines()\\nm.drawcountries()\\nshp = ShapeFile('../states/statesp020')\\ndbf = dbflib.open('../states/statesp020')\\nfor npoly in range(shp.info()[0]):\\n    # Draw colored polygons on the map\\n    shpsegs = []\\n    shp_object = shp.read_object(npoly)\\n    verts = shp_object.vertices()\\n    rings = len(verts)\\n    for ring in range(rings):\\n        lons, lats = zip(*verts[ring])\\n        x, y = m(lons, lats)\\n        shpsegs.append(zip(x,y))\\n        if ring == 0:\\n            shapedict = dbf.read_record(npoly)\\n        name = shapedict['STATE']\\n    lines = LineCollection(shpsegs,antialiaseds=(1,))\\n    # state_to_code dict, e.g. 'ALASKA' -> 'AK', omitted\\n    try:\\n        per = obama[state_to_code[name.upper()]]\\n    except KeyError:\\n        continue\\n    lines.set_facecolors('k')\\n    lines.set_alpha(0.75 * per) # Shrink the percentage a bit\\n    lines.set_edgecolors('k')\\n    lines.set_linewidth(0.3)\\n    ax.add_collection(lines)\\nplt.show()\\nSee Figure 9-4 for the result.\\nExample: 2012 Federal Election Commission Database | 287\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Figure 9-4. US map aggregated donation statistics overlay (darker means more Democratic)\\n288 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'CHAPTER 10\\nTime Series\\nTime series data is an important form of structured data in many different fields, such\\nas finance, economics, ecology, neuroscience, or physics. Anything that is observed or\\nmeasured at many points in time forms a time series. Many time series are fixed fre-\\nquency, which is to say that data points occur at regular intervals according to some\\nrule, such as every 15 seconds, every 5 minutes, or once per month. Time series can\\nalso be irregular without a fixed unit or time or offset between units. How you mark\\nand refer to time series data depends on the application and you may have one of the\\nfollowing:\\n• Timestamps, specific instants in time\\n• Fixed periods, such as the month January 2007 or the full year 2010\\n• Intervals of time, indicated by a start and end timestamp. Periods can be thought\\nof as special cases of intervals\\n• Experiment or elapsed time; each timestamp is a measure of time relative to a\\nparticular start time. For example, the diameter of a cookie baking each second\\nsince being placed in the oven\\nIn this chapter, I am mainly concerned with time series in the first 3 categories, though\\nmany of the techniques can be applied to experimental time series where the index may\\nbe an integer or floating point number indicating elapsed time from the start of the\\nexperiment. The simplest and most widely used kind of time series are those indexed\\nby timestamp.\\npandas provides a standard set of time series tools and data algorithms. With this, you\\ncan efficiently work with very large time series and easily slice and dice, aggregate, and\\nresample irregular and fixed frequency time series. As you might guess, many of these\\ntools are especially useful for financial and economics applications, but you could cer-\\ntainly use them to analyze server log data, too.\\n289\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Some of the features and code, in particular period logic, presented in\\nthis chapter were derived from the now defunct scikits.timeseries li-\\nbrary.\\nDate and Time Data Types and Tools\\nThe Python standard library includes data types for date and time data, as well as\\ncalendar-related functionality. The datetime, time, and calendar modules are the main\\nplaces to start. The datetime.datetime type, or simply datetime, is widely used:\\nIn [317]: from datetime import datetime\\nIn [318]: now = datetime.now()\\nIn [319]: now\\nOut[319]: datetime.datetime(2012, 8, 4, 17, 9, 21, 832092)\\nIn [320]: now.year, now.month, now.day\\nOut[320]: (2012, 8, 4)\\ndatetime stores both the date and time down to the microsecond. datetime.time\\ndelta represents the temporal difference between two datetime objects:\\nIn [321]: delta = datetime(2011, 1, 7) - datetime(2008, 6, 24, 8, 15)\\nIn [322]: delta\\nOut[322]: datetime.timedelta(926, 56700)\\nIn [323]: delta.days        In [324]: delta.seconds\\nOut[323]: 926               Out[324]: 56700\\nYou can add (or subtract) a timedelta or multiple thereof to a datetime object to yield\\na new shifted object:\\nIn [325]: from datetime import timedelta\\nIn [326]: start = datetime(2011, 1, 7)\\nIn [327]: start + timedelta(12)\\nOut[327]: datetime.datetime(2011, 1, 19, 0, 0)\\nIn [328]: start - 2 * timedelta(12)\\nOut[328]: datetime.datetime(2010, 12, 14, 0, 0)\\nThe data types in the datetime module are summarized in Table 10-1. While this chap-\\nter is mainly concerned with the data types in pandas and higher level time series ma-\\nnipulation, you will undoubtedly encounter the datetime-based types in many other\\nplaces in Python the wild.\\n290 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Table 10-1. Types in datetime module\\nType Description\\ndate Store calendar date (year, month, day) using the Gregorian calendar.\\ntime Store time of day as hours, minutes, seconds, and microseconds\\ndatetime Stores both date and time\\ntimedelta Represents the difference between two datetime values (as days, seconds, and micro-\\nseconds)\\nConverting between string and datetime\\ndatetime objects and pandas Timestamp objects, which I’ll introduce later, can be for-\\nmatted as strings using str or the strftime method, passing a format specification:\\nIn [329]: stamp = datetime(2011, 1, 3)\\nIn [330]: str(stamp)                   In [331]: stamp.strftime('%Y-%m-%d')\\nOut[330]: '2011-01-03 00:00:00'        Out[331]: '2011-01-03'\\nSee Table 10-2 for a complete list of the format codes. These same format codes can be\\nused to convert strings to dates using datetime.strptime:\\nIn [332]: value = '2011-01-03'\\nIn [333]: datetime.strptime(value, '%Y-%m-%d')\\nOut[333]: datetime.datetime(2011, 1, 3, 0, 0)\\nIn [334]: datestrs = ['7/6/2011', '8/6/2011']\\nIn [335]: [datetime.strptime(x, '%m/%d/%Y') for x in datestrs]\\nOut[335]: [datetime.datetime(2011, 7, 6, 0, 0), datetime.datetime(2011, 8, 6, 0, 0)]\\ndatetime.strptime is the best way to parse a date with a known format. However, it\\ncan be a bit annoying to have to write a format spec each time, especially for common\\ndate formats. In this case, you can use the parser.parse method in the third party \\ndateutil package:\\nIn [336]: from dateutil.parser import parse\\nIn [337]: parse('2011-01-03')\\nOut[337]: datetime.datetime(2011, 1, 3, 0, 0)\\ndateutil is capable of parsing almost any human-intelligible date representation:\\nIn [338]: parse('Jan 31, 1997 10:45 PM')\\nOut[338]: datetime.datetime(1997, 1, 31, 22, 45)\\nIn international locales, day appearing before month is very common, so you can pass\\ndayfirst=True to indicate this:\\nIn [339]: parse('6/12/2011', dayfirst=True)\\nOut[339]: datetime.datetime(2011, 12, 6, 0, 0)\\nDate and Time Data Types and Tools | 291\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"pandas is generally oriented toward working with arrays of dates, whether used as an\\naxis index or a column in a DataFrame. The to_datetime method parses many different\\nkinds of date representations. Standard date formats like ISO8601 can be parsed very\\nquickly.\\nIn [340]: datestrs\\nOut[340]: ['7/6/2011', '8/6/2011']\\nIn [341]: pd.to_datetime(datestrs)\\nOut[341]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n[2011-07-06 00:00:00, 2011-08-06 00:00:00]\\nLength: 2, Freq: None, Timezone: None\\nIt also handles values that should be considered missing (None, empty string, etc.):\\nIn [342]: idx = pd.to_datetime(datestrs + [None])\\nIn [343]: idx\\nOut[343]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n[2011-07-06 00:00:00, ..., NaT]\\nLength: 3, Freq: None, Timezone: None\\nIn [344]: idx[2]\\nOut[344]: NaT\\nIn [345]: pd.isnull(idx)\\nOut[345]: array([False, False, True], dtype=bool)\\nNaT (Not a Time) is pandas’s NA value for timestamp data.\\ndateutil.parser is a useful, but not perfect tool. Notably, it will recog-\\nnize some strings as dates that you might prefer that it didn’t, like\\n'42' will be parsed as the year 2042 with today’s calendar date.\\nTable 10-2. Datetime format specification (ISO C89 compatible)\\nType Description\\n%Y 4-digit year\\n%y 2-digit year\\n%m 2-digit month [01, 12]\\n%d 2-digit day [01, 31]\\n%H Hour (24-hour clock) [00, 23]\\n%I Hour (12-hour clock) [01, 12]\\n%M 2-digit minute [00, 59]\\n%S Second [00, 61] (seconds 60, 61 account for leap seconds)\\n%w Weekday as integer [0 (Sunday), 6]\\n292 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Type Description\\n%U Week number of the year [00, 53]. Sunday is considered the first day of the week, and days before the first\\nSunday of the year are “week 0”.\\n%W Week number of the year [00, 53]. Monday is considered the first day of the week, and days before the first\\nMonday of the year are “week 0”.\\n%z UTC time zone offset as +HHMM or -HHMM, empty if time zone naive\\n%F Shortcut for %Y-%m-%d, for example 2012-4-18\\n%D Shortcut for %m/%d/%y, for example 04/18/12\\ndatetime objects also have a number of locale-specific formatting options for systems\\nin other countries or languages. For example, the abbreviated month names will be\\ndifferent on German or French systems compared with English systems. \\nTable 10-3. Locale-specific date formatting\\nType Description\\n%a Abbreviated weekday name\\n%A Full weekday name\\n%b Abbreviated month name\\n%B Full month name\\n%c Full date and time, for example ‘Tue 01 May 2012 04:20:57 PM’\\n%p Locale equivalent of AM or PM\\n%x Locale-appropriate formatted date; e.g. in US May 1, 2012 yields ’05/01/2012’\\n%X Locale-appropriate time, e.g. ’04:24:12 PM’\\nTime Series Basics\\nThe most basic kind of time series object in pandas is a Series indexed by timestamps,\\nwhich is often represented external to pandas as Python strings or datetime objects:\\nIn [346]: from datetime import datetime\\nIn [347]: dates = [datetime(2011, 1, 2), datetime(2011, 1, 5), datetime(2011, 1, 7),\\n   .....:          datetime(2011, 1, 8), datetime(2011, 1, 10), datetime(2011, 1, 12)]\\nIn [348]: ts = Series(np.random.randn(6), index=dates)\\nIn [349]: ts\\nOut[349]: \\n2011-01-02    0.690002\\n2011-01-05    1.001543\\n2011-01-07   -0.503087\\n2011-01-08   -0.622274\\nTime Series Basics | 293\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"2011-01-10   -0.921169\\n2011-01-12   -0.726213\\nUnder the hood, these datetime objects have been put in a DatetimeIndex, and the\\nvariable ts is now of type TimeSeries:\\nIn [350]: type(ts)\\nOut[350]: pandas.core.series.TimeSeries\\nIn [351]: ts.index\\nOut[351]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n[2011-01-02 00:00:00, ..., 2011-01-12 00:00:00]\\nLength: 6, Freq: None, Timezone: None\\nIt’s not necessary to use the TimeSeries constructor explicitly; when\\ncreating a Series with a DatetimeIndex, pandas knows that the object is\\na time series.\\nLike other Series, arithmetic operations between differently-indexed time series auto-\\nmatically align on the dates:\\nIn [352]: ts + ts[::2]\\nOut[352]: \\n2011-01-02    1.380004\\n2011-01-05         NaN\\n2011-01-07   -1.006175\\n2011-01-08         NaN\\n2011-01-10   -1.842337\\n2011-01-12         NaN\\npandas stores timestamps using NumPy’s datetime64 data type at the nanosecond res-\\nolution:\\nIn [353]: ts.index.dtype\\nOut[353]: dtype('datetime64[ns]')\\nScalar values from a DatetimeIndex are pandas Timestamp objects\\nIn [354]: stamp = ts.index[0]\\nIn [355]: stamp\\nOut[355]: <Timestamp: 2011-01-02 00:00:00>\\nA Timestamp can be substituted anywhere you would use a datetime object. Addition-\\nally, it can store frequency information (if any) and understands how to do time zone\\nconversions and other kinds of manipulations. More on both of these things later.\\nIndexing, Selection, Subsetting\\nTimeSeries is a subclass of Series and thus behaves in the same way with regard to\\nindexing and selecting data based on label:\\n294 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [356]: stamp = ts.index[2]\\nIn [357]: ts[stamp]\\nOut[357]: -0.50308739136034464\\nAs a convenience, you can also pass a string that is interpretable as a date:\\nIn [358]: ts['1/10/2011']             In [359]: ts['20110110']      \\nOut[358]: -0.92116860801301081        Out[359]: -0.92116860801301081\\nFor longer time series, a year or only a year and month can be passed to easily select\\nslices of data:\\nIn [360]: longer_ts = Series(np.random.randn(1000),\\n   .....:                    index=pd.date_range('1/1/2000', periods=1000))\\nIn [361]: longer_ts\\nOut[361]: \\n2000-01-01    0.222896\\n2000-01-02    0.051316\\n2000-01-03   -1.157719\\n2000-01-04    0.816707\\n...\\n2002-09-23   -0.395813\\n2002-09-24   -0.180737\\n2002-09-25    1.337508\\n2002-09-26   -0.416584\\nFreq: D, Length: 1000\\nIn [362]: longer_ts['2001']        In [363]: longer_ts['2001-05']\\nOut[362]:                          Out[363]:                     \\n2001-01-01   -1.499503             2001-05-01    1.662014        \\n2001-01-02    0.545154             2001-05-02   -1.189203        \\n2001-01-03    0.400823             2001-05-03    0.093597        \\n2001-01-04   -1.946230             2001-05-04   -0.539164        \\n...                                ...                           \\n2001-12-28   -1.568139             2001-05-28   -0.683066        \\n2001-12-29   -0.900887             2001-05-29   -0.950313        \\n2001-12-30    0.652346             2001-05-30    0.400710        \\n2001-12-31    0.871600             2001-05-31   -0.126072        \\nFreq: D, Length: 365               Freq: D, Length: 31\\nSlicing with dates works just like with a regular Series:\\nIn [364]: ts[datetime(2011, 1, 7):]\\nOut[364]: \\n2011-01-07   -0.503087\\n2011-01-08   -0.622274\\n2011-01-10   -0.921169\\n2011-01-12   -0.726213\\nBecause most time series data is ordered chronologically, you can slice with timestamps\\nnot contained in a time series to perform a range query:\\nIn [365]: ts                  In [366]: ts['1/6/2011':'1/11/2011']\\nOut[365]:                     Out[366]:                           \\n2011-01-02    0.690002        2011-01-07   -0.503087              \\nTime Series Basics | 295\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"2011-01-05    1.001543        2011-01-08   -0.622274              \\n2011-01-07   -0.503087        2011-01-10   -0.921169              \\n2011-01-08   -0.622274                                            \\n2011-01-10   -0.921169                                            \\n2011-01-12   -0.726213\\nAs before you can pass either a string date, datetime, or Timestamp. Remember that\\nslicing in this manner produces views on the source time series just like slicing NumPy\\narrays. There is an equivalent instance method truncate which slices a TimeSeries be-\\ntween two dates:\\nIn [367]: ts.truncate(after='1/9/2011')\\nOut[367]: \\n2011-01-02    0.690002\\n2011-01-05    1.001543\\n2011-01-07   -0.503087\\n2011-01-08   -0.622274\\nAll of the above holds true for DataFrame as well, indexing on its rows:\\nIn [368]: dates = pd.date_range('1/1/2000', periods=100, freq='W-WED')\\nIn [369]: long_df = DataFrame(np.random.randn(100, 4),\\n   .....:                     index=dates,\\n   .....:                     columns=['Colorado', 'Texas', 'New York', 'Ohio'])\\nIn [370]: long_df.ix['5-2001']\\nOut[370]: \\n            Colorado     Texas  New York      Ohio\\n2001-05-02  0.943479 -0.349366  0.530412 -0.508724\\n2001-05-09  0.230643 -0.065569 -0.248717 -0.587136\\n2001-05-16 -1.022324  1.060661  0.954768 -0.511824\\n2001-05-23 -1.387680  0.767902 -1.164490  1.527070\\n2001-05-30  0.287542  0.715359 -0.345805  0.470886\\nTime Series with Duplicate Indices\\nIn some applications, there may be multiple data observations falling on a particular\\ntimestamp. Here is an example:\\nIn [371]: dates = pd.DatetimeIndex(['1/1/2000', '1/2/2000', '1/2/2000', '1/2/2000',\\n   .....:                           '1/3/2000'])\\nIn [372]: dup_ts = Series(np.arange(5), index=dates)\\nIn [373]: dup_ts\\nOut[373]: \\n2000-01-01    0\\n2000-01-02    1\\n2000-01-02    2\\n2000-01-02    3\\n2000-01-03    4\\nWe can tell that the index is not unique by checking its is_unique property:\\n296 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [374]: dup_ts.index.is_unique\\nOut[374]: False\\nIndexing into this time series will now either produce scalar values or slices depending\\non whether a timestamp is duplicated:\\nIn [375]: dup_ts['1/3/2000']  # not duplicated\\nOut[375]: 4\\nIn [376]: dup_ts['1/2/2000']  # duplicated\\nOut[376]: \\n2000-01-02    1\\n2000-01-02    2\\n2000-01-02    3\\nSuppose you wanted to aggregate the data having non-unique timestamps. One way\\nto do this is to use groupby and pass level=0 (the only level of indexing!):\\nIn [377]: grouped = dup_ts.groupby(level=0)\\nIn [378]: grouped.mean()      In [379]: grouped.count()\\nOut[378]:                     Out[379]:                \\n2000-01-01    0               2000-01-01    1          \\n2000-01-02    2               2000-01-02    3          \\n2000-01-03    4               2000-01-03    1\\nDate Ranges, Frequencies, and Shifting\\nGeneric time series in pandas are assumed to be irregular; that is, they have no fixed\\nfrequency. For many applications this is sufficient. However, it’s often desirable to work\\nrelative to a fixed frequency, such as daily, monthly, or every 15 minutes, even if that\\nmeans introducing missing values into a time series. Fortunately pandas has a full suite\\nof standard time series frequencies and tools for resampling, inferring frequencies, and\\ngenerating fixed frequency date ranges. For example, in the example time series, con-\\nverting it to be fixed daily frequency can be accomplished by calling resample:\\nIn [380]: ts                  In [381]: ts.resample('D')\\nOut[380]:                     Out[381]:                 \\n2011-01-02    0.690002        2011-01-02    0.690002    \\n2011-01-05    1.001543        2011-01-03         NaN    \\n2011-01-07   -0.503087        2011-01-04         NaN    \\n2011-01-08   -0.622274        2011-01-05    1.001543    \\n2011-01-10   -0.921169        2011-01-06         NaN    \\n2011-01-12   -0.726213        2011-01-07   -0.503087    \\n                              2011-01-08   -0.622274    \\n                              2011-01-09         NaN    \\n                              2011-01-10   -0.921169    \\n                              2011-01-11         NaN    \\n                              2011-01-12   -0.726213    \\n                              Freq: D\\nConversion between frequencies or resampling is a big enough topic to have its own\\nsection later. Here I’ll show you how to use the base frequencies and multiples thereof.\\nDate Ranges, Frequencies, and Shifting | 297\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Generating Date Ranges\\nWhile I used it previously without explanation, you may have guessed that pan\\ndas.date_range is responsible for generating a DatetimeIndex with an indicated length\\naccording to a particular frequency:\\nIn [382]: index = pd.date_range('4/1/2012', '6/1/2012')\\nIn [383]: index\\nOut[383]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n[2012-04-01 00:00:00, ..., 2012-06-01 00:00:00]\\nLength: 62, Freq: D, Timezone: None\\nBy default, date_range generates daily timestamps. If you pass only a start or end date,\\nyou must pass a number of periods to generate:\\nIn [384]: pd.date_range(start='4/1/2012', periods=20)\\nOut[384]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n[2012-04-01 00:00:00, ..., 2012-04-20 00:00:00]\\nLength: 20, Freq: D, Timezone: None\\nIn [385]: pd.date_range(end='6/1/2012', periods=20)\\nOut[385]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n[2012-05-13 00:00:00, ..., 2012-06-01 00:00:00]\\nLength: 20, Freq: D, Timezone: None\\nThe start and end dates define strict boundaries for the generated date index. For ex-\\nample, if you wanted a date index containing the last business day of each month, you\\nwould pass the 'BM' frequency (business end of month) and only dates falling on or\\ninside the date interval will be included:\\nIn [386]: pd.date_range('1/1/2000', '12/1/2000', freq='BM')\\nOut[386]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n[2000-01-31 00:00:00, ..., 2000-11-30 00:00:00]\\nLength: 11, Freq: BM, Timezone: None\\ndate_range by default preserves the time (if any) of the start or end timestamp:\\nIn [387]: pd.date_range('5/2/2012 12:56:31', periods=5)\\nOut[387]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n[2012-05-02 12:56:31, ..., 2012-05-06 12:56:31]\\nLength: 5, Freq: D, Timezone: None\\nSometimes you will have start or end dates with time information but want to generate\\na set of timestamps normalized to midnight as a convention. To do this, there is a\\nnormalize option:\\nIn [388]: pd.date_range('5/2/2012 12:56:31', periods=5, normalize=True)\\nOut[388]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n298 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"[2012-05-02 00:00:00, ..., 2012-05-06 00:00:00]\\nLength: 5, Freq: D, Timezone: None\\nFrequencies and Date Offsets\\nFrequencies in pandas are composed of a base frequency and a multiplier. Base fre-\\nquencies are typically referred to by a string alias, like 'M' for monthly or 'H' for hourly.\\nFor each base frequency, there is an object defined generally referred to as a date off-\\nset. For example, hourly frequency can be represented with the Hour class:\\nIn [389]: from pandas.tseries.offsets import Hour, Minute\\nIn [390]: hour = Hour()\\nIn [391]: hour\\nOut[391]: <1 Hour>\\nYou can define a multiple of an offset by passing an integer:\\nIn [392]: four_hours = Hour(4)\\nIn [393]: four_hours\\nOut[393]: <4 Hours>\\nIn most applications, you would never need to explicitly create one of these objects,\\ninstead using a string alias like 'H' or '4H'. Putting an integer before the base frequency\\ncreates a multiple:\\nIn [394]: pd.date_range('1/1/2000', '1/3/2000 23:59', freq='4h')\\nOut[394]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n[2000-01-01 00:00:00, ..., 2000-01-03 20:00:00]\\nLength: 18, Freq: 4H, Timezone: None\\nMany offsets can be combined together by addition:\\nIn [395]: Hour(2) + Minute(30)\\nOut[395]: <150 Minutes>\\nSimilarly, you can pass frequency strings like '2h30min' which will effectively be parsed\\nto the same expression:\\nIn [396]: pd.date_range('1/1/2000', periods=10, freq='1h30min')\\nOut[396]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n[2000-01-01 00:00:00, ..., 2000-01-01 13:30:00]\\nLength: 10, Freq: 90T, Timezone: None\\nSome frequencies describe points in time that are not evenly spaced. For example,\\n'M' (calendar month end) and 'BM' (last business/weekday of month) depend on the\\nnumber of days in a month and, in the latter case, whether the month ends on a weekend\\nor not. For lack of a better term, I call these anchored offsets.\\nSee Table 10-4 for a listing of frequency codes and date offset classes available in pandas.\\nDate Ranges, Frequencies, and Shifting | 299\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Users can define their own custom frequency classes to provide date\\nlogic not available in pandas, though the full details of that are outside\\nthe scope of this book.\\nTable 10-4. Base Time Series Frequencies\\nAlias Offset Type Description\\nD Day Calendar daily\\nB BusinessDay Business daily\\nH Hour Hourly\\nT or min Minute Minutely\\nS Second Secondly\\nL or ms Milli Millisecond (1/1000th of 1 second)\\nU Micro Microsecond (1/1000000th of 1 second)\\nM MonthEnd Last calendar day of month\\nBM BusinessMonthEnd Last business day (weekday) of month\\nMS MonthBegin First calendar day of month\\nBMS BusinessMonthBegin First weekday of month\\nW-MON, W-TUE, ... Week Weekly on given day of week: MON, TUE, WED, THU, FRI, SAT,\\nor SUN.\\nWOM-1MON, WOM-2MON, ... WeekOfMonth Generate weekly dates in the first, second, third, or fourth week\\nof the month. For example, WOM-3FRI for the 3rd Friday of\\neach month.\\nQ-JAN, Q-FEB, ... QuarterEnd Quarterly dates anchored on last calendar day of each month,\\nfor year ending in indicated month: JAN, FEB, MAR, APR, MAY,\\nJUN, JUL, AUG, SEP, OCT, NOV, or DEC.\\nBQ-JAN, BQ-FEB, ... BusinessQuarterEnd Quarterly dates anchored on last weekday day of each month,\\nfor year ending in indicated month\\nQS-JAN, QS-FEB, ... QuarterBegin Quarterly dates anchored on first calendar day of each month,\\nfor year ending in indicated month\\nBQS-JAN, BQS-FEB, ... BusinessQuarterBegin Quarterly dates anchored on first weekday day of each month,\\nfor year ending in indicated month\\nA-JAN, A-FEB, ... YearEnd Annual dates anchored on last calendar day of given month:\\nJAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, NOV, or DEC.\\nBA-JAN, BA-FEB, ... BusinessYearEnd Annual dates anchored on last weekday of given month\\nAS-JAN, AS-FEB, ... YearBegin Annual dates anchored on first day of given month\\nBAS-JAN, BAS-FEB, ... BusinessYearBegin Annual dates anchored on first weekday of given month\\n300 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Week of month dates\\nOne useful frequency class is “week of month”, starting with WOM. This enables you to\\nget dates like the third Friday of each month:\\nIn [397]: rng = pd.date_range('1/1/2012', '9/1/2012', freq='WOM-3FRI')\\nIn [398]: list(rng)\\nOut[398]: \\n[<Timestamp: 2012-01-20 00:00:00>,\\n <Timestamp: 2012-02-17 00:00:00>,\\n <Timestamp: 2012-03-16 00:00:00>,\\n <Timestamp: 2012-04-20 00:00:00>,\\n <Timestamp: 2012-05-18 00:00:00>,\\n <Timestamp: 2012-06-15 00:00:00>,\\n <Timestamp: 2012-07-20 00:00:00>,\\n <Timestamp: 2012-08-17 00:00:00>]\\nTraders of US equity options will recognize these dates as the standard dates of monthly\\nexpiry.\\nShifting (Leading and Lagging) Data\\n“Shifting” refers to moving data backward and forward through time. Both Series and\\nDataFrame have a shift method for doing naive shifts forward or backward, leaving\\nthe index unmodified:\\nIn [399]: ts = Series(np.random.randn(4),\\n   .....:             index=pd.date_range('1/1/2000', periods=4, freq='M'))\\nIn [400]: ts                In [401]: ts.shift(2)       In [402]: ts.shift(-2)\\nOut[400]:                   Out[401]:                   Out[402]:             \\n2000-01-31    0.575283      2000-01-31         NaN      2000-01-31    1.814582\\n2000-02-29    0.304205      2000-02-29         NaN      2000-02-29    1.634858\\n2000-03-31    1.814582      2000-03-31    0.575283      2000-03-31         NaN\\n2000-04-30    1.634858      2000-04-30    0.304205      2000-04-30         NaN\\nFreq: M                     Freq: M                     Freq: M\\nA common use of shift is computing percent changes in a time series or multiple time\\nseries as DataFrame columns. This is expressed as\\nts / ts.shift(1) - 1\\nBecause naive shifts leave the index unmodified, some data is discarded. Thus if the\\nfrequency is known, it can be passed to shift to advance the timestamps instead of\\nsimply the data:\\nIn [403]: ts.shift(2, freq='M')\\nOut[403]: \\n2000-03-31    0.575283\\n2000-04-30    0.304205\\n2000-05-31    1.814582\\n2000-06-30    1.634858\\nFreq: M\\nDate Ranges, Frequencies, and Shifting | 301\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Other frequencies can be passed, too, giving you a lot of flexibility in how to lead and\\nlag the data:\\nIn [404]: ts.shift(3, freq='D')        In [405]: ts.shift(1, freq='3D')\\nOut[404]:                              Out[405]:                       \\n2000-02-03    0.575283                 2000-02-03    0.575283          \\n2000-03-03    0.304205                 2000-03-03    0.304205          \\n2000-04-03    1.814582                 2000-04-03    1.814582          \\n2000-05-03    1.634858                 2000-05-03    1.634858          \\n                                                                       \\nIn [406]: ts.shift(1, freq='90T')\\nOut[406]: \\n2000-01-31 01:30:00    0.575283\\n2000-02-29 01:30:00    0.304205\\n2000-03-31 01:30:00    1.814582\\n2000-04-30 01:30:00    1.634858\\nShifting dates with offsets\\nThe pandas date offsets can also be used with datetime or Timestamp objects:\\nIn [407]: from pandas.tseries.offsets import Day, MonthEnd\\nIn [408]: now = datetime(2011, 11, 17)\\nIn [409]: now + 3 * Day()\\nOut[409]: datetime.datetime(2011, 11, 20, 0, 0)\\nIf you add an anchored offset like MonthEnd, the first increment will roll forward a date\\nto the next date according to the frequency rule:\\nIn [410]: now + MonthEnd()\\nOut[410]: datetime.datetime(2011, 11, 30, 0, 0)\\nIn [411]: now + MonthEnd(2)\\nOut[411]: datetime.datetime(2011, 12, 31, 0, 0)\\nAnchored offsets can explicitly “roll” dates forward or backward using their rollfor\\nward and rollback methods, respectively:\\nIn [412]: offset = MonthEnd()\\nIn [413]: offset.rollforward(now)\\nOut[413]: datetime.datetime(2011, 11, 30, 0, 0)\\nIn [414]: offset.rollback(now)\\nOut[414]: datetime.datetime(2011, 10, 31, 0, 0)\\nA clever use of date offsets is to use these methods with groupby:\\nIn [415]: ts = Series(np.random.randn(20),\\n   .....:             index=pd.date_range('1/15/2000', periods=20, freq='4d'))\\nIn [416]: ts.groupby(offset.rollforward).mean()\\nOut[416]: \\n2000-01-31   -0.448874\\n302 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"2000-02-29   -0.683663\\n2000-03-31    0.251920\\nOf course, an easier and faster way to do this is using resample (much more on this later):\\nIn [417]: ts.resample('M', how='mean')\\nOut[417]: \\n2000-01-31   -0.448874\\n2000-02-29   -0.683663\\n2000-03-31    0.251920\\nFreq: M\\nTime Zone Handling\\nWorking with time zones is generally considered one of the most unpleasant parts of\\ntime series manipulation. In particular, daylight savings time (DST) transitions are a\\ncommon source of complication. As such, many time series users choose to work with\\ntime series in coordinated universal time or UTC, which is the successor to Greenwich\\nMean Time and is the current international standard. Time zones are expressed as\\noffsets from UTC; for example, New York is four hours behind UTC during daylight\\nsavings time and 5 hours the rest of the year.\\nIn Python, time zone information comes from the 3rd party pytz library, which exposes\\nthe Olson database, a compilation of world time zone information. This is especially\\nimportant for historical data because the DST transition dates (and even UTC offsets)\\nhave been changed numerous times depending on the whims of local governments. In\\nthe United States,the DST transition times have been changed many times since 1900!\\nFor detailed information about pytz library, you’ll need to look at that library’s docu-\\nmentation. As far as this book is concerned, pandas wraps pytz’s functionality so you\\ncan ignore its API outside of the time zone names. Time zone names can be found\\ninteractively and in the docs:\\nIn [418]: import pytz\\nIn [419]: pytz.common_timezones[-5:]\\nOut[419]: ['US/Eastern', 'US/Hawaii', 'US/Mountain', 'US/Pacific', 'UTC']\\nTo get a time zone object from pytz, use pytz.timezone:\\nIn [420]: tz = pytz.timezone('US/Eastern')\\nIn [421]: tz\\nOut[421]: <DstTzInfo 'US/Eastern' EST-1 day, 19:00:00 STD>\\nMethods in pandas will accept either time zone names or these objects. I recommend\\njust using the names.\\nTime Zone Handling | 303\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Localization and Conversion\\nBy default, time series in pandas are time zone naive. Consider the following time series:\\nrng = pd.date_range('3/9/2012 9:30', periods=6, freq='D')\\nts = Series(np.random.randn(len(rng)), index=rng)\\nThe index’s tz field is None:\\nIn [423]: print(ts.index.tz)\\nNone\\nDate ranges can be generated with a time zone set:\\nIn [424]: pd.date_range('3/9/2012 9:30', periods=10, freq='D', tz='UTC')\\nOut[424]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n[2012-03-09 09:30:00, ..., 2012-03-18 09:30:00]\\nLength: 10, Freq: D, Timezone: UTC\\nConversion from naive to localized is handled by the tz_localize method:\\nIn [425]: ts_utc = ts.tz_localize('UTC')\\nIn [426]: ts_utc\\nOut[426]: \\n2012-03-09 09:30:00+00:00    0.414615\\n2012-03-10 09:30:00+00:00    0.427185\\n2012-03-11 09:30:00+00:00    1.172557\\n2012-03-12 09:30:00+00:00   -0.351572\\n2012-03-13 09:30:00+00:00    1.454593\\n2012-03-14 09:30:00+00:00    2.043319\\nFreq: D\\nIn [427]: ts_utc.index\\nOut[427]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n[2012-03-09 09:30:00, ..., 2012-03-14 09:30:00]\\nLength: 6, Freq: D, Timezone: UTC\\nOnce a time series has been localized to a particular time zone, it can be converted to\\nanother time zone using tz_convert:\\nIn [428]: ts_utc.tz_convert('US/Eastern')\\nOut[428]: \\n2012-03-09 04:30:00-05:00    0.414615\\n2012-03-10 04:30:00-05:00    0.427185\\n2012-03-11 05:30:00-04:00    1.172557\\n2012-03-12 05:30:00-04:00   -0.351572\\n2012-03-13 05:30:00-04:00    1.454593\\n2012-03-14 05:30:00-04:00    2.043319\\nFreq: D\\nIn the case of the above time series, which straddles a DST transition in the US/Eastern\\ntime zone, we could localize to EST and convert to, say, UTC or Berlin time:\\nIn [429]: ts_eastern = ts.tz_localize('US/Eastern')\\n304 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [430]: ts_eastern.tz_convert('UTC')\\nOut[430]: \\n2012-03-09 14:30:00+00:00    0.414615\\n2012-03-10 14:30:00+00:00    0.427185\\n2012-03-11 13:30:00+00:00    1.172557\\n2012-03-12 13:30:00+00:00   -0.351572\\n2012-03-13 13:30:00+00:00    1.454593\\n2012-03-14 13:30:00+00:00    2.043319\\nFreq: D\\nIn [431]: ts_eastern.tz_convert('Europe/Berlin')\\nOut[431]: \\n2012-03-09 15:30:00+01:00    0.414615\\n2012-03-10 15:30:00+01:00    0.427185\\n2012-03-11 14:30:00+01:00    1.172557\\n2012-03-12 14:30:00+01:00   -0.351572\\n2012-03-13 14:30:00+01:00    1.454593\\n2012-03-14 14:30:00+01:00    2.043319\\nFreq: D\\ntz_localize and tz_convert are also instance methods on DatetimeIndex:\\nIn [432]: ts.index.tz_localize('Asia/Shanghai')\\nOut[432]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n[2012-03-09 09:30:00, ..., 2012-03-14 09:30:00]\\nLength: 6, Freq: D, Timezone: Asia/Shanghai\\nLocalizing naive timestamps also checks for ambiguous or non-existent\\ntimes around daylight savings time transitions.\\nOperations with Time Zone−aware Timestamp Objects\\nSimilar to time series and date ranges, individual Timestamp objects similarly can be\\nlocalized from naive to time zone-aware and converted from one time zone to another:\\nIn [433]: stamp = pd.Timestamp('2011-03-12 04:00')\\nIn [434]: stamp_utc = stamp.tz_localize('utc')\\nIn [435]: stamp_utc.tz_convert('US/Eastern')\\nOut[435]: <Timestamp: 2011-03-11 23:00:00-0500 EST, tz=US/Eastern>\\nYou can also pass a time zone when creating the Timestamp:\\nIn [436]: stamp_moscow = pd.Timestamp('2011-03-12 04:00', tz='Europe/Moscow')\\nIn [437]: stamp_moscow\\nOut[437]: <Timestamp: 2011-03-12 04:00:00+0300 MSK, tz=Europe/Moscow>\\nTime zone-aware Timestamp objects internally store a UTC timestamp value as nano-\\nseconds since the UNIX epoch (January 1, 1970); this UTC value is invariant between\\ntime zone conversions:\\nTime Zone Handling | 305\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [438]: stamp_utc.value\\nOut[438]: 1299902400000000000\\nIn [439]: stamp_utc.tz_convert('US/Eastern').value\\nOut[439]: 1299902400000000000\\nWhen performing time arithmetic using pandas’s DateOffset objects, daylight savings\\ntime transitions are respected where possible:\\n# 30 minutes before DST transition\\nIn [440]: from pandas.tseries.offsets import Hour\\nIn [441]: stamp = pd.Timestamp('2012-03-12 01:30', tz='US/Eastern')\\nIn [442]: stamp\\nOut[442]: <Timestamp: 2012-03-12 01:30:00-0400 EDT, tz=US/Eastern>\\nIn [443]: stamp + Hour()\\nOut[443]: <Timestamp: 2012-03-12 02:30:00-0400 EDT, tz=US/Eastern>\\n# 90 minutes before DST transition\\nIn [444]: stamp = pd.Timestamp('2012-11-04 00:30', tz='US/Eastern')\\nIn [445]: stamp\\nOut[445]: <Timestamp: 2012-11-04 00:30:00-0400 EDT, tz=US/Eastern>\\nIn [446]: stamp + 2 * Hour()\\nOut[446]: <Timestamp: 2012-11-04 01:30:00-0500 EST, tz=US/Eastern>\\nOperations between Different Time Zones\\nIf two time series with different time zones are combined, the result will be UTC. Since\\nthe timestamps are stored under the hood in UTC, this is a straightforward operation\\nand requires no conversion to happen:\\nIn [447]: rng = pd.date_range('3/7/2012 9:30', periods=10, freq='B')\\nIn [448]: ts = Series(np.random.randn(len(rng)), index=rng)\\nIn [449]: ts\\nOut[449]: \\n2012-03-07 09:30:00   -1.749309\\n2012-03-08 09:30:00   -0.387235\\n2012-03-09 09:30:00   -0.208074\\n2012-03-12 09:30:00   -1.221957\\n2012-03-13 09:30:00   -0.067460\\n2012-03-14 09:30:00    0.229005\\n2012-03-15 09:30:00   -0.576234\\n2012-03-16 09:30:00    0.816895\\n2012-03-19 09:30:00   -0.772192\\n2012-03-20 09:30:00   -1.333576\\nFreq: B\\nIn [450]: ts1 = ts[:7].tz_localize('Europe/London')\\n306 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [451]: ts2 = ts1[2:].tz_convert('Europe/Moscow')\\nIn [452]: result = ts1 + ts2\\nIn [453]: result.index\\nOut[453]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n[2012-03-07 09:30:00, ..., 2012-03-15 09:30:00]\\nLength: 7, Freq: B, Timezone: UTC\\nPeriods and Period Arithmetic\\nPeriods represent time spans, like days, months, quarters, or years. The Period class\\nrepresents this data type, requiring a string or integer and a frequency from the above\\ntable:\\nIn [454]: p = pd.Period(2007, freq='A-DEC')\\nIn [455]: p\\nOut[455]: Period('2007', 'A-DEC')\\nIn this case, the Period object represents the full timespan from January 1, 2007 to\\nDecember 31, 2007, inclusive. Conveniently, adding and subtracting integers from pe-\\nriods has the effect of shifting by their frequency:\\nIn [456]: p + 5                          In [457]: p - 2                  \\nOut[456]: Period('2012', 'A-DEC')        Out[457]: Period('2005', 'A-DEC')\\nIf two periods have the same frequency, their difference is the number of units between\\nthem:\\nIn [458]: pd.Period('2014', freq='A-DEC') - p\\nOut[458]: 7\\nRegular ranges of periods can be constructed using the period_range function:\\nIn [459]: rng = pd.period_range('1/1/2000', '6/30/2000', freq='M')\\nIn [460]: rng\\nOut[460]: \\n<class 'pandas.tseries.period.PeriodIndex'>\\nfreq: M\\n[2000-01, ..., 2000-06]\\nlength: 6\\nThe PeriodIndex class stores a sequence of periods and can serve as an axis index in\\nany pandas data structure:\\nIn [461]: Series(np.random.randn(6), index=rng)\\nOut[461]: \\n2000-01   -0.309119\\n2000-02    0.028558\\n2000-03    1.129605\\n2000-04   -0.374173\\n2000-05   -0.011401\\nPeriods and Period Arithmetic | 307\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"2000-06    0.272924\\nFreq: M\\nIf you have an array of strings, you can also appeal to the PeriodIndex class itself:\\nIn [462]: values = ['2001Q3', '2002Q2', '2003Q1']\\nIn [463]: index = pd.PeriodIndex(values, freq='Q-DEC')\\nIn [464]: index\\nOut[464]: \\n<class 'pandas.tseries.period.PeriodIndex'>\\nfreq: Q-DEC\\n[2001Q3, ..., 2003Q1]\\nlength: 3\\nPeriod Frequency Conversion\\nPeriods and PeriodIndex objects can be converted to another frequency using their \\nasfreq method. As an example, suppose we had an annual period and wanted to convert\\nit into a monthly period either at the start or end of the year. This is fairly straightfor-\\nward:\\nIn [465]: p = pd.Period('2007', freq='A-DEC')\\nIn [466]: p.asfreq('M', how='start')      In [467]: p.asfreq('M', how='end')\\nOut[466]: Period('2007-01', 'M')         Out[467]: Period('2007-12', 'M')\\nYou can think of Period('2007', 'A-DEC') as being a cursor pointing to a span of time,\\nsubdivided by monthly periods. See Figure 10-1 for an illustration of this. For a fiscal\\nyear ending on a month other than December, the monthly subperiods belonging are\\ndifferent:\\nIn [468]: p = pd.Period('2007', freq='A-JUN')\\nIn [469]: p.asfreq('M', 'start')       In [470]: p.asfreq('M', 'end')   \\nOut[469]: Period('2006-07', 'M')      Out[470]: Period('2007-07', 'M')\\nWhen converting from high to low frequency, the superperiod will be determined de-\\npending on where the subperiod “belongs”. For example, in A-JUN frequency, the month\\nAug-2007 is actually part of the 2008 period:\\nIn [471]: p = pd.Period('2007-08', 'M')\\nIn [472]: p.asfreq('A-JUN')\\nOut[472]: Period('2008', 'A-JUN')\\nWhole PeriodIndex objects or TimeSeries can be similarly converted with the same\\nsemantics:\\nIn [473]: rng = pd.period_range('2006', '2009', freq='A-DEC')\\nIn [474]: ts = Series(np.random.randn(len(rng)), index=rng)\\nIn [475]: ts\\n308 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Out[475]: \\n2006   -0.601544\\n2007    0.574265\\n2008   -0.194115\\n2009    0.202225\\nFreq: A-DEC\\nIn [476]: ts.asfreq('M', how='start')      In [477]: ts.asfreq('B', how='end')\\nOut[476]:                                  Out[477]:                          \\n2006-01   -0.601544                        2006-12-29   -0.601544            \\n2007-01    0.574265                        2007-12-31    0.574265            \\n2008-01   -0.194115                        2008-12-31   -0.194115            \\n2009-01    0.202225                        2009-12-31    0.202225            \\nFreq: M                                    Freq: B\\nFigure 10-1. Period frequency conversion illustration\\nQuarterly Period Frequencies\\nQuarterly data is standard in accounting, finance, and other fields. Much quarterly data\\nis reported relative to a fiscal year end, typically the last calendar or business day of one\\nof the 12 months of the year. As such, the period 2012Q4 has a different meaning de-\\npending on fiscal year end. pandas supports all 12 possible quarterly frequencies as Q-\\nJAN through Q-DEC:\\nIn [478]: p = pd.Period('2012Q4', freq='Q-JAN')\\nIn [479]: p\\nOut[479]: Period('2012Q4', 'Q-JAN')\\nIn the case of fiscal year ending in January, 2012Q4 runs from November through Jan-\\nuary, which you can check by converting to daily frequency. See Figure 10-2 for an\\nillustration:\\nIn [480]: p.asfreq('D', 'start')          In [481]: p.asfreq('D', 'end')      \\nOut[480]: Period('2011-11-01', 'D')      Out[481]: Period('2012-01-31', 'D')\\nPeriods and Period Arithmetic | 309\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Thus, it’s possible to do period arithmetic very easily; for example, to get the timestamp\\nat 4PM on the 2nd to last business day of the quarter, you could do:\\nIn [482]: p4pm = (p.asfreq('B', 'e') - 1).asfreq('T', 's') + 16 * 60\\nIn [483]: p4pm\\nOut[483]: Period('2012-01-30 16:00', 'T')\\nIn [484]: p4pm.to_timestamp()\\nOut[484]: <Timestamp: 2012-01-30 16:00:00>\\nFigure 10-2. Different quarterly frequency conventions\\nGenerating quarterly ranges works as you would expect using period_range. Arithmetic\\nis identical, too:\\nIn [485]: rng = pd.period_range('2011Q3', '2012Q4', freq='Q-JAN')\\nIn [486]: ts = Series(np.arange(len(rng)), index=rng)\\nIn [487]: ts\\nOut[487]: \\n2011Q3    0\\n2011Q4    1\\n2012Q1    2\\n2012Q2    3\\n2012Q3    4\\n2012Q4    5\\nFreq: Q-JAN\\nIn [488]: new_rng = (rng.asfreq('B', 'e') - 1).asfreq('T', 's') + 16 * 60\\nIn [489]: ts.index = new_rng.to_timestamp()\\nIn [490]: ts\\nOut[490]: \\n2010-10-28 16:00:00    0\\n2011-01-28 16:00:00    1\\n2011-04-28 16:00:00    2\\n2011-07-28 16:00:00    3\\n2011-10-28 16:00:00    4\\n2012-01-30 16:00:00    5\\n310 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Converting Timestamps to Periods (and Back)\\nSeries and DataFrame objects indexed by timestamps can be converted to periods using\\nthe to_period method:\\nIn [491]: rng = pd.date_range('1/1/2000', periods=3, freq='M')\\nIn [492]: ts = Series(randn(3), index=rng)\\nIn [493]: pts = ts.to_period()\\nIn [494]: ts                  In [495]: pts       \\nOut[494]:                     Out[495]:           \\n2000-01-31   -0.505124        2000-01   -0.505124\\n2000-02-29    2.954439        2000-02    2.954439\\n2000-03-31   -2.630247        2000-03   -2.630247\\nFreq: M                       Freq: M\\nSince periods always refer to non-overlapping timespans, a timestamp can only belong\\nto a single period for a given frequency. While the frequency of the new PeriodIndex is\\ninferred from the timestamps by default, you can specify any frequency you want. There\\nis also no problem with having duplicate periods in the result:\\nIn [496]: rng = pd.date_range('1/29/2000', periods=6, freq='D')\\nIn [497]: ts2 = Series(randn(6), index=rng)\\nIn [498]: ts2.to_period('M')\\nOut[498]: \\n2000-01   -0.352453\\n2000-01   -0.477808\\n2000-01    0.161594\\n2000-02    1.686833\\n2000-02    0.821965\\n2000-02   -0.667406\\nFreq: M\\nTo convert back to timestamps, use to_timestamp:\\nIn [499]: pts = ts.to_period()\\nIn [500]: pts\\nOut[500]: \\n2000-01   -0.505124\\n2000-02    2.954439\\n2000-03   -2.630247\\nFreq: M\\nIn [501]: pts.to_timestamp(how='end')\\nOut[501]: \\n2000-01-31   -0.505124\\n2000-02-29    2.954439\\n2000-03-31   -2.630247\\nFreq: M\\nPeriods and Period Arithmetic | 311\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Creating a PeriodIndex from Arrays\\nFixed frequency data sets are sometimes stored with timespan information spread\\nacross multiple columns. For example, in this macroeconomic data set, the year and\\nquarter are in different columns:\\nIn [502]: data = pd.read_csv('ch08/macrodata.csv')\\nIn [503]: data.year            In [504]: data.quarter    \\nOut[503]:                      Out[504]:                 \\n0    1959                      0    1                    \\n1    1959                      1    2                    \\n2    1959                      2    3                    \\n3    1959                      3    4                    \\n...                            ...                       \\n199    2008                    199    4                  \\n200    2009                    200    1                  \\n201    2009                    201    2                  \\n202    2009                    202    3                  \\nName: year, Length: 203        Name: quarter, Length: 203\\nBy passing these arrays to PeriodIndex with a frequency, they can be combined to form\\nan index for the DataFrame:\\nIn [505]: index = pd.PeriodIndex(year=data.year, quarter=data.quarter, freq='Q-DEC')\\nIn [506]: index\\nOut[506]: \\n<class 'pandas.tseries.period.PeriodIndex'>\\nfreq: Q-DEC\\n[1959Q1, ..., 2009Q3]\\nlength: 203\\nIn [507]: data.index = index\\nIn [508]: data.infl\\nOut[508]: \\n1959Q1    0.00\\n1959Q2    2.34\\n1959Q3    2.74\\n1959Q4    0.27\\n...\\n2008Q4   -8.79\\n2009Q1    0.94\\n2009Q2    3.37\\n2009Q3    3.56\\nFreq: Q-DEC, Name: infl, Length: 203\\nResampling and Frequency Conversion\\nResampling refers to the process of converting a time series from one frequency to\\nanother. Aggregating higher frequency data to lower frequency is called downsam-\\npling, while converting lower frequency to higher frequency is called upsampling. Not\\n312 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"all resampling falls into either of these categories; for example, converting W-WED (weekly\\non Wednesday) to W-FRI is neither upsampling nor downstampling.\\npandas objects are equipped with a resample method, which is the workhorse function\\nfor all frequency conversion:\\nIn [509]: rng = pd.date_range('1/1/2000', periods=100, freq='D')\\nIn [510]: ts = Series(randn(len(rng)), index=rng)\\nIn [511]: ts.resample('M', how='mean')\\nOut[511]: \\n2000-01-31    0.170876\\n2000-02-29    0.165020\\n2000-03-31    0.095451\\n2000-04-30    0.363566\\nFreq: M\\nIn [512]: ts.resample('M', how='mean', kind='period')\\nOut[512]: \\n2000-01    0.170876\\n2000-02    0.165020\\n2000-03    0.095451\\n2000-04    0.363566\\nFreq: M\\nresample is a flexible and high-performance method that can be used to process very\\nlarge time series. I’ll illustrate its semantics and use through a series of examples.\\nTable 10-5. Resample method arguments\\nArgument Description\\nfreq String or DateOffset indicating desired resampled frequency, e.g. ‘M', ’5min', or Sec\\nond(15)\\nhow='mean' Function name or array function producing aggregated value, for example 'mean',\\n'ohlc', np.max. Defaults to 'mean'. Other common values: 'first', 'last',\\n'median', 'ohlc', 'max', 'min'.\\naxis=0 Axis to resample on, default axis=0\\nfill_method=None How to interpolate when upsampling, as in 'ffill' or 'bfill'. By default does no\\ninterpolation.\\nclosed='right' In downsampling, which end of each interval is closed (inclusive), 'right' or\\n'left'. Defaults to 'right'\\nlabel='right' In downsampling, how to label the aggregated result, with the 'right' or 'left'\\nbin edge. For example, the 9:30 to 9:35 5-minute interval could be labeled 9:30 or\\n9:35. Defaults to 'right' (or 9:35, in this example).\\nloffset=None Time adjustment to the bin labels, such as '-1s' / Second(-1) to shift the aggregate\\nlabels one second earlier\\nlimit=None When forward or backward filling, the maximum number of periods to fill\\nResampling and Frequency Conversion | 313\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Argument Description\\nkind=None Aggregate to periods ('period') or timestamps ('timestamp'); defaults to kind of\\nindex the time series has\\nconvention=None When resampling periods, the convention ('start' or 'end') for converting the low\\nfrequency period to high frequency. Defaults to 'end'\\nDownsampling\\nAggregating data to a regular, lower frequency is a pretty normal time series task. The\\ndata you’re aggregating doesn’t need to be fixed frequently; the desired frequency de-\\nfines bin edges that are used to slice the time series into pieces to aggregate. For example,\\nto convert to monthly, 'M' or 'BM', the data need to be chopped up into one month\\nintervals. Each interval is said to be half-open; a data point can only belong to one\\ninterval, and the union of the intervals must make up the whole time frame. There are\\na couple things to think about when using resample to downsample data:\\n• Which side of each interval is closed\\n• How to label each aggregated bin, either with the start of the interval or the end\\nTo illustrate, let’s look at some one-minute data:\\nIn [513]: rng = pd.date_range('1/1/2000', periods=12, freq='T')\\nIn [514]: ts = Series(np.arange(12), index=rng)\\nIn [515]: ts\\nOut[515]: \\n2000-01-01 00:00:00     0\\n2000-01-01 00:01:00     1\\n2000-01-01 00:02:00     2\\n2000-01-01 00:03:00     3\\n2000-01-01 00:04:00     4\\n2000-01-01 00:05:00     5\\n2000-01-01 00:06:00     6\\n2000-01-01 00:07:00     7\\n2000-01-01 00:08:00     8\\n2000-01-01 00:09:00     9\\n2000-01-01 00:10:00    10\\n2000-01-01 00:11:00    11\\nFreq: T\\nSuppose you wanted to aggregate this data into five-minute chunks or bars by taking\\nthe sum of each group:\\nIn [516]: ts.resample('5min', how='sum')\\nOut[516]: \\n2000-01-01 00:00:00     0\\n2000-01-01 00:05:00    15\\n2000-01-01 00:10:00    40\\n2000-01-01 00:15:00    11\\nFreq: 5T\\n314 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"The frequency you pass defines bin edges in five-minute increments. By default, the\\nright bin edge is inclusive, so the 00:05 value is included in the 00:00 to 00:05 inter-\\nval.1 Passing closed='left' changes the interval to be closed on the left:\\nIn [517]: ts.resample('5min', how='sum', closed='left')\\nOut[517]: \\n2000-01-01 00:05:00    10\\n2000-01-01 00:10:00    35\\n2000-01-01 00:15:00    21\\nFreq: 5T\\nAs you can see, the resulting time series is labeled by the timestamps from the right side\\nof each bin. By passing label='left' you can label them with the left bin edge:\\nIn [518]: ts.resample('5min', how='sum', closed='left', label='left')\\nOut[518]: \\n2000-01-01 00:00:00    10\\n2000-01-01 00:05:00    35\\n2000-01-01 00:10:00    21\\nFreq: 5T\\nSee Figure 10-3 for an illustration of minutely data being resampled to five-minute.\\nFigure 10-3. 5-minute resampling illustration of closed, label conventions\\nLastly, you might want to shift the result index by some amount, say subtracting one\\nsecond from the right edge to make it more clear which interval the timestamp refers\\nto. To do this, pass a string or date offset to loffset:\\nIn [519]: ts.resample('5min', how='sum', loffset='-1s')\\nOut[519]: \\n1999-12-31 23:59:59     0\\n2000-01-01 00:04:59    15\\n2000-01-01 00:09:59    40\\n2000-01-01 00:14:59    11\\nFreq: 5T\\n1. The choice of closed='right', label='right' as the default might seem a bit odd to some users. In\\npractice the choice is somewhat arbitrary; for some target frequencies, closed='left' is preferable, while\\nfor others closed='right' makes more sense. The important thing is that you keep in mind exactly how\\nyou are segmenting the data.\\nResampling and Frequency Conversion | 315\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"This also could have been accomplished by calling the shift method on the result\\nwithout the loffset.\\nOpen-High-Low-Close (OHLC) resampling\\nIn finance, an ubiquitous way to aggregate a time series is to compute four values for\\neach bucket: the first (open), last (close), maximum (high), and minimal (low) values.\\nBy passing how='ohlc' you will obtain a DataFrame having columns containing these\\nfour aggregates, which are efficiently computed in a single sweep of the data:\\nIn [520]: ts.resample('5min', how='ohlc')\\nOut[520]: \\n                     open  high  low  close\\n2000-01-01 00:00:00     0     0    0      0\\n2000-01-01 00:05:00     1     5    1      5\\n2000-01-01 00:10:00     6    10    6     10\\n2000-01-01 00:15:00    11    11   11     11\\nResampling with GroupBy\\nAn alternate way to downsample is to use pandas’s groupby functionality. For example,\\nyou can group by month or weekday by passing a function that accesses those fields\\non the time series’s index:\\nIn [521]: rng = pd.date_range('1/1/2000', periods=100, freq='D')\\nIn [522]: ts = Series(np.arange(100), index=rng)\\nIn [523]: ts.groupby(lambda x: x.month).mean()\\nOut[523]: \\n1    15\\n2    45\\n3    75\\n4    95\\nIn [524]: ts.groupby(lambda x: x.weekday).mean()\\nOut[524]: \\n0    47.5\\n1    48.5\\n2    49.5\\n3    50.5\\n4    51.5\\n5    49.0\\n6    50.0\\nUpsampling and Interpolation\\nWhen converting from a low frequency to a higher frequency, no aggregation is needed.\\nLet’s consider a DataFrame with some weekly data:\\nIn [525]: frame = DataFrame(np.random.randn(2, 4),\\n   .....:                   index=pd.date_range('1/1/2000', periods=2, freq='W-WED'),\\n   .....:                   columns=['Colorado', 'Texas', 'New York', 'Ohio'])\\n316 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [526]: frame[:5]\\nOut[526]: \\n            Colorado     Texas  New York     Ohio\\n2000-01-05 -0.609657 -0.268837  0.195592  0.85979\\n2000-01-12 -0.263206  1.141350 -0.101937 -0.07666\\nWhen resampling this to daily frequency, by default missing values are introduced:\\nIn [527]: df_daily = frame.resample('D')\\nIn [528]: df_daily\\nOut[528]: \\n            Colorado     Texas  New York     Ohio\\n2000-01-05 -0.609657 -0.268837  0.195592  0.85979\\n2000-01-06       NaN       NaN       NaN      NaN\\n2000-01-07       NaN       NaN       NaN      NaN\\n2000-01-08       NaN       NaN       NaN      NaN\\n2000-01-09       NaN       NaN       NaN      NaN\\n2000-01-10       NaN       NaN       NaN      NaN\\n2000-01-11       NaN       NaN       NaN      NaN\\n2000-01-12 -0.263206  1.141350 -0.101937 -0.07666\\nSuppose you wanted to fill forward each weekly value on the non-Wednesdays. The\\nsame filling or interpolation methods available in the fillna and reindex methods are\\navailable for resampling:\\nIn [529]: frame.resample('D', fill_method='ffill')\\nOut[529]: \\n            Colorado     Texas  New York     Ohio\\n2000-01-05 -0.609657 -0.268837  0.195592  0.85979\\n2000-01-06 -0.609657 -0.268837  0.195592  0.85979\\n2000-01-07 -0.609657 -0.268837  0.195592  0.85979\\n2000-01-08 -0.609657 -0.268837  0.195592  0.85979\\n2000-01-09 -0.609657 -0.268837  0.195592  0.85979\\n2000-01-10 -0.609657 -0.268837  0.195592  0.85979\\n2000-01-11 -0.609657 -0.268837  0.195592  0.85979\\n2000-01-12 -0.263206  1.141350 -0.101937 -0.07666\\nYou can similarly choose to only fill a certain number of periods forward to limit how\\nfar to continue using an observed value:\\nIn [530]: frame.resample('D', fill_method='ffill', limit=2)\\nOut[530]: \\n            Colorado     Texas  New York     Ohio\\n2000-01-05 -0.609657 -0.268837  0.195592  0.85979\\n2000-01-06 -0.609657 -0.268837  0.195592  0.85979\\n2000-01-07 -0.609657 -0.268837  0.195592  0.85979\\n2000-01-08       NaN       NaN       NaN      NaN\\n2000-01-09       NaN       NaN       NaN      NaN\\n2000-01-10       NaN       NaN       NaN      NaN\\n2000-01-11       NaN       NaN       NaN      NaN\\n2000-01-12 -0.263206  1.141350 -0.101937 -0.07666\\nNotably, the new date index need not overlap with the old one at all:\\nResampling and Frequency Conversion | 317\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [531]: frame.resample('W-THU', fill_method='ffill')\\nOut[531]: \\n            Colorado     Texas  New York     Ohio\\n2000-01-06 -0.609657 -0.268837  0.195592  0.85979\\n2000-01-13 -0.263206  1.141350 -0.101937 -0.07666\\nResampling with Periods\\nResampling data indexed by periods is reasonably straightforward and works as you\\nwould hope:\\nIn [532]: frame = DataFrame(np.random.randn(24, 4),\\n   .....:                   index=pd.period_range('1-2000', '12-2001', freq='M'),\\n   .....:                   columns=['Colorado', 'Texas', 'New York', 'Ohio'])\\nIn [533]: frame[:5]\\nOut[533]: \\n         Colorado     Texas  New York      Ohio\\n2000-01  0.120837  1.076607  0.434200  0.056432\\n2000-02 -0.378890  0.047831  0.341626  1.567920\\n2000-03 -0.047619 -0.821825 -0.179330 -0.166675\\n2000-04  0.333219 -0.544615 -0.653635 -2.311026\\n2000-05  1.612270 -0.806614  0.557884  0.580201\\nIn [534]: annual_frame = frame.resample('A-DEC', how='mean')\\nIn [535]: annual_frame\\nOut[535]: \\n      Colorado     Texas  New York      Ohio\\n2000  0.352070 -0.553642  0.196642 -0.094099\\n2001  0.158207  0.042967 -0.360755  0.184687\\nUpsampling is more nuanced as you must make a decision about which end of the\\ntimespan in the new frequency to place the values before resampling, just like the \\nasfreq method. The convention argument defaults to 'end' but can also be 'start':\\n# Q-DEC: Quarterly, year ending in December\\nIn [536]: annual_frame.resample('Q-DEC', fill_method='ffill')\\nOut[536]: \\n        Colorado     Texas  New York      Ohio\\n2000Q4  0.352070 -0.553642  0.196642 -0.094099\\n2001Q1  0.352070 -0.553642  0.196642 -0.094099\\n2001Q2  0.352070 -0.553642  0.196642 -0.094099\\n2001Q3  0.352070 -0.553642  0.196642 -0.094099\\n2001Q4  0.158207  0.042967 -0.360755  0.184687\\nIn [537]: annual_frame.resample('Q-DEC', fill_method='ffill', convention='start')\\nOut[537]: \\n        Colorado     Texas  New York      Ohio\\n2000Q1  0.352070 -0.553642  0.196642 -0.094099\\n2000Q2  0.352070 -0.553642  0.196642 -0.094099\\n2000Q3  0.352070 -0.553642  0.196642 -0.094099\\n2000Q4  0.352070 -0.553642  0.196642 -0.094099\\n2001Q1  0.158207  0.042967 -0.360755  0.184687\\n318 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Since periods refer to timespans, the rules about upsampling and downsampling are\\nmore rigid:\\n• In downsampling, the target frequency must be a subperiod of the source frequency.\\n• In upsampling, the target frequency must be a superperiod of the source frequency.\\nIf these rules are not satisfied, an exception will be raised. This mainly affects the quar-\\nterly, annual, and weekly frequencies; for example, the timespans defined by Q-MAR only\\nline up with A-MAR, A-JUN, A-SEP, and A-DEC:\\nIn [538]: annual_frame.resample('Q-MAR', fill_method='ffill')\\nOut[538]: \\n        Colorado     Texas  New York      Ohio\\n2001Q3  0.352070 -0.553642  0.196642 -0.094099\\n2001Q4  0.352070 -0.553642  0.196642 -0.094099\\n2002Q1  0.352070 -0.553642  0.196642 -0.094099\\n2002Q2  0.352070 -0.553642  0.196642 -0.094099\\n2002Q3  0.158207  0.042967 -0.360755  0.184687\\nTime Series Plotting\\nPlots with pandas time series have improved date formatting compared with matplotlib\\nout of the box. As an example, I downloaded some stock price data on a few common\\nUS stock from Yahoo! Finance:\\nIn [539]: close_px_all = pd.read_csv('ch09/stock_px.csv', parse_dates=True, index_col=0)\\nIn [540]: close_px = close_px_all[['AAPL', 'MSFT', 'XOM']]\\nIn [541]: close_px = close_px.resample('B', fill_method='ffill')\\nIn [542]: close_px\\nOut[542]: \\n<class 'pandas.core.frame.DataFrame'>\\nDatetimeIndex: 2292 entries, 2003-01-02 00:00:00 to 2011-10-14 00:00:00\\nFreq: B\\nData columns:\\nAAPL    2292  non-null values\\nMSFT    2292  non-null values\\nXOM     2292  non-null values\\ndtypes: float64(3)\\nCalling plot on one of the columns grenerates a simple plot, seen in Figure 10-4.\\nIn [544]: close_px['AAPL'].plot()\\nWhen called on a DataFrame, as you would expect, all of the time series are drawn on\\na single subplot with a legend indicating which is which. I’ll plot only the year 2009\\ndata so you can see how both months and years are formatted on the X axis; see\\nFigure 10-5.\\nIn [546]: close_px.ix['2009'].plot()\\nTime Series Plotting | 319\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [548]: close_px['AAPL'].ix['01-2011':'03-2011'].plot()\\nQuarterly frequency data is also more nicely formatted with quarterly markers, some-\\nthing that would be quite a bit more work to do by hand. See Figure 10-7.\\nIn [550]: appl_q = close_px['AAPL'].resample('Q-DEC', fill_method='ffill')\\nIn [551]: appl_q.ix['2009':].plot()\\nA last feature of time series plotting in pandas is that by right-clicking and dragging to\\nzoom in and out, the dates will be dynamically expanded or contracted and reformat-\\nting depending on the timespan contained in the plot view. This is of course only true\\nwhen using matplotlib in interactive mode.\\nMoving Window Functions\\nA common class of array transformations intended for time series operations are sta-\\ntistics and other functions evaluated over a sliding window or with exponentially de-\\nFigure 10-4. AAPL Daily Price\\nFigure 10-5. Stock Prices in 2009\\n320 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'caying weights. I call these moving window functions, even though it includes functions\\nwithout a fixed-length window like exponentially-weighted moving average. Like other\\nstatistical functions, these also automatically exclude missing data.\\nrolling_mean is one of the simplest such functions. It takes a TimeSeries or DataFrame\\nalong with a window (expressed as a number of periods):\\nIn [555]: close_px.AAPL.plot()\\nOut[555]: <matplotlib.axes.AxesSubplot at 0x1099b3990>\\nIn [556]: pd.rolling_mean(close_px.AAPL, 250).plot()\\nSee Figure 10-8 for the plot. By default functions like rolling_mean require the indicated\\nnumber of non-NA observations. This behavior can be changed to account for missing\\ndata and, in particular, the fact that you will have fewer than window periods of data at\\nthe beginning of the time series (see Figure 10-9):\\nFigure 10-6. Apple Daily Price in 1/2011-3/2011\\nFigure 10-7. Apple Quarterly Price 2009-2011\\nMoving Window Functions | 321\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'In [558]: appl_std250 = pd.rolling_std(close_px.AAPL, 250, min_periods=10)\\nIn [559]: appl_std250[5:12]\\nOut[559]: \\n2003-01-09         NaN\\n2003-01-10         NaN\\n2003-01-13         NaN\\n2003-01-14         NaN\\n2003-01-15    0.077496\\n2003-01-16    0.074760\\n2003-01-17    0.112368\\nFreq: B\\nIn [560]: appl_std250.plot()\\nFigure 10-8. Apple Price with 250-day MA\\nFigure 10-9. Apple 250-day daily return standard deviation\\nTo compute an expanding window mean, you can see that an expanding window is just\\na special case where the window is the length of the time series, but only one or more\\nperiods is required to compute a value:\\n322 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': '# Define expanding mean in terms of rolling_mean\\nIn [561]: expanding_mean = lambda x: rolling_mean(x, len(x), min_periods=1)\\nCalling rolling_mean and friends on a DataFrame applies the transformation to each\\ncolumn (see Figure 10-10):\\nIn [563]: pd.rolling_mean(close_px, 60).plot(logy=True)\\nFigure 10-10. Stocks Prices 60-day MA (log Y-axis)\\nSee Table 10-6 for a listing of related functions in pandas.\\nTable 10-6. Moving window and exponentially-weighted functions\\nFunction Description\\nrolling_count Returns number of non-NA observations in each trailing window.\\nrolling_sum Moving window sum.\\nrolling_mean Moving window mean.\\nrolling_median Moving window median.\\nrolling_var, rolling_std Moving window variance and standard deviation, respectively. Uses n - 1 denom-\\ninator.\\nrolling_skew, rolling_kurt Moving window skewness (3rd moment) and kurtosis (4th moment), respectively.\\nrolling_min, rolling_max Moving window minimum and maximum.\\nrolling_quantile Moving window score at percentile/sample quantile.\\nrolling_corr, rolling_cov Moving window correlation and covariance.\\nrolling_apply Apply generic array function over a moving window.\\newma Exponentially-weighted moving average.\\newmvar, ewmstd Exponentially-weighted moving variance and standard deviation.\\newmcorr, ewmcov Exponentially-weighted moving correlation and covariance.\\nMoving Window Functions | 323\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"bottleneck, a Python library by Keith Goodman, provides an alternate\\nimplementation of NaN-friendly moving window functions and may be\\nworth looking at depending on your application.\\nExponentially-weighted functions\\nAn alternative to using a static window size with equally-weighted observations is to\\nspecify a constant decay factor to give more weight to more recent observations. In\\nmathematical terms, if mat is the moving average result at time t and x is the time series\\nin question, each value in the result is computed as mat = a * mat - 1 + (a - 1) * x_t, where\\na is the decay factor. There are a couple of ways to specify the decay factor, a popular\\none is using a span, which makes the result comparable to a simple moving window\\nfunction with window size equal to the span.\\nSince an exponentially-weighted statistic places more weight on more recent observa-\\ntions, it “adapts” faster to changes compared with the equal-weighted version. Here’s\\nan example comparing a 60-day moving average of Apple’s stock price with an EW\\nmoving average with span=60 (see Figure 10-11):\\nfig, axes = plt.subplots(nrows=2, ncols=1, sharex=True, sharey=True,\\n                         figsize=(12, 7))\\naapl_px = close_px.AAPL['2005':'2009']\\nma60 = pd.rolling_mean(aapl_px, 60, min_periods=50)\\newma60 = pd.ewma(aapl_px, span=60)\\naapl_px.plot(style='k-', ax=axes[0])\\nma60.plot(style='k--', ax=axes[0])\\naapl_px.plot(style='k-', ax=axes[1])\\newma60.plot(style='k--', ax=axes[1])\\naxes[0].set_title('Simple MA')\\naxes[1].set_title('Exponentially-weighted MA')\\nBinary Moving Window Functions\\nSome statistical operators, like correlation and covariance, need to operate on two time\\nseries. As an example, financial analysts are often interested in a stock’s correlation to\\na benchmark index like the S&P 500. We can compute that by computing the percent\\nchanges and using rolling_corr (see Figure 10-12):\\nIn [570]: spx_rets = spx_px / spx_px.shift(1) - 1\\nIn [571]: returns = close_px.pct_change()\\nIn [572]: corr = pd.rolling_corr(returns.AAPL, spx_rets, 125, min_periods=100)\\nIn [573]: corr.plot()\\n324 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Suppose you wanted to compute the correlation of the S&P 500 index with many stocks\\nat once. Writing a loop and creating a new DataFrame would be easy but maybe get\\nrepetitive, so if you pass a TimeSeries and a DataFrame, a function like rolling_corr\\nwill compute the correlation of the TimeSeries (spx_rets in this case) with each column\\nin the DataFrame. See Figure 10-13 for the plot of the result:\\nIn [575]: corr = pd.rolling_corr(returns, spx_rets, 125, min_periods=100)\\nIn [576]: corr.plot()\\nFigure 10-11. Simple moving average versus exponentially-weighted\\nFigure 10-12. Six-month AAPL return correlation to S&P 500\\nMoving Window Functions | 325\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'User-Defined Moving Window Functions\\nThe rolling_apply function provides a means to apply an array function of your own\\ndevising over a moving window. The only requirement is that the function produce a\\nsingle value (a reduction) from each piece of the array. For example, while we can\\ncompute sample quantiles using rolling_quantile, we might be interested in the per-\\ncentile rank of a particular value over the sample. The scipy.stats.percentileof\\nscore function does just this:\\nIn [578]: from scipy.stats import percentileofscore\\nIn [579]: score_at_2percent = lambda x: percentileofscore(x, 0.02)\\nIn [580]: result = pd.rolling_apply(returns.AAPL, 250, score_at_2percent)\\nIn [581]: result.plot()\\nFigure 10-13. Six-month return correlations to S&P 500\\nFigure 10-14. Percentile rank of 2% AAPL return over 1 year window\\n326 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Performance and Memory Usage Notes\\nTimestamps and periods are represented as 64-bit integers using NumPy’s date\\ntime64 dtype. This means that for each data point, there is an associated 8 bytes of\\nmemory per timestamp. Thus, a time series with 1 million float64 data points has a\\nmemory footprint of approximately 16 megabytes. Since pandas makes every effort to\\nshare indexes among time series, creating views on existing time series do not cause\\nany more memory to be used. Additionally, indexes for lower frequencies (daily and\\nup) are stored in a central cache, so that any fixed-frequency index is a view on the date\\ncache. Thus, if you have a large collection of low-frequency time series, the memory\\nfootprint of the indexes will not be as significant.\\nPerformance-wise, pandas has been highly optimized for data alignment operations\\n(the behind-the-scenes work of differently indexed ts1 + ts2) and resampling. Here is\\nan example of aggregating 10MM data points to OHLC:\\nIn [582]: rng = pd.date_range('1/1/2000', periods=10000000, freq='10ms')\\nIn [583]: ts = Series(np.random.randn(len(rng)), index=rng)\\nIn [584]: ts\\nOut[584]: \\n2000-01-01 00:00:00          -1.402235\\n2000-01-01 00:00:00.010000    2.424667\\n2000-01-01 00:00:00.020000   -1.956042\\n2000-01-01 00:00:00.030000   -0.897339\\n...\\n2000-01-02 03:46:39.960000    0.495530\\n2000-01-02 03:46:39.970000    0.574766\\n2000-01-02 03:46:39.980000    1.348374\\n2000-01-02 03:46:39.990000    0.665034\\nFreq: 10L, Length: 10000000\\nIn [585]: ts.resample('15min', how='ohlc')\\nOut[585]: \\n<class 'pandas.core.frame.DataFrame'>\\nDatetimeIndex: 113 entries, 2000-01-01 00:00:00 to 2000-01-02 04:00:00\\nFreq: 15T\\nData columns:\\nopen     113  non-null values\\nhigh     113  non-null values\\nlow      113  non-null values\\nclose    113  non-null values\\ndtypes: float64(4)\\nIn [586]: %timeit ts.resample('15min', how='ohlc')\\n10 loops, best of 3: 61.1 ms per loop\\nThe runtime may depend slightly on the relative size of the aggregated result; higher\\nfrequency aggregates unsurprisingly take longer to compute:\\nIn [587]: rng = pd.date_range('1/1/2000', periods=10000000, freq='1s')\\nPerformance and Memory Usage Notes | 327\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [588]: ts = Series(np.random.randn(len(rng)), index=rng)\\nIn [589]: %timeit ts.resample('15s', how='ohlc')\\n1 loops, best of 3: 88.2 ms per loop\\nIt’s possible that by the time you read this, the performance of these algorithms may\\nbe even further improved. As an example, there are currently no optimizations for\\nconversions between regular frequencies, but that would be fairly straightforward to do.\\n328 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'CHAPTER 11\\nFinancial and Economic Data\\nApplications\\nThe use of Python in the financial industry has been increasing rapidly since 2005, led\\nlargely by the maturation of libraries (like NumPy and pandas) and the availability of\\nskilled Python programmers. Institutions have found that Python is well-suited both\\nas an interactive analysis environment as well as enabling robust systems to be devel-\\noped often in a fraction of the time it would have taken in Java or C++. Python is also\\nan ideal glue layer; it is easy to build Python interfaces to legacy libraries built in C or\\nC++.\\nWhile the field of financial analysis is broad enough to fill an entire book, I hope to\\nshow you how the tools in this book can be applied to a number of specific problems\\nin finance. As with other research and analysis domains, too much programming effort\\nis often spent wrangling data rather than solving the core modeling and research prob-\\nlems. I personally got started building pandas in 2008 while grappling with inadequate\\ndata tools.\\nIn these examples, I’ll use the term cross-section to refer to data at a fixed point in time.\\nFor example, the closing prices of all the stocks in the S&P 500 index on a particular\\ndate form a cross-section. Cross-sectional data at multiple points in time over multiple\\ndata items (for example, prices together with volume) form a panel. Panel data can\\neither be represented as a hierarchically-indexed DataFrame or using the three-dimen-\\nsional Panel pandas object.\\nData Munging Topics\\nMany helpful data munging tools for financial applications are spread across the earlier\\nchapters. Here I’ll highlight a number of topics as they relate to this problem domain.\\n329\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Time Series and Cross-Section Alignment\\nOne of the most time-consuming issues in working with financial data is the so-called\\ndata alignment problem. Two related time series may have indexes that don’t line up\\nperfectly, or two DataFrame objects might have columns or row labels that don’t match.\\nUsers of MATLAB, R, and other matrix-programming languages often invest significant\\neffort in wrangling data into perfectly aligned forms. In my experience, having to align\\ndata by hand (and worse, having to verify that data is aligned) is a far too rigid and\\ntedious way to work. It is also rife with potential for bugs due to combining misaligned\\ndata.\\npandas take an alternate approach by automatically aligning data in arithmetic opera-\\ntions. In practice, this grants immense freedom and enhances your productivity. As an\\nexample, let’s consider a couple of DataFrames containing time series of stock prices\\nand volume:\\nIn [16]: prices\\nOut[16]: \\n              AAPL    JNJ      SPX    XOM\\n2011-09-06  379.74  64.64  1165.24  71.15\\n2011-09-07  383.93  65.43  1198.62  73.65\\n2011-09-08  384.14  64.95  1185.90  72.82\\n2011-09-09  377.48  63.64  1154.23  71.01\\n2011-09-12  379.94  63.59  1162.27  71.84\\n2011-09-13  384.62  63.61  1172.87  71.65\\n2011-09-14  389.30  63.73  1188.68  72.64\\nIn [17]: volume\\nOut[17]: \\n                AAPL       JNJ       XOM\\n2011-09-06  18173500  15848300  25416300\\n2011-09-07  12492000  10759700  23108400\\n2011-09-08  14839800  15551500  22434800\\n2011-09-09  20171900  17008200  27969100\\n2011-09-12  16697300  13448200  26205800\\nSuppose you wanted to compute a volume-weighted average price using all available\\ndata (and making the simplifying assumption that the volume data is a subset of the\\nprice data). Since pandas aligns the data automatically in arithmetic and excludes\\nmissing data in functions like sum, we can express this concisely as:\\nIn [18]: prices * volume\\nOut[18]: \\n                  AAPL         JNJ  SPX         XOM\\n2011-09-06  6901204890  1024434112  NaN  1808369745\\n2011-09-07  4796053560   704007171  NaN  1701933660\\n2011-09-08  5700560772  1010069925  NaN  1633702136\\n2011-09-09  7614488812  1082401848  NaN  1986085791\\n2011-09-12  6343972162   855171038  NaN  1882624672\\n2011-09-13         NaN         NaN  NaN         NaN\\n2011-09-14         NaN         NaN  NaN         NaN\\nIn [19]: vwap = (prices * volume).sum() / volume.sum()\\n330 | Chapter 11: \\u2002Financial and Economic Data Applications\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [20]: vwap             In [21]: vwap.dropna()\\nOut[20]:                  Out[21]:              \\nAAPL    380.655181        AAPL    380.655181    \\nJNJ      64.394769        JNJ      64.394769    \\nSPX            NaN        XOM      72.024288    \\nXOM      72.024288\\nSince SPX wasn’t found in volume, you can choose to explicitly discard that at any point.\\nShould you wish to align by hand, you can use DataFrame’s align method, which\\nreturns a tuple of reindexed versions of the two objects:\\nIn [22]: prices.align(volume, join='inner')\\nOut[22]: \\n(              AAPL    JNJ    XOM\\n2011-09-06  379.74  64.64  71.15\\n2011-09-07  383.93  65.43  73.65\\n2011-09-08  384.14  64.95  72.82\\n2011-09-09  377.48  63.64  71.01\\n2011-09-12  379.94  63.59  71.84,\\n                 AAPL       JNJ       XOM\\n2011-09-06  18173500  15848300  25416300\\n2011-09-07  12492000  10759700  23108400\\n2011-09-08  14839800  15551500  22434800\\n2011-09-09  20171900  17008200  27969100\\n2011-09-12  16697300  13448200  26205800)\\nAnother indispensable feature is constructing a DataFrame from a collection of poten-\\ntially differently indexed Series:\\nIn [23]: s1 = Series(range(3), index=['a', 'b', 'c'])\\nIn [24]: s2 = Series(range(4), index=['d', 'b', 'c', 'e'])\\nIn [25]: s3 = Series(range(3), index=['f', 'a', 'c'])\\nIn [26]: DataFrame({'one': s1, 'two': s2, 'three': s3})\\nOut[26]: \\n   one  three  two\\na    0      1  NaN\\nb    1    NaN    1\\nc    2      2    2\\nd  NaN    NaN    0\\ne  NaN    NaN    3\\nf  NaN      0  NaN\\nAs you have seen earlier, you can of course specify explicitly the index of the result,\\ndiscarding the rest of the data:\\nIn [27]: DataFrame({'one': s1, 'two': s2, 'three': s3}, index=list('face'))\\nOut[27]: \\n   one  three  two\\nf  NaN      0  NaN\\na    0      1  NaN\\nc    2      2    2\\ne  NaN    NaN    3\\nData Munging Topics | 331\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Operations with Time Series of Different Frequencies\\nEconomic time series are often of annual, quarterly, monthly, daily, or some other more\\nspecialized frequency. Some are completely irregular; for example, earnings revisions\\nfor a stock may arrive at any time. The two main tools for frequency conversion and\\nrealignment are the resample and reindex methods. resample converts data to a fixed\\nfrequency while reindex conforms data to a new index. Both support optional inter-\\npolation (such as forward filling) logic.\\nLet’s consider a small weekly time series:\\nIn [28]: ts1 = Series(np.random.randn(3),\\n   ....:              index=pd.date_range('2012-6-13', periods=3, freq='W-WED'))\\nIn [29]: ts1\\nOut[29]: \\n2012-06-13   -1.124801\\n2012-06-20    0.469004\\n2012-06-27   -0.117439\\nFreq: W-WED\\nIf you resample this to business daily (Monday-Friday) frequency, you get holes on the\\ndays where there is no data:\\nIn [30]: ts1.resample('B')\\nOut[30]: \\n2012-06-13   -1.124801\\n2012-06-14         NaN\\n2012-06-15         NaN\\n2012-06-18         NaN\\n2012-06-19         NaN\\n2012-06-20    0.469004\\n2012-06-21         NaN\\n2012-06-22         NaN\\n2012-06-25         NaN\\n2012-06-26         NaN\\n2012-06-27   -0.117439\\nFreq: B\\nOf course, using 'ffill' as the fill_method forward fills values in those gaps. This is\\na common practice with lower frequency data as you compute a time series of values\\non each timestamp having the latest valid or “as of” value:\\nIn [31]: ts1.resample('B', fill_method='ffill')\\nOut[31]: \\n2012-06-13   -1.124801\\n2012-06-14   -1.124801\\n2012-06-15   -1.124801\\n2012-06-18   -1.124801\\n2012-06-19   -1.124801\\n2012-06-20    0.469004\\n2012-06-21    0.469004\\n2012-06-22    0.469004\\n2012-06-25    0.469004\\n2012-06-26    0.469004\\n332 | Chapter 11: \\u2002Financial and Economic Data Applications\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"2012-06-27   -0.117439\\nFreq: B\\nIn practice, upsampling lower frequency data to a higher, regular frequency is a fine\\nsolution, but in the more general irregular time series case it may be a poor fit. Consider\\nan irregularly sampled time series from the same general time period:\\nIn [32]: dates = pd.DatetimeIndex(['2012-6-12', '2012-6-17', '2012-6-18',\\n   ....:                           '2012-6-21', '2012-6-22', '2012-6-29'])\\nIn [33]: ts2 = Series(np.random.randn(6), index=dates)\\nIn [34]: ts2\\nOut[34]: \\n2012-06-12   -0.449429\\n2012-06-17    0.459648\\n2012-06-18   -0.172531\\n2012-06-21    0.835938\\n2012-06-22   -0.594779\\n2012-06-29    0.027197\\nIf you wanted to add the “as of” values in ts1 (forward filling) to ts2. One option would\\nbe to resample both to a regular frequency then add, but if you want to maintain the\\ndate index in ts2, using reindex is a more precise solution:\\nIn [35]: ts1.reindex(ts2.index, method='ffill')\\nOut[35]: \\n2012-06-12         NaN\\n2012-06-17   -1.124801\\n2012-06-18   -1.124801\\n2012-06-21    0.469004\\n2012-06-22    0.469004\\n2012-06-29   -0.117439\\nIn [36]: ts2 + ts1.reindex(ts2.index, method='ffill')\\nOut[36]: \\n2012-06-12         NaN\\n2012-06-17   -0.665153\\n2012-06-18   -1.297332\\n2012-06-21    1.304942\\n2012-06-22   -0.125775\\n2012-06-29   -0.090242\\nUsing periods instead of timestamps\\nPeriods (representing time spans) provide an alternate means of working with different\\nfrequency time series, especially financial or economic series with annual or quarterly\\nfrequency having a particular reporting convention. For example, a company might\\nannounce its quarterly earnings with fiscal year ending in June, thus having Q-JUN fre-\\nquency. Consider a pair of macroeconomic time series related to GDP and inflation:\\nIn [37]: gdp = Series([1.78, 1.94, 2.08, 2.01, 2.15, 2.31, 2.46],\\n   ....:              index=pd.period_range('1984Q2', periods=7, freq='Q-SEP'))\\nData Munging Topics | 333\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [38]: infl = Series([0.025, 0.045, 0.037, 0.04],\\n   ....:               index=pd.period_range('1982', periods=4, freq='A-DEC'))\\nIn [39]: gdp          In [40]: infl\\nOut[39]:              Out[40]:     \\n1984Q2    1.78        1982    0.025\\n1984Q3    1.94        1983    0.045\\n1984Q4    2.08        1984    0.037\\n1985Q1    2.01        1985    0.040\\n1985Q2    2.15        Freq: A-DEC  \\n1985Q3    2.31                     \\n1985Q4    2.46                     \\nFreq: Q-SEP\\nUnlike time series with timestamps, operations between different-frequency time series\\nindexed by periods are not possible without explicit conversions. In this case, if we\\nknow that infl values were observed at the end of each year, we can then convert to\\nQ-SEP to get the right periods in that frequency:\\nIn [41]: infl_q = infl.asfreq('Q-SEP', how='end')\\nIn [42]: infl_q\\nOut[42]: \\n1983Q1    0.025\\n1984Q1    0.045\\n1985Q1    0.037\\n1986Q1    0.040\\nFreq: Q-SEP\\nThat time series can then be reindexed with forward-filling to match gdp:\\nIn [43]: infl_q.reindex(gdp.index, method='ffill')\\nOut[43]: \\n1984Q2    0.045\\n1984Q3    0.045\\n1984Q4    0.045\\n1985Q1    0.037\\n1985Q2    0.037\\n1985Q3    0.037\\n1985Q4    0.037\\nFreq: Q-SEP\\nTime of Day and “as of” Data Selection\\nSuppose you have a long time series containing intraday market data and you want to\\nextract the prices at a particular time of day on each day of the data. What if the data\\nare irregular such that observations do not fall exactly on the desired time? In practice\\nthis task can make for error-prone data munging if you are not careful. Here is an\\nexample for illustration purposes:\\n# Make an intraday date range and time series\\nIn [44]: rng = pd.date_range('2012-06-01 09:30', '2012-06-01 15:59', freq='T')\\n# Make a 5-day series of 9:30-15:59 values\\n334 | Chapter 11: \\u2002Financial and Economic Data Applications\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'In [45]: rng = rng.append([rng + pd.offsets.BDay(i) for i in range(1, 4)])\\nIn [46]: ts = Series(np.arange(len(rng), dtype=float), index=rng)\\nIn [47]: ts\\nOut[47]: \\n2012-06-01 09:30:00    0\\n2012-06-01 09:31:00    1\\n2012-06-01 09:32:00    2\\n2012-06-01 09:33:00    3\\n...\\n2012-06-06 15:56:00    1556\\n2012-06-06 15:57:00    1557\\n2012-06-06 15:58:00    1558\\n2012-06-06 15:59:00    1559\\nLength: 1560\\nIndexing with a Python datetime.time object will extract values at those times:\\nIn [48]: from datetime import time\\nIn [49]: ts[time(10, 0)]\\nOut[49]: \\n2012-06-01 10:00:00      30\\n2012-06-04 10:00:00     420\\n2012-06-05 10:00:00     810\\n2012-06-06 10:00:00    1200\\nUnder the hood, this uses an instance method at_time (available on individual time\\nseries and DataFrame objects alike):\\nIn [50]: ts.at_time(time(10, 0))\\nOut[50]: \\n2012-06-01 10:00:00      30\\n2012-06-04 10:00:00     420\\n2012-06-05 10:00:00     810\\n2012-06-06 10:00:00    1200\\nYou can select values between two times using the related between_time method:\\nIn [51]: ts.between_time(time(10, 0), time(10, 1))\\nOut[51]: \\n2012-06-01 10:00:00      30\\n2012-06-01 10:01:00      31\\n2012-06-04 10:00:00     420\\n2012-06-04 10:01:00     421\\n2012-06-05 10:00:00     810\\n2012-06-05 10:01:00     811\\n2012-06-06 10:00:00    1200\\n2012-06-06 10:01:00    1201\\nAs mentioned above, it might be the case that no data actually fall exactly at a time like\\n10 AM, but you might want to know the last known value at 10 AM:\\n# Set most of the time series randomly to NA\\nIn [53]: indexer = np.sort(np.random.permutation(len(ts))[700:])\\nData Munging Topics | 335\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [54]: irr_ts = ts.copy()\\nIn [55]: irr_ts[indexer] = np.nan\\nIn [56]: irr_ts['2012-06-01 09:50':'2012-06-01 10:00']\\nOut[56]: \\n2012-06-01 09:50:00    20\\n2012-06-01 09:51:00   NaN\\n2012-06-01 09:52:00    22\\n2012-06-01 09:53:00    23\\n2012-06-01 09:54:00   NaN\\n2012-06-01 09:55:00    25\\n2012-06-01 09:56:00   NaN\\n2012-06-01 09:57:00   NaN\\n2012-06-01 09:58:00   NaN\\n2012-06-01 09:59:00   NaN\\n2012-06-01 10:00:00   NaN\\nBy passing an array of timestamps to the asof method, you will obtain an array of the\\nlast valid (non-NA) values at or before each timestamp. So we construct a date range\\nat 10 AM for each day and pass that to asof:\\nIn [57]: selection = pd.date_range('2012-06-01 10:00', periods=4, freq='B')\\nIn [58]: irr_ts.asof(selection)\\nOut[58]: \\n2012-06-01 10:00:00      25\\n2012-06-04 10:00:00     420\\n2012-06-05 10:00:00     810\\n2012-06-06 10:00:00    1197\\nFreq: B\\nSplicing Together Data Sources\\nIn Chapter 7, I described a number of strategies for merging together two related data\\nsets. In a financial or economic context, there are a few widely occurring use cases:\\n• Switching from one data source (a time series or collection of time series) to another\\nat a specific point in time\\n• “Patching” missing values in a time series at the beginning, middle, or end using\\nanother time series\\n• Completely replacing the data for a subset of symbols (countries, asset tickers, and\\nso on)\\nIn the first case, switching from one set of time series to another at a specific instant, it\\nis a matter of splicing together two TimeSeries or DataFrame objects using pandas.con\\ncat:\\nIn [59]: data1 = DataFrame(np.ones((6, 3), dtype=float),\\n   ....:                   columns=['a', 'b', 'c'],\\n   ....:                   index=pd.date_range('6/12/2012', periods=6))\\n336 | Chapter 11: \\u2002Financial and Economic Data Applications\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [60]: data2 = DataFrame(np.ones((6, 3), dtype=float) * 2,\\n   ....:                   columns=['a', 'b', 'c'],\\n   ....:                   index=pd.date_range('6/13/2012', periods=6))\\nIn [61]: spliced = pd.concat([data1.ix[:'2012-06-14'], data2.ix['2012-06-15':]])\\nIn [62]: spliced\\nOut[62]: \\n            a  b  c\\n2012-06-12  1  1  1\\n2012-06-13  1  1  1\\n2012-06-14  1  1  1\\n2012-06-15  2  2  2\\n2012-06-16  2  2  2\\n2012-06-17  2  2  2\\n2012-06-18  2  2  2\\nSuppose in a similar example that data1 was missing a time series present in data2:\\nIn [63]: data2 = DataFrame(np.ones((6, 4), dtype=float) * 2,\\n   ....:                   columns=['a', 'b', 'c', 'd'],\\n   ....:                   index=pd.date_range('6/13/2012', periods=6))\\nIn [64]: spliced = pd.concat([data1.ix[:'2012-06-14'], data2.ix['2012-06-15':]])\\nIn [65]: spliced\\nOut[65]: \\n            a  b  c   d\\n2012-06-12  1  1  1 NaN\\n2012-06-13  1  1  1 NaN\\n2012-06-14  1  1  1 NaN\\n2012-06-15  2  2  2   2\\n2012-06-16  2  2  2   2\\n2012-06-17  2  2  2   2\\n2012-06-18  2  2  2   2\\nUsing combine_first, you can bring in data from before the splice point to extend the\\nhistory for 'd' item:\\nIn [66]: spliced_filled = spliced.combine_first(data2)\\nIn [67]: spliced_filled\\nOut[67]: \\n            a  b  c   d\\n2012-06-12  1  1  1 NaN\\n2012-06-13  1  1  1   2\\n2012-06-14  1  1  1   2\\n2012-06-15  2  2  2   2\\n2012-06-16  2  2  2   2\\n2012-06-17  2  2  2   2\\n2012-06-18  2  2  2   2\\nSince data2 does not have any values for 2012-06-12, no values are filled on that day.\\nDataFrame has a related method update for performing in-place updates. You have to\\npass overwrite=False to make it only fill the holes:\\nData Munging Topics | 337\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [68]: spliced.update(data2, overwrite=False)\\nIn [69]: spliced\\nOut[69]: \\n            a  b  c   d\\n2012-06-12  1  1  1 NaN\\n2012-06-13  1  1  1   2\\n2012-06-14  1  1  1   2\\n2012-06-15  2  2  2   2\\n2012-06-16  2  2  2   2\\n2012-06-17  2  2  2   2\\n2012-06-18  2  2  2   2\\nTo replace the data for a subset of symbols, you can use any of the above techniques,\\nbut sometimes it’s simpler to just set the columns directly with DataFrame indexing:\\nIn [70]: cp_spliced = spliced.copy()\\nIn [71]: cp_spliced[['a', 'c']] = data1[['a', 'c']]\\nIn [72]: cp_spliced\\nOut[72]: \\n             a  b   c   d\\n2012-06-12   1  1   1 NaN\\n2012-06-13   1  1   1   2\\n2012-06-14   1  1   1   2\\n2012-06-15   1  2   1   2\\n2012-06-16   1  2   1   2\\n2012-06-17   1  2   1   2\\n2012-06-18 NaN  2 NaN   2\\nReturn Indexes and Cumulative Returns\\nIn a financial context, returns usually refer to percent changes in the price of an asset.\\nLet’s consider price data for Apple in 2011 and 2012:\\nIn [73]: import pandas.io.data as web\\nIn [74]: price = web.get_data_yahoo('AAPL', '2011-01-01')['Adj Close']\\nIn [75]: price[-5:]\\nOut[75]: \\nDate\\n2012-07-23    603.83\\n2012-07-24    600.92\\n2012-07-25    574.97\\n2012-07-26    574.88\\n2012-07-27    585.16\\nName: Adj Close\\nFor Apple, which has no dividends, computing the cumulative percent return between\\ntwo points in time requires computing only the percent change in the price:\\nIn [76]: price['2011-10-03'] / price['2011-3-01'] - 1\\nOut[76]: 0.072399874037388123\\n338 | Chapter 11: \\u2002Financial and Economic Data Applications\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"For other stocks with dividend payouts, computing how much money you make from\\nholding a stock can be more complicated. The adjusted close values used here have\\nbeen adjusted for splits and dividends, however. In all cases, it’s quite common to derive\\na return index, which is a time series indicating the value of a unit investment (one\\ndollar, say). Many assumptions can underlie the return index; for example, some will\\nchoose to reinvest profit and others not. In the case of Apple, we can compute a simple\\nreturn index using cumprod:\\nIn [77]: returns = price.pct_change()\\nIn [78]: ret_index = (1 + returns).cumprod()\\nIn [79]: ret_index[0] = 1  # Set first value to 1\\nIn [80]: ret_index\\nOut[80]: \\nDate\\n2011-01-03    1.000000\\n2011-01-04    1.005219\\n2011-01-05    1.013442\\n2011-01-06    1.012623\\n...\\n2012-07-24    1.823346\\n2012-07-25    1.744607\\n2012-07-26    1.744334\\n2012-07-27    1.775526\\nLength: 396\\nWith a return index in hand, computing cumulative returns at a particular resolution\\nis simple:\\nIn [81]: m_returns = ret_index.resample('BM', how='last').pct_change()\\nIn [82]: m_returns['2012']\\nOut[82]: \\nDate\\n2012-01-31    0.127111\\n2012-02-29    0.188311\\n2012-03-30    0.105284\\n2012-04-30   -0.025969\\n2012-05-31   -0.010702\\n2012-06-29    0.010853\\n2012-07-31    0.001986\\nFreq: BM\\nOf course, in this simple case (no dividends or other adjustments to take into account)\\nthese could have been computed from the daily percent changed by resampling with\\naggregation (here, to periods):\\nIn [83]: m_rets = (1 + returns).resample('M', how='prod', kind='period') - 1\\nIn [84]: m_rets['2012']\\nOut[84]: \\nDate\\nData Munging Topics | 339\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"2012-01    0.127111\\n2012-02    0.188311\\n2012-03    0.105284\\n2012-04   -0.025969\\n2012-05   -0.010702\\n2012-06    0.010853\\n2012-07    0.001986\\nFreq: M\\nIf you had dividend dates and percentages, including them in the total return per day\\nwould look like:\\nreturns[dividend_dates] += dividend_pcts\\nGroup Transforms and Analysis\\nIn Chapter 9, you learned the basics of computing group statistics and applying your\\nown transformations to groups in a dataset.\\nLet’s consider a collection of hypothetical stock portfolios. I first randomly generate a\\nbroad universe of 2000 tickers:\\nimport random; random.seed(0)\\nimport string\\nN = 1000\\ndef rands(n):\\n    choices = string.ascii_uppercase\\n    return ''.join([random.choice(choices) for _ in xrange(n)])\\ntickers = np.array([rands(5) for _ in xrange(N)])\\nI then create a DataFrame containing 3 columns representing hypothetical, but random\\nportfolios for a subset of tickers:\\nM = 500\\ndf = DataFrame({'Momentum' : np.random.randn(M) / 200 + 0.03,\\n                'Value' : np.random.randn(M) / 200 + 0.08,\\n                'ShortInterest' : np.random.randn(M) / 200 - 0.02},\\n                index=tickers[:M])\\nNext, let’s create a random industry classification for the tickers. To keep things simple,\\nI’ll just keep it to 2 industries, storing the mapping in a Series:\\nind_names = np.array(['FINANCIAL', 'TECH'])\\nsampler = np.random.randint(0, len(ind_names), N)\\nindustries = Series(ind_names[sampler], index=tickers,\\n                    name='industry')\\nNow we can group by industries and carry out group aggregation and transformations:\\nIn [90]: by_industry = df.groupby(industries)\\nIn [91]: by_industry.mean()\\nOut[91]: \\n           Momentum  ShortInterest     Value\\n340 | Chapter 11: \\u2002Financial and Economic Data Applications\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"industry                                    \\nFINANCIAL  0.029485      -0.020739  0.079929\\nTECH       0.030407      -0.019609  0.080113\\nIn [92]: by_industry.describe()\\nOut[92]: \\n                   Momentum  ShortInterest       Value\\nindustry                                              \\nFINANCIAL count  246.000000     246.000000  246.000000\\n          mean     0.029485      -0.020739    0.079929\\n          std      0.004802       0.004986    0.004548\\n          min      0.017210      -0.036997    0.067025\\n          25%      0.026263      -0.024138    0.076638\\n          50%      0.029261      -0.020833    0.079804\\n          75%      0.032806      -0.017345    0.082718\\n          max      0.045884      -0.006322    0.093334\\nTECH      count  254.000000     254.000000  254.000000\\n          mean     0.030407      -0.019609    0.080113\\n          std      0.005303       0.005074    0.004886\\n          min      0.016778      -0.032682    0.065253\\n          25%      0.026456      -0.022779    0.076737\\n          50%      0.030650      -0.019829    0.080296\\n          75%      0.033602      -0.016923    0.083353\\n          max      0.049638      -0.003698    0.093081\\nBy defining transformation functions, it’s easy to transform these portfolios by industry.\\nFor example, standardizing within industry is widely used in equity portfolio construc-\\ntion:\\n# Within-Industry Standardize\\ndef zscore(group):\\n    return (group - group.mean()) / group.std()\\ndf_stand = by_industry.apply(zscore)\\nYou can verify that each industry has mean 0 and standard deviation 1:\\nIn [94]: df_stand.groupby(industries).agg(['mean', 'std'])\\nOut[94]: \\n           Momentum       ShortInterest       Value     \\n               mean  std           mean  std   mean  std\\nindustry                                                \\nFINANCIAL         0    1              0    1      0    1\\nTECH             -0    1             -0    1     -0    1\\nOther, built-in kinds of transformations, like rank, can be used more concisely:\\n# Within-industry rank descending\\nIn [95]: ind_rank = by_industry.rank(ascending=False)\\nIn [96]: ind_rank.groupby(industries).agg(['min', 'max'])\\nOut[96]: \\n           Momentum       ShortInterest       Value     \\n                min  max            min  max    min  max\\nindustry                                                \\nFINANCIAL         1  246              1  246      1  246\\nTECH              1  254              1  254      1  254\\nGroup Transforms and Analysis | 341\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In quantitative equity, “rank and standardize” is a common sequence of transforms.\\nYou could do this by chaining together rank and zscore like so:\\n# Industry rank and standardize\\nIn [97]: by_industry.apply(lambda x: zscore(x.rank()))\\nOut[97]: \\n<class 'pandas.core.frame.DataFrame'>\\nIndex: 500 entries, VTKGN to PTDQE\\nData columns:\\nMomentum         500  non-null values\\nShortInterest    500  non-null values\\nValue            500  non-null values\\ndtypes: float64(3)\\nGroup Factor Exposures\\nFactor analysis is a technique in quantitative portfolio management. Portfolio holdings\\nand performance (profit and less) are decomposed using one or more factors (risk fac-\\ntors are one example) represented as a portfolio of weights. For example, a stock price’s\\nco-movement with a benchmark (like S&P 500 index) is known as its beta, a common\\nrisk factor. Let’s consider a contrived example of a portfolio constructed from 3 ran-\\ndomly-generated factors (usually called the factor loadings) and some weights:\\nfrom numpy.random import rand\\nfac1, fac2, fac3 = np.random.rand(3, 1000)\\nticker_subset = tickers.take(np.random.permutation(N)[:1000])\\n# Weighted sum of factors plus noise\\nport = Series(0.7 * fac1 - 1.2 * fac2 + 0.3 * fac3 + rand(1000),\\n              index=ticker_subset)\\nfactors = DataFrame({'f1': fac1, 'f2': fac2, 'f3': fac3},\\n                    index=ticker_subset)\\nVector correlations between each factor and the portfolio may not indicate too much:\\nIn [99]: factors.corrwith(port)\\nOut[99]: \\nf1    0.402377\\nf2   -0.680980\\nf3    0.168083\\nThe standard way to compute the factor exposures is by least squares regression; using\\npandas.ols with factors as the explanatory variables we can compute exposures over\\nthe entire set of tickers:\\nIn [100]: pd.ols(y=port, x=factors).beta\\nOut[100]: \\nf1           0.761789\\nf2          -1.208760\\nf3           0.289865\\nintercept    0.484477\\n342 | Chapter 11: \\u2002Financial and Economic Data Applications\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"As you can see, the original factor weights can nearly be recovered since there was not\\ntoo much additional random noise added to the portfolio. Using groupby you can com-\\npute exposures industry by industry. To do so, write a function like so:\\ndef beta_exposure(chunk, factors=None):\\n    return pd.ols(y=chunk, x=factors).beta\\nThen, group by industries and apply that function, passing the DataFrame of factor\\nloadings:\\nIn [102]: by_ind = port.groupby(industries)\\nIn [103]: exposures = by_ind.apply(beta_exposure, factors=factors)\\nIn [104]: exposures.unstack()\\nOut[104]: \\n                 f1        f2        f3  intercept\\nindustry                                          \\nFINANCIAL  0.790329 -1.182970  0.275624   0.455569\\nTECH       0.740857 -1.232882  0.303811   0.508188\\nDecile and Quartile Analysis\\nAnalyzing data based on sample quantiles is another important tool for financial ana-\\nlysts. For example, the performance of a stock portfolio could be broken down into\\nquartiles (four equal-sized chunks) based on each stock’s price-to-earnings. Using pan\\ndas.qcut combined with groupby makes quantile analysis reasonably straightforward.\\nAs an example, let’s consider a simple trend following or momentum strategy trading\\nthe S&P 500 index via the SPY exchange-traded fund. You can download the price\\nhistory from Yahoo! Finance:\\nIn [105]: import pandas.io.data as web\\nIn [106]: data = web.get_data_yahoo('SPY', '2006-01-01')\\nIn [107]: data\\nOut[107]: \\n<class 'pandas.core.frame.DataFrame'>\\nDatetimeIndex: 1655 entries, 2006-01-03 00:00:00 to 2012-07-27 00:00:00\\nData columns:\\nOpen         1655  non-null values\\nHigh         1655  non-null values\\nLow          1655  non-null values\\nClose        1655  non-null values\\nVolume       1655  non-null values\\nAdj Close    1655  non-null values\\ndtypes: float64(5), int64(1)\\nNow, we’ll compute daily returns and a function for transforming the returns into a\\ntrend signal formed from a lagged moving sum:\\npx = data['Adj Close']\\nreturns = px.pct_change()\\nGroup Transforms and Analysis | 343\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"def to_index(rets):\\n    index = (1 + rets).cumprod()\\n    first_loc = max(index.notnull().argmax() - 1, 0)\\n    index.values[first_loc] = 1\\n    return index\\ndef trend_signal(rets, lookback, lag):\\n    signal = pd.rolling_sum(rets, lookback, min_periods=lookback - 5)\\n    return signal.shift(lag)\\nUsing this function, we can (naively) create and test a trading strategy that trades this\\nmomentum signal every Friday:\\nIn [109]: signal = trend_signal(returns, 100, 3)\\nIn [110]: trade_friday = signal.resample('W-FRI').resample('B', fill_method='ffill')\\nIn [111]: trade_rets = trade_friday.shift(1) * returns\\nWe can then convert the strategy returns to a return index and plot them (see Fig-\\nure 11-1):\\nIn [112]: to_index(trade_rets).plot()\\nFigure 11-1. SPY momentum strategy return index\\nSuppose you wanted to decompose the strategy performance into more and less volatile\\nperiods of trading. Trailing one-year annualized standard deviation is a simple measure\\nof volatility, and we can compute Sharpe ratios to assess the reward-to-risk ratio in\\nvarious volatility regimes:\\nvol = pd.rolling_std(returns, 250, min_periods=200) * np.sqrt(250)\\n344 | Chapter 11: \\u2002Financial and Economic Data Applications\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"def sharpe(rets, ann=250):\\n    return rets.mean() / rets.std()  * np.sqrt(ann)\\nNow, dividing vol into quartiles with qcut and aggregating with sharpe we obtain:\\nIn [114]: trade_rets.groupby(pd.qcut(vol, 4)).agg(sharpe)\\nOut[114]: \\n[0.0955, 0.16]    0.490051\\n(0.16, 0.188]     0.482788\\n(0.188, 0.231]   -0.731199\\n(0.231, 0.457]    0.570500\\nThese results show that the strategy performed the best during the period when the\\nvolatility was the highest.\\nMore Example Applications\\nHere is a small set of additional examples.\\nSignal Frontier Analysis\\nIn this section, I’ll describe a simplified cross-sectional momentum portfolio and show\\nhow you might explore a grid of model parameterizations. First, I’ll load historical\\nprices for a portfolio of financial and technology stocks:\\nnames = ['AAPL', 'GOOG', 'MSFT', 'DELL', 'GS', 'MS', 'BAC', 'C']\\ndef get_px(stock, start, end):\\n    return web.get_data_yahoo(stock, start, end)['Adj Close']\\npx = DataFrame({n: get_px(n, '1/1/2009', '6/1/2012') for n in names})\\nWe can easily plot the cumulative returns of each stock (see Figure 11-2):\\nIn [117]: px = px.asfreq('B').fillna(method='pad')\\nIn [118]: rets = px.pct_change()\\nIn [119]: ((1 + rets).cumprod() - 1).plot()\\nFor the portfolio construction, we’ll compute momentum over a certain lookback, then\\nrank in descending order and standardize:\\ndef calc_mom(price, lookback, lag):\\n    mom_ret = price.shift(lag).pct_change(lookback)\\n    ranks = mom_ret.rank(axis=1, ascending=False)\\n    demeaned = ranks - ranks.mean(axis=1)\\n    return demeaned / demeaned.std(axis=1)\\nWith this transform function in hand, we can set up a strategy backtesting function\\nthat computes a portfolio for a particular lookback and holding period (days between\\ntrading), returning the overall Sharpe ratio:\\ncompound = lambda x : (1 + x).prod() - 1\\ndaily_sr = lambda x: x.mean() / x.std()\\nMore Example Applications | 345\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"def strat_sr(prices, lb, hold):\\n    # Compute portfolio weights\\n    freq = '%dB' % hold\\n    port = calc_mom(prices, lb, lag=1)\\n    daily_rets = prices.pct_change()\\n    # Compute portfolio returns\\n    port = port.shift(1).resample(freq, how='first')\\n    returns = daily_rets.resample(freq, how=compound)\\n    port_rets = (port * returns).sum(axis=1)\\n    return daily_sr(port_rets) * np.sqrt(252 / hold)\\nFigure 11-2. Cumulative returns for each of the stocks\\nWhen called with the prices and a parameter combination, this function returns a scalar\\nvalue:\\nIn [122]: strat_sr(px, 70, 30)\\nOut[122]: 0.27421582756800583\\nFrom there, you can evaluate the strat_sr function over a grid of parameters, storing\\nthem as you go in a defaultdict and finally putting the results in a DataFrame:\\nfrom collections import defaultdict\\nlookbacks = range(20, 90, 5)\\nholdings = range(20, 90, 5)\\ndd = defaultdict(dict)\\nfor lb in lookbacks:\\n    for hold in holdings:\\n        dd[lb][hold] = strat_sr(px, lb, hold)\\n346 | Chapter 11: \\u2002Financial and Economic Data Applications\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"ddf = DataFrame(dd)\\nddf.index.name = 'Holding Period'\\nddf.columns.name = 'Lookback Period'\\nTo visualize the results and get an idea of what’s going on, here is a function that uses\\nmatplotlib to produce a heatmap with some adornments:\\nimport matplotlib.pyplot as plt\\ndef heatmap(df, cmap=plt.cm.gray_r):\\n    fig = plt.figure()\\n    ax = fig.add_subplot(111)\\n    axim = ax.imshow(df.values, cmap=cmap, interpolation='nearest')\\n    ax.set_xlabel(df.columns.name)\\n    ax.set_xticks(np.arange(len(df.columns)))\\n    ax.set_xticklabels(list(df.columns))\\n    ax.set_ylabel(df.index.name)\\n    ax.set_yticks(np.arange(len(df.index)))\\n    ax.set_yticklabels(list(df.index))\\n    plt.colorbar(axim)\\nCalling this function on the backtest results, we get Figure 11-3:\\nIn [125]: heatmap(ddf)\\nFigure 11-3. Heatmap of momentum strategy Sharpe ratio (higher is better) over various lookbacks\\nand holding periods\\nFuture Contract Rolling\\nA future is an ubiquitous form of derivative contract; it is an agreement to take delivery\\nof a certain asset (such as oil, gold, or shares of the FTSE 100 index) on a particular\\ndate. In practice, modeling and trading futures contracts on equities, currencies,\\nMore Example Applications | 347\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"commodities, bonds, and other asset classes is complicated by the time-limited nature\\nof each contract. For example, at any given time for a type of future (say silver or copper\\nfutures) multiple contracts with different expiration dates may be traded. In many cases,\\nthe future contract expiring next (the near contract) will be the most liquid (highest\\nvolume and lowest bid-ask spread).\\nFor the purposes of modeling and forecasting, it can be much easier to work with a \\ncontinuous return index indicating the profit and loss associated with always holding\\nthe near contract. Transitioning from an expiring contract to the next (or far) contract\\nis referred to as rolling. Computing a continuous future series from the individual con-\\ntract data is not necessarily a straightforward exercise and typically requires a deeper\\nunderstanding of the market and how the instruments are traded. For example, in\\npractice when and how quickly would you trade out of an expiring contract and into\\nthe next contract? Here I describe one such process.\\nFirst, I’ll use scaled prices for the SPY exchange-traded fund as a proxy for the S&P 500\\nindex:\\nIn [127]: import pandas.io.data as web\\n# Approximate price of S&P 500 index\\nIn [128]: px = web.get_data_yahoo('SPY')['Adj Close'] * 10\\nIn [129]: px\\nOut[129]: \\nDate\\n2011-08-01    1261.0\\n2011-08-02    1228.8\\n2011-08-03    1235.5\\n...\\n2012-07-25    1339.6\\n2012-07-26    1361.7\\n2012-07-27    1386.8\\nName: Adj Close, Length: 251\\nNow, a little bit of setup. I put a couple of S&P 500 future contracts and expiry dates\\nin a Series:\\nfrom datetime import datetime\\nexpiry = {'ESU2': datetime(2012, 9, 21),\\n          'ESZ2': datetime(2012, 12, 21)}\\nexpiry = Series(expiry).order()\\nexpiry then looks like:\\nIn [131]: expiry\\nOut[131]: \\nESU2    2012-09-21 00:00:00\\nESZ2    2012-12-21 00:00:00\\n348 | Chapter 11: \\u2002Financial and Economic Data Applications\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Then, I use the Yahoo! Finance prices along with a random walk and some noise to\\nsimulate the two contracts into the future:\\nnp.random.seed(12347)\\nN = 200\\nwalk = (np.random.randint(0, 200, size=N) - 100) * 0.25\\nperturb = (np.random.randint(0, 20, size=N) - 10) * 0.25\\nwalk = walk.cumsum()\\nrng = pd.date_range(px.index[0], periods=len(px) + N, freq='B')\\nnear = np.concatenate([px.values, px.values[-1] + walk])\\nfar = np.concatenate([px.values, px.values[-1] + walk + perturb])\\nprices = DataFrame({'ESU2': near, 'ESZ2': far}, index=rng)\\nprices then has two time series for the contracts that differ from each other by a random\\namount:\\nIn [133]: prices.tail()\\nOut[133]: \\n               ESU2     ESZ2\\n2013-04-16  1416.05  1417.80\\n2013-04-17  1402.30  1404.55\\n2013-04-18  1410.30  1412.05\\n2013-04-19  1426.80  1426.05\\n2013-04-22  1406.80  1404.55\\nOne way to splice time series together into a single continuous series is to construct a\\nweighting matrix. Active contracts would have a weight of 1 until the expiry date ap-\\nproaches. At that point you have to decide on a roll convention. Here is a function that\\ncomputes a weighting matrix with linear decay over a number of periods leading up to\\nexpiry:\\ndef get_roll_weights(start, expiry, items, roll_periods=5):\\n    # start : first date to compute weighting DataFrame\\n    # expiry : Series of ticker -> expiration dates\\n    # items : sequence of contract names\\n    dates = pd.date_range(start, expiry[-1], freq='B')\\n    weights = DataFrame(np.zeros((len(dates), len(items))),\\n                        index=dates, columns=items)\\n    prev_date = weights.index[0]\\n    for i, (item, ex_date) in enumerate(expiry.iteritems()):\\n        if i < len(expiry) - 1:\\n            weights.ix[prev_date:ex_date - pd.offsets.BDay(), item] = 1\\n            roll_rng = pd.date_range(end=ex_date - pd.offsets.BDay(),\\n                                     periods=roll_periods + 1, freq='B')\\n            decay_weights = np.linspace(0, 1, roll_periods + 1)\\n            weights.ix[roll_rng, item] = 1 - decay_weights\\n            weights.ix[roll_rng, expiry.index[i + 1]] = decay_weights\\n        else:\\n            weights.ix[prev_date:, item] = 1\\n        prev_date = ex_date\\nMore Example Applications | 349\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"return weights\\nThe weights look like this around the ESU2 expiry:\\nIn [135]: weights = get_roll_weights('6/1/2012', expiry, prices.columns)\\nIn [136]: weights.ix['2012-09-12':'2012-09-21']\\nOut[136]: \\n            ESU2  ESZ2\\n2012-09-12   1.0   0.0\\n2012-09-13   1.0   0.0\\n2012-09-14   0.8   0.2\\n2012-09-17   0.6   0.4\\n2012-09-18   0.4   0.6\\n2012-09-19   0.2   0.8\\n2012-09-20   0.0   1.0\\n2012-09-21   0.0   1.0\\nFinally, the rolled future returns are just a weighted sum of the contract returns:\\nIn [137]: rolled_returns = (prices.pct_change() * weights).sum(1)\\nRolling Correlation and Linear Regression\\nDynamic models play an important role in financial modeling as they can be used to\\nsimulate trading decisions over a historical period. Moving window and exponentially-\\nweighted time series functions are an example of tools that are used for dynamic models.\\nCorrelation is one way to look at the co-movement between the changes in two asset\\ntime series. pandas’s rolling_corr function can be called with two return series to\\ncompute the moving window correlation. First, I load some price series from Yahoo!\\nFinance and compute daily returns:\\naapl = web.get_data_yahoo('AAPL', '2000-01-01')['Adj Close']\\nmsft = web.get_data_yahoo('MSFT', '2000-01-01')['Adj Close']\\naapl_rets = aapl.pct_change()\\nmsft_rets = msft.pct_change()\\nThen, I compute and plot the one-year moving correlation (see Figure 11-4):\\nIn [140]: pd.rolling_corr(aapl_rets, msft_rets, 250).plot()\\nOne issue with correlation between two assets is that it does not capture differences in\\nvolatility. Least-squares regression provides another means for modeling the dynamic\\nrelationship between a variable and one or more other predictor variables.\\nIn [142]: model = pd.ols(y=aapl_rets, x={'MSFT': msft_rets}, window=250)\\nIn [143]: model.beta\\nOut[143]: \\n<class 'pandas.core.frame.DataFrame'>\\nDatetimeIndex: 2913 entries, 2000-12-28 00:00:00 to 2012-07-27 00:00:00\\nData columns:\\n350 | Chapter 11: \\u2002Financial and Economic Data Applications\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"MSFT         2913  non-null values\\nintercept    2913  non-null values\\ndtypes: float64(2)\\nIn [144]: model.beta['MSFT'].plot()\\nFigure 11-4. One-year correlation of Apple with Microsoft\\nFigure 11-5. One-year beta (OLS regression coefficient) of Apple to Microsoft\\npandas’s ols function implements static and dynamic (expanding or rolling window)\\nleast squares regressions. For more sophisticated statistical and econometrics models,\\nsee the statsmodels project (http://statsmodels.sourceforge.net).\\nMore Example Applications | 351\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf', 'page_content': 'www.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'CHAPTER 12\\nAdvanced NumPy\\nndarray Object Internals\\nThe NumPy ndarray provides a means to interpret a block of homogeneous data (either\\ncontiguous or strided, more on this later) as a multidimensional array object. As you’ve\\nseen, the data type, or dtype, determines how the data is interpreted as being floating\\npoint, integer, boolean, or any of the other types we’ve been looking at.\\nPart of what makes ndarray powerful is that every array object is a strided view on a\\nblock of data. You might wonder, for example, how the array view arr[::2, ::-1] does\\nnot copy any data. Simply put, the ndarray is more than just a chunk of memory and\\na dtype; it also has striding information which enables the array to move through\\nmemory with varying step sizes. More precisely, the ndarray internally consists of the\\nfollowing:\\n• A pointer to data, that is a block of system memory\\n• The data type or dtype\\n• A tuple indicating the array’s shape; For example, a 10 by 5 array would have shape\\n(10, 5)\\nIn [8]: np.ones((10, 5)).shape\\nOut[8]: (10, 5)\\n• A tuple of strides, integers indicating the number of bytes to “step” in order to\\nadvance one element along a dimension; For example, a typical (C order, more on\\nthis later) 3 x 4 x 5 array of float64 (8-byte) values has strides (160, 40, 8)\\nIn [9]: np.ones((3, 4, 5), dtype=np.float64).strides\\nOut[9]: (160, 40, 8)\\nWhile it is rare that a typical NumPy user would be interested in the array strides,\\nthey are the critical ingredient in constructing copyless array views. Strides can\\neven be negative which enables an array to move backward through memory, which\\nwould be the case in a slice like obj[::-1] or obj[:, ::-1].\\n353\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'See Figure 12-1 for a simple mockup the ndarray innards.\\nFigure 12-1. The NumPy ndarray object\\nNumPy dtype Hierarchy\\nYou may occasionally have code which needs to check whether an array contains in-\\ntegers, floating point numbers, strings, or Python objects. Because there are many types\\nof floating point numbers (float16 through float128), checking that the dtype is among\\na list of types would be very verbose. Fortunately, the dtypes have superclasses such as\\nnp.integer and np.floating which can be used in conjunction with the np.issubd\\ntype function:\\nIn [10]: ints = np.ones(10, dtype=np.uint16)\\nIn [11]: floats = np.ones(10, dtype=np.float32)\\nIn [12]: np.issubdtype(ints.dtype, np.integer)\\nOut[12]: True\\nIn [13]: np.issubdtype(floats.dtype, np.floating)\\nOut[13]: True\\nYou can see all of the parent classes of a specific dtype by calling the type’s mro method:\\nIn [14]: np.float64.mro()\\nOut[14]: \\n[numpy.float64,\\n numpy.floating,\\n numpy.inexact,\\n numpy.number,\\n numpy.generic,\\n float,\\n object]\\nMost NumPy users will never have to know about this, but it occasionally comes in\\nhandy. See Figure 12-2  for a graph of the dtype hierarchy and parent-subclass\\nrelationships 1.\\n1. Some of the dtypes have trailing underscores in their names. These are there to avoid variable name\\nconflicts between the NumPy-specific types and the Python built-in ones.\\n354 | Chapter 12: \\u2002Advanced NumPy\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Advanced Array Manipulation\\nThere are many ways to work with arrays beyond fancy indexing, slicing, and boolean\\nsubsetting. While much of the heavy lifting for data analysis applications is handled by\\nhigher level functions in pandas, you may at some point need to write a data algorithm\\nthat is not found in one of the existing libraries.\\nReshaping Arrays\\nGiven what we know about NumPy arrays, it should come as little surprise that you\\ncan convert an array from one shape to another without copying any data. To do this,\\npass a tuple indicating the new shape to the reshape array instance method. For exam-\\nple, suppose we had a one-dimensional array of values that we wished to rearrange into\\na matrix:\\nIn [15]: arr = np.arange(8)\\nIn [16]: arr\\nOut[16]: array([0, 1, 2, 3, 4, 5, 6, 7])\\nIn [17]: arr.reshape((4, 2))\\nOut[17]: \\narray([[0, 1],\\n       [2, 3],\\n       [4, 5],\\n       [6, 7]])\\nA multidimensional array can also be reshaped:\\nIn [18]: arr.reshape((4, 2)).reshape((2, 4))\\nOut[18]: \\nFigure 12-2. The NumPy dtype class hierarchy\\nAdvanced Array Manipulation | 355\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'array([[0, 1, 2, 3],\\n       [4, 5, 6, 7]])\\nOne of the passed shape dimensions can be -1, in which case the value used for that\\ndimension will be inferred from the data:\\nIn [19]: arr = np.arange(15)      In [20]: arr.reshape((5, -1))\\n                                  Out[20]:                     \\n                                  array([[ 0,  1,  2],         \\n                                         [ 3,  4,  5],         \\n                                         [ 6,  7,  8],         \\n                                         [ 9, 10, 11],         \\n                                         [12, 13, 14]])\\nSince an array’s shape attribute is a tuple, it can be passed to reshape, too:\\nIn [21]: other_arr = np.ones((3, 5))\\nIn [22]: other_arr.shape\\nOut[22]: (3, 5)\\nIn [23]: arr.reshape(other_arr.shape)\\nOut[23]: \\narray([[ 0,  1,  2,  3,  4],\\n       [ 5,  6,  7,  8,  9],\\n       [10, 11, 12, 13, 14]])\\nThe opposite operation of reshape from one-dimensional to a higher dimension is typ-\\nically known as flattening or raveling:\\nIn [24]: arr = np.arange(15).reshape((5, 3))      In [25]: arr         \\n                                                  Out[25]:             \\n                                                  array([[ 0,  1,  2], \\n                                                         [ 3,  4,  5], \\n                                                         [ 6,  7,  8], \\n                                                         [ 9, 10, 11], \\n                                                         [12, 13, 14]])\\n                                                                       \\nIn [26]: arr.ravel()\\nOut[26]: array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])\\nravel does not produce a copy of the underlying data if it does not have to (more on\\nthis below). The flatten method behaves like ravel except it always returns a copy of\\nthe data:\\nIn [27]: arr.flatten()\\nOut[27]: array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])\\nThe data can be reshaped or raveled in different orders. This is a slightly nuanced topic\\nfor new NumPy users and is therefore the next subtopic.\\nC versus Fortran Order\\nContrary to some other scientific computing environments like R and MATLAB,\\nNumPy gives you much more control and flexibility over the layout of your data in\\n356 | Chapter 12: \\u2002Advanced NumPy\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"memory. By default, NumPy arrays are created in row major order. Spatially this means\\nthat if you have a two-dimensional array of data, the items in each row of the array are\\nstored in adjacent memory locations. The alternative to row major ordering is column\\nmajor order, which means that (you guessed it) values within each column of data are\\nstored in adjacent memory locations.\\nFor historical reasons, row and column major order are also know as C and Fortran\\norder, respectively. In FORTRAN 77, the language of our forebears, matrices were all\\ncolumn major.\\nFunctions like reshape and ravel, accept an order argument indicating the order to use\\nthe data in the array. This can be 'C' or 'F' in most cases (there are also less commonly-\\nused options 'A' and 'K'; see the NumPy documentation). These are illustrated in\\nFigure 12-3.\\nIn [28]: arr = np.arange(12).reshape((3, 4))\\nIn [29]: arr\\nOut[29]: \\narray([[ 0,  1,  2,  3],\\n       [ 4,  5,  6,  7],\\n       [ 8,  9, 10, 11]])\\nIn [30]: arr.ravel()\\nOut[30]: array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\\nIn [31]: arr.ravel('F')\\nOut[31]: array([ 0,  4,  8,  1,  5,  9,  2,  6, 10,  3,  7, 11])\\nReshaping arrays with more than two dimensions can be a bit mind-bending. The key\\ndifference between C and Fortran order is the order in which the dimensions are\\nwalked:\\n• C / row major order: traverse higher dimensions first (e.g. axis 1 before advancing\\non axis 0).\\n• Fortran / column major order:  traverse higher dimensions last (e.g. axis 0 before\\nadvancing on axis 1).\\nConcatenating and Splitting Arrays\\nnumpy.concatenate takes a sequence (tuple, list, etc.) of arrays and joins them together\\nin order along the input axis.\\nIn [32]: arr1 = np.array([[1, 2, 3], [4, 5, 6]])\\nIn [33]: arr2 = np.array([[7, 8, 9], [10, 11, 12]])\\nIn [34]: np.concatenate([arr1, arr2], axis=0)\\nOut[34]: \\narray([[ 1,  2,  3],\\n       [ 4,  5,  6],\\nAdvanced Array Manipulation | 357\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': '[ 7,  8,  9],\\n       [10, 11, 12]])\\nIn [35]: np.concatenate([arr1, arr2], axis=1)\\nOut[35]: \\narray([[ 1,  2,  3,  7,  8,  9],\\n       [ 4,  5,  6, 10, 11, 12]])\\nFigure 12-3. Reshaping in C (row major) or Fortran (column major) order\\nThere are some convenience functions, like vstack and hstack, for common kinds of\\nconcatenation. The above operations could have been expressed as:\\nIn [36]: np.vstack((arr1, arr2))      In [37]: np.hstack((arr1, arr2)) \\nOut[36]:                              Out[37]:                         \\narray([[ 1,  2,  3],                  array([[ 1,  2,  3,  7,  8,  9], \\n       [ 4,  5,  6],                         [ 4,  5,  6, 10, 11, 12]])\\n       [ 7,  8,  9],                                                   \\n       [10, 11, 12]])\\nsplit, on the other hand, slices apart an array into multiple arrays along an axis:\\nIn [38]: from numpy.random import randn\\nIn [39]: arr = randn(5, 2)      In [40]: arr               \\n                                Out[40]:                   \\n                                array([[ 0.1689,  0.3287], \\n                                       [ 0.4703,  0.8989], \\n                                       [ 0.1535,  0.0243], \\n                                       [-0.2832,  1.1536], \\n                                       [ 0.2707,  0.8075]])\\n                                                           \\nIn [41]: first, second, third = np.split(arr, [1, 3])\\nIn [42]: first\\n358 | Chapter 12: \\u2002Advanced NumPy\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Out[42]: array([[ 0.1689,  0.3287]])\\nIn [43]: second                  In [44]: third             \\nOut[43]:                         Out[44]:                   \\narray([[ 0.4703,  0.8989],       array([[-0.2832,  1.1536], \\n       [ 0.1535,  0.0243]])             [ 0.2707,  0.8075]])\\nSee Table 12-1 for a list of all relevant concatenation and splitting functions, some of\\nwhich are provided only as a convenience of the very general purpose concatenate.\\nTable 12-1. Array concatenation functions\\nFunction Description\\nconcatenate Most general function, concatenates collection of arrays along one axis\\nvstack, row_stack Stack arrays row-wise (along axis 0)\\nhstack Stack arrays column-wise (along axis 1)\\ncolumn_stack Like hstack, but converts 1D arrays to 2D column vectors first\\ndstack Stack arrays “depth\"-wise (along axis 2)\\nsplit Split array at passed locations along a particular axis\\nhsplit / vsplit / dsplit Convenience functions for splitting on axis 0, 1, and 2, respectively.\\nStacking helpers: r_ and c_\\nThere are two special objects in the NumPy namespace, r_ and c_, that make stacking\\narrays more concise:\\nIn [45]: arr = np.arange(6)\\nIn [46]: arr1 = arr.reshape((3, 2))\\nIn [47]: arr2 = randn(3, 2)\\nIn [48]: np.r_[arr1, arr2]       In [49]: np.c_[np.r_[arr1, arr2], arr]\\nOut[48]:                         Out[49]:                              \\narray([[ 0.    ,  1.    ],       array([[ 0.    ,  1.    ,  0.    ],   \\n       [ 2.    ,  3.    ],              [ 2.    ,  3.    ,  1.    ],   \\n       [ 4.    ,  5.    ],              [ 4.    ,  5.    ,  2.    ],   \\n       [ 0.7258, -1.5325],              [ 0.7258, -1.5325,  3.    ],   \\n       [-0.4696, -0.2127],              [-0.4696, -0.2127,  4.    ],   \\n       [-0.1072,  1.2871]])             [-0.1072,  1.2871,  5.    ]])\\nThese additionally can translate slices to arrays:\\nIn [50]: np.c_[1:6, -10:-5]\\nOut[50]: \\narray([[  1, -10],\\n       [  2,  -9],\\n       [  3,  -8],\\n       [  4,  -7],\\n       [  5,  -6]])\\nSee the docstring for more on what you can do with c_ and r_.\\nAdvanced Array Manipulation | 359\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Repeating Elements: Tile and Repeat\\nThe need to replicate or repeat arrays is less common with NumPy than\\nit is with other popular array programming languages like MATLAB.\\nThe main reason for this is that broadcasting fulfills this need better,\\nwhich is the subject of the next section.\\nThe two main tools for repeating or replicating arrays to produce larger arrays are the \\nrepeat and tile functions. repeat replicates each element in an array some number of\\ntimes, producing a larger array:\\nIn [51]: arr = np.arange(3)\\nIn [52]: arr.repeat(3)\\nOut[52]: array([0, 0, 0, 1, 1, 1, 2, 2, 2])\\nBy default, if you pass an integer, each element will be repeated that number of times.\\nIf you pass an array of integers, each element can be repeated a different number of\\ntimes:\\nIn [53]: arr.repeat([2, 3, 4])\\nOut[53]: array([0, 0, 1, 1, 1, 2, 2, 2, 2])\\nMultidimensional arrays can have their elements repeated along a particular axis.\\nIn [54]: arr = randn(2, 2)\\nIn [55]: arr                       In [56]: arr.repeat(2, axis=0)\\nOut[55]:                           Out[56]:                      \\narray([[ 0.7157, -0.6387],         array([[ 0.7157, -0.6387],    \\n       [ 0.3626,  0.849 ]])               [ 0.7157, -0.6387],    \\n                                          [ 0.3626,  0.849 ],    \\n                                          [ 0.3626,  0.849 ]])\\nNote that if no axis is passed, the array will be flattened first, which is likely not what\\nyou want. Similarly you can pass an array of integers when repeating a multidimen-\\nsional array to repeat a given slice a different number of times:\\nIn [57]: arr.repeat([2, 3], axis=0)\\nOut[57]: \\narray([[ 0.7157, -0.6387],\\n       [ 0.7157, -0.6387],\\n       [ 0.3626,  0.849 ],\\n       [ 0.3626,  0.849 ],\\n       [ 0.3626,  0.849 ]])\\nIn [58]: arr.repeat([2, 3], axis=1)\\nOut[58]: \\narray([[ 0.7157,  0.7157, -0.6387, -0.6387, -0.6387],\\n       [ 0.3626,  0.3626,  0.849 ,  0.849 ,  0.849 ]])\\n360 | Chapter 12: \\u2002Advanced NumPy\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'tile, on the other hand, is a shortcut for stacking copies of an array along an axis. You\\ncan visually think about it as like “laying down tiles”:\\nIn [59]: arr\\nOut[59]: \\narray([[ 0.7157, -0.6387],\\n       [ 0.3626,  0.849 ]])\\nIn [60]: np.tile(arr, 2)\\nOut[60]: \\narray([[ 0.7157, -0.6387,  0.7157, -0.6387],\\n       [ 0.3626,  0.849 ,  0.3626,  0.849 ]])\\nThe second argument is the number of tiles; with a scalar, the tiling is made row-by-\\nrow, rather than column by column: The second argument to tile can be a tuple in-\\ndicating the layout of the “tiling”:\\nIn [61]: arr\\nOut[61]: \\narray([[ 0.7157, -0.6387],\\n       [ 0.3626,  0.849 ]])\\nIn [62]: np.tile(arr, (2, 1))      In [63]: np.tile(arr, (3, 2))                \\nOut[62]:                           Out[63]:                                     \\narray([[ 0.7157, -0.6387],         array([[ 0.7157, -0.6387,  0.7157, -0.6387], \\n       [ 0.3626,  0.849 ],                [ 0.3626,  0.849 ,  0.3626,  0.849 ], \\n       [ 0.7157, -0.6387],                [ 0.7157, -0.6387,  0.7157, -0.6387], \\n       [ 0.3626,  0.849 ]])               [ 0.3626,  0.849 ,  0.3626,  0.849 ], \\n                                          [ 0.7157, -0.6387,  0.7157, -0.6387], \\n                                          [ 0.3626,  0.849 ,  0.3626,  0.849 ]])\\nFancy Indexing Equivalents: Take and Put\\nAs you may recall from Chapter 4, one way to get and set subsets of arrays is by \\nfancy indexing using integer arrays:\\nIn [64]: arr = np.arange(10) * 100\\nIn [65]: inds = [7, 1, 2, 6]        In [66]: arr[inds]                  \\n                                    Out[66]: array([700, 100, 200, 600])\\nThere are alternate ndarray methods that are useful in the special case of only making\\na selection on a single axis:\\nIn [67]: arr.take(inds)\\nOut[67]: array([700, 100, 200, 600])\\nIn [68]: arr.put(inds, 42)\\nIn [69]: arr\\nOut[69]: array([  0,  42,  42, 300, 400, 500,  42,  42, 800, 900])\\nIn [70]: arr.put(inds, [40, 41, 42, 43])\\nAdvanced Array Manipulation | 361\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'In [71]: arr\\nOut[71]: array([  0,  41,  42, 300, 400, 500,  43,  40, 800, 900])\\nTo use take along other axes, you can pass the axis keyword:\\nIn [72]: inds = [2, 0, 2, 1]\\nIn [73]: arr = randn(2, 4)\\nIn [74]: arr\\nOut[74]: \\narray([[-0.8237,  2.6047, -0.4578, -1.    ],\\n       [ 2.3198, -1.0792,  0.518 ,  0.2527]])\\nIn [75]: arr.take(inds, axis=1)\\nOut[75]: \\narray([[-0.4578, -0.8237, -0.4578,  2.6047],\\n       [ 0.518 ,  2.3198,  0.518 , -1.0792]])\\nput does not accept an axis argument but rather indexes into the flattened (one-di-\\nmensional, C order) version of the array (this could be changed in principle). Thus,\\nwhen you need to set elements using an index array on other axes, you will want to use\\nfancy indexing.\\nAs of this writing, the take and put functions in general have better\\nperformance than their fancy indexing equivalents by a significant mar-\\ngin. I regard this as a “bug” and something to be fixed in NumPy, but\\nit’s something worth keeping in mind if you’re selecting subsets of large\\narrays using integer arrays:\\nIn [76]: arr = randn(1000, 50)\\n# Random sample of 500 rows\\nIn [77]: inds = np.random.permutation(1000)[:500]\\nIn [78]: %timeit arr[inds]\\n1000 loops, best of 3: 356 us per loop\\nIn [79]: %timeit arr.take(inds, axis=0)\\n10000 loops, best of 3: 34 us per loop\\nBroadcasting\\nBroadcasting describes how arithmetic works between arrays of different shapes. It is\\na very powerful feature, but one that can be easily misunderstood, even by experienced\\nusers. The simplest example of broadcasting occurs when combining a scalar value\\nwith an array:\\nIn [80]: arr = np.arange(5)\\nIn [81]: arr                           In [82]: arr * 4                    \\nOut[81]: array([0, 1, 2, 3, 4])        Out[82]: array([ 0,  4,  8, 12, 16])\\n362 | Chapter 12: \\u2002Advanced NumPy\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Here we say that the scalar value 4 has been broadcast to all of the other elements in\\nthe multiplication operation.\\nFor example, we can demean each column of an array by subtracting the column means.\\nIn this case, it is very simple:\\nIn [83]: arr = randn(4, 3)\\nIn [84]: arr.mean(0)\\nOut[84]: array([ 0.1321,  0.552 ,  0.8571])\\nIn [85]: demeaned = arr - arr.mean(0)\\nIn [86]: demeaned                           In [87]: demeaned.mean(0)      \\nOut[86]:                                    Out[87]: array([ 0., -0., -0.])\\narray([[ 0.1718, -0.1972, -1.3669],                                        \\n       [-0.1292,  1.6529, -0.3429],                                        \\n       [-0.2891, -0.0435,  1.2322],                                        \\n       [ 0.2465, -1.4122,  0.4776]])\\nSee Figure 12-4 for an illustration of this operation. Demeaning the rows as a broadcast\\noperation requires a bit more care. Fortunately, broadcasting potentially lower dimen-\\nsional values across any dimension of an array (like subtracting the row means from\\neach column of a two-dimensional array) is possible as long as you follow the rules.\\nThis brings us to:\\nFigure 12-4. Broadcasting over axis 0 with a 1D array\\nThe Broadcasting Ru\\nTwo arrays are compatible for broadcasting if for each trailing dimension (that is, start-\\ning from the end), the axis lengths match or if either of the lengths is 1. Broadcasting\\nis then performed over the missing and / or length 1 dimensions.\\nEven as an experienced NumPy user, I often must stop to draw pictures and think about\\nthe broadcasting rule. Consider the last example and suppose we wished instead to\\nsubtract the mean value from each row. Since arr.mean(0) has length 3, it is compatible\\nBroadcasting | 363\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'for broadcasting across axis 0 because the trailing dimension in arr is 3 and therefore\\nmatches. According to the rules, to subtract over axis 1 (that is, subtract the row mean\\nfrom each row), the smaller array must have shape (4, 1):\\nIn [88]: arr\\nOut[88]: \\narray([[ 0.3039,  0.3548, -0.5097],\\n       [ 0.0029,  2.2049,  0.5142],\\n       [-0.1571,  0.5085,  2.0893],\\n       [ 0.3786, -0.8602,  1.3347]])\\nIn [89]: row_means = arr.mean(1)        In [90]: row_means.reshape((4, 1))\\n                                        Out[90]:                          \\n                                        array([[ 0.0496],                 \\n                                               [ 0.9073],                 \\n                                               [ 0.8136],                 \\n                                               [ 0.2844]])                \\n                                                                          \\nIn [91]: demeaned = arr - row_means.reshape((4, 1))\\nIn [92]: demeaned.mean(1)\\nOut[92]: array([ 0.,  0.,  0.,  0.])\\nHas your head exploded yet? See Figure 12-5 for an illustration of this operation.\\nFigure 12-5. Broadcasting over axis 1 of a 2D array\\nSee Figure 12-6 for another illustration, this time subtracting a two-dimensional array\\nfrom a three-dimensional one across axis 0.\\nBroadcasting Over Other Axes\\nBroadcasting with higher dimensional arrays can seem even more mind-bending, but\\nit is really a matter of following the rules. If you don’t, you’ll get an error like this:\\nIn [93]: arr - arr.mean(1)\\n---------------------------------------------------------------------------\\nValueError                                Traceback (most recent call last)\\n<ipython-input-93-7b87b85a20b2> in <module>()\\n364 | Chapter 12: \\u2002Advanced NumPy\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': '----> 1 arr - arr.mean(1)\\nValueError: operands could not be broadcast together with shapes (4,3) (4)\\nFigure 12-6. Broadcasting over axis 0 of a 3D array\\nIt’s quite common to want to perform an arithmetic operation with a lower dimensional\\narray across axes other than axis 0. According to the broadcasting rule, the “broadcast\\ndimensions” must be 1 in the smaller array. In the example of row demeaning above\\nthis meant reshaping the row means to be shape (4, 1) instead of (4,):\\nIn [94]: arr - arr.mean(1).reshape((4, 1))\\nOut[94]: \\narray([[ 0.2542,  0.3051, -0.5594],\\n       [-0.9044,  1.2976, -0.3931],\\n       [-0.9707, -0.3051,  1.2757],\\n       [ 0.0942, -1.1446,  1.0503]])\\nIn the three-dimensional case, broadcasting over any of the three dimensions is only a\\nmatter of reshaping the data to be shape-compatible. See Figure 12-7 for a nice visual-\\nization of the shapes required to broadcast over each axis of a three-dimensional array.\\nA very common problem, therefore, is needing to add a new axis with length 1 specif-\\nically for broadcasting purposes, especially in generic algorithms. Using reshape is one\\noption, but inserting an axis requires constructing a tuple indicating the new shape.\\nThis can often be a tedious exercise. Thus, NumPy arrays offer a special syntax for\\ninserting new axes by indexing. We use the special np.newaxis attribute along with\\n“full” slices to insert the new axis:\\nIn [95]: arr = np.zeros((4, 4))\\nIn [96]: arr_3d = arr[:, np.newaxis, :]      In [97]: arr_3d.shape\\n                                             Out[97]: (4, 1, 4)\\nIn [98]: arr_1d = np.random.normal(size=3)\\nIn [99]: arr_1d[:, np.newaxis]      In [100]: arr_1d[np.newaxis, :]               \\nOut[99]:                            Out[100]: array([[-0.3899,  0.396 , -0.1852]])\\nBroadcasting | 365\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'array([[-0.3899],                                                                 \\n       [ 0.396 ],                                                                 \\n       [-0.1852]])\\nFigure 12-7. Compatible 2D array shapes for broadcasting over a 3D array\\nThus, if we had a three-dimensional array and wanted to demean axis 2, say, we would\\nonly need to write:\\nIn [101]: arr = randn(3, 4, 5)\\nIn [102]: depth_means = arr.mean(2)\\nIn [103]: depth_means\\nOut[103]: \\narray([[ 0.1097,  0.3118, -0.5473,  0.2663],\\n       [ 0.1747,  0.1379,  0.1146, -0.4224],\\n       [ 0.0217,  0.3686, -0.0468,  1.3026]])\\nIn [104]: demeaned = arr - depth_means[:, :, np.newaxis]\\nIn [105]: demeaned.mean(2)\\nOut[105]: \\narray([[ 0.,  0., -0.,  0.],\\n       [ 0., -0., -0.,  0.],\\n       [-0., -0.,  0.,  0.]])\\nIf you’re completely confused by this, don’t worry. With practice you will get the hang\\nof it!\\n366 | Chapter 12: \\u2002Advanced NumPy\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Some readers might wonder if there’s a way to generalize demeaning over an axis\\nwithout sacrificing performance. There is, in fact, but it requires some indexing\\ngymnastics:\\ndef demean_axis(arr, axis=0):\\n    means = arr.mean(axis)\\n    # This generalized things like [:, :, np.newaxis] to N dimensions\\n    indexer = [slice(None)] * arr.ndim\\n    indexer[axis] = np.newaxis\\n    return arr - means[indexer]\\nSetting Array Values by Broadcasting\\nThe same broadcasting rule governing arithmetic operations also applies to setting\\nvalues via array indexing. In the simplest case, we can do things like:\\nIn [106]: arr = np.zeros((4, 3))\\nIn [107]: arr[:] = 5        In [108]: arr           \\n                            Out[108]:               \\n                            array([[ 5.,  5.,  5.], \\n                                   [ 5.,  5.,  5.], \\n                                   [ 5.,  5.,  5.], \\n                                   [ 5.,  5.,  5.]])\\nHowever, if we had a one-dimensional array of values we wanted to set into the columns\\nof the array, we can do that as long as the shape is compatible:\\nIn [109]: col = np.array([1.28, -0.42, 0.44, 1.6])\\nIn [110]: arr[:] = col[:, np.newaxis]       In [111]: arr                 \\n                                            Out[111]:                     \\n                                            array([[ 1.28,  1.28,  1.28], \\n                                                   [-0.42, -0.42, -0.42], \\n                                                   [ 0.44,  0.44,  0.44], \\n                                                   [ 1.6 ,  1.6 ,  1.6 ]])\\n                                                                          \\nIn [112]: arr[:2] = [[-1.37], [0.509]]      In [113]: arr                    \\n                                            Out[113]:                        \\n                                            array([[-1.37 , -1.37 , -1.37 ], \\n                                                   [ 0.509,  0.509,  0.509], \\n                                                   [ 0.44 ,  0.44 ,  0.44 ], \\n                                                   [ 1.6  ,  1.6  ,  1.6  ]])\\nAdvanced ufunc Usage\\nWhile many NumPy users will only make use of the fast element-wise operations pro-\\nvided by the universal functions, there are a number of additional features that occa-\\nsionally can help you write more concise code without loops.\\nAdvanced ufunc Usage | 367\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'ufunc Instance Methods\\nEach of NumPy’s binary ufuncs has special methods for performing certain kinds of\\nspecial vectorized operations. These are summarized in Table 12-2, but I’ll give a few\\nconcrete examples to illustrate how they work.\\nreduce takes a single array and aggregates its values, optionally along an axis, by per-\\nforming a sequence of binary operations. For example, an alternate way to sum ele-\\nments in an array is to use np.add.reduce:\\nIn [114]: arr = np.arange(10)\\nIn [115]: np.add.reduce(arr)\\nOut[115]: 45\\nIn [116]: arr.sum()\\nOut[116]: 45\\nThe starting value (0 for add) depends on the ufunc. If an axis is passed, the reduction\\nis performed along that axis. This allows you to answer certain kinds of questions in a\\nconcise way. As a less trivial example, we can use np.logical_and to check whether the\\nvalues in each row of an array are sorted:\\nIn [118]: arr = randn(5, 5)\\nIn [119]: arr[::2].sort(1) # sort a few rows\\nIn [120]: arr[:, :-1] < arr[:, 1:]\\nOut[120]: \\narray([[ True,  True,  True,  True],\\n       [False,  True, False, False],\\n       [ True,  True,  True,  True],\\n       [ True, False,  True,  True],\\n       [ True,  True,  True,  True]], dtype=bool)\\nIn [121]: np.logical_and.reduce(arr[:, :-1] < arr[:, 1:], axis=1)\\nOut[121]: array([ True, False,  True, False,  True], dtype=bool)\\nOf course, logical_and.reduce is equivalent to the all method.\\naccumulate is related to reduce like cumsum is related to sum. It produces an array of the\\nsame size with the intermediate “accumulated” values:\\nIn [122]: arr = np.arange(15).reshape((3, 5))\\nIn [123]: np.add.accumulate(arr, axis=1)\\nOut[123]: \\narray([[ 0,  1,  3,  6, 10],\\n       [ 5, 11, 18, 26, 35],\\n       [10, 21, 33, 46, 60]])\\nouter performs a pairwise cross-product between two arrays:\\nIn [124]: arr = np.arange(3).repeat([1, 2, 2])\\n368 | Chapter 12: \\u2002Advanced NumPy\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'In [125]: arr\\nOut[125]: array([0, 1, 1, 2, 2])\\nIn [126]: np.multiply.outer(arr, np.arange(5))\\nOut[126]: \\narray([[0, 0, 0, 0, 0],\\n       [0, 1, 2, 3, 4],\\n       [0, 1, 2, 3, 4],\\n       [0, 2, 4, 6, 8],\\n       [0, 2, 4, 6, 8]])\\nThe output of outer will have a dimension that is the sum of the dimensions of the\\ninputs:\\nIn [127]: result = np.subtract.outer(randn(3, 4), randn(5))\\nIn [128]: result.shape\\nOut[128]: (3, 4, 5)\\nThe last method, reduceat, performs a “local reduce”, in essence an array groupby op-\\neration in which slices of the array are aggregated together. While it’s less flexible than\\nthe GroupBy capabilities in pandas, it can be very fast and powerful in the right cir-\\ncumstances. It accepts a sequence of “bin edges” which indicate how to split and ag-\\ngregate the values:\\nIn [129]: arr = np.arange(10)\\nIn [130]: np.add.reduceat(arr, [0, 5, 8])\\nOut[130]: array([10, 18, 17])\\nThe results are the reductions (here, sums) performed over arr[0:5], arr[5:8], and\\narr[8:]. Like the other methods, you can pass an axis argument:\\nIn [131]: arr = np.multiply.outer(np.arange(4), np.arange(5))\\nIn [132]: arr                      In [133]: np.add.reduceat(arr, [0, 2, 4], axis=1)\\nOut[132]:                          Out[133]:                                        \\narray([[ 0,  0,  0,  0,  0],       array([[ 0,  0,  0],                             \\n       [ 0,  1,  2,  3,  4],              [ 1,  5,  4],                             \\n       [ 0,  2,  4,  6,  8],              [ 2, 10,  8],                             \\n       [ 0,  3,  6,  9, 12]])             [ 3, 15, 12]])\\nTable 12-2. ufunc methods\\nMethod Description\\nreduce(x) Aggregate values by successive applications of the operation\\naccumulate(x) Aggregate values, preserving all partial aggregates\\nreduceat(x, bins) “Local” reduce or “group by”. Reduce contiguous slices of data to produce aggregated\\narray.\\nouter(x, y) Apply operation to all pairs of elements in x and y. Result array has shape x.shape +\\ny.shape\\nAdvanced ufunc Usage | 369\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Custom ufuncs\\nThere are a couple facilities for creating your own functions with ufunc-like semantics. \\nnumpy.frompyfunc accepts a Python function along with a specification for the number\\nof inputs and outputs. For example, a simple function that adds element-wise would\\nbe specified as:\\nIn [134]: def add_elements(x, y):\\n   .....:     return x + y\\nIn [135]: add_them = np.frompyfunc(add_elements, 2, 1)\\nIn [136]: add_them(np.arange(8), np.arange(8))\\nOut[136]: array([0, 2, 4, 6, 8, 10, 12, 14], dtype=object)\\nFunctions created using frompyfunc always return arrays of Python objects which isn’t\\nvery convenient. Fortunately, there is an alternate, but slightly less featureful function \\nnumpy.vectorize that is a bit more intelligent about type inference:\\nIn [137]: add_them = np.vectorize(add_elements, otypes=[np.float64])\\nIn [138]: add_them(np.arange(8), np.arange(8))\\nOut[138]: array([  0.,   2.,   4.,   6.,   8.,  10.,  12.,  14.])\\nThese functions provide a way to create ufunc-like functions, but they are very slow\\nbecause they require a Python function call to compute each element, which is a lot\\nslower than NumPy’s C-based ufunc loops:\\nIn [139]: arr = randn(10000)\\nIn [140]: %timeit add_them(arr, arr)\\n100 loops, best of 3: 2.12 ms per loop\\nIn [141]: %timeit np.add(arr, arr)\\n100000 loops, best of 3: 11.6 us per loop\\nThere are a number of projects under way in the scientific Python community to make\\nit easier to define new ufuncs whose performance is closer to that of the built-in ones.\\nStructured and Record Arrays\\nYou may have noticed up until now that ndarray is a homogeneous data container; that\\nis, it represents a block of memory in which each element takes up the same number\\nof bytes, determined by the dtype. On the surface, this would appear to not allow you\\nto represent heterogeneous or tabular-like data. A structured array is an ndarray in\\nwhich each element can be thought of as representing a struct in C (hence the “struc-\\ntured” name) or a row in a SQL table with multiple named fields:\\nIn [142]: dtype = [('x', np.float64), ('y', np.int32)]\\nIn [143]: sarr = np.array([(1.5, 6), (np.pi, -2)], dtype=dtype)\\n370 | Chapter 12: \\u2002Advanced NumPy\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [144]: sarr\\nOut[144]: \\narray([(1.5, 6), (3.141592653589793, -2)], \\n      dtype=[('x', '<f8'), ('y', '<i4')])\\nThere are several ways to specify a structured dtype (see the online NumPy documen-\\ntation). One typical way is as a list of tuples with (field_name, field_data_type). Now,\\nthe elements of the array are tuple-like objects whose elements can be accessed like a\\ndictionary:\\nIn [145]: sarr[0]\\nOut[145]: (1.5, 6)\\nIn [146]: sarr[0]['y']\\nOut[146]: 6\\nThe field names are stored in the dtype.names attribute. On accessing a field on the\\nstructured array, a strided view on the data is returned thus copying nothing:\\nIn [147]: sarr['x']\\nOut[147]: array([ 1.5   ,  3.1416])\\nNested dtypes and Multidimensional Fields\\nWhen specifying a structured dtype, you can additionally pass a shape (as an int or\\ntuple):\\nIn [148]: dtype = [('x', np.int64, 3), ('y', np.int32)]\\nIn [149]: arr = np.zeros(4, dtype=dtype)\\nIn [150]: arr\\nOut[150]: \\narray([([0, 0, 0], 0), ([0, 0, 0], 0), ([0, 0, 0], 0), ([0, 0, 0], 0)], \\n      dtype=[('x', '<i8', (3,)), ('y', '<i4')])\\nIn this case, the x field now refers to an array of length three for each record:\\nIn [151]: arr[0]['x']\\nOut[151]: array([0, 0, 0])\\nConveniently, accessing arr['x'] then returns a two-dimensional array instead of a\\none-dimensional array as in prior examples:\\nIn [152]: arr['x']\\nOut[152]: \\narray([[0, 0, 0],\\n       [0, 0, 0],\\n       [0, 0, 0],\\n       [0, 0, 0]])\\nThis enables you to express more complicated, nested structures as a single block of\\nmemory in an array. Though, since dtypes can be arbitrarily complex, why not nested\\ndtypes? Here is a simple example:\\nStructured and Record Arrays | 371\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [153]: dtype = [('x', [('a', 'f8'), ('b', 'f4')]), ('y', np.int32)]\\nIn [154]: data = np.array([((1, 2), 5), ((3, 4), 6)], dtype=dtype)\\nIn [155]: data['x']\\nOut[155]: \\narray([(1.0, 2.0), (3.0, 4.0)], \\n      dtype=[('a', '<f8'), ('b', '<f4')])\\nIn [156]: data['y']\\nOut[156]: array([5, 6], dtype=int32)\\nIn [157]: data['x']['a']\\nOut[157]: array([ 1.,  3.])\\nAs you can see, variable-shape fields and nested records is a very rich feature that can\\nbe the right tool in certain circumstances. A DataFrame from pandas, by contrast, does\\nnot support this feature directly, though it is similar to hierarchical indexing.\\nWhy Use Structured Arrays?\\nCompared with, say, a DataFrame from pandas, NumPy structured arrays are a com-\\nparatively low-level tool. They provide a means to interpreting a block of memory as a\\ntabular structure with arbitrarily complex nested columns. Since each element in the\\narray is represented in memory as a fixed number of bytes, structured arrays provide a\\nvery fast and efficient way of writing data to and from disk (including memory maps,\\nmore on this later), transporting it over the network, and other such use.\\nAs another common use for structured arrays, writing data files as fixed length record\\nbyte streams is a common way to serialize data in C and C++ code, which is commonly\\nfound in legacy systems in industry. As long as the format of the file is known (the size\\nof each record and the order, byte size, and data type of each element), the data can be\\nread into memory using np.fromfile. Specialized uses like this are beyond the scope of\\nthis book, but it’s worth knowing that such things are possible.\\nStructured Array Manipulations: numpy.lib.recfunctions\\nWhile there is not as much functionality available for structured arrays as for Data-\\nFrames, the NumPy module numpy.lib.recfunctions has some helpful tools for adding\\nand dropping fields or doing basic join-like operations. The thing to remember with\\nthese tools is that it is typically necessary to create a new array to make any modifica-\\ntions to the dtype (like adding or dropping a column). These functions are left to the\\ninterested reader to explore as I do not use them anywhere in this book.\\n372 | Chapter 12: \\u2002Advanced NumPy\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'More About Sorting\\nLike Python’s built-in list, the ndarray sort instance method is an in-place sort, meaning\\nthat the array contents are rearranged without producing a new array:\\nIn [158]: arr = randn(6)\\nIn [159]: arr.sort()\\nIn [160]: arr\\nOut[160]: array([-1.082 ,  0.3759,  0.8014,  1.1397,  1.2888,  1.8413])\\nWhen sorting arrays in-place, remember that if the array is a view on a different ndarray,\\nthe original array will be modified:\\nIn [161]: arr = randn(3, 5)\\nIn [162]: arr\\nOut[162]: \\narray([[-0.3318, -1.4711,  0.8705, -0.0847, -1.1329],\\n       [-1.0111, -0.3436,  2.1714,  0.1234, -0.0189],\\n       [ 0.1773,  0.7424,  0.8548,  1.038 , -0.329 ]])\\nIn [163]: arr[:, 0].sort()  # Sort first column values in-place\\nIn [164]: arr\\nOut[164]: \\narray([[-1.0111, -1.4711,  0.8705, -0.0847, -1.1329],\\n       [-0.3318, -0.3436,  2.1714,  0.1234, -0.0189],\\n       [ 0.1773,  0.7424,  0.8548,  1.038 , -0.329 ]])\\nOn the other hand, numpy.sort creates a new, sorted copy of an array. Otherwise it\\naccepts the same arguments (such as kind, more on this below) as ndarray.sort:\\nIn [165]: arr = randn(5)\\nIn [166]: arr\\nOut[166]: array([-1.1181, -0.2415, -2.0051,  0.7379, -1.0614])\\nIn [167]: np.sort(arr)\\nOut[167]: array([-2.0051, -1.1181, -1.0614, -0.2415,  0.7379])\\nIn [168]: arr\\nOut[168]: array([-1.1181, -0.2415, -2.0051,  0.7379, -1.0614])\\nAll of these sort methods take an axis argument for sorting the sections of data along\\nthe passed axis independently:\\nIn [169]: arr = randn(3, 5)\\nIn [170]: arr\\nOut[170]: \\narray([[ 0.5955, -0.2682,  1.3389, -0.1872,  0.9111],\\n       [-0.3215,  1.0054, -0.5168,  1.1925, -0.1989],\\n       [ 0.3969, -1.7638,  0.6071, -0.2222, -0.2171]])\\nMore About Sorting | 373\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'In [171]: arr.sort(axis=1)\\nIn [172]: arr\\nOut[172]: \\narray([[-0.2682, -0.1872,  0.5955,  0.9111,  1.3389],\\n       [-0.5168, -0.3215, -0.1989,  1.0054,  1.1925],\\n       [-1.7638, -0.2222, -0.2171,  0.3969,  0.6071]])\\nYou may notice that none of the sort methods have an option to sort in descending\\norder. This is not actually a big deal because array slicing produces views, thus not\\nproducing a copy or requiring any computational work. Many Python users are familiar\\nwith the “trick” that for a list values, values[::-1] returns a list in reverse order. The\\nsame is true for ndarrays:\\nIn [173]: arr[:, ::-1]\\nOut[173]: \\narray([[ 1.3389,  0.9111,  0.5955, -0.1872, -0.2682],\\n       [ 1.1925,  1.0054, -0.1989, -0.3215, -0.5168],\\n       [ 0.6071,  0.3969, -0.2171, -0.2222, -1.7638]])\\nIndirect Sorts: argsort and lexsort\\nIn data analysis it’s very common to need to reorder data sets by one or more keys. For\\nexample, a table of data about some students might need to be sorted by last name then\\nby first name. This is an example of an indirect sort, and if you’ve read the pandas-\\nrelated chapters you have already seen many higher-level examples. Given a key or keys\\n(an array or values or multiple arrays of values), you wish to obtain an array of integer\\nindices (I refer to them colloquially as indexers) that tells you how to reorder the data\\nto be in sorted order. The two main methods for this are argsort and numpy.lexsort.\\nAs a trivial example:\\nIn [174]: values = np.array([5, 0, 1, 3, 2])\\nIn [175]: indexer = values.argsort()\\nIn [176]: indexer\\nOut[176]: array([1, 2, 4, 3, 0])\\nIn [177]: values[indexer]\\nOut[177]: array([0, 1, 2, 3, 5])\\nAs a less trivial example, this code reorders a 2D array by its first row:\\nIn [178]: arr = randn(3, 5)\\nIn [179]: arr[0] = values\\nIn [180]: arr\\nOut[180]: \\narray([[ 5.    ,  0.    ,  1.    ,  3.    ,  2.    ],\\n       [-0.3636, -0.1378,  2.1777, -0.4728,  0.8356],\\n       [-0.2089,  0.2316,  0.728 , -1.3918,  1.9956]])\\n374 | Chapter 12: \\u2002Advanced NumPy\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [181]: arr[:, arr[0].argsort()]\\nOut[181]: \\narray([[ 0.    ,  1.    ,  2.    ,  3.    ,  5.    ],\\n       [-0.1378,  2.1777,  0.8356, -0.4728, -0.3636],\\n       [ 0.2316,  0.728 ,  1.9956, -1.3918, -0.2089]])\\nlexsort is similar to argsort, but it performs an indirect lexicographical sort on multiple\\nkey arrays. Suppose we wanted to sort some data identified by first and last names:\\nIn [182]: first_name = np.array(['Bob', 'Jane', 'Steve', 'Bill', 'Barbara'])\\nIn [183]: last_name = np.array(['Jones', 'Arnold', 'Arnold', 'Jones', 'Walters'])\\nIn [184]: sorter = np.lexsort((first_name, last_name))\\nIn [185]: zip(last_name[sorter], first_name[sorter])\\nOut[185]: \\n[('Arnold', 'Jane'),\\n ('Arnold', 'Steve'),\\n ('Jones', 'Bill'),\\n ('Jones', 'Bob'),\\n ('Walters', 'Barbara')]\\nlexsort can be a bit confusing the first time you use it because the order in which the\\nkeys are used to order the data starts with the last array passed. As you can see,\\nlast_name was used before first_name.\\npandas methods like Series’s and DataFrame’s sort_index methods and\\nthe Series order method are implemented with variants of these func-\\ntions (which also must take into account missing values)\\nAlternate Sort Algorithms\\nA stable sorting algorithm preserves the relative position of equal elements. This can\\nbe especially important in indirect sorts where the relative ordering is meaningful:\\nIn [186]: values = np.array(['2:first', '2:second', '1:first', '1:second', '1:third'])\\nIn [187]: key = np.array([2, 2, 1, 1, 1])\\nIn [188]: indexer = key.argsort(kind='mergesort')\\nIn [189]: indexer\\nOut[189]: array([2, 3, 4, 0, 1])\\nIn [190]: values.take(indexer)\\nOut[190]: \\narray(['1:first', '1:second', '1:third', '2:first', '2:second'], \\n      dtype='|S8')\\nThe only stable sort available is mergesort which has guaranteed O(n log n) performance\\n(for complexity buffs), but its performance is on average worse than the default\\nMore About Sorting | 375\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"quicksort method. See Table 12-3 for a summary of available methods and their relative\\nperformance (and performance guarantees). This is not something that most users will\\never have to think about but useful to know that it’s there.\\nTable 12-3. Array sorting methods\\nKind Speed Stable Work space Worst-case\\n'quicksort' 1 No 0 O(n2)\\n'mergesort' 2 Yes n / 2 O(n log n)\\n'heapsort' 3 No 0 O(n log n)\\nAt the time of this writing, sort algorithms other than quicksort are not\\navailable on arrays of Python objects (dtype=object). This means occa-\\nsionally that algorithms requiring stable sorting will require work-\\narounds when dealing with Python objects.\\nnumpy.searchsorted: Finding elements in a Sorted Array\\nsearchsorted is an array method that performs a binary search on a sorted array, re-\\nturning the location in the array where the value would need to be inserted to maintain\\nsortedness:\\nIn [191]: arr = np.array([0, 1, 7, 12, 15])\\nIn [192]: arr.searchsorted(9)\\nOut[192]: 3\\nAs you might expect, you can also pass an array of values to get an array of indices back:\\nIn [193]: arr.searchsorted([0, 8, 11, 16])\\nOut[193]: array([0, 3, 3, 5])\\nYou might have noticed that searchsorted returned 0 for the 0 element. This is because\\nthe default behavior is to return the index at the left side of a group of equal values:\\nIn [194]: arr = np.array([0, 0, 0, 1, 1, 1, 1])\\nIn [195]: arr.searchsorted([0, 1])\\nOut[195]: array([0, 3])\\nIn [196]: arr.searchsorted([0, 1], side='right')\\nOut[196]: array([3, 7])\\nAs another application of searchsorted, suppose we had an array of values between 0\\nand 10,000) and a separate array of “bucket edges” that we wanted to use to bin the\\ndata:\\nIn [197]: data = np.floor(np.random.uniform(0, 10000, size=50))\\nIn [198]: bins = np.array([0, 100, 1000, 5000, 10000])\\nIn [199]: data\\n376 | Chapter 12: \\u2002Advanced NumPy\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Out[199]: \\narray([ 8304.,  4181.,  9352.,  4907.,  3250.,  8546.,  2673.,  6152.,\\n        2774.,  5130.,  9553.,  4997.,  1794.,  9688.,   426.,  1612.,\\n         651.,  8653.,  1695.,  4764.,  1052.,  4836.,  8020.,  3479.,\\n        1513.,  5872.,  8992.,  7656.,  4764.,  5383.,  2319.,  4280.,\\n        4150.,  8601.,  3946.,  9904.,  7286.,  9969.,  6032.,  4574.,\\n        8480.,  4298.,  2708.,  7358.,  6439.,  7916.,  3899.,  9182.,\\n         871.,  7973.])\\nTo then get a labeling of which interval each data point belongs to (where 1 would\\nmean the bucket [0, 100)), we can simply use searchsorted:\\nIn [200]: labels = bins.searchsorted(data)\\nIn [201]: labels\\nOut[201]: \\narray([4, 3, 4, 3, 3, 4, 3, 4, 3, 4, 4, 3, 3, 4, 2, 3, 2, 4, 3, 3, 3, 3, 4,\\n       3, 3, 4, 4, 4, 3, 4, 3, 3, 3, 4, 3, 4, 4, 4, 4, 3, 4, 3, 3, 4, 4, 4,\\n       3, 4, 2, 4])\\nThis, combined with pandas’s groupby, can be used to easily bin data:\\nIn [202]: Series(data).groupby(labels).mean()\\nOut[202]: \\n2     649.333333\\n3    3411.521739\\n4    7935.041667\\nNote that NumPy actually has a function digitize that computes this bin labeling:\\nIn [203]: np.digitize(data, bins)\\nOut[203]: \\narray([4, 3, 4, 3, 3, 4, 3, 4, 3, 4, 4, 3, 3, 4, 2, 3, 2, 4, 3, 3, 3, 3, 4,\\n       3, 3, 4, 4, 4, 3, 4, 3, 3, 3, 4, 3, 4, 4, 4, 4, 3, 4, 3, 3, 4, 4, 4,\\n       3, 4, 2, 4])\\nNumPy Matrix Class\\nCompared with other languages for matrix operations and linear algebra, like MAT-\\nLAB, Julia, and GAUSS, NumPy’s linear algebra syntax can often be quite verbose. One\\nreason is that matrix multiplication requires using numpy.dot. Also NumPy’s indexing\\nsemantics are different, which makes porting code to Python less straightforward at\\ntimes. Selecting a single row (e.g. X[1, :]) or column (e.g. X[:, 1]) from a 2D array\\nyields a 1D array compared with a 2D array as in, say, MATLAB.\\nIn [204]: X =  np.array([[ 8.82768214,  3.82222409, -1.14276475,  2.04411587],\\n   .....:                [ 3.82222409,  6.75272284,  0.83909108,  2.08293758],\\n   .....:                [-1.14276475,  0.83909108,  5.01690521,  0.79573241],\\n   .....:                [ 2.04411587,  2.08293758,  0.79573241,  6.24095859]])\\nIn [205]: X[:, 0]  # one-dimensional\\nOut[205]: array([ 8.8277,  3.8222, -1.1428,  2.0441])\\nIn [206]: y = X[:, :1]  # two-dimensional by slicing\\nNumPy Matrix Class | 377\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'In [207]: X\\nOut[207]: \\narray([[ 8.8277,  3.8222, -1.1428,  2.0441],\\n       [ 3.8222,  6.7527,  0.8391,  2.0829],\\n       [-1.1428,  0.8391,  5.0169,  0.7957],\\n       [ 2.0441,  2.0829,  0.7957,  6.241 ]])\\nIn [208]: y\\nOut[208]: \\narray([[ 8.8277],\\n       [ 3.8222],\\n       [-1.1428],\\n       [ 2.0441]])\\nIn this case, the product yT X y would be expressed like so:\\nIn [209]: np.dot(y.T, np.dot(X, y))\\nOut[209]: array([[ 1195.468]])\\nTo aid in writing code with a lot of matrix operations, NumPy has a matrix class which\\nhas modified indexing behavior to make it more MATLAB-like: single rows and col-\\numns come back two-dimensional and multiplication with * is matrix multiplication.\\nThe above operation with numpy.matrix would look like:\\nIn [210]: Xm = np.matrix(X)\\nIn [211]: ym = Xm[:, 0]\\nIn [212]: Xm\\nOut[212]: \\nmatrix([[ 8.8277,  3.8222, -1.1428,  2.0441],\\n        [ 3.8222,  6.7527,  0.8391,  2.0829],\\n        [-1.1428,  0.8391,  5.0169,  0.7957],\\n        [ 2.0441,  2.0829,  0.7957,  6.241 ]])\\nIn [213]: ym\\nOut[213]: \\nmatrix([[ 8.8277],\\n        [ 3.8222],\\n        [-1.1428],\\n        [ 2.0441]])\\nIn [214]: ym.T * Xm * ym\\nOut[214]: matrix([[ 1195.468]])\\nmatrix also has a special attribute I which returns the matrix inverse:\\nIn [215]: Xm.I * X\\nOut[215]: \\nmatrix([[ 1., -0., -0., -0.],\\n        [ 0.,  1.,  0.,  0.],\\n        [ 0.,  0.,  1.,  0.],\\n        [ 0.,  0.,  0.,  1.]])\\n378 | Chapter 12: \\u2002Advanced NumPy\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"I do not recommend using numpy.matrix as a replacement for regular ndarrays because\\nthey are generally more seldom used. In individual functions with lots of linear algebra,\\nit may be helpful to convert the function argument to matrix type, then cast back to\\nregular arrays with np.asarray (which does not copy any data) before returning them.\\nAdvanced Array Input and Output\\nIn Chapter 4, I introduced you to np.save and np.load for storing arrays in binary format\\non disk. There are a number of additional options to consider for more sophisticated\\nuse. In particular, memory maps have the additional benefit of enabling you to work\\nwith data sets that do not fit into RAM.\\nMemory-mapped Files\\nA memory-mapped file is a method for treating potentially very large binary data on\\ndisk as an in-memory array. NumPy implements a memmap object that is ndarray-like,\\nenabling small segments of a large file to be read and written without reading the whole\\narray into memory. Additionally, a memmap has the same methods as an in-memory array\\nand thus can be substituted into many algorithms where an ndarray would be expected.\\nTo create a new memmap, use the function np.memmap and pass a file path, dtype, shape,\\nand file mode:\\nIn [216]: mmap = np.memmap('mymmap', dtype='float64', mode='w+', shape=(10000, 10000))\\nIn [217]: mmap\\nOut[217]: \\nmemmap([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\\n       ..., \\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])\\nSlicing a memmap returns views on the data on disk:\\nIn [218]: section = mmap[:5]\\nIf you assign data to these, it will be buffered in memory (like a Python file object), but\\ncan be written to disk by calling flush:\\nIn [219]: section[:] = np.random.randn(5, 10000)\\nIn [220]: mmap.flush()\\nIn [221]: mmap\\nOut[221]: \\nmemmap([[-0.1614, -0.1768,  0.422 , ..., -0.2195, -0.1256, -0.4012],\\n       [ 0.4898, -2.2219, -0.7684, ..., -2.3517, -1.0782,  1.3208],\\n       [-0.6875,  1.6901, -0.7444, ..., -1.4218, -0.0509,  1.2224],\\nAdvanced Array Input and Output | 379\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"..., \\n       [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\\n       [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\\n       [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ]])\\nIn [222]: del mmap\\nWhenever a memory map falls out of scope and is garbage-collected, any changes will\\nbe flushed to disk also. When opening an existing memory map, you still have to specify\\nthe dtype and shape as the file is just a block of binary data with no metadata on disk:\\nIn [223]: mmap = np.memmap('mymmap', dtype='float64', shape=(10000, 10000))\\nIn [224]: mmap\\nOut[224]: \\nmemmap([[-0.1614, -0.1768,  0.422 , ..., -0.2195, -0.1256, -0.4012],\\n       [ 0.4898, -2.2219, -0.7684, ..., -2.3517, -1.0782,  1.3208],\\n       [-0.6875,  1.6901, -0.7444, ..., -1.4218, -0.0509,  1.2224],\\n       ..., \\n       [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\\n       [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\\n       [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ]])\\nSince a memory map is just an on-disk ndarray, there are no issues using a structured\\ndtype as described above.\\nHDF5 and Other Array Storage Options\\nPyTables and h5py are two Python projects providing NumPy-friendly interfaces for\\nstoring array data in the efficient and compressible HDF5 format (HDF stands for\\nhierarchical data format). You can safely store hundreds of gigabytes or even terabytes\\nof data in HDF5 format. The use of these libraries is unfortunately outside the scope\\nof the book.\\nPyTables provides a rich facility for working with structured arrays with advanced\\nquerying features and the ability to add column indexes to accelerate queries. This is\\nvery similar to the table indexing capabilities provided by relational databases.\\nPerformance Tips\\nGetting good performance out of code utilizing NumPy is often straightforward, as\\narray operations typically replace otherwise comparatively extremely slow pure Python\\nloops. Here is a brief list of some of the things to keep in mind:\\n• Convert Python loops and conditional logic to array operations and boolean array\\noperations\\n• Use broadcasting whenever possible\\n• Avoid copying data using array views (slicing)\\n• Utilize ufuncs and ufunc methods\\n380 | Chapter 12: \\u2002Advanced NumPy\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"If you can’t get the performance you require after exhausting the capabilities provided\\nby NumPy alone, writing code in C, Fortran, or especially Cython (see a bit more on\\nthis below) may be in order. I personally use Cython (http://cython.org) heavily in my\\nown work as an easy way to get C-like performance with minimal development.\\nThe Importance of Contiguous Memory\\nWhile the full extent of this topic is a bit outside the scope of this book, in some ap-\\nplications the memory layout of an array can significantly affect the speed of compu-\\ntations. This is based partly on performance differences having to do with the cache\\nhierarchy of the CPU; operations accessing contiguous blocks of memory (for example,\\nsumming the rows of a C order array) will generally be the fastest because the memory\\nsubsystem will buffer the appropriate blocks of memory into the ultrafast L1 or L2 CPU\\ncache. Also, certain code paths inside NumPy’s C codebase have been optimized for\\nthe contiguous case in which generic strided memory access can be avoided.\\nTo say that an array’s memory layout is contiguous means that the elements are stored\\nin memory in the order that they appear in the array with respect to Fortran (column\\nmajor) or C (row major) ordering. By default, NumPy arrays are created as C-contigu-\\nous or just simply contiguous. A column major array, such as the transpose of a C-\\ncontiguous array, is thus said to be Fortran-contiguous. These properties can be ex-\\nplicitly checked via the flags attribute on the ndarray:\\nIn [227]: arr_c = np.ones((1000, 1000), order='C')\\nIn [228]: arr_f = np.ones((1000, 1000), order='F')\\nIn [229]: arr_c.flags         In [230]: arr_f.flags \\nOut[229]:                     Out[230]:             \\n  C_CONTIGUOUS : True           C_CONTIGUOUS : False\\n  F_CONTIGUOUS : False          F_CONTIGUOUS : True \\n  OWNDATA : True                OWNDATA : True      \\n  WRITEABLE : True              WRITEABLE : True    \\n  ALIGNED : True                ALIGNED : True      \\n  UPDATEIFCOPY : False          UPDATEIFCOPY : False\\n                                                    \\nIn [231]: arr_f.flags.f_contiguous\\nOut[231]: True\\nIn this example, summing the rows of these arrays should, in theory, be faster for\\narr_c than arr_f since the rows are contiguous in memory. Here I check for sure using\\n%timeit in IPython:\\nIn [232]: %timeit arr_c.sum(1)\\n1000 loops, best of 3: 1.33 ms per loop\\nIn [233]: %timeit arr_f.sum(1)\\n100 loops, best of 3: 8.75 ms per loop\\nPerformance Tips | 381\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"When looking to squeeze more performance out of NumPy, this is often a place to\\ninvest some effort. If you have an array that does not have the desired memory order,\\nyou can use copy and pass either 'C' or 'F':\\nIn [234]: arr_f.copy('C').flags\\nOut[234]: \\n  C_CONTIGUOUS : True\\n  F_CONTIGUOUS : False\\n  OWNDATA : True\\n  WRITEABLE : True\\n  ALIGNED : True\\n  UPDATEIFCOPY : False\\nWhen constructing a view on an array, keep in mind that the result is not guaranteed\\nto be contiguous:\\nIn [235]: arr_c[:50].flags.contiguous      In [236]: arr_c[:, :50].flags\\nOut[235]: True                             Out[236]:                    \\n                                             C_CONTIGUOUS : False       \\n                                             F_CONTIGUOUS : False       \\n                                             OWNDATA : False            \\n                                             WRITEABLE : True           \\n                                             ALIGNED : True             \\n                                             UPDATEIFCOPY : False\\nOther Speed Options: Cython, f2py, C\\nIn recent years, the Cython project (( http://cython.org) has become the tool of choice\\nfor many scientific Python programmers for implementing fast code that may need to\\ninteract with C or C++ libraries, but without having to write pure C code. You can\\nthink of Cython as Python with static types and the ability to interleave functions im-\\nplemented in C into Python-like code. For example, a simple Cython function to sum\\nthe elements of a one-dimensional array might look like:\\nfrom numpy cimport ndarray, float64_t\\ndef sum_elements(ndarray[float64_t] arr):\\n    cdef Py_ssize_t i, n = len(arr)\\n    cdef float64_t result = 0\\n    for i in range(n):\\n        result += arr[i]\\n    return result\\nCython takes this code, translates it to C, then compiles the generated C code to create\\na Python extension. Cython is an attractive option for performance computing because\\nthe code is only slightly more time-consuming to write than pure Python code and it\\nintegrates closely with NumPy. A common workflow is to get an algorithm working in\\nPython, then translate it to Cython by adding type declarations and a handful of other\\ntweaks. For more, see the project documentation.\\n382 | Chapter 12: \\u2002Advanced NumPy\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Some other options for writing high performance code with NumPy include f2py, a\\nwrapper generator for Fortran 77 and 90 code, and writing pure C extensions.\\nPerformance Tips | 383\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf', 'page_content': 'www.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'APPENDIX\\nPython Language Essentials\\nKnowledge is a treasure, but practice is the key to it.\\n—Thomas Fuller\\nPeople often ask me about good resources for learning Python for data-centric appli-\\ncations. While there are many excellent Python language books, I am usually hesitant\\nto recommend some of them as they are intended for a general audience rather than\\ntailored for someone who wants to load in some data sets, do some computations, and\\nplot some of the results. There are actually a couple of books on “scientific program-\\nming in Python”, but they are geared toward numerical computing and engineering\\napplications: solving differential equations, computing integrals, doing Monte Carlo\\nsimulations, and various topics that are more mathematically-oriented rather than be-\\ning about data analysis and statistics. As this is a book about becoming proficient at\\nworking with data in Python, I think it is valuable to spend some time highlighting the\\nmost important features of Python’s built-in data structures and libraries from the per-\\nspective of processing and manipulating structured and unstructured data. As such, I\\nwill only present roughly enough information to enable you to follow along with the\\nrest of the book.\\nThis chapter is not intended to be an exhaustive introduction to the Python language\\nbut rather a biased, no-frills overview of features which are used repeatedly throughout\\nthis book. For new Python programmers, I recommend that you supplement this chap-\\nter with the official Python tutorial ( http://docs.python.org) and potentially one of the\\nmany excellent (and much longer) books on general purpose Python programming. In\\nmy opinion, it is not necessary to become proficient at building good software in Python\\nto be able to productively do data analysis. I encourage you to use IPython to experi-\\nment with the code examples and to explore the documentation for the various types,\\nfunctions, and methods. Note that some of the code used in the examples may not\\nnecessarily be fully-introduced at this point.\\nMuch of this book focuses on high performance array-based computing tools for work-\\ning with large data sets. In order to use those tools you must often first do some munging\\nto corral messy data into a more nicely structured form. Fortunately, Python is one of\\n385\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'the easiest-to-use languages for rapidly whipping your data into shape. The greater your\\nfacility with Python, the language, the easier it will be for you to prepare new data sets\\nfor analysis.\\nThe Python Interpreter\\nPython is an interpreted language. The Python interpreter runs a program by executing\\none statement at a time. The standard interactive Python interpreter can be invoked on\\nthe command line with the python command:\\n$ python\\nPython 2.7.2 (default, Oct  4 2011, 20:06:09)\\n[GCC 4.6.1] on linux2\\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\\n>>> a = 5\\n>>> print a\\n5\\nThe >>> you see is the prompt where you’ll type expressions. To exit the Python inter-\\npreter and return to the command prompt, you can either type exit() or press Ctrl-D.\\nRunning Python programs is as simple as calling python with a .py file as its first argu-\\nment. Suppose we had created hello_world.py with these contents:\\nprint \\'Hello world\\'\\nThis can be run from the terminal simply as:\\n$ python hello_world.py\\nHello world\\nWhile many Python programmers execute all of their Python code in this way, many\\nscientific Python programmers make use of IPython, an enhanced interactive Python\\ninterpreter. Chapter 3 is dedicated to the IPython system. By using the %run command,\\nIPython executes the code in the specified file in the same process, enabling you to\\nexplore the results interactively when it’s done.\\n$ ipython\\nPython 2.7.2 |EPD 7.1-2 (64-bit)| (default, Jul  3 2011, 15:17:51)\\nType \"copyright\", \"credits\" or \"license\" for more information.\\nIPython 0.12 -- An enhanced Interactive Python.\\n?         -> Introduction and overview of IPython\\'s features.\\n%quickref -> Quick reference.\\nhelp      -> Python\\'s own help system.\\nobject?   -> Details about \\'object\\', use \\'object??\\' for extra details.\\nIn [1]: %run hello_world.py\\nHello world\\nIn [2]:\\n386 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'The default IPython prompt adopts the numbered In [2]: style compared with the\\nstandard >>> prompt.\\nThe Basics\\nLanguage Semantics\\nThe Python language design is distinguished by its emphasis on readability, simplicity,\\nand explicitness. Some people go so far as to liken it to “executable pseudocode”.\\nIndentation, not braces\\nPython uses whitespace (tabs or spaces) to structure code instead of using braces as in\\nmany other languages like R, C++, Java, and Perl. Take the for loop in the above\\nquicksort algorithm:\\nfor x in array:\\n    if x < pivot:\\n        less.append(x)\\n    else:\\n        greater.append(x)\\nA colon denotes the start of an indented code block after which all of the code must be\\nindented by the same amount until the end of the block. In another language, you might\\ninstead have something like:\\nfor x in array {\\n        if x < pivot {\\n            less.append(x)\\n        } else {\\n            greater.append(x)\\n        }\\n    }\\nOne major reason that whitespace matters is that it results in most Python code looking\\ncosmetically similar, which means less cognitive dissonance when you read a piece of\\ncode that you didn’t write yourself (or wrote in a hurry a year ago!). In a language\\nwithout significant whitespace, you might stumble on some differently formatted code\\nlike:\\nfor x in array\\n    {\\n      if x < pivot\\n      {\\n        less.append(x)\\n      }\\n      else\\n      {\\n        greater.append(x)\\nThe Basics | 387\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"}\\n    }\\nLove it or hate it, significant whitespace is a fact of life for Python programmers, and\\nin my experience it helps make Python code a lot more readable than other languages\\nI’ve used. While it may seem foreign at first, I suspect that it will grow on you after a\\nwhile.\\nI strongly recommend that you use 4 spaces to as your default indenta-\\ntion and that your editor replace tabs with 4 spaces. Many text editors\\nhave a setting that will replace tab stops with spaces automatically (do\\nthis!). Some people use tabs or a different number of spaces, with 2\\nspaces not being terribly uncommon. 4 spaces is by and large the stan-\\ndard adopted by the vast majority of Python programmers, so I recom-\\nmend doing that in the absence of a compelling reason otherwise.\\nAs you can see by now, Python statements also do not need to be terminated by sem-\\nicolons. Semicolons can be used, however, to separate multiple statements on a single\\nline:\\na = 5; b = 6; c = 7\\nPutting multiple statements on one line is generally discouraged in Python as it often\\nmakes code less readable.\\nEverything is an object\\nAn important characteristic of the Python language is the consistency of its object\\nmodel. Every number, string, data structure, function, class, module, and so on exists\\nin the Python interpreter in its own “box” which is referred to as a Python object. Each\\nobject has an associated type (for example, string or function) and internal data. In\\npractice this makes the language very flexible, as even functions can be treated just like\\nany other object.\\nComments\\nAny text preceded by the hash mark (pound sign) # is ignored by the Python interpreter.\\nThis is often used to add comments to code. At times you may also want to exclude\\ncertain blocks of code without deleting them. An easy solution is to comment out the\\ncode:\\nresults = []\\nfor line in file_handle:\\n    # keep the empty lines for now\\n    # if len(line) == 0:\\n    #   continue\\n    results.append(line.replace('foo', 'bar'))\\n388 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Function and object method calls\\nFunctions are called using parentheses and passing zero or more arguments, optionally\\nassigning the returned value to a variable:\\nresult = f(x, y, z)\\ng()\\nAlmost every object in Python has attached functions, known as methods, that have\\naccess to the object’s internal contents. They can be called using the syntax:\\nobj.some_method(x, y, z)\\nFunctions can take both positional and keyword arguments:\\nresult = f(a, b, c, d=5, e='foo')\\nMore on this later.\\nVariables and pass-by-reference\\nWhen assigning a variable (or name) in Python, you are creating a reference to the object\\non the right hand side of the equals sign. In practical terms, consider a list of integers:\\nIn [241]: a = [1, 2, 3]\\nSuppose we assign a to a new variable b:\\nIn [242]: b = a\\nIn some languages, this assignment would cause the data [1, 2, 3] to be copied. In\\nPython, a and b actually now refer to the same object, the original list [1, 2, 3] (see\\nFigure A-1 for a mockup). You can prove this to yourself by appending an element to\\na and then examining b:\\nIn [243]: a.append(4)\\nIn [244]: b\\nOut[244]: [1, 2, 3, 4]\\nFigure A-1. Two references for the same object\\nUnderstanding the semantics of references in Python and when, how, and why data is\\ncopied is especially critical when working with larger data sets in Python.\\nThe Basics | 389\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Assignment is also referred to as binding, as we are binding a name to\\nan object. Variables names that have been assigned may occasionally be\\nreferred to as bound variables.\\nWhen you pass objects as arguments to a function, you are only passing references; no\\ncopying occurs. Thus, Python is said to pass by reference, whereas some other languages\\nsupport both pass by value (creating copies) and pass by reference. This means that a\\nfunction can mutate the internals of its arguments. Suppose we had the following func-\\ntion:\\ndef append_element(some_list, element):\\n    some_list.append(element)\\nThen given what’s been said, this should not come as a surprise:\\nIn [2]: data = [1, 2, 3]\\nIn [3]: append_element(data, 4)\\nIn [4]: data\\nOut[4]: [1, 2, 3, 4]\\nDynamic references, strong types\\nIn contrast with many compiled languages, such as Java and C++, object references in\\nPython have no type associated with them. There is no problem with the following:\\nIn [245]: a = 5        In [246]: type(a)\\n                       Out[246]: int    \\n                                        \\nIn [247]: a = 'foo'    In [248]: type(a)\\n                       Out[248]: str\\nVariables are names for objects within a particular namespace; the type information is\\nstored in the object itself. Some observers might hastily conclude that Python is not a\\n“typed language”. This is not true; consider this example:\\nIn [249]: '5' + 5\\n---------------------------------------------------------------------------\\nTypeError                                 Traceback (most recent call last)\\n<ipython-input-249-f9dbf5f0b234> in <module>()\\n----> 1 '5' + 5\\nTypeError: cannot concatenate 'str' and 'int' objects\\nIn some languages, such as Visual Basic, the string '5' might get implicitly converted\\n(or casted) to an integer, thus yielding 10. Yet in other languages, such as JavaScript,\\nthe integer 5 might be casted to a string, yielding the concatenated string '55'. In this\\nregard Python is considered a strongly-typed language, which means that every object\\nhas a specific type (or class), and implicit conversions will occur only in certain obvious\\ncircumstances, such as the following:\\n390 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [250]: a = 4.5\\nIn [251]: b = 2\\n# String formatting, to be visited later\\nIn [252]: print 'a is %s, b is %s' % (type(a), type(b))\\na is <type 'float'>, b is <type 'int'>\\nIn [253]: a / b\\nOut[253]: 2.25\\nKnowing the type of an object is important, and it’s useful to be able to write functions\\nthat can handle many different kinds of input. You can check that an object is an\\ninstance of a particular type using the isinstance function:\\nIn [254]: a = 5        In [255]: isinstance(a, int)\\n                       Out[255]: True\\nisinstance can accept a tuple of types if you want to check that an object’s type is\\namong those present in the tuple:\\nIn [256]: a = 5; b = 4.5\\nIn [257]: isinstance(a, (int, float))      In [258]: isinstance(b, (int, float))\\nOut[257]: True                             Out[258]: True\\nAttributes and methods\\nObjects in Python typically have both attributes, other Python objects stored “inside”\\nthe object, and methods, functions associated with an object which can have access to\\nthe object’s internal data. Both of them are accessed via the syntax obj.attribute_name:\\nIn [1]: a = 'foo'\\nIn [2]: a.<Tab>\\na.capitalize  a.format      a.isupper     a.rindex      a.strip\\na.center      a.index       a.join        a.rjust       a.swapcase\\na.count       a.isalnum     a.ljust       a.rpartition  a.title\\na.decode      a.isalpha     a.lower       a.rsplit      a.translate\\na.encode      a.isdigit     a.lstrip      a.rstrip      a.upper\\na.endswith    a.islower     a.partition   a.split       a.zfill\\na.expandtabs  a.isspace     a.replace     a.splitlines\\na.find        a.istitle     a.rfind       a.startswith\\nAttributes and methods can also be accessed by name using the getattr function:\\n>>> getattr(a, 'split')\\n<function split>\\nWhile we will not extensively use the functions getattr and related functions hasattr\\nand setattr in this book, they can be used very effectively to write generic, reusable\\ncode.\\nThe Basics | 391\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"“Duck” typing\\nOften you may not care about the type of an object but rather only whether it has certain\\nmethods or behavior. For example, you can verify that an object is iterable if it imple-\\nmented the iterator protocol. For many objects, this means it has a __iter__ “magic\\nmethod”, though an alternative and better way to check is to try using the iter function:\\ndef isiterable(obj):\\n    try:\\n        iter(obj)\\n        return True\\n    except TypeError: # not iterable\\n        return False\\nThis function would return True for strings as well as most Python collection types:\\nIn [260]: isiterable('a string')        In [261]: isiterable([1, 2, 3])\\nOut[260]: True                          Out[261]: True                 \\n                                                                       \\nIn [262]: isiterable(5)\\nOut[262]: False\\nA place where I use this functionality all the time is to write functions that can accept\\nmultiple kinds of input. A common case is writing a function that can accept any kind\\nof sequence (list, tuple, ndarray) or even an iterator. You can first check if the object is\\na list (or a NumPy array) and, if it is not, convert it to be one:\\nif not isinstance(x, list) and isiterable(x):\\n    x = list(x)\\nImports\\nIn Python a module is simply a .py file containing function and variable definitions\\nalong with such things imported from other .py files. Suppose that we had the following\\nmodule:\\n# some_module.py\\nPI = 3.14159\\ndef f(x):\\n    return x + 2\\ndef g(a, b):\\n    return a + b\\nIf we wanted to access the variables and functions defined in some_module.py, from\\nanother file in the same directory we could do:\\nimport some_module\\nresult = some_module.f(5)\\npi = some_module.PI\\nOr equivalently:\\nfrom some_module import f, g, PI\\nresult = g(5, PI)\\n392 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'By using the as keyword you can give imports different variable names:\\nimport some_module as sm\\nfrom some_module import PI as pi, g as gf\\nr1 = sm.f(pi)\\nr2 = gf(6, pi)\\nBinary operators and comparisons\\nMost of the binary math operations and comparisons are as you might expect:\\nIn [263]: 5 - 7        In [264]: 12 + 21.5\\nOut[263]: -2           Out[264]: 33.5     \\n                                          \\nIn [265]: 5 <= 2\\nOut[265]: False\\nSee Table A-1 for all of the available binary operators.\\nTo check if two references refer to the same object, use the is keyword. is not is also\\nperfectly valid if you want to check that two objects are not the same:\\nIn [266]: a = [1, 2, 3]\\nIn [267]: b = a\\n# Note, the list function always creates a new list\\nIn [268]: c = list(a)\\nIn [269]: a is b        In [270]: a is not c\\nOut[269]: True          Out[270]: True\\nNote this is not the same thing is comparing with ==, because in this case we have:\\nIn [271]: a == c\\nOut[271]: True\\nA very common use of is and is not is to check if a variable is None, since there is only\\none instance of None:\\nIn [272]: a = None\\nIn [273]: a is None\\nOut[273]: True\\nTable A-1. Binary operators\\nOperation Description\\na + b Add a and b\\na - b Subtract b from a\\na * b Multiply a by b\\na / b Divide a by b\\na // b Floor-divide a by b, dropping any fractional remainder\\nThe Basics | 393\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Operation Description\\na ** b Raise a to the b power\\na & b True if both a and b are True. For integers, take the bitwise AND.\\na | b True if either a or b is True. For integers, take the bitwise OR.\\na ^ b For booleans, True if a or b is True, but not both. For integers, take the bitwise EXCLUSIVE-OR.\\na == b True if a equals b\\na != b True if a is not equal to b\\na <= b, a < b True if a is less than (less than or equal) to b\\na > b, a >= b True if a is greater than (greater than or equal) to b\\na is b True if a and b reference same Python object\\na is not b True if a and b reference different Python objects\\nStrictness versus laziness\\nWhen using any programming language, it’s important to understand when expressions\\nare evaluated. Consider the simple expression:\\na = b = c = 5\\nd = a + b * c\\nIn Python, once these statements are evaluated, the calculation is immediately (or\\nstrictly) carried out, setting the value of d to 30. In another programming paradigm,\\nsuch as in a pure functional programming language like Haskell, the value of d might\\nnot be evaluated until it is actually used elsewhere. The idea of deferring computations\\nin this way is commonly known as lazy evaluation. Python, on the other hand, is a very \\nstrict (or eager) language. Nearly all of the time, computations and expressions are\\nevaluated immediately. Even in the above simple expression, the result of b * c is\\ncomputed as a separate step before adding it to a.\\nThere are Python techniques, especially using iterators and generators, which can be\\nused to achieve laziness. When performing very expensive computations which are only\\nnecessary some of the time, this can be an important technique in data-intensive ap-\\nplications.\\nMutable and immutable objects\\nMost objects in Python are mutable, such as lists, dicts, NumPy arrays, or most user-\\ndefined types (classes). This means that the object or values that they contain can be\\nmodified.\\nIn [274]: a_list = ['foo', 2, [4, 5]]\\nIn [275]: a_list[2] = (3, 4)\\nIn [276]: a_list\\nOut[276]: ['foo', 2, (3, 4)]\\n394 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Others, like strings and tuples, are immutable:\\nIn [277]: a_tuple = (3, 5, (4, 5))\\nIn [278]: a_tuple[1] = 'four'\\n---------------------------------------------------------------------------\\nTypeError                                 Traceback (most recent call last)\\n<ipython-input-278-b7966a9ae0f1> in <module>()\\n----> 1 a_tuple[1] = 'four'\\nTypeError: 'tuple' object does not support item assignment\\nRemember that just because you can mutate an object does not mean that you always\\nshould. Such actions are known in programming as side effects. For example, when\\nwriting a function, any side effects should be explicitly communicated to the user in\\nthe function’s documentation or comments. If possible, I recommend trying to avoid\\nside effects and favor immutability, even though there may be mutable objects involved.\\nScalar Types\\nPython has a small set of built-in types for handling numerical data, strings, boolean\\n(True or False) values, and dates and time. See Table A-2 for a list of the main scalar\\ntypes. Date and time handling will be discussed separately as these are provided by the \\ndatetime module in the standard library.\\nTable A-2. Standard Python Scalar Types\\nType Description\\nNone The Python “null” value (only one instance of the None object exists)\\nstr String type. ASCII-valued only in Python 2.x and Unicode in Python 3\\nunicode Unicode string type\\nfloat Double-precision (64-bit) floating point number. Note there is no separate double type.\\nbool A True or False value\\nint Signed integer with maximum value determined by the platform.\\nlong Arbitrary precision signed integer. Large int values are automatically converted to long.\\nNumeric types\\nThe primary Python types for numbers are int and float. The size of the integer which\\ncan be stored as an int is dependent on your platform (whether 32 or 64-bit), but Python\\nwill transparently convert a very large integer to long, which can store arbitrarily large\\nintegers.\\nIn [279]: ival = 17239871\\nIn [280]: ival ** 6\\nOut[280]: 26254519291092456596965462913230729701102721L\\nThe Basics | 395\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Floating point numbers are represented with the Python float type. Under the hood\\neach one is a double-precision (64 bits) value. They can also be expressed using scien-\\ntific notation:\\nIn [281]: fval = 7.243\\nIn [282]: fval2 = 6.78e-5\\nIn Python 3, integer division not resulting in a whole number will always yield a floating\\npoint number:\\nIn [284]: 3 / 2\\nOut[284]: 1.5\\nIn Python 2.7 and below (which some readers will likely be using), you can enable this\\nbehavior by default by putting the following cryptic-looking statement at the top of\\nyour module:\\nfrom __future__ import division\\nWithout this in place, you can always explicitly convert the denominator into a floating\\npoint number:\\nIn [285]: 3 / float(2)\\nOut[285]: 1.5\\nTo get C-style integer division (which drops the fractional part if the result is not a\\nwhole number), use the floor division operator //:\\nIn [286]: 3 // 2\\nOut[286]: 1\\nComplex numbers are written using j for the imaginary part:\\nIn [287]: cval = 1 + 2j\\nIn [288]: cval * (1 - 2j)\\nOut[288]: (5+0j)\\nStrings\\nMany people use Python for its powerful and flexible built-in string processing capa-\\nbilities. You can write string literal using either single quotes \\' or double quotes \":\\na = \\'one way of writing a string\\'\\nb = \"another way\"\\nFor multiline strings with line breaks, you can use triple quotes, either \\'\\'\\' or \"\"\":\\nc = \"\"\"\\nThis is a longer string that\\nspans multiple lines\\n\"\"\"\\nPython strings are immutable; you cannot modify a string without creating a new string:\\n396 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [289]: a = 'this is a string'\\nIn [290]: a[10] = 'f'\\n---------------------------------------------------------------------------\\nTypeError                                 Traceback (most recent call last)\\n<ipython-input-290-5ca625d1e504> in <module>()\\n----> 1 a[10] = 'f'\\nTypeError: 'str' object does not support item assignment\\nIn [291]: b = a.replace('string', 'longer string')\\nIn [292]: b\\nOut[292]: 'this is a longer string'\\nMany Python objects can be converted to a string using the str function:\\nIn [293]: a = 5.6        In [294]: s = str(a)\\n                                             \\nIn [295]: s\\nOut[295]: '5.6'\\nStrings are a sequence of characters and therefore can be treated like other sequences,\\nsuch as lists and tuples:\\nIn [296]: s = 'python'        In [297]: list(s)                       \\n                              Out[297]: ['p', 'y', 't', 'h', 'o', 'n']\\n                                                                      \\nIn [298]: s[:3]\\nOut[298]: 'pyt'\\nThe backslash character \\\\ is an escape character, meaning that it is used to specify\\nspecial characters like newline \\\\n or unicode characters. To write a string literal with\\nbackslashes, you need to escape them:\\nIn [299]: s = '12\\\\\\\\34'\\nIn [300]: print s\\n12\\\\34\\nIf you have a string with a lot of backslashes and no special characters, you might find\\nthis a bit annoying. Fortunately you can preface the leading quote of the string with r\\nwhich means that the characters should be interpreted as is:\\nIn [301]: s = r'this\\\\has\\\\no\\\\special\\\\characters'\\nIn [302]: s\\nOut[302]: 'this\\\\\\\\has\\\\\\\\no\\\\\\\\special\\\\\\\\characters'\\nAdding two strings together concatenates them and produces a new string:\\nIn [303]: a = 'this is the first half '\\nIn [304]: b = 'and this is the second half'\\nIn [305]: a + b\\nOut[305]: 'this is the first half and this is the second half'\\nThe Basics | 397\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"String templating or formatting is another important topic. The number of ways to do\\nso has expanded with the advent of Python 3, here I will briefly describe the mechanics\\nof one of the main interfaces. Strings with a % followed by one or more format characters\\nis a target for inserting a value into that string (this is quite similar to the printf function\\nin C). As an example, consider this string:\\nIn [306]: template = '%.2f %s are worth $%d'\\nIn this string, %s means to format an argument as a string, %.2f a number with 2 decimal\\nplaces, and %d an integer. To substitute arguments for these format parameters, use the\\nbinary operator % with a tuple of values:\\nIn [307]: template % (4.5560, 'Argentine Pesos', 1)\\nOut[307]: '4.56 Argentine Pesos are worth $1'\\nString formatting is a broad topic; there are multiple methods and numerous options\\nand tweaks available to control how values are formatted in the resulting string. To\\nlearn more, I recommend you seek out more information on the web.\\nI discuss general string processing as it relates to data analysis in more detail in Chap-\\nter 7.\\nBooleans\\nThe two boolean values in Python are written as True and False. Comparisons and\\nother conditional expressions evaluate to either True or False. Boolean values are com-\\nbined with the and and or keywords:\\nIn [308]: True and True\\nOut[308]: True\\nIn [309]: False or True\\nOut[309]: True\\nAlmost all built-in Python tops and any class defining the __nonzero__ magic method\\nhave a True or False interpretation in an if statement:\\nIn [310]: a = [1, 2, 3]\\n   .....: if a:\\n   .....:     print 'I found something!'\\n   .....:\\nI found something!\\nIn [311]: b = []\\n   .....: if not b:\\n   .....:     print 'Empty!'\\n   .....:\\nEmpty!\\nMost objects in Python have a notion of true- or falseness. For example, empty se-\\nquences (lists, dicts, tuples, etc.) are treated as False if used in control flow (as above\\nwith the empty list b). You can see exactly what boolean value an object coerces to by\\ninvoking bool on it:\\n398 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [312]: bool([]), bool([1, 2, 3])\\nOut[312]: (False, True)\\nIn [313]: bool('Hello world!'), bool('')\\nOut[313]: (True, False)\\nIn [314]: bool(0), bool(1)\\nOut[314]: (False, True)\\nType casting\\nThe str, bool, int and float types are also functions which can be used to cast values\\nto those types:\\nIn [315]: s = '3.14159'\\nIn [316]: fval = float(s)        In [317]: type(fval)\\n                                 Out[317]: float     \\n                                                     \\nIn [318]: int(fval)        In [319]: bool(fval)        In [320]: bool(0)\\nOut[318]: 3                Out[319]: True              Out[320]: False\\nNone\\nNone is the Python null value type. If a function does not explicitly return a value, it\\nimplicitly returns None.\\nIn [321]: a = None      In [322]: a is None\\n                        Out[322]: True     \\n                                           \\nIn [323]: b = 5         In [324]: b is not None\\n                        Out[324]: True\\nNone is also a common default value for optional function arguments:\\ndef add_and_maybe_multiply(a, b, c=None):\\n    result = a + b\\n    if c is not None:\\n        result = result * c\\n    return result\\nWhile a technical point, it’s worth bearing in mind that None is not a reserved keyword\\nbut rather a unique instance of NoneType.\\nDates and times\\nThe built-in Python datetime module provides datetime, date, and time types. The\\ndatetime type as you may imagine combines the information stored in date and time\\nand is the most commonly used:\\nIn [325]: from datetime import datetime, date, time\\nIn [326]: dt = datetime(2011, 10, 29, 20, 30, 21)\\nThe Basics | 399\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [327]: dt.day    In [328]: dt.minute\\nOut[327]: 29        Out[328]: 30\\nGiven a datetime instance, you can extract the equivalent date and time objects by\\ncalling methods on the datetime of the same name:\\nIn [329]: dt.date()                      In [330]: dt.time()                \\nOut[329]: datetime.date(2011, 10, 29)    Out[330]: datetime.time(20, 30, 21)\\nThe strftime method formats a datetime as a string:\\nIn [331]: dt.strftime('%m/%d/%Y %H:%M')\\nOut[331]: '10/29/2011 20:30'\\nStrings can be converted (parsed) into datetime objects using the strptime function:\\nIn [332]: datetime.strptime('20091031', '%Y%m%d')\\nOut[332]: datetime.datetime(2009, 10, 31, 0, 0)\\nSee Table 10-2 for a full list of format specifications.\\nWhen aggregating of otherwise grouping time series data, it will occasionally be useful\\nto replace fields of a series of datetimes, for example replacing the minute and second\\nfields with zero, producing a new object:\\nIn [333]: dt.replace(minute=0, second=0)\\nOut[333]: datetime.datetime(2011, 10, 29, 20, 0)\\nThe difference of two datetime objects produces a datetime.timedelta type:\\nIn [334]: dt2 = datetime(2011, 11, 15, 22, 30)\\nIn [335]: delta = dt2 - dt\\nIn [336]: delta                             In [337]: type(delta)       \\nOut[336]: datetime.timedelta(17, 7179)      Out[337]: datetime.timedelta\\nAdding a timedelta to a datetime produces a new shifted datetime:\\nIn [338]: dt\\nOut[338]: datetime.datetime(2011, 10, 29, 20, 30, 21)\\nIn [339]: dt + delta\\nOut[339]: datetime.datetime(2011, 11, 15, 22, 30)\\nControl Flow\\nif, elif, and else\\nThe if statement is one of the most well-known control flow statement types. It checks\\na condition which, if True, evaluates the code in the block that follows:\\nif x < 0:\\n    print 'It's negative'\\n400 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"An if statement can be optionally followed by one or more elif blocks and a catch-all\\nelse block if all of the conditions are False:\\nif x < 0:\\n    print 'It's negative'\\nelif x == 0:\\n    print 'Equal to zero'\\nelif 0 < x < 5:\\n    print 'Positive but smaller than 5'\\nelse:\\n    print 'Positive and larger than or equal to 5'\\nIf any of the conditions is True, no further elif or else blocks will be reached. With a\\ncompound condition using and or or, conditions are evaluated left-to-right and will\\nshort circuit:\\nIn [340]: a = 5; b = 7\\nIn [341]: c = 8; d = 4\\nIn [342]: if a < b or c > d:\\n   .....:     print 'Made it'\\nMade it\\nIn this example, the comparison c > d never gets evaluated because the first comparison\\nwas True.\\nfor loops\\nfor loops are for iterating over a collection (like a list or tuple) or an iterater. The\\nstandard syntax for a for loop is:\\nfor value in collection:\\n    # do something with value\\nA for loop can be advanced to the next iteration, skipping the remainder of the block,\\nusing the continue keyword. Consider this code which sums up integers in a list and\\nskips None values:\\nsequence = [1, 2, None, 4, None, 5]\\ntotal = 0\\nfor value in sequence:\\n    if value is None:\\n        continue\\n    total += value\\nA for loop can be exited altogether using the break keyword. This code sums elements\\nof the list until a 5 is reached:\\nsequence = [1, 2, 0, 4, 6, 5, 2, 1]\\ntotal_until_5 = 0\\nfor value in sequence:\\n    if value == 5:\\n        break\\n    total_until_5 += value\\nThe Basics | 401\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"As we will see in more detail, if the elements in the collection or iterator are sequences\\n(tuples or lists, say), they can be conveniently unpacked into variables in the for loop\\nstatement:\\nfor a, b, c in iterator:\\n    # do something\\nwhile loops\\nA while loop specifies a condition and a block of code that is to be executed until the\\ncondition evaluates to False or the loop is explicitly ended with break:\\nx = 256\\ntotal = 0\\nwhile x > 0:\\n    if total > 500:\\n        break\\n    total += x\\n    x = x // 2\\npass\\npass is the “no-op” statement in Python. It can be used in blocks where no action is to\\nbe taken; it is only required because Python uses whitespace to delimit blocks:\\nif x < 0:\\n    print 'negative!'\\nelif x == 0:\\n    # TODO: put something smart here\\n    pass\\nelse:\\n    print 'positive!'\\nIt’s common to use pass as a place-holder in code while working on a new piece of\\nfunctionality:\\ndef f(x, y, z):\\n    # TODO: implement this function!\\n    pass\\nException handling\\nHandling Python errors or exceptions gracefully is an important part of building robust\\nprograms. In data analysis applications, many functions only work on certain kinds of\\ninput. As an example, Python’s float function is capable of casting a string to a floating\\npoint number, but fails with ValueError on improper inputs:\\nIn [343]: float('1.2345')\\nOut[343]: 1.2345\\nIn [344]: float('something')\\n---------------------------------------------------------------------------\\nValueError                                Traceback (most recent call last)\\n<ipython-input-344-439904410854> in <module>()\\n402 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"----> 1 float('something')\\nValueError: could not convert string to float: something\\nSuppose we wanted a version of float that fails gracefully, returning the input argu-\\nment. We can do this by writing a function that encloses the call to float in a try/\\nexcept block:\\ndef attempt_float(x):\\n    try:\\n        return float(x)\\n    except:\\n        return x\\nThe code in the except part of the block will only be executed if float(x) raises an\\nexception:\\nIn [346]: attempt_float('1.2345')\\nOut[346]: 1.2345\\nIn [347]: attempt_float('something')\\nOut[347]: 'something'\\nYou might notice that float can raise exceptions other than ValueError:\\nIn [348]: float((1, 2))\\n---------------------------------------------------------------------------\\nTypeError                                 Traceback (most recent call last)\\n<ipython-input-348-842079ebb635> in <module>()\\n----> 1 float((1, 2))\\nTypeError: float() argument must be a string or a number\\nYou might want to only suppress ValueError, since a TypeError (the input was not a\\nstring or numeric value) might indicate a legitimate bug in your program. To do that,\\nwrite the exception type after except:\\ndef attempt_float(x):\\n    try:\\n        return float(x)\\n    except ValueError:\\n        return x\\nWe have then:\\nIn [350]: attempt_float((1, 2))\\n---------------------------------------------------------------------------\\nTypeError                                 Traceback (most recent call last)\\n<ipython-input-350-9bdfd730cead> in <module>()\\n----> 1 attempt_float((1, 2))\\n<ipython-input-349-3e06b8379b6b> in attempt_float(x)\\n      1 def attempt_float(x):\\n      2     try:\\n----> 3         return float(x)\\n      4     except ValueError:\\n      5         return x\\nTypeError: float() argument must be a string or a number\\nThe Basics | 403\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"You can catch multiple exception types by writing a tuple of exception types instead\\n(the parentheses are required):\\ndef attempt_float(x):\\n    try:\\n        return float(x)\\n    except (TypeError, ValueError):\\n        return x\\nIn some cases, you may not want to suppress an exception, but you want some code\\nto be executed regardless of whether the code in the try block succeeds or not. To do\\nthis, use finally:\\nf = open(path, 'w')\\ntry:\\n    write_to_file(f)\\nfinally:\\n    f.close()\\nHere, the file handle f will always get closed. Similarly, you can have code that executes\\nonly if the try: block succeeds using else:\\nf = open(path, 'w')\\ntry:\\n    write_to_file(f)\\nexcept:\\n    print 'Failed'\\nelse:\\n    print 'Succeeded'\\nfinally:\\n    f.close()\\nrange and xrange\\nThe range function produces a list of evenly-spaced integers:\\nIn [352]: range(10)\\nOut[352]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\\nBoth a start, end, and step can be given:\\nIn [353]: range(0, 20, 2)\\nOut[353]: [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\\nAs you can see, range produces integers up to but not including the endpoint. A com-\\nmon use of range is for iterating through sequences by index:\\nseq = [1, 2, 3, 4]\\nfor i in range(len(seq)):\\n    val = seq[i]\\nFor very long ranges, it’s recommended to use xrange, which takes the same arguments\\nas range but returns an iterator that generates integers one by one rather than generating\\n404 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"all of them up-front and storing them in a (potentially very large) list. This snippet sums\\nall numbers from 0 to 9999 that are multiples of 3 or 5:\\nsum = 0\\nfor i in xrange(10000):\\n    # % is the modulo operator\\n    if x % 3 == 0 or x % 5 == 0:\\n        sum += i\\nIn Python 3, range always returns an iterator, and thus it is not necessary\\nto use the xrange function\\nTernary Expressions\\nA ternary expression in Python allows you combine an if-else block which produces\\na value into a single line or expression. The syntax for this in Python is\\nvalue = true-expr if condition else\\nfalse-expr\\nHere, true-expr and false-expr can be any Python expressions. It has the identical\\neffect as the more verbose\\nif condition:\\n    value = true-expr\\nelse:\\n    value = false-expr\\nThis is a more concrete example:\\nIn [354]: x = 5\\nIn [355]: 'Non-negative' if x >= 0 else 'Negative'\\nOut[355]: 'Non-negative'\\nAs with if-else blocks, only one of the expressions will be evaluated. While it may be\\ntempting to always use ternary expressions to condense your code, realize that you may\\nsacrifice readability if the condition as well and the true and false expressions are very\\ncomplex.\\nData Structures and Sequences\\nPython’s data structures are simple, but powerful. Mastering their use is a critical part\\nof becoming a proficient Python programmer.\\nData Structures and Sequences | 405\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Tuple\\nA tuple is a one-dimensional, fixed-length, immutable sequence of Python objects. The\\neasiest way to create one is with a comma-separated sequence of values:\\nIn [356]: tup = 4, 5, 6\\nIn [357]: tup\\nOut[357]: (4, 5, 6)\\nWhen defining tuples in more complicated expressions, it’s often necessary to enclose\\nthe values in parentheses, as in this example of creating a tuple of tuples:\\nIn [358]: nested_tup = (4, 5, 6), (7, 8)\\nIn [359]: nested_tup\\nOut[359]: ((4, 5, 6), (7, 8))\\nAny sequence or iterator can be converted to a tuple by invoking tuple:\\nIn [360]: tuple([4, 0, 2])\\nOut[360]: (4, 0, 2)\\nIn [361]: tup = tuple('string')\\nIn [362]: tup\\nOut[362]: ('s', 't', 'r', 'i', 'n', 'g')\\nElements can be accessed with square brackets [] as with most other sequence types.\\nLike C, C++, Java, and many other languages, sequences are 0-indexed in Python:\\nIn [363]: tup[0]\\nOut[363]: 's'\\nWhile the objects stored in a tuple may be mutable themselves, once created it’s not\\npossible to modify which object is stored in each slot:\\nIn [364]: tup = tuple(['foo', [1, 2], True])\\nIn [365]: tup[2] = False\\n---------------------------------------------------------------------------\\nTypeError                                 Traceback (most recent call last)\\n<ipython-input-365-c7308343b841> in <module>()\\n----> 1 tup[2] = False\\nTypeError: 'tuple' object does not support item assignment\\n# however\\nIn [366]: tup[1].append(3)\\nIn [367]: tup\\nOut[367]: ('foo', [1, 2, 3], True)\\nTuples can be concatenated using the + operator to produce longer tuples:\\nIn [368]: (4, None, 'foo') + (6, 0) + ('bar',)\\nOut[368]: (4, None, 'foo', 6, 0, 'bar')\\n406 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Multiplying a tuple by an integer, as with lists, has the effect of concatenating together\\nthat many copies of the tuple.\\nIn [369]: ('foo', 'bar') * 4\\nOut[369]: ('foo', 'bar', 'foo', 'bar', 'foo', 'bar', 'foo', 'bar')\\nNote that the objects themselves are not copied, only the references to them.\\nUnpacking tuples\\nIf you try to assign to a tuple-like expression of variables, Python will attempt to un-\\npack the value on the right-hand side of the equals sign:\\nIn [370]: tup = (4, 5, 6)\\nIn [371]: a, b, c = tup\\nIn [372]: b\\nOut[372]: 5\\nEven sequences with nested tuples can be unpacked:\\nIn [373]: tup = 4, 5, (6, 7)\\nIn [374]: a, b, (c, d) = tup\\nIn [375]: d\\nOut[375]: 7\\nUsing this functionality it’s easy to swap variable names, a task which in many lan-\\nguages might look like:\\ntmp = a\\na = b\\nb = tmp\\nb, a = a, b\\nOne of the most common uses of variable unpacking when iterating over sequences of\\ntuples or lists:\\nseq = [(1, 2, 3), (4, 5, 6), (7, 8, 9)]\\nfor a, b, c in seq:\\n    pass\\nAnother common use is for returning multiple values from a function. More on this\\nlater.\\nTuple methods\\nSince the size and contents of a tuple cannot be modified, it is very light on instance\\nmethods. One particularly useful one (also available on lists) is count, which counts the\\nnumber of occurrences of a value:\\nIn [376]: a = (1, 2, 2, 2, 3, 4, 2)\\nData Structures and Sequences | 407\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [377]: a.count(2)\\nOut[377]: 4\\nList\\nIn contrast with tuples, lists are variable-length and their contents can be modified.\\nThey can be defined using square brackets [] or using the list type function:\\nIn [378]: a_list = [2, 3, 7, None]\\nIn [379]: tup = ('foo', 'bar', 'baz')\\nIn [380]: b_list = list(tup)        In [381]: b_list               \\n                                    Out[381]: ['foo', 'bar', 'baz']\\n                                                                   \\nIn [382]: b_list[1] = 'peekaboo'    In [383]: b_list                    \\n                                    Out[383]: ['foo', 'peekaboo', 'baz']\\nLists and tuples are semantically similar as one-dimensional sequences of objects and\\nthus can be used interchangeably in many functions.\\nAdding and removing elements\\nElements can be appended to the end of the list with the append method:\\nIn [384]: b_list.append('dwarf')\\nIn [385]: b_list\\nOut[385]: ['foo', 'peekaboo', 'baz', 'dwarf']\\nUsing insert you can insert an element at a specific location in the list:\\nIn [386]: b_list.insert(1, 'red')\\nIn [387]: b_list\\nOut[387]: ['foo', 'red', 'peekaboo', 'baz', 'dwarf']\\ninsert is computationally expensive compared with append as references\\nto subsequent elements have to be shifted internally to make room for\\nthe new element.\\nThe inverse operation to insert is pop, which removes and returns an element at a\\nparticular index:\\nIn [388]: b_list.pop(2)\\nOut[388]: 'peekaboo'\\nIn [389]: b_list\\nOut[389]: ['foo', 'red', 'baz', 'dwarf']\\nElements can be removed by value using remove, which locates the first such value and\\nremoves it from the last:\\n408 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [390]: b_list.append('foo')\\nIn [391]: b_list.remove('foo')\\nIn [392]: b_list\\nOut[392]: ['red', 'baz', 'dwarf', 'foo']\\nIf performance is not a concern, by using append and remove, a Python list can be used\\nas a perfectly suitable “multi-set” data structure.\\nYou can check if a list contains a value using the in keyword:\\nIn [393]: 'dwarf' in b_list\\nOut[393]: True\\nNote that checking whether a list contains a value is a lot slower than dicts and sets as\\nPython makes a linear scan across the values of the list, whereas the others (based on\\nhash tables) can make the check in constant time.\\nConcatenating and combining lists\\nSimilar to tuples, adding two lists together with + concatenates them:\\nIn [394]: [4, None, 'foo'] + [7, 8, (2, 3)]\\nOut[394]: [4, None, 'foo', 7, 8, (2, 3)]\\nIf you have a list already defined, you can append multiple elements to it using the \\nextend method:\\nIn [395]: x = [4, None, 'foo']\\nIn [396]: x.extend([7, 8, (2, 3)])\\nIn [397]: x\\nOut[397]: [4, None, 'foo', 7, 8, (2, 3)]\\nNote that list concatenation is a compartively expensive operation since a new list must\\nbe created and the objects copied over. Using extend to append elements to an existing\\nlist, especially if you are building up a large list, is usually preferable. Thus,\\neverything = []\\nfor chunk in list_of_lists:\\n    everything.extend(chunk)\\nis faster than than the concatenative alternative\\neverything = []\\nfor chunk in list_of_lists:\\n    everything = everything + chunk\\nSorting\\nA list can be sorted in-place (without creating a new object) by calling its sort function:\\nIn [398]: a = [7, 2, 5, 1, 3]\\nData Structures and Sequences | 409\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [399]: a.sort()\\nIn [400]: a\\nOut[400]: [1, 2, 3, 5, 7]\\nsort has a few options that will occasionally come in handy. One is the ability to pass\\na secondary sort key, i.e. a function that produces a value to use to sort the objects. For\\nexample, we could sort a collection of strings by their lengths:\\nIn [401]: b = ['saw', 'small', 'He', 'foxes', 'six']\\nIn [402]: b.sort(key=len)\\nIn [403]: b\\nOut[403]: ['He', 'saw', 'six', 'small', 'foxes']\\nBinary search and maintaining a sorted list\\nThe built-in bisect module implements binary-search and insertion into a sorted list.\\nbisect.bisect finds the location where an element should be inserted to keep it sorted,\\nwhile bisect.insort actually inserts the element into that location:\\nIn [404]: import bisect\\nIn [405]: c = [1, 2, 2, 2, 3, 4, 7]\\nIn [406]: bisect.bisect(c, 2)        In [407]: bisect.bisect(c, 5)\\nOut[406]: 4                          Out[407]: 6                  \\n                                                                  \\nIn [408]: bisect.insort(c, 6)\\nIn [409]: c\\nOut[409]: [1, 2, 2, 2, 3, 4, 6, 7]\\nThe bisect module functions do not check whether the list is sorted as\\ndoing so would be computationally expensive. Thus, using them with\\nan unsorted list will succeed without error but may lead to incorrect\\nresults.\\nSlicing\\nYou can select sections of list-like types (arrays, tuples, NumPy arrays) by using slice\\nnotation, which in its basic form consists of start:stop passed to the indexing operator\\n[]:\\nIn [410]: seq = [7, 2, 3, 7, 5, 6, 0, 1]\\nIn [411]: seq[1:5]\\nOut[411]: [2, 3, 7, 5]\\nSlices can also be assigned to with a sequence:\\nIn [412]: seq[3:4] = [6, 3]\\n410 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'In [413]: seq\\nOut[413]: [7, 2, 3, 6, 3, 5, 6, 0, 1]\\nWhile element at the start index is included, the stop index is not included, so that\\nthe number of elements in the result is stop - start.\\nEither the start or stop can be omitted in which case they default to the start of the\\nsequence and the end of the sequence, respectively:\\nIn [414]: seq[:5]                In [415]: seq[3:]           \\nOut[414]: [7, 2, 3, 6, 3]        Out[415]: [6, 3, 5, 6, 0, 1]\\nNegative indices slice the sequence relative to the end:\\nIn [416]: seq[-4:]            In [417]: seq[-6:-2]  \\nOut[416]: [5, 6, 0, 1]        Out[417]: [6, 3, 5, 6]\\nSlicing semantics takes a bit of getting used to, especially if you’re coming from R or\\nMATLAB. See Figure A-2 for a helpful illustrating of slicing with positive and negative\\nintegers.\\nA step can also be used after a second colon to, say, take every other element:\\nIn [418]: seq[::2]\\nOut[418]: [7, 3, 3, 6, 1]\\nA clever use of this is to pass -1 which has the useful effect of reversing a list or tuple:\\nIn [419]: seq[::-1]\\nOut[419]: [1, 0, 6, 5, 3, 6, 3, 2, 7]\\nFigure A-2. Illustration of Python slicing conventions\\nBuilt-in Sequence Functions\\nPython has a handful of useful sequence functions that you should familiarize yourself\\nwith and use at any opportunity.\\nData Structures and Sequences | 411\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"enumerate\\nIt’s common when iterating over a sequence to want to keep track of the index of the\\ncurrent item. A do-it-yourself approach would look like:\\ni = 0\\nfor value in collection:\\n   # do something with value\\n   i += 1\\nSince this is so common, Python has a built-in function enumerate which returns a\\nsequence of (i, value) tuples:\\nfor i, value in enumerate(collection):\\n   # do something with value\\nWhen indexing data, a useful pattern that uses enumerate is computing a dict mapping\\nthe values of a sequence (which are assumed to be unique) to their locations in the\\nsequence:\\nIn [420]: some_list = ['foo', 'bar', 'baz']\\nIn [421]: mapping = dict((v, i) for i, v in enumerate(some_list))\\nIn [422]: mapping\\nOut[422]: {'bar': 1, 'baz': 2, 'foo': 0}\\nsorted\\nThe sorted function returns a new sorted list from the elements of any sequence:\\nIn [423]: sorted([7, 1, 2, 6, 0, 3, 2])\\nOut[423]: [0, 1, 2, 2, 3, 6, 7]\\nIn [424]: sorted('horse race')\\nOut[424]: [' ', 'a', 'c', 'e', 'e', 'h', 'o', 'r', 'r', 's']\\nA common pattern for getting a sorted list of the unique elements in a sequence is to\\ncombine sorted with set:\\nIn [425]: sorted(set('this is just some string'))\\nOut[425]: [' ', 'e', 'g', 'h', 'i', 'j', 'm', 'n', 'o', 'r', 's', 't', 'u']\\nzip\\nzip “pairs” up the elements of a number of lists, tuples, or other sequences, to create\\na list of tuples:\\nIn [426]: seq1 = ['foo', 'bar', 'baz']\\nIn [427]: seq2 = ['one', 'two', 'three']\\nIn [428]: zip(seq1, seq2)\\nOut[428]: [('foo', 'one'), ('bar', 'two'), ('baz', 'three')]\\n412 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"zip can take an arbitrary number of sequences, and the number of elements it produces\\nis determined by the shortest sequence:\\nIn [429]: seq3 = [False, True]\\nIn [430]: zip(seq1, seq2, seq3)\\nOut[430]: [('foo', 'one', False), ('bar', 'two', True)]\\nA very common use of zip is for simultaneously iterating over multiple sequences,\\npossibly also combined with enumerate:\\nIn [431]: for i, (a, b) in enumerate(zip(seq1, seq2)):\\n   .....:     print('%d: %s, %s' % (i, a, b))\\n   .....:\\n0: foo, one\\n1: bar, two\\n2: baz, three\\nGiven a “zipped” sequence, zip can be applied in a clever way to “unzip” the sequence.\\nAnother way to think about this is converting a list of rows into a list of columns. The\\nsyntax, which looks a bit magical, is:\\nIn [432]: pitchers = [('Nolan', 'Ryan'), ('Roger', 'Clemens'),\\n   .....:             ('Schilling', 'Curt')]\\nIn [433]: first_names, last_names = zip(*pitchers)\\nIn [434]: first_names\\nOut[434]: ('Nolan', 'Roger', 'Schilling')\\nIn [435]: last_names\\nOut[435]: ('Ryan', 'Clemens', 'Curt')\\nWe’ll look in more detail at the use of * in a function call. It is equivalent to the fol-\\nlowing:\\nzip(seq[0], seq[1], ..., seq[len(seq) - 1])\\nreversed\\nreversed iterates over the elements of a sequence in reverse order:\\nIn [436]: list(reversed(range(10)))\\nOut[436]: [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\\nDict\\ndict is likely the most important built-in Python data structure. A more common name\\nfor it is hash map or associative array. It is a flexibly-sized collection of key-value pairs,\\nwhere key and value are Python objects. One way to create one is by using curly braces \\n{} and using colons to separate keys and values:\\nIn [437]: empty_dict = {}\\nIn [438]: d1 = {'a' : 'some value', 'b' : [1, 2, 3, 4]}\\nData Structures and Sequences | 413\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [439]: d1\\nOut[439]: {'a': 'some value', 'b': [1, 2, 3, 4]}\\nElements can be accessed and inserted or set using the same syntax as accessing ele-\\nments of a list or tuple:\\nIn [440]: d1[7] = 'an integer'\\nIn [441]: d1\\nOut[441]: {7: 'an integer', 'a': 'some value', 'b': [1, 2, 3, 4]}\\nIn [442]: d1['b']\\nOut[442]: [1, 2, 3, 4]\\nYou can check if a dict contains a key using the same syntax as with checking whether\\na list or tuple contains a value:\\nIn [443]: 'b' in d1\\nOut[443]: True\\nValues can be deleted either using the del keyword or the pop method (which simulta-\\nneously returns the value and deletes the key):\\nIn [444]: d1[5] = 'some value'\\nIn [445]: d1['dummy'] = 'another value'\\nIn [446]: del d1[5]\\nIn [447]: ret = d1.pop('dummy')        In [448]: ret            \\n                                       Out[448]: 'another value'\\nThe keys and values method give you lists of the keys and values, respectively. While\\nthe key-value pairs are not in any particular order, these functions output the keys and\\nvalues in the same order:\\nIn [449]: d1.keys()            In [450]: d1.values()                               \\nOut[449]: ['a', 'b', 7]        Out[450]: ['some value', [1, 2, 3, 4], 'an integer']\\nIf you’re using Python 3, dict.keys() and dict.values() are iterators\\ninstead of lists.\\nOne dict can be merged into another using the update method:\\nIn [451]: d1.update({'b' : 'foo', 'c' : 12})\\nIn [452]: d1\\nOut[452]: {7: 'an integer', 'a': 'some value', 'b': 'foo', 'c': 12}\\n414 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Creating dicts from sequences\\nIt’s common to occasionally end up with two sequences that you want to pair up ele-\\nment-wise in a dict. As a first cut, you might write code like this:\\nmapping = {}\\nfor key, value in zip(key_list, value_list):\\n    mapping[key] = value\\nSince a dict is essentially a collection of 2-tuples, it should be no shock that the dict\\ntype function accepts a list of 2-tuples:\\nIn [453]: mapping = dict(zip(range(5), reversed(range(5))))\\nIn [454]: mapping\\nOut[454]: {0: 4, 1: 3, 2: 2, 3: 1, 4: 0}\\nIn a later section we’ll talk about dict comprehensions, another elegant way to construct\\ndicts.\\nDefault values\\nIt’s very common to have logic like:\\nif key in some_dict:\\n    value = some_dict[key]\\nelse:\\n    value = default_value\\nThus, the dict methods get and pop can take a default value to be returned, so that the\\nabove if-else block can be written simply as:\\nvalue = some_dict.get(key, default_value)\\nget by default will return None if the key is not present, while pop will raise an exception.\\nWith setting values, a common case is for the values in a dict to be other collections,\\nlike lists. For example, you could imagine categorizing a list of words by their first letters\\nas a dict of lists:\\nIn [455]: words = ['apple', 'bat', 'bar', 'atom', 'book']\\nIn [456]: by_letter = {}\\nIn [457]: for word in words:\\n   .....:     letter = word[0]\\n   .....:     if letter not in by_letter:\\n   .....:         by_letter[letter] = [word]\\n   .....:     else:\\n   .....:         by_letter[letter].append(word)\\n   .....:\\nIn [458]: by_letter\\nOut[458]: {'a': ['apple', 'atom'], 'b': ['bat', 'bar', 'book']}\\nThe setdefault dict method is for precisely this purpose. The if-else block above can\\nbe rewritten as:\\nData Structures and Sequences | 415\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"by_letter.setdefault(letter, []).append(word)\\nThe built-in collections module has a useful class, defaultdict, which makes this even\\neasier. One is created by passing a type or function for generating the default value for\\neach slot in the dict:\\nfrom collections import defaultdict\\nby_letter = defaultdict(list)\\nfor word in words:\\n    by_letter[word[0]].append(word)\\nThe initializer to defaultdict only needs to be a callable object (e.g. any function), not\\nnecessarily a type. Thus, if you wanted the default value to be 4 you could pass a\\nfunction returning 4\\ncounts = defaultdict(lambda: 4)\\nValid dict key types\\nWhile the values of a dict can be any Python object, the keys have to be immutable\\nobjects like scalar types (int, float, string) or tuples (all the objects in the tuple need to\\nbe immutable, too). The technical term here is hashability. You can check whether an\\nobject is hashable (can be used as a key in a dict) with the hash function:\\nIn [459]: hash('string')\\nOut[459]: -9167918882415130555\\nIn [460]: hash((1, 2, (2, 3)))\\nOut[460]: 1097636502276347782\\nIn [461]: hash((1, 2, [2, 3])) # fails because lists are mutable\\n---------------------------------------------------------------------------\\nTypeError                                 Traceback (most recent call last)\\n<ipython-input-461-800cd14ba8be> in <module>()\\n----> 1 hash((1, 2, [2, 3])) # fails because lists are mutable\\nTypeError: unhashable type: 'list'\\nTo use a list as a key, an easy fix is to convert it to a tuple:\\nIn [462]: d = {}\\nIn [463]: d[tuple([1, 2, 3])] = 5\\nIn [464]: d\\nOut[464]: {(1, 2, 3): 5}\\nSet\\nA set is an unordered collection of unique elements. You can think of them like dicts,\\nbut keys only, no values. A set can be created in two ways: via the set function or using\\na set literal with curly braces:\\nIn [465]: set([2, 2, 2, 1, 3, 3])\\nOut[465]: set([1, 2, 3])\\n416 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'In [466]: {2, 2, 2, 1, 3, 3}\\nOut[466]: set([1, 2, 3])\\nSets support mathematical set operations like union, intersection, difference, and sym-\\nmetric difference. See Table A-3 for a list of commonly used set methods.\\nIn [467]: a = {1, 2, 3, 4, 5}\\nIn [468]: b = {3, 4, 5, 6, 7, 8}\\nIn [469]: a | b  # union (or)\\nOut[469]: set([1, 2, 3, 4, 5, 6, 7, 8])\\nIn [470]: a & b  # intersection (and)\\nOut[470]: set([3, 4, 5])\\nIn [471]: a - b  # difference\\nOut[471]: set([1, 2])\\nIn [472]: a ^ b  # symmetric difference (xor)\\nOut[472]: set([1, 2, 6, 7, 8])\\nYou can also check if a set is a subset of (is contained in) or a superset of (contains all\\nelements of) another set:\\nIn [473]: a_set = {1, 2, 3, 4, 5}\\nIn [474]: {1, 2, 3}.issubset(a_set)\\nOut[474]: True\\nIn [475]: a_set.issuperset({1, 2, 3})\\nOut[475]: True\\nAs you might guess, sets are equal if their contents are equal:\\nIn [476]: {1, 2, 3} == {3, 2, 1}\\nOut[476]: True\\nTable A-3. Python Set Operations\\nFunction Alternate Syntax Description\\na.add(x) N/A Add element x to the set a\\na.remove(x) N/A Remove element x from the set a\\na.union(b) a | b All of the unique elements in a and b.\\na.intersection(b) a & b All of the elements in both  a and b.\\na.difference(b) a - b The elements in a that are not in b.\\na.symmetric_difference(b) a ^ b All of the elements in a or b but not both.\\na.issubset(b) N/A True if the elements of a are all contained in b.\\na.issuperset(b) N/A True if the elements of b are all contained in a.\\na.isdisjoint(b) N/A True if a and b have no elements in common.\\nData Structures and Sequences | 417\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"List, Set, and Dict Comprehensions\\nList comprehensions are one of the most-loved Python language features. They allow\\nyou to concisely form a new list by filtering the elements of a collection and transforming\\nthe elements passing the filter in one conscise expression. They take the basic form:\\n[expr for val in collection if condition]\\nThis is equivalent to the following for loop:\\nresult = []\\nfor val in collection:\\n    if condition:\\n        result.append(expr)\\nThe filter condition can be omitted, leaving only the expression. For example, given a\\nlist of strings, we could filter out strings with length 2 or less and also convert them to\\nuppercase like this:\\nIn [477]: strings = ['a', 'as', 'bat', 'car', 'dove', 'python']\\nIn [478]: [x.upper() for x in strings if len(x) > 2]\\nOut[478]: ['BAT', 'CAR', 'DOVE', 'PYTHON']\\nSet and dict comprehensions are a natural extension, producing sets and dicts in a\\nidiomatically similar way instead of lists. A dict comprehension looks like this:\\ndict_comp = {key-expr : value-expr for value in collection\\n             if condition}\\nA set comprehension looks like the equivalent list comprehension except with curly\\nbraces instead of square brackets:\\nset_comp = {expr for value in collection if condition}\\nLike list comprehensions, set and dict comprehensions are just syntactic sugar, but they\\nsimilarly can make code both easier to write and read. Consider the list of strings above.\\nSuppose we wanted a set containing just the lengths of the strings contained in the\\ncollection; this could be easily computed using a set comprehension:\\nIn [479]: unique_lengths = {len(x) for x in strings}\\nIn [480]: unique_lengths\\nOut[480]: set([1, 2, 3, 4, 6])\\nAs a simple dict comprehension example, we could create a lookup map of these strings\\nto their locations in the list:\\nIn [481]: loc_mapping = {val : index for index, val in enumerate(strings)}\\nIn [482]: loc_mapping\\nOut[482]: {'a': 0, 'as': 1, 'bat': 2, 'car': 3, 'dove': 4, 'python': 5}\\nNote that this dict could be equivalently constructed by:\\nloc_mapping = dict((val, idx) for idx, val in enumerate(strings)}\\n418 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"The dict comprehension version is shorter and cleaner in my opinion.\\nDict and set comprehensions were added to Python fairly recently in\\nPython 2.7 and Python 3.1+.\\nNested list comprehensions\\nSuppose we have a list of lists containing some boy and girl names:\\nIn [483]: all_data = [['Tom', 'Billy', 'Jefferson', 'Andrew', 'Wesley', 'Steven', 'Joe'],\\n   .....:             ['Susie', 'Casey', 'Jill', 'Ana', 'Eva', 'Jennifer', 'Stephanie']]\\nYou might have gotten these names from a couple of files and decided to keep the boy\\nand girl names separate. Now, suppose we wanted to get a single list containing all\\nnames with two or more e’s in them. We could certainly do this with a simple for loop:\\nnames_of_interest = []\\nfor names in all_data:\\n    enough_es = [name for name in names if name.count('e') > 2]\\n    names_of_interest.extend(enough_es)\\nYou can actually wrap this whole operation up in a single nested list comprehension,\\nwhich will look like:\\nIn [484]: result = [name for names in all_data for name in names\\n   .....:           if name.count('e') >= 2]\\nIn [485]: result\\nOut[485]: ['Jefferson', 'Wesley', 'Steven', 'Jennifer', 'Stephanie']\\nAt first, nested list comprehensions are a bit hard to wrap your head around. The for\\nparts of the list comprehension are arranged according to the order of nesting, and any\\nfilter condition is put at the end as before. Here is another example where we “flatten”\\na list of tuples of integers into a simple list of integers:\\nIn [486]: some_tuples = [(1, 2, 3), (4, 5, 6), (7, 8, 9)]\\nIn [487]: flattened = [x for tup in some_tuples for x in tup]\\nIn [488]: flattened\\nOut[488]: [1, 2, 3, 4, 5, 6, 7, 8, 9]\\nKeep in mind that the order of the for expressions would be the same if you wrote a\\nnested for loop instead of a list comprehension:\\nflattened = []\\nfor tup in some_tuples:\\n    for x in tup:\\n        flattened.append(x)\\nData Structures and Sequences | 419\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'You can have arbitrarily many levels of nesting, though if you have more than two or\\nthree levels of nesting you should probably start to question your data structure design.\\nIt’s important to distinguish the above syntax from a list comprehension inside a list\\ncomprehension, which is also perfectly valid:\\nIn [229]: [[x for x in tup] for tup in some_tuples]\\nFunctions\\nFunctions are the primary and most important method of code organization and reuse\\nin Python. There may not be such a thing as having too many functions. In fact, I would\\nargue that most programmers doing data analysis don’t write enough functions! As you\\nhave likely inferred from prior examples, functions are declared using the def keyword\\nand returned from using the return keyword:\\ndef my_function(x, y, z=1.5):\\n    if z > 1:\\n        return z * (x + y)\\n    else:\\n        return z / (x + y)\\nThere is no issue with having multiple return statements. If the end of a function is\\nreached without encountering a return statement, None is returned.\\nEach function can have some number of positional arguments and some number of \\nkeyword arguments. Keyword arguments are most commonly used to specify default\\nvalues or optional arguments. In the above function, x and y are positional arguments\\nwhile z is a keyword argument. This means that it can be called in either of these\\nequivalent ways:\\nmy_function(5, 6, z=0.7)\\nmy_function(3.14, 7, 3.5)\\nThe main restriction on function arguments it that the keyword arguments must follow\\nthe positional arguments (if any). You can specify keyword arguments in any order;\\nthis frees you from having to remember which order the function arguments were\\nspecified in and only what their names are.\\nNamespaces, Scope, and Local Functions\\nFunctions can access variables in two different scopes: global and local. An alternate\\nand more descriptive name describing a variable scope in Python is a namespace. Any\\nvariables that are assigned within a function by default are assigned to the local name-\\nspace. The local namespace is created when the function is called and immediately\\npopulated by the function’s arguments. After the function is finished, the local name-\\nspace is destroyed (with some exceptions, see section on closures below). Consider the\\nfollowing function:\\n420 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'def func():\\n    a = []\\n    for i in range(5):\\n        a.append(i)\\nUpon calling func(), the empty list a is created, 5 elements are appended, then a is\\ndestroyed when the function exits. Suppose instead we had declared a\\na = []\\ndef func():\\n    for i in range(5):\\n        a.append(i)\\nAssigning global variables within a function is possible, but those variables must be\\ndeclared as global using the global keyword:\\nIn [489]: a = None\\nIn [490]: def bind_a_variable():\\n   .....:     global a\\n   .....:     a = []\\n   .....: bind_a_variable()\\n   .....:\\nIn [491]: print a\\n[]\\nI generally discourage people from using the global keyword frequently.\\nTypically global variables are used to store some kind of state in a sys-\\ntem. If you find yourself using a lot of them, it’s probably a sign that\\nsome object-oriented programming (using classes) is in order.\\nFunctions can be declared anywhere, and there is no problem with having local func-\\ntions that are dynamically created when a function is called:\\ndef outer_function(x, y, z):\\n    def inner_function(a, b, c):\\n        pass\\n    pass\\nIn the above code, the inner_function will not exist until outer_function is called. As\\nsoon as outer_function is done executing, the inner_function is destroyed.\\nNested inner functions can access the local namespace of the enclosing function, but\\nthey cannot bind new variables in it. I’ll talk a bit more about this in the section on\\nclosures.\\nIn a strict sense, all functions are local to some scope, that scope may just be the module\\nlevel scope.\\nFunctions | 421\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Returning Multiple Values\\nWhen I first programmed in Python after having programmed in Java and C++, one of\\nmy favorite features was the ability to return multiple values from a function. Here’s a\\nsimple example:\\ndef f():\\n    a = 5\\n    b = 6\\n    c = 7\\n    return a, b, c\\na, b, c = f()\\nIn data analysis and other scientific applications, you will likely find yourself doing this\\nvery often as many functions may have multiple outputs, whether those are data struc-\\ntures or other auxiliary data computed inside the function. If you think about tuple\\npacking and unpacking from earlier in this chapter, you may realize that what’s hap-\\npening here is that the function is actually just returning one object, namely a tuple,\\nwhich is then being unpacked into the result variables. In the above example, we could\\nhave done instead:\\nreturn_value = f()\\nIn this case, return_value would be, as you may guess, a 3-tuple with the three returned\\nvariables. A potentially attractive alternative to returning multiple values like above\\nmight be to return a dict instead:\\ndef f():\\n    a = 5\\n    b = 6\\n    c = 7\\n    return {'a' : a, 'b' : b, 'c' : c}\\nFunctions Are Objects\\nSince Python functions are objects, many constructs can be easily expressed that are\\ndifficult to do in other languages. Suppose we were doing some data cleaning and\\nneeded to apply a bunch of transformations to the following list of strings:\\nstates = ['   Alabama ', 'Georgia!', 'Georgia', 'georgia', 'FlOrIda',\\n          'south   carolina##', 'West virginia?']\\nAnyone who has ever worked with user-submitted survey data can expect messy results\\nlike these. Lots of things need to happen to make this list of strings uniform and ready\\nfor analysis: whitespace stripping, removing punctuation symbols, and proper capital-\\nization. As a first pass, we might write some code like:\\nimport re  # Regular expression module\\ndef clean_strings(strings):\\n    result = []\\n422 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"for value in strings:\\n        value = value.strip()\\n        value = re.sub('[!#?]', '', value) # remove punctuation\\n        value = value.title()\\n        result.append(value)\\n    return result\\nThe result looks like this:\\nIn [15]: clean_strings(states)\\nOut[15]:\\n['Alabama',\\n 'Georgia',\\n 'Georgia',\\n 'Georgia',\\n 'Florida',\\n 'South Carolina',\\n 'West Virginia']\\nAn alternate approach that you may find useful is to make a list of the operations you\\nwant to apply to a particular set of strings:\\ndef remove_punctuation(value):\\n    return re.sub('[!#?]', '', value)\\nclean_ops = [str.strip, remove_punctuation, str.title]\\ndef clean_strings(strings, ops):\\n    result = []\\n    for value in strings:\\n        for function in ops:\\n            value = function(value)\\n        result.append(value)\\n    return result\\nThen we have\\nIn [22]: clean_strings(states, clean_ops)\\nOut[22]:\\n['Alabama',\\n 'Georgia',\\n 'Georgia',\\n 'Georgia',\\n 'Florida',\\n 'South Carolina',\\n 'West Virginia']\\nA more functional pattern like this enables you to easily modify how the strings are\\ntransformed at a very high level. The clean_strings function is also now more reusable!\\nYou can naturally use functions as arguments to other functions like the built-in map\\nfunction, which applies a function to a collection of some kind:\\nIn [23]: map(remove_punctuation, states)\\nOut[23]:\\n['   Alabama ',\\n 'Georgia',\\nFunctions | 423\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"'Georgia',\\n 'georgia',\\n 'FlOrIda',\\n 'south   carolina',\\n 'West virginia']\\nAnonymous (lambda) Functions\\nPython has support for so-called anonymous or lambda functions, which are really just\\nsimple functions consisting of a single statement, the result of which is the return value.\\nThey are defined using the lambda keyword, which has no meaning other than “we are\\ndeclaring an anonymous function.”\\ndef short_function(x):\\n    return x * 2\\nequiv_anon = lambda x: x * 2\\nI usually refer to these as lambda functions in the rest of the book. They are especially\\nconvenient in data analysis because, as you’ll see, there are many cases where data\\ntransformation functions will take functions as arguments. It’s often less typing (and\\nclearer) to pass a lambda function as opposed to writing a full-out function declaration\\nor even assigning the lambda function to a local variable. For example, consider this\\nsilly example:\\ndef apply_to_list(some_list, f):\\n    return [f(x) for x in some_list]\\nints = [4, 0, 1, 5, 6]\\napply_to_list(ints, lambda x: x * 2)\\nYou could also have written [x * 2 for x in ints], but here we were able to succintly\\npass a custom operator to the apply_to_list function.\\nAs another example, suppose you wanted to sort a collection of strings by the number\\nof distinct letters in each string:\\nIn [492]: strings = ['foo', 'card', 'bar', 'aaaa', 'abab']\\nHere we could pass a lambda function to the list’s sort method:\\nIn [493]: strings.sort(key=lambda x: len(set(list(x))))\\nIn [494]: strings\\nOut[494]: ['aaaa', 'foo', 'abab', 'bar', 'card']\\nOne reason lambda functions are called anonymous functions is that\\nthe function object itself is never given a name attribute.\\n424 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Closures: Functions that Return Functions\\nClosures are nothing to fear. They can actually be a very useful and powerful tool in\\nthe right circumstance! In a nutshell, a closure is any dynamically-generated function\\nreturned by another function. The key property is that the returned function has access\\nto the variables in the local namespace where it was created. Here is a very simple\\nexample:\\ndef make_closure(a):\\n    def closure():\\n        print('I know the secret: %d' % a)\\n    return closure\\nclosure = make_closure(5)\\nThe difference between a closure and a regular Python function is that the closure\\ncontinues to have access to the namespace (the function) where it was created, even\\nthough that function is done executing. So in the above case, the returned closure will\\nalways print I know the secret: 5 whenever you call it. While it’s common to create\\nclosures whose internal state (in this example, only the value of a) is static, you can just\\nas easily have a mutable object like a dict, set, or list that can be modified. For example,\\nhere’s a function that returns a function that keeps track of arguments it has been called\\nwith:\\ndef make_watcher():\\n    have_seen = {}\\n    def has_been_seen(x):\\n        if x in have_seen:\\n            return True\\n        else:\\n            have_seen[x] = True\\n            return False\\n    return has_been_seen\\nUsing this on a sequence of integers I obtain:\\nIn [496]: watcher = make_watcher()\\nIn [497]: vals = [5, 6, 1, 5, 1, 6, 3, 5]\\nIn [498]: [watcher(x) for x in vals]\\nOut[498]: [False, False, False, True, True, True, False, True]\\nHowever, one technical limitation to keep in mind is that while you can mutate any\\ninternal state objects (like adding key-value pairs to a dict), you cannot bind variables\\nin the enclosing function scope. One way to work around this is to modify a dict or list\\nrather than binding variables:\\ndef make_counter():\\n    count = [0]\\n    def counter():\\nFunctions | 425\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': '# increment and return the current count\\n        count[0] += 1\\n        return count[0]\\n    return counter\\ncounter = make_counter()\\nYou might be wondering why this is useful. In practice, you can write very general\\nfunctions with lots of options, then fabricate simpler, more specialized functions.\\nHere’s an example of creating a string formatting function:\\ndef format_and_pad(template, space):\\n    def formatter(x):\\n        return (template % x).rjust(space)\\n    return formatter\\nYou could then create a floating point formatter that always returns a length-15 string\\nlike so:\\nIn [500]: fmt = format_and_pad(\\'%.4f\\', 15)\\nIn [501]: fmt(1.756)\\nOut[501]: \\'         1.7560\\'\\nIf you learn more about object-oriented programming in Python, you might observe\\nthat these patterns also could be implemented (albeit more verbosely) using classes.\\nExtended Call Syntax with *args, **kwargs\\nThe way that function arguments work under the hood in Python is actually very sim-\\nple. When you write func(a, b, c, d=some, e=value) , the positional and keyword\\narguments are actually packed up into a tuple and dict, respectively. So the internal\\nfunction receives a tuple args and dict kwargs and internally does the equivalent of:\\na, b, c = args\\nd = kwargs.get(\\'d\\', d_default_value)\\ne = kwargs.get(\\'e\\', e_default_value)\\nThis all happens nicely behind the scenes. Of course, it also does some error checking\\nand allows you to specify some of the positional arguments as keywords also (even if\\nthey aren’t keyword in the function declaration!).\\ndef say_hello_then_call_f(f, *args, **kwargs):\\n    print \\'args is\\', args\\n    print \\'kwargs is\\', kwargs\\n    print(\"Hello! Now I\\'m going to call %s\" % f)\\n    return f(*args, **kwargs)\\ndef g(x, y, z=1):\\n    return (x + y) / z\\nThen if we call g with say_hello_then_call_f we get:\\n426 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [8]:  say_hello_then_call_f(g, 1, 2, z=5.)\\nargs is (1, 2)\\nkwargs is {'z': 5.0}\\nHello! Now I'm going to call <function g at 0x2dd5cf8>\\nOut[8]: 0.6\\nCurrying: Partial Argument Application\\nCurrying is a fun computer science term which means deriving new functions from\\nexisting ones by partial argument application. For example, suppose we had a trivial\\nfunction that adds two numbers together:\\ndef add_numbers(x, y):\\n    return x + y\\nUsing this function, we could derive a new function of one variable, add_five, that adds\\n5 to its argument:\\nadd_five = lambda y: add_numbers(5, y)\\nThe second argument to add_numbers is said to be curried. There’s nothing very fancy\\nhere as we really only have defined a new function that calls an existing function. The\\nbuilt-in functools module can simplify this process using the partial function:\\nfrom functools import partial\\nadd_five = partial(add_numbers, 5)\\nWhen discussing pandas and time series data, we’ll use this technique to create speci-\\nalized functions for transforming data series\\n# compute 60-day moving average of time series x\\nma60 = lambda x: pandas.rolling_mean(x, 60)\\n# Take the 60-day moving average of of all time series in data\\ndata.apply(ma60)\\nGenerators\\nHaving a consistent way to iterate over sequences, like objects in a list or lines in a file,\\nis an important Python feature. This is accomplished by means of the iterator proto-\\ncol, a generic way to make objects iterable. For example, iterating over a dict yields the\\ndict keys:\\nIn [502]: some_dict = {'a': 1, 'b': 2, 'c': 3}\\nIn [503]: for key in some_dict:\\n   .....:     print key,\\na c b\\nWhen you write for key in some_dict, the Python interpreter first attempts to create\\nan iterator out of some_dict:\\nIn [504]: dict_iterator = iter(some_dict)\\nFunctions | 427\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [505]: dict_iterator\\nOut[505]: <dictionary-keyiterator at 0x10a0a1578>\\nAny iterator is any object that will yield objects to the Python interpreter when used in\\na context like a for loop. Most methods expecting a list or list-like object will also accept\\nany iterable object. This includes built-in methods such as min, max, and sum, and type\\nconstructors like list and tuple:\\nIn [506]: list(dict_iterator)\\nOut[506]: ['a', 'c', 'b']\\nA generator is a simple way to construct a new iterable object. Whereas normal func-\\ntions execute and return a single value, generators return a sequence of values lazily,\\npausing after each one until the next one is requested. To create a generator, use the\\nyield keyword instead of return in a function:\\ndef squares(n=10):\\n    for i in xrange(1, n + 1):\\n        print 'Generating squares from 1 to %d' % (n ** 2)\\n        yield i ** 2\\nWhen you actually call the generator, no code is immediately executed:\\nIn [2]: gen = squares()\\nIn [3]: gen\\nOut[3]: <generator object squares at 0x34c8280>\\nIt is not until you request elements from the generator that it begins executing its code:\\nIn [4]: for x in gen:\\n   ...:     print x,\\n   ...:\\nGenerating squares from 0 to 100\\n1 4 9 16 25 36 49 64 81 100\\nAs a less trivial example, suppose we wished to find all unique ways to make change\\nfor $1 (100 cents) using an arbitrary set of coins. You can probably think of various\\nways to implement this and how to store the unique combinations as you come up with\\nthem. One way is to write a generator that yields lists of coins (represented as integers):\\ndef make_change(amount, coins=[1, 5, 10, 25], hand=None):\\n    hand = [] if hand is None else hand\\n    if amount == 0:\\n        yield hand\\n    for coin in coins:\\n        # ensures we don't give too much change, and combinations are unique\\n        if coin > amount or (len(hand) > 0 and hand[-1] < coin):\\n            continue\\n        for result in make_change(amount - coin, coins=coins,\\n                                  hand=hand + [coin]):\\n            yield result\\nThe details of the algorithm are not that important (can you think of a shorter way?).\\nThen we can write:\\n428 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"In [508]: for way in make_change(100, coins=[10, 25, 50]):\\n   .....:     print way\\n[10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\\n[25, 25, 10, 10, 10, 10, 10]\\n[25, 25, 25, 25]\\n[50, 10, 10, 10, 10, 10]\\n[50, 25, 25]\\n[50, 50]\\nIn [509]: len(list(make_change(100)))\\nOut[509]: 242\\nGenerator expresssions\\nA simple way to make a generator is by using a generator expression. This is a generator\\nanalogue to list, dict and set comprehensions; to create one, enclose what would other-\\nwise be a list comprehension with parenthesis instead of brackets:\\nIn [510]: gen = (x ** 2 for x in xrange(100))\\nIn [511]: gen\\nOut[511]: <generator object <genexpr> at 0x10a0a31e0>\\nThis is completely equivalent to the following more verbose generator:\\ndef _make_gen():\\n    for x in xrange(100):\\n        yield x ** 2\\ngen = _make_gen()\\nGenerator expressions can be used inside any Python function that will accept a gen-\\nerator:\\nIn [512]: sum(x ** 2 for x in xrange(100))\\nOut[512]: 328350\\nIn [513]: dict((i, i **2) for i in xrange(5))\\nOut[513]: {0: 0, 1: 1, 2: 4, 3: 9, 4: 16}\\nitertools module\\nThe standard library itertools module has a collection of generators for many common\\ndata algorithms. For example, groupby takes any sequence and a function; this groups\\nconsecutive elements in the sequence by return value of the function. Here’s an exam-\\nple:\\nIn [514]: import itertools\\nIn [515]: first_letter = lambda x: x[0]\\nIn [516]: names = ['Alan', 'Adam', 'Wes', 'Will', 'Albert', 'Steven']\\nIn [517]: for letter, names in itertools.groupby(names, first_letter):\\n   .....:     print letter, list(names) # names is a generator\\nA ['Alan', 'Adam']\\nFunctions | 429\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"W ['Wes', 'Will']\\nA ['Albert']\\nS ['Steven']\\nSee Table A-4 for a list of a few other itertools functions I’ve frequently found useful.\\nTable A-4. Some useful itertools functions\\nFunction Description\\nimap(func, *iterables) Generator version of the built-in map; applies func to each zipped tuple of\\nthe passed sequences.\\nifilter(func, iterable) Generator version of the built-in filter; yields elements x for which\\nfunc(x) is True.\\ncombinations(iterable, k) Generates a sequence of all possible k-tuples of elements in the iterable,\\nignoring order.\\npermutations(iterable, k) Generates a sequence of all possible k-tuples of elements in the iterable,\\nrespecting order.\\ngroupby(iterable[, keyfunc]) Generates (key, sub-iterator) for each unique key\\nIn Python 3, several built-in functions ( zip, map, filter) producing\\nlists have been replaced by their generator versions found in itertools\\nin Python 2.\\nFiles and the operating system\\nMost of this book uses high-level tools like pandas.read_csv to read data files from disk\\ninto Python data structures. However, it’s important to understand the basics of how\\nto work with files in Python. Fortunately, it’s very simple, which is part of why Python\\nis so popular for text and file munging.\\nTo open a file for reading or writing, use the built-in open function with either a relative\\nor absolute file path:\\nIn [518]: path = 'ch13/segismundo.txt'\\nIn [519]: f = open(path)\\nBy default, the file is opened in read-only mode 'r'. We can then treat the file handle\\nf like a list and iterate over the lines like so\\nfor line in f:\\n    pass\\nThe lines come out of the file with the end-of-line (EOL) markers intact, so you’ll often\\nsee code to get an EOL-free list of lines in a file like\\nIn [520]: lines = [x.rstrip() for x in open(path)]\\nIn [521]: lines\\n430 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"Out[521]: \\n['Sue\\\\xc3\\\\xb1a el rico en su riqueza,',\\n 'que m\\\\xc3\\\\xa1s cuidados le ofrece;',\\n '',\\n 'sue\\\\xc3\\\\xb1a el pobre que padece',\\n 'su miseria y su pobreza;',\\n '',\\n 'sue\\\\xc3\\\\xb1a el que a medrar empieza,',\\n 'sue\\\\xc3\\\\xb1a el que afana y pretende,',\\n 'sue\\\\xc3\\\\xb1a el que agravia y ofende,',\\n '',\\n 'y en el mundo, en conclusi\\\\xc3\\\\xb3n,',\\n 'todos sue\\\\xc3\\\\xb1an lo que son,',\\n 'aunque ninguno lo entiende.',\\n '']\\nIf we had typed f = open(path, 'w'), a new file at ch13/segismundo.txt would have\\nbeen created, overwriting any one in its place. See below for a list of all valid file read/\\nwrite modes.\\nTable A-5. Python file modes\\nMode Description\\nr Read-only mode\\nw Write-only mode. Creates a new file (deleting any file with the same name)\\na Append to existing file (create it if it does not exist)\\nr+ Read and write\\nb Add to mode for binary files, that is 'rb' or 'wb'\\nU Use universal newline mode. Pass by itself 'U' or appended to one of the read modes like 'rU'\\nTo write text to a file, you can use either the file’s write or writelines methods. For\\nexample, we could create a version of prof_mod.py with no blank lines like so:\\nIn [522]: with open('tmp.txt', 'w') as handle:\\n   .....:     handle.writelines(x for x in open(path) if len(x) > 1)\\nIn [523]: open('tmp.txt').readlines()\\nOut[523]: \\n['Sue\\\\xc3\\\\xb1a el rico en su riqueza,\\\\n',\\n 'que m\\\\xc3\\\\xa1s cuidados le ofrece;\\\\n',\\n 'sue\\\\xc3\\\\xb1a el pobre que padece\\\\n',\\n 'su miseria y su pobreza;\\\\n',\\n 'sue\\\\xc3\\\\xb1a el que a medrar empieza,\\\\n',\\n 'sue\\\\xc3\\\\xb1a el que afana y pretende,\\\\n',\\n 'sue\\\\xc3\\\\xb1a el que agravia y ofende,\\\\n',\\n 'y en el mundo, en conclusi\\\\xc3\\\\xb3n,\\\\n',\\n 'todos sue\\\\xc3\\\\xb1an lo que son,\\\\n',\\n 'aunque ninguno lo entiende.\\\\n']\\nSee Table A-6 for many of the most commonly-used file methods.\\nFiles and the operating system | 431\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Table A-6. Important Python file methods or attributes\\nMethod Description\\nread([size]) Return data from file as a string, with optional size argument indicating the number of bytes\\nto read\\nreadlines([size]) Return list of lines in the file, with optional size argument\\nreadlines([size]) Return list of lines (as strings) in the file\\nwrite(str) Write passed string to file.\\nwritelines(strings) Write passed sequence of strings to the file.\\nclose() Close the handle\\nflush() Flush the internal I/O buffer to disk\\nseek(pos) Move to indicated file position (integer).\\ntell() Return current file position as integer.\\nclosed True is the file is closed.\\n432 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Index\\nSymbols\\n! character, 60, 61, 64\\n!= operator, 91\\n!cmd command, 60\\n\"two-language\" problem, 2–3\\n# (hash mark), 388\\n$PATH variable, 10\\n% character, 398\\n%a datetime format, 293\\n%A datetime format, 293\\n%alias magic function, 61\\n%automagic magic function, 55\\n%b datetime format, 293\\n%B datetime format, 293\\n%bookmark magic function, 60, 62\\n%c datetime format, 293\\n%cd magic function, 60\\n%cpaste magic function, 51–52, 55\\n%d datetime format, 292\\n%D datetime format, 293\\n%d format character, 398\\n%debug magic function, 54–55, 62\\n%dhist magic function, 60\\n%dirs magic function, 60\\n%env magic function, 60\\n%F datetime format, 293\\n%gui magic function, 57\\n%H datetime format, 292\\n%hist magic function, 55, 59\\n%I datetime format, 292\\n%logstart magic function, 60\\n%logstop magic function, 60\\n%lprun magic function, 70, 72\\n%m datetime format, 292\\n%M datetime format, 292\\n%magic magic function, 55\\n%p datetime format, 293\\n%page magic function, 55\\n%paste magic function, 51, 55\\n%pdb magic function, 54, 63\\n%popd magic function, 60\\n%prun magic function, 55, 70\\n%pushd magic function, 60\\n%pwd magic function, 60\\n%quickref magic function, 55\\n%reset magic function, 55, 59\\n%run magic function, 49–50, 55, 386\\n%S datetime format, 292\\n%s format character, 398\\n%time magic function, 55, 67\\n%timeit magic function, 54, 67, 68\\n%U datetime format, 293\\n%w datetime format, 292\\n%W datetime format, 293\\n%who magic function, 55\\n%whos magic function, 55\\n%who_ls magic function, 55\\n%x datetime format, 293\\n%X datetime format, 293\\n%xdel magic function, 55, 59\\n%xmode magic function, 54\\n%Y datetime format, 292\\n%y datetime format, 292\\n%z datetime format, 293\\n& operator, 91\\n* operator, 105\\n+ operator, 406, 409\\n2012 Federal Election Commission database\\nexample, 278–287\\nWe’d like to hear your suggestions for improving our indexes. Send email to index@oreilly.com.\\n433\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'bucketing donation amounts, 283–285\\ndonation statistics by occupation and\\nemployer, 280–283\\ndonation statistics by state, 285–287\\n== operator, 393\\n>>> prompt, 386\\n? (question mark), 49\\n[] (brackets), 406, 408\\n\\\\ (backslash), 397\\n_ (underscore), 48, 58\\n__ (two underscores), 58\\n{} (braces), 413\\n| operator, 91\\nA\\na file mode, 431\\nabs function, 96\\naccumulate method, 368\\nadd method, 95, 130, 417\\nadd_patch method, 229\\nadd_subplot method, 221\\naggfunc option, 277\\naggregate method, 260, 262\\naggregations, 100\\nalgorithms for sorting, 375–376\\nalignment of data, 330–331\\nall method, 101, 368\\nalpha argument, 233\\nand keyword, 398, 401\\nannotating in matplotlib, 228–230\\nanonymous functions, 424\\nany method, 101, 110, 201\\nappend method, 122, 408\\napply method, 39, 132, 142, 266–268, 270\\napt package management tool, 10\\narange function, 82\\narccos function, 96\\narccosh function, 96\\narcsin function, 96\\narcsinh function, 96\\narctan function, 96\\narctanh function, 96\\nargmax method, 101\\nargmin method, 101, 139\\nargsort method, 135, 374\\narithmetic, 128–132\\noperations between DataFrame and Series,\\n130–132\\nwith fill values, 129–130\\narrays\\nboolean arrays, 101\\nboolean indexing for, 89–92\\nconditional logic as operation, 98–100\\ncreating, 81–82\\ncreating PeriodIndex from, 312\\ndata types for, 83–85\\nfancy indexing, 92–93\\nfile input and output with, 103–105\\nsaving and loading text files, 104–105\\nstoring on disk in binary format, 103–\\n104\\nfinding elements in sorted array, 376–377\\nin NumPy, 355–362\\nconcatenating, 357–359\\nc_ object, 359\\nlayout of in memory, 356–357\\nreplicating, 360–361\\nreshaping, 355–356\\nr_ object, 359\\nsaving to file, 379–380\\nsplitting, 357–359\\nsubsets for, 361–362\\nindexes for, 86–89\\noperations between, 85–86\\nsetting values by broadcasting, 367\\nslicing, 86–89\\nsorting, 101–102\\nstatistical methods for, 100\\nstructured arrays, 370–372\\nbenefits of, 372\\nmainpulating, 372\\nnested data types, 371–372\\nswapping axes in, 93–94\\ntransposing, 93–94\\nunique function, 102–103\\nwhere function, 98–100\\narrow function, 229\\nas keyword, 393\\nasarray function, 82, 379\\nasfreq method, 308, 318\\nasof method, 334–336\\nastype method, 84, 85\\nattributes\\nin Python, 391\\nstarting with underscore, 48\\naverage method, 136\\nax argument, 233\\naxes\\n434 | Index\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'broadcasting over, 364–367\\nconcatenating along, 185–188\\nlabels for, 226–227\\nrenaming indexes for, 197–198\\nswapping in arrays, 93–94\\nAxesSubplot object, 221\\naxis argument, 188\\naxis method, 138\\nB\\nb file mode, 431\\nbackslash (\\\\), 397\\nbar plots, 235–238\\nBasemap object, 245, 246\\n.bashrc file, 10\\n.bash_profile file, 9\\nbbox_inches option, 231\\nbenefits\\nof Python, 2–3\\nglue for code, 2\\nsolving \"two-language\" problem with, 2–\\n3\\nof structured arrays, 372\\nbeta function, 107\\ndefined, 342\\nbetween_time method, 335\\nbfill method, 123\\nbin edges, 314\\nbinary data formats, 171–172\\nHDF5, 171–172\\nMicrosoft Excel files, 172\\nstoring arrays in, 103–104\\nbinary moving window functions, 324–325\\nbinary search of lists, 410\\nbinary universal functions, 96\\nbinding\\ndefined, 390\\nvariables, 425\\nbinomial function, 107\\nbisect module, 410\\nbookmarking directories in IPython, 62\\nBoolean\\narrays, 101\\ndata type, 84, 398\\nindexing for arrays, 89–92\\nbottleneck library, 324\\nbraces ({}), 413\\nbrackets ([]), 406, 408\\nbreak keyword, 401\\nbroadcasting, 362–367\\ndefined, 86, 360, 362\\nover other axes, 364–367\\nsetting array values by, 367\\nbucketing, 283–285\\nC\\ncalendar module, 290\\ncasting, 84\\ncat method, 156, 212\\nCategorical object, 199\\nceil function, 96\\ncenter method, 212\\nChaco, 248\\nchisquare function, 107\\nchunksize argument, 160, 161\\nclearing screen shortcut, 53\\nclipboard, executing code from, 50–52\\nclock function, 67\\nclose method, 220, 432\\nclosures, 425–426\\ncmd.exe, 7\\ncollections module, 416\\ncolons, 387\\ncols option, 277\\ncolumns, grouping on, 256–257\\ncolumn_stack function, 359\\ncombinations function, 430\\ncombine_first method, 177, 189\\ncombining\\ndata sources, 336–338\\ndata sources, with overlap, 188–189\\nlists, 409\\ncommands, 65\\n(see also magic commands)\\ndebugger, 65\\nhistory in IPython, 58–60\\ninput and output variables, 58–59\\nlogging of, 59–60\\nreusing command history, 58\\nsearching for, 53\\ncomment argument, 160\\ncomments in Python, 388\\ncompile method, 208\\ncomplex128 data type, 84\\ncomplex256 data type, 84\\ncomplex64 data type, 84\\nconcat function, 34, 177, 184, 185, 186, 267,\\n357, 359\\nIndex | 435\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'concatenating\\nalong axis, 185–188\\narrays, 357–359\\nconditional logic as array operation, 98–100\\nconferences, 12\\nconfiguring matplotlib, 231–232\\nconforming, 122\\ncontains method, 212\\ncontiguous memory, 381–382\\ncontinue keyword, 401\\ncontinuous return, 348\\nconvention argument, 314\\nconverting\\nbetween string and datetime, 291–293\\ntimestamps to periods, 311\\ncoordinated universal time (UTC), 303\\ncopy argument, 181\\ncopy method, 118\\ncopysign function, 96\\ncorr method, 140\\ncorrelation, 139–141\\ncorrwith method, 140\\ncos function, 96\\ncosh function, 96\\ncount method, 139, 206, 212, 261, 407\\nCounter class, 21\\ncov method, 140\\ncovariance, 139–141\\nCPython, 7\\ncross-section, 329\\ncrosstab function, 277–278\\ncrowdsourcing, 241\\nCSV files, 163–165, 242\\nCtrl-A keyboard shortcut, 53\\nCtrl-B keyboard shortcut, 53\\nCtrl-C keyboard shortcut, 53\\nCtrl-E keyboard shortcut, 53\\nCtrl-F keyboard shortcut, 53\\nCtrl-K keyboard shortcut, 53\\nCtrl-L keyboard shortcut, 53\\nCtrl-N keyboard shortcut, 53\\nCtrl-P keyboard shortcut, 53\\nCtrl-R keyboard shortcut, 53\\nCtrl-Shift-V keyboard shortcut, 53\\nCtrl-U keyboard shortcut, 53\\ncummax method, 139\\ncummin method, 139\\ncumprod method, 100, 139\\ncumsum method, 100, 139\\ncumulative returns, 338–340\\ncurrying, 427\\ncursor, moving with keyboard, 53\\ncustom universal functions, 370\\ncut function, 199, 200, 201, 268, 283\\nCython project, 2, 382–383\\nc_ object, 359\\nD\\ndata aggregation, 259–264\\nreturning data in unindexed form, 264\\nusing multiple functions, 262–264\\ndata alignment, 128–132\\narithmetic methods with fill values, 129–\\n130\\noperations between DataFrame and Series,\\n130–132\\ndata munging, 329–340\\nasof method, 334–336\\ncombining data, 336–338\\nfor data alignment, 330–331\\nfor specialized frequencies, 332–334\\ndata structures for pandas, 112–121\\nDataFrame, 115–120\\nIndex objects, 120–121\\nPanel, 152–154\\nSeries, 112–115\\ndata types\\nfor arrays, 83–85\\nfor ndarray, 83–85\\nfor NumPy, 353–354\\nhierarchy of, 354\\nfor Python, 395–400\\nboolean data type, 398\\ndates and times, 399–400\\nNone data type, 399\\nnumeric data types, 395–396\\nstr data type, 396–398\\ntype casting in, 399\\nfor time series data, 290–293\\nconverting between string and datetime,\\n291–293\\nnested, 371–372\\ndata wrangling\\nmanipulating strings, 205–211\\nmethods for, 206–207\\nvectorized string methods, 210–211\\nwith regular expressions, 207–210\\nmerging data, 177–189\\n436 | Index\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'combining data with overlap, 188–189\\nconcatenating along axis, 185–188\\nDataFrame merges, 178–181\\non index, 182–184\\npivoting, 192–193\\nreshaping, 190–191\\ntransforming data, 194–205\\ndiscretization, 199–201\\ndummy variables, 203–205\\nfiltering outliers, 201–202\\nmapping, 195–196\\npermutation, 202\\nremoving duplicates, 194–195\\nrenaming axis indexes, 197–198\\nreplacing values, 196–197\\nUSDA food database example, 212–217\\ndatabases\\nreading and writing to, 174–176\\nDataFrame data structure, 22, 27, 112, 115–\\n120\\narithmetic operations between Series and,\\n130–132\\nhierarchical indexing using, 150–151\\nmerging data with, 178–181\\ndates and times, 291\\n(see also time series data)\\ndata types for, 291, 399–400\\ndate ranges, 298\\ndatetime type, 291–293, 395, 399\\nDatetimeIndex Index object, 121\\ndateutil package, 291\\ndate_parser argument, 160\\ndate_range function, 298\\ndayfirst argument, 160\\ndebug function, 66\\ndebugger, IPython\\nin IPython, 62–66\\ndef keyword, 420\\ndefaults\\nprofiles, 77\\nvalues for dicts, 415–416\\ndel keyword, 59, 118, 414\\ndelete method, 122\\ndelimited formats, 163–165\\ndensity plots, 238–239\\ndescribe method, 138, 243, 267\\ndesign tips, 74–76\\nflat is better than nested, 75\\nkeeping relevant objects and data alive, 75\\novercoming fear of longer files, 75–76\\ndet function, 106\\ndevelopment tools in IPython, 62–72\\ndebugger, 62–66\\nprofiling code, 68–70\\nprofiling function line-by-line, 70–72\\ntiming code, 67–68\\ndiag function, 106\\ndicts, 413–416\\ncreating, 415\\ndefault values for, 415–416\\ndict comprehensions, 418–420\\ngrouping on, 257–258\\nkeys for, 416\\nreturning system environment variables as,\\n60\\ndiff method, 122, 139\\ndifference method, 417\\ndigitize function, 377\\ndirectories\\nbookmarking in IPython, 62\\nchanging, commands for, 60\\ndiscretization, 199–201\\ndiv method, 130\\ndivide function, 96\\n.dmg file, 9\\ndonation statistics\\nby occupation and employer, 280–283\\nby state, 285–287\\ndot function, 105, 106, 377\\ndoublequote option, 165\\ndownsampling, 312\\ndpi (dots-per-inch) option, 231\\ndreload function, 74\\ndrop method, 122, 125\\ndropna method, 143\\ndrop_duplicates method, 194\\ndsplit function, 359\\ndstack function, 359\\ndtype object (see data types)\\n“duck” typing in Python, 392\\ndummy variables, 203–205\\ndumps function, 165\\nduplicated method, 194\\nduplicates\\nindices, 296–297\\nremoving from data, 194–195\\ndynamically-generated functions, 425\\nIndex | 437\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'E\\nedgecolo option, 231\\nedit-compile-run workflow, 45\\neig function, 106\\nelif blocks (see if statements)\\nelse block (see if statements)\\nempty function, 82, 83\\nempty namespace, 50\\nencoding argument, 160\\nendswith method, 207, 212\\nenumerate function, 412\\nenvironment variables, 8, 60\\nEPD (Enthought Python Distribution), 7–9\\nequal function, 96\\nescapechar option, 165\\newma function, 323\\newmcorr function, 323\\newmcov function, 323\\newmstd function, 323\\newmvar function, 323\\nExcelFile class, 172\\nexcept block, 403\\nexceptions\\nautomatically entering debugger after, 55\\ndefined, 402\\nhandling in Python, 402–404\\nexec keyword, 59\\nexecute-explore workflow, 45\\nexecution time\\nof code, 55\\nof single statement, 55\\nexit command, 386\\nexp function, 96\\nexpanding window mean, 322\\nexponentially-weighted functions, 324\\nextend method, 409\\nextensible markup language (XML) files, 169–\\n170\\neye function, 83\\nF\\nfabs function, 96\\nfacecolor option, 231\\nfactor analysis, 342–343\\nFactor object, 269\\nfactors, 342\\nfancy indexing\\ndefined, 361\\nfor arrays, 92–93\\nffill method, 123\\nfigsize argument, 234\\nFigure object, 220, 223\\nfile input/output\\nbinary data formats for, 171–172\\nHDF5, 171–172\\nMicrosoft Excel files, 172\\nfor arrays, 103–105\\nHDF5, 380\\nmemory-mapped files, 379–380\\nsaving and loading text files, 104–105\\nstoring on disk in binary format, 103–\\n104\\nin Python, 430–431\\nsaving plot to file, 231\\ntext files, 155–170\\ndelimited formats, 163–165\\nHTML files, 166–170\\nJSON data, 165–166\\nlxml library, 166–170\\nreading in pieces, 160–162\\nwriting to, 162–163\\nXML files, 169–170\\nwith databases, 174–176\\nwith Web APIs, 173–174\\nfilling in missing data, 145–146, 270–271\\nfillna method, 22, 143, 145, 146, 196, 270,\\n317\\nfill_method argument, 313\\nfill_value option, 277\\nfiltering\\nin pandas, 125–128\\nmissing data, 143–144\\noutliers, 201–202\\nfinancial applications\\ncumulative returns, 338–340\\ndata munging, 329–340\\nasof method, 334–336\\ncombining data, 336–338\\nfor data alignment, 330–331\\nfor specialized frequencies, 332–334\\nfuture contract rolling, 347–350\\ngrouping for, 340–345\\nfactor analysis with, 342–343\\nquartile analysis, 343–345\\nlinear regression, 350–351\\nreturn indexes, 338–340\\nrolling correlation, 350–351\\n438 | Index\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'signal frontier analysis, 345–347\\nfind method, 206, 207\\nfindall method, 167, 208, 210, 212\\nfinditer method, 210\\nfirst crossing time, 109\\nfirst method, 136, 261\\nflat is better than nested, 75\\nflattening, 356\\nfloat data type, 83, 354, 395, 396, 399\\nfloat function, 402\\nfloat128 data type, 84\\nfloat16 data type, 84\\nfloat32 data type, 84\\nfloat64 data type, 84\\nfloor function, 96\\nfloor_divide function, 96\\nflow control, 400–405\\nexception handling, 402–404\\nfor loops, 401–402\\nif statements, 400–401\\npass statements, 402\\nrange function, 404–405\\nternary expressions, 405\\nwhile loops, 402\\nxrange function, 404–405\\nflush method, 432\\nfmax function, 96\\nfmin function, 96\\nfname option, 231\\nfor loops, 85, 100, 401–402, 418, 419\\nformat option, 231\\nfrequencies, 299–301\\nconverting, 308\\nspecialized frequencies, 332–334\\nweek of month dates, 301\\nfrompyfunc function, 370\\nfrom_csv method, 163\\nfunctions, 389, 420–430\\nanonymous functions, 424\\nare objects, 422–423\\nclosures, 425–426\\ncurrying of, 427\\nextended call syntax for, 426\\nlambda functions, 424\\nnamespaces for, 420–421\\nparsing in pandas, 155\\nreturning multiple values from, 422\\nscope of, 420–421\\nfunctools module, 427\\nfuture contract rolling, 347–350\\nfutures, 347\\nG\\ngamma function, 107\\ngcc command, 9, 11\\ngenerators, 427–430\\ndefined, 428\\ngenerator expressions, 429\\nitertools module for, 429–430\\nget method, 167, 172, 212, 415\\ngetattr function, 391\\nget_chunk method, 162\\nget_dummies function, 203, 205\\nget_value method, 128\\nget_xlim method, 226\\nGIL (global interpreter lock), 3\\nglobal scope, 420, 421\\nglue for code\\nPython as, 2\\n.gov domain, 17\\nGranger, Brian, 72\\ngraphics\\nChaco, 248\\nmayavi, 248\\ngreater function, 96\\ngreater_equal function, 96\\ngrid argument, 234\\ngroup keys, 268\\ngroupby method, 39, 252–259, 297, 316, 343,\\n377, 429\\niterating over groups, 255–256\\non column, 256–257\\non dict, 257–258\\non levels, 259\\nresampling with, 316\\nusing functions with, 258–259\\nwith Series, 257–258\\ngrouping\\n2012 Federal Election Commission database\\nexample, 278–287\\nbucketing donation amounts, 283–285\\ndonation statistics by occupation and\\nemployer, 280–283\\ndonation statistics by state, 285–287\\napply method, 266–268\\ndata aggregation, 259–264\\nreturning data in unindexed form, 264\\nusing multiple functions, 262–264\\nIndex | 439\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'filling missing values with group-specific\\nvalues, 270–271\\nfor financial applications, 340–345\\nfactor analysis with, 342–343\\nquartile analysis, 343–345\\ngroup weighted average, 273–274\\ngroupby method, 252–259\\niterating over groups, 255–256\\non column, 256–257\\non dict, 257–258\\non levels, 259\\nusing functions with, 258–259\\nwith Series, 257–258\\nlinear regression for, 274–275\\npivot tables, 275–278\\ncross-tabulation, 277–278\\nquantile analysis with, 268–269\\nrandom sampling with, 271–272\\nH\\nHaiti earthquake crisis data example, 241–247\\nhalf-open, 314\\nhasattr function, 391\\nhash mark (#), 388\\nhashability, 416\\nHDF5 (hierarchical data format), 171–172,\\n380\\nHDFStore class, 171\\nheader argument, 160\\nheapsort sorting method, 376\\nhierarchical data format (HDF5), 171–172,\\n380\\nhierarchical indexing\\nin pandas, 147–151\\nsorting levels, 149–150\\nsummary statistics by level, 150\\nwith DataFrame columns, 150–151\\nreshaping data with, 190–191\\nhist method, 238\\nhistograms, 238–239\\nhistory of commands, searching, 53\\nhomogeneous data container, 370\\nhow argument, 181, 313, 316\\nhsplit function, 359\\nhstack function, 358\\nHTML files, 166–170\\nHTML Notebook in IPython, 72\\nHunter, John D., 5, 219\\nhyperbolic trigonometric functions, 96\\nI\\nicol method, 128, 152\\nIDEs (Integrated Development Environments),\\n11, 52\\nidxmax method, 138\\nidxmin method, 138\\nif statements, 400–401, 415\\nifilter function, 430\\niget_value method, 152\\nignore_index argument, 188\\nimap function, 430\\nimport directive\\nin Python, 392–393\\nusage of in this book, 13\\nimshow function, 98\\nin keyword, 409\\nin-place sort, 373\\nin1d method, 103\\nindentation\\nin Python, 387–388\\nIndentationError event, 51\\nindex method, 206, 207\\nIndex objects data structure, 120–121\\nindexes\\ndefined, 112\\nfor arrays, 86–89\\nfor axis, 197–198\\nfor TimeSeries class, 294–296\\nhierarchical indexing, 147–151\\nreshaping data with, 190–191\\nsorting levels, 149–150\\nsummary statistics by level, 150\\nwith DataFrame columns, 150–151\\nin pandas, 136\\ninteger indexing, 151–152\\nmerging data on, 182–184\\nindex_col argument, 160\\nindirect sorts, 374–375, 374\\ninput variables, 58–59\\ninsert method, 122, 408\\ninsort method, 410\\nint data type, 83, 395, 399\\nint16 data type, 84\\nint32 data type, 84\\nint64 data type, 84\\nInt64Index Index object, 121\\nint8 data type, 84\\ninteger arrays, indexing using (see fancy\\nindexing)\\n440 | Index\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'integer indexing, 151–152\\nIntegrated Development Environments (IDEs),\\n11, 52\\ninterpreted languages\\ndefined, 386\\nPython interpreter, 386\\ninterrupting code, 50, 53\\nintersect1d method, 103\\nintersection method, 122, 417\\nintervals of time, 289\\ninv function, 106\\ninverse trigonometric functions, 96\\n.ipynb files, 72\\nIPython, 5\\nbookmarking directories, 62\\ncommand history in, 58–60\\ninput and output variables, 58–59\\nlogging of, 59–60\\nreusing command history, 58\\ndesign tips, 74–76\\nflat is better than nested, 75\\nkeeping relevant objects and data alive,\\n75\\novercoming fear of longer files, 75–76\\ndevelopment tools, 62–72\\ndebugger, 62–66\\nprofiling code, 68–70\\nprofiling function line-by-line, 70–72\\ntiming code, 67–68\\nexecuting code from clipboard, 50–52\\nHTML Notebook in, 72\\nintegration with IDEs and editors, 52\\nintegration with mathplotlib, 56–57\\nkeyboard shortcuts for, 52\\nmagic commands in, 54–55\\nmaking classes output correctly, 76\\nobject introspection in, 48–49\\nprofiles for, 77–78\\nQt console for, 55\\nQuick Reference Card for, 55\\nreloading module dependencies, 74\\n%run command in, 49–50\\nshell commands in, 60–61\\ntab completion in, 47–48\\ntracebacks in, 53–54\\nipython_config.py file, 77\\nirow method, 128, 152\\nis keyword, 393\\nisdisjoint method, 417\\nisfinite function, 96\\nisin method, 141–142\\nisinf function, 96\\nisinstance function, 391\\nisnull method, 96, 114, 143\\nissubdtype function, 354\\nissubset method, 417\\nissuperset method, 417\\nis_monotonic method, 122\\nis_unique method, 122\\niter function, 392\\niterating over groups, 255–256\\niterator argument, 160\\niterator protocol, 392, 427\\nitertools module, 429–430, 429\\nix_ function, 93\\nJ\\njoin method, 184, 206, 212\\nJSON (JavaScript Object Notation), 18, 165–\\n166, 213\\nK\\nKDE (kernel density estimate) plots, 239\\nkeep_date_col argument, 160\\nkernels, 239\\nkey-value pairs, 413\\nkeyboard shortcuts, 53\\nfor deleting text, 53\\nfor IPython, 52\\nKeyboardInterrupt event, 50\\nkeys\\nargument, 188\\nfor dicts, 416\\nmethod, 414\\nkeyword arguments, 389, 420\\nkind argument, 234, 314\\nkurt method, 139\\nL\\nlabel argument, 233, 313, 315\\nlambda functions, 211, 262, 424\\nlast method, 261\\nlayout of arrays in memory, 356–357\\nleft argument, 181\\nleft_index argument, 181\\nleft_on argument, 181\\nlegends in matplotlib, 228\\nIndex | 441\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'len function, 212, 258\\nless function, 96\\nless_equal function, 96\\nlevel keyword, 259\\nlevels\\ndefined, 147\\ngrouping on, 259\\nsorting, 149–150\\nsummary statistics by, 150\\nlexicographical sort\\ndefined, 375\\nlexsort method, 374\\nlibraries, 3–6\\nIPython, 5\\nmatplotlib, 5\\nNumPy, 4\\npandas, 4–5\\nSciPy, 6\\nlimit argument, 313\\nlinalg function, 105\\nline plots, 232–235\\nlinear algebra, 105–106\\nlinear regression, 274–275, 350–351\\nlineterminator option, 164\\nline_profiler extension, 70\\nLinux, setting up on, 10–11\\nlist comprehensions, 418–420\\nnested list comprehensions, 419–420\\nlist function, 408\\nlists, 408–411\\nadding elements to, 408–409\\nbinary search of, 410\\ncombining, 409\\ninsertion into sorted, 410\\nlist comprehensions, 418–420\\nremoving elements from, 408–409\\nslicing, 410–411\\nsorting, 409–410\\nljust method, 207\\nload function, 103, 379\\nload method, 171\\nloads function, 18\\nlocal scope, 420\\nlocalizing time series data, 304–305\\nloffset argument, 313, 316\\nlog function, 96\\nlog1p function, 96\\nlog2 function, 96\\nlogging command history in IPython, 59–60\\nlogical_and function, 96\\nlogical_not function, 96\\nlogical_or function, 96\\nlogical_xor function, 96\\nlogy argument, 234\\nlong format, 192\\nlong type, 395\\nlonger files overcoming fear of, 75–76\\nlower method, 207, 212\\nlstrip method, 207, 212\\nlstsq function, 106\\nlxml library, 166–170\\nM\\nmad method, 139\\nmagic methods, 48, 54–55\\nmain function, 75\\nmainpulating structured arrays, 372\\nmany-to-many merge, 179\\nmany-to-one merge, 178\\nmap method, 133, 195–196, 211, 280, 423\\nmargins, 275\\nmarkers, 224\\nmatch method, 208–212\\nmatplotlib, 5, 219–232\\nannotating in, 228–230\\naxis labels in, 226–227\\nconfiguring, 231–232\\nintegrating with IPython, 56–57\\nlegends in, 228\\nsaving to file, 231\\nstyling for, 224–225\\nsubplots in, 220–224\\nticks in, 226–227\\ntitle in, 226–227\\nmatplotlibrc file, 232\\nmatrix operations in NumPy, 377–379\\nmax method, 101, 136, 139, 261, 428\\nmaximum function, 95, 96\\nmayavi, 248\\nmean method, 100, 139, 253, 259, 261, 265\\nmedian method, 139, 261\\nmemmap object, 379\\nmemory, layout of arrays in, 356–357\\nmemory-mapped files\\ndefined, 379\\nsaving arrays to file, 379–380\\nmergesort sorting method, 375, 376\\nmerging data, 177–189\\n442 | Index\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'combining data with overlap, 188–189\\nconcatenating along axis, 185–188\\nDataFrame merges, 178–181\\non index, 182–184\\nmeshgrid function, 97\\nmethods\\ndefined, 389\\nfor tuples, 407\\nin Python, 389\\nstarting with underscore, 48\\nMicrosoft Excel files, 172\\n.mil domain, 17\\nmin method, 101, 136, 139, 261, 428\\nminimum function, 96\\nmissing data, 142–146\\nfilling in, 145–146\\nfiltering out, 143–144\\nmod function, 96\\nmodf function, 95\\nmodules, 392\\nmomentum, 343\\nMongoDB, 176\\nMovieLens 1M data set example, 26–31\\nmoving window functions, 320–326\\nbinary moving window functions, 324–325\\nexponentially-weighted functions, 324\\nuser-defined, 326\\n.mpkg file, 9\\nmro method, 354\\nmul method, 130\\nMultiIndex Index object, 121, 147, 149\\nmultiple profiles, 77\\nmultiply function, 96\\nmunging, 13\\nmutable objects, 394–395\\nN\\nNA data type, 143\\nnames argument, 160, 188\\nnamespaces\\ndefined, 420\\nin Python, 420–421\\nnaming trends\\nin US baby names 1880-2010 example, 36–\\n43\\nboy names that became girl names, 42–\\n43\\nmeasuring increase in diversity, 37–40\\nrevolution of last letter, 40–41\\nNaN (not a number), 101, 114, 143\\nna_values argument, 160\\nncols option, 223\\nndarray, 80\\nBoolean indexing, 89–92\\ncreating arrays, 81–82\\ndata types for, 83–85\\nfancy indexing, 92–93\\nindexes for, 86–89\\noperations between arrays, 85–86\\nslicing arrays, 86–89\\nswapping axes in, 93–94\\ntransposing, 93–94\\nnested code, 75\\nnested data types, 371–372\\nnested list comprehensions, 419–420\\nNew York MTA (Metropolitan Transportation\\nAuthority), 169\\nNone data type, 395, 399\\nnormal function, 107, 110\\nnormalized timestamps, 298\\nNoSQL databases, 176\\nnot a number (NaN), 101, 114, 143\\nNotebookCloud, 72\\nnotnull method, 114, 143\\nnot_equal function, 96\\n.npy files, 103\\n.npz files, 104\\nnrows argument, 160, 223\\nnuisance column, 254\\nnumeric data types, 395–396\\nNumPy, 4\\narrays in, 355–362\\nconcatenating, 357–359\\nc_ object, 359\\nlayout of in memory, 356–357\\nreplicating, 360–361\\nreshaping, 355–356\\nr_ object, 359\\nsaving to file, 379–380\\nsplitting, 357–359\\nsubsets for, 361–362\\nbroadcasting, 362–367\\nover other axes, 364–367\\nsetting array values by, 367\\ndata processing using\\nwhere function, 98–100\\ndata processing using arrays, 97–103\\nIndex | 443\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'conditional logic as array operation, 98–\\n100\\nmethods for boolean arrays, 101\\nsorting arrays, 101–102\\nstatistical methods, 100\\nunique function, 102–103\\ndata types for, 353–354\\nfile input and output with arrays, 103–105\\nsaving and loading text files, 104–105\\nstoring on disk in binary format, 103–\\n104\\nlinear algebra, 105–106\\nmatrix operations in, 377–379\\nndarray arrays, 80\\nBoolean indexing, 89–92\\ncreating, 81–82\\ndata types for, 83–85\\nfancy indexing, 92–93\\nindexes for, 86–89\\noperations between arrays, 85–86\\nslicing arrays, 86–89\\nswapping axes in, 93–94\\ntransposing, 93–94\\nnumpy-discussion (mailing list), 12\\nperformance of, 380–383\\ncontiguous memory, 381–382\\nCython project, 382–383\\nrandom number generation, 106–107\\nrandom walks example, 108–110\\nsorting, 373–377\\nalgorithms for, 375–376\\nfinding elements in sorted array, 376–\\n377\\nindirect sorts, 374–375\\nstructured arrays in, 370–372\\nbenefits of, 372\\nmainpulating, 372\\nnested data types, 371–372\\nuniversal functions for, 95–96, 367–370\\ncustom, 370\\nin pandas, 132–133\\ninstance methods for, 368–369\\nO\\nobject introspection, 48–49\\nobject model, 388\\nobject type, 84\\nobjectify function, 166, 169\\nobjs argument, 188\\noffsets for time series data, 302–303\\nOHLC (Open-High-Low-Close) resampling,\\n316\\nols function, 351\\nOlson database, 303\\non argument, 181\\nones function, 82\\nopen function, 430\\nOpen-High-Low-Close (OHLC) resampling,\\n316\\noperators in Python, 393\\nor keyword, 401\\norder method, 375\\nOS X, setting up Python on, 9–10\\nouter method, 368, 369\\noutliers, filtering, 201–202\\noutput variables, 58–59\\nP\\npad method, 212\\npairs plot, 241\\npandas, 4–5\\narithmetic and data alignment, 128–132\\narithmetic methods with fill values, 129–\\n130\\noperations between DataFrame and\\nSeries, 130–132\\ndata structures for, 112–121\\nDataFrame, 115–120\\nIndex objects, 120–121\\nPanel, 152–154\\nSeries, 112–115\\ndrop function, 125\\nfiltering in, 125–128\\nhandling missing data, 142–146\\nfilling in, 145–146\\nfiltering out, 143–144\\nhierarchical indexing in, 147–151\\nsorting levels, 149–150\\nsummary statistics by level, 150\\nwith DataFrame columns, 150–151\\nindexes in, 136\\nindexing options, 125–128\\ninteger indexing, 151–152\\nNumPy universal functions with, 132–133\\nplotting with, 232\\nbar plots, 235–238\\ndensity plots, 238–239\\nhistograms, 238–239\\n444 | Index\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'line plots, 232–235\\nscatter plots, 239–241\\nranking data in, 133–135\\nreductions in, 137–142\\nreindex function, 122–124\\nselecting in objects, 125–128\\nsorting in, 133–135\\nsummary statistics in\\ncorrelation and covariance, 139–141\\nisin function, 141–142\\nunique function, 141–142\\nvalue_counts function, 141–142\\nusa.gov data from bit.ly example with, 21–\\n26\\nPanel data structure, 152–154\\npanels, 329\\nparse method, 291\\nparse_dates argument, 160\\npartial function, 427\\npartial indexing, 147\\npass statements, 402\\npassing by reference, 390\\npasting\\nkeyboard shortcut for, 53\\nmagic command for, 55\\npatches, 229\\npath argument, 160\\nPath variable, 8\\npct_change method, 139\\npdb debugger, 62\\n.pdf files, 231\\npercentileofscore function, 326\\nPérez, Fernando, 45, 219\\nperformance\\nand time series data, 327–328\\nof NumPy, 380–383\\ncontiguous memory, 381–382\\nCython project, 382–383\\nPeriod class, 307\\nPeriodIndex Index object, 121, 311, 312\\nperiods, 307–312\\nconverting timestamps to, 311\\ncreating PeriodIndex from arrays, 312\\ndefined, 289, 307\\nfrequency conversion for, 308\\ninstead of timestamps, 333–334\\nquarterly periods, 309–310\\nresampling with, 318–319\\nperiod_range function, 307, 310\\npermutation, 202\\npickle serialization, 170\\npinv function, 106\\npivoting data\\ncross-tabulation, 277–278\\ndefined, 189\\npivot method, 192–193\\npivot_table method, 29, 275–278\\npivot_table aggregation type, 275\\nplot method, 23, 36, 41, 220, 224, 232, 239,\\n246, 319\\nplotting\\nHaiti earthquake crisis data example, 241–\\n247\\ntime series data, 319–320\\nwith matplotlib, 219–232\\nannotating in, 228–230\\naxis labels in, 226–227\\nconfiguring, 231–232\\nlegends in, 228\\nsaving to file, 231\\nstyling for, 224–225\\nsubplots in, 220–224\\nticks in, 226–227\\ntitle in, 226–227\\nwith pandas, 232\\nbar plots, 235–238\\ndensity plots, 238–239\\nhistograms, 238–239\\nline plots, 232–235\\nscatter plots, 239–241\\n.png files, 231\\npop method, 408, 414\\npositional arguments, 389\\npower function, 96\\npprint module, 76\\npretty printing\\nand displaying through pager, 55\\ndefined, 47\\nprivate attributes, 48\\nprivate methods, 48\\nprod method, 261\\nprofiles\\ndefined, 77\\nfor IPython, 77–78\\nprofile_default directory, 77\\nprofiling code\\nin IPython, 68–70\\npseudocode, 14\\nIndex | 445\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'put function, 362\\nput method, 362\\n.py files, 50, 386, 392\\npydata (Google group), 12\\npylab mode, 219\\npymongo driver, 175\\npyplot module, 220\\npystatsmodels (mailing list), 12\\nPython\\nbenefits of using, 2–3\\nglue for code, 2\\nsolving \"two-language\" problem with, 2–\\n3\\ndata types for, 395–400\\nboolean data type, 398\\ndates and times, 399–400\\nNone data type, 399\\nnumeric data types, 395–396\\nstr data type, 396–398\\ntype casting in, 399\\ndict comprehensions in, 418–420\\ndicts in, 413–416\\ncreating, 415\\ndefault values for, 415–416\\nkeys for, 416\\nfile input/output in, 430–431\\nflow control in, 400–405\\nexception handling, 402–404\\nfor loops, 401–402\\nif statements, 400–401\\npass statements, 402\\nrange function, 404–405\\nternary expressions, 405\\nwhile loops, 402\\nxrange function, 404–405\\nfunctions in, 420–430\\nanonymous functions, 424\\nare objects, 422–423\\nclosures, 425–426\\ncurrying of, 427\\nextended call syntax for, 426\\nlambda functions, 424\\nnamespaces for, 420–421\\nreturning multiple values from, 422\\nscope of, 420–421\\ngenerators in, 427–430\\ngenerator expressions, 429\\nitertools module for, 429–430\\nIDEs for, 11\\ninterpreter for, 386\\nlist comprehensions in, 418–420\\nlists in, 408–411\\nadding elements to, 408–409\\nbinary search of, 410\\ncombining, 409\\ninsertion into sorted, 410\\nremoving elements from, 408–409\\nslicing, 410–411\\nsorting, 409–410\\nPython 2 vs. Python 3, 11\\nrequired libraries, 3–6\\nIPython, 5\\nmatplotlib, 5\\nNumPy, 4\\npandas, 4–5\\nSciPy, 6\\nsemantics of, 387–395\\nattributes in, 391\\ncomments in, 388\\nfunctions in, 389\\nimport directive, 392–393\\nindentation, 387–388\\nmethods in, 389\\nmutable objects in, 394–395\\nobject model, 388\\noperators for, 393\\nreferences in, 389–390\\nstrict evaluation, 394\\nstrongly-typed language, 390–391\\nvariables in, 389–390\\n“duck” typing, 392\\nsequence functions in, 411–413\\nenumerate function, 412\\nreversed function, 413\\nsorted function, 412\\nzip function, 412–413\\nset comprehensions in, 418–420\\nsets in, 416–417\\nsetting up, 6–11\\non Linux, 10–11\\non OS X, 9–10\\non Windows, 7–9\\ntuples in, 406–407\\nmethods for, 407\\nunpacking, 407\\npytz library, 303\\n446 | Index\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Q\\nqcut method, 200, 201, 268, 269, 343\\nqr function, 106\\nQt console for IPython, 55\\nquantile analysis, 268–269\\nquarterly periods, 309–310\\nquartile analysis, 343–345\\nquestion mark (?), 49\\nquicksort sorting method, 376\\nquotechar option, 164\\nquoting option, 164\\nR\\nr file mode, 431\\nr+ file mode, 431\\nRamachandran, Prabhu, 248\\nrand function, 107\\nrandint function, 107, 202\\nrandn function, 89, 107\\nrandom number generation, 106–107\\nrandom sampling with grouping, 271–272\\nrandom walks example, 108–110\\nrange function, 82, 404–405\\nranking data\\ndefined, 135\\nin pandas, 133–135\\nravel method, 356, 357\\nrc method, 231, 232\\nre module, 207\\nread method, 432\\nread-only mode, 431\\nreading\\nfrom databases, 174–176\\nfrom text files in pieces, 160–162\\nreadline functionality, 58\\nreadlines method, 432\\nreadshapefile method, 246\\nread_clipboard function, 155\\nread_csv function, 104, 155, 161, 163, 261,\\n430\\nread_frame function, 175\\nread_fwf function, 155\\nread_table function, 104, 155, 158, 163\\nrecfunctions module, 372\\nreduce method, 368, 369\\nreduceat method, 369\\nreductions, 137\\n(see also aggregations)\\ndefined, 137\\nin pandas, 137–142\\nreferences\\ndefined, 389, 390\\nin Python, 389–390\\nregress function, 274\\nregular expressions (regex)\\ndefined, 207\\nmanipulating strings with, 207–210\\nreindex method, 122–124, 317, 332\\nreload function, 74\\nremove method, 408, 417\\nrename method, 198\\nrenaming axis indexes, 197–198\\nrepeat method, 212, 360\\nreplace method, 196, 206, 212\\nreplicating arrays, 360–361\\nresampling, 312–319, 332\\ndefined, 312\\nOHLC (Open-High-Low-Close)\\nresampling, 316\\nupsampling, 316–317\\nwith groupby method, 316\\nwith periods, 318–319\\nreset_index function, 151\\nreshape method, 190–191, 355, 365\\nreshaping\\narrays, 355–356\\ndefined, 189\\nwith hierarchical indexing, 190–191\\nresources, 12\\nreturn statements, 420\\nreturns\\ncumulative returns, 338–340\\ndefined, 338\\nreturn indexes, 338–340\\nreversed function, 413\\nrfind method, 207\\nright argument, 181\\nright_index argument, 181\\nright_on argument, 181\\nrint function, 96\\nrjust method, 207\\nrollback method, 302\\nrollforward method, 302\\nrolling, 348\\nrolling correlation, 350–351\\nrolling_apply function, 323, 326\\nrolling_corr function, 323, 350\\nIndex | 447\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'rolling_count function, 323\\nrolling_cov function, 323\\nrolling_kurt function, 323\\nrolling_mean function, 321, 323\\nrolling_median function, 323\\nrolling_min function, 323\\nrolling_mint function, 323\\nrolling_quantile function, 323, 326\\nrolling_skew function, 323\\nrolling_std function, 323\\nrolling_sum function, 323\\nrolling_var function, 323\\nrot argument, 234\\nrows option, 277\\nrow_stack function, 359\\nrstrip method, 207, 212\\nr_ object, 359\\nS\\nsave function, 103, 379\\nsave method, 171, 176\\nsavefig method, 231\\nsavez function, 104\\nsaving text files, 104–105\\nscatter method, 239\\nscatter plots, 239–241\\nscatter_matrix function, 241\\nScientific Python base, 7\\nSciPy library, 6\\nscipy-user (mailing list), 12\\nscope, 420–421\\nscreen, clearing, 53\\nscripting languages, 2\\nscripts, 2\\nsearch method, 208, 210\\nsearchsorted method, 376\\nseed function, 107\\nseek method, 432\\nsemantics, 387–395\\nattributes in, 391\\ncomments in, 388\\n“duck” typing, 392\\nfunctions in, 389\\nimport directive, 392–393\\nindentation, 387–388\\nmethods in, 389\\nmutable objects in, 394–395\\nobject model, 388\\noperators for, 393\\nreferences in, 389–390\\nstrict evaluation, 394\\nstrongly-typed language, 390–391\\nvariables in, 389–390\\nsemicolons, 388\\nsentinels, 143, 159\\nsep argument, 160\\nsequence functions, 411–413\\nenumerate function, 412\\nreversed function, 413\\nsorted function, 412\\nzip function, 412–413\\nSeries data structure, 112–115\\narithmetic operations between DataFrame\\nand, 130–132\\ngrouping with, 257–258\\nset comprehensions, 418–420\\nset function, 416\\nsetattr function, 391\\nsetdefault method, 415\\nsetdiff1d method, 103\\nsets/set comprehensions, 416–417\\nsetxor1d method, 103\\nset_index function, 151\\nset_index method, 193\\nset_title method, 226\\nset_trace function, 65\\nset_value method, 128\\nset_xlabel method, 226\\nset_xlim method, 226\\nset_xticklabels method, 226\\nset_xticks method, 226\\nshapefiles, 246\\nshapes, 80, 353\\nsharex option, 223, 234\\nsharey option, 223, 234\\nshell commands in IPython, 60–61\\nshifting in time series data, 301–303\\nshortcuts, keyboard, 53\\nfor deleting text, 53\\nfor IPython, 52\\nshuffle function, 107\\nsign function, 96, 202\\nsignal frontier analysis, 345–347\\nsin function, 96\\nsinh function, 96\\nsize method, 255\\nskew method, 139\\nskipinitialspace option, 165\\n448 | Index\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'skipna method, 138\\nskipna option, 137\\nskiprows argument, 160\\nskip_footer argument, 160\\nslice method, 212\\nslicing\\narrays, 86–89\\nlists, 410–411\\nSocial Security Administration (SSA), 32\\nsolve function, 106\\nsort argument, 181\\nsort method, 101, 373, 409, 424\\nsorted function, 412\\nsorting\\narrays, 101–102\\nfinding elements in sorted array, 376–377\\nin NumPy, 373–377\\nalgorithms for, 375–376\\nfinding elements in sorted array, 376–\\n377\\nindirect sorts, 374–375\\nin pandas, 133–135\\nlevels, 149–150\\nlists, 409–410\\nsortlevel function, 149\\nsort_columns argument, 235\\nsort_index method, 133, 150, 375\\nspaces, structuring code with, 387–388\\nspacing around subplots, 223–224\\nspan, 324\\nspecialized frequencies\\ndata munging for, 332–334\\nsplit method, 165, 206, 210, 212, 358\\nsplit-apply-combine, 252\\nsplitting arrays, 357–359\\nSQL databases, 175\\nsql module, 175\\nSQLite databases, 174\\nsqrt function, 95, 96\\nsquare function, 96\\nsqueeze argument, 160\\nSSA (Social Security Administration), 32\\nstable sorting, 375\\nstacked format, 192\\nstart index, 411\\nstartswith method, 207, 212\\nstatistical methods, 100\\nstd method, 101, 139, 261\\nstdout, 162\\nstep index, 411\\nstop index, 411\\nstrftime method, 291, 400\\nstrict evaluation/language, 394\\nstrides/strided view, 353\\nstrings\\nconverting to datetime, 291–293\\ndata types for, 84, 396–398\\nmanipulating, 205–211\\nmethods for, 206–207\\nvectorized string methods, 210–211\\nwith regular expressions, 207–210\\nstrip method, 207, 212\\nstrongly-typed languages, 390–391, 390\\nstrptime method, 291, 400\\nstructs, 370\\nstructured arrays, 370–372\\nbenefits of, 372\\ndefined, 370\\nmainpulating, 372\\nnested data types, 371–372\\nstyle argument, 233\\nstyling for matplotlib, 224–225\\nsub method, 130, 209\\nsubn method, 210\\nsubperiod, 319\\nsubplots, 220–224\\nsubplots method, 222\\nsubplots_adjust method, 223\\nsubplot_kw option, 223\\nsubsets for arrays, 361–362\\nsubtract function, 96\\nsudo command, 11\\nsuffixes argument, 181\\nsum method, 100, 132, 137, 139, 259, 261, 330,\\n428\\nsummary statistics, 137\\nby level, 150\\ncorrelation and covariance, 139–141\\nisin function, 141–142\\nunique function, 141–142\\nvalue_counts function, 141–142\\nsuperperiod, 319\\nsvd function, 106\\nswapaxes method, 94\\nswaplevel function, 149\\nswapping axes in arrays, 93–94\\nsymmetric_difference method, 417\\nsyntactic sugar, 14\\nIndex | 449\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'system commands, defining alias for, 60\\nT\\ntab completion in IPython, 47–48\\ntabs, structuring code with, 387–388\\ntake method, 202, 362\\ntan function, 96\\ntanh function, 96\\ntell method, 432\\nterminology, 13–14\\nternary expressions, 405\\ntext editors, integrating with IPython, 52\\ntext files, 155–170\\ndelimited formats, 163–165\\nHTML files, 166–170\\nJSON data, 165–166\\nlxml library, 166–170\\nreading in pieces, 160–162\\nsaving and loading, 104–105\\nwriting to, 162–163\\nXML files, 169–170\\nTextParser class, 160, 162, 168\\ntext_content method, 167\\nthousands argument, 160\\nthresh argument, 144\\nticks, 226–227\\ntile function, 360, 361\\ntime series data\\nand performance, 327–328\\ndata types for, 290–293\\nconverting between string and datetime,\\n291–293\\ndate ranges, 298\\nfrequencies, 299–301\\nweek of month dates, 301\\nmoving window functions, 320–326\\nbinary moving window functions, 324–\\n325\\nexponentially-weighted functions, 324\\nuser-defined, 326\\nperiods, 307–312\\nconverting timestamps to, 311\\ncreating PeriodIndex from arrays, 312\\nfrequency conversion for, 308\\nquarterly periods, 309–310\\nplotting, 319–320\\nresampling, 312–319\\nOHLC (Open-High-Low-Close)\\nresampling, 316\\nupsampling, 316–317\\nwith groupby method, 316\\nwith periods, 318–319\\nshifting in, 301–303\\nwith offsets, 302–303\\ntime zones in, 303–306\\nlocalizing objects, 304–305\\nmethods for time zone-aware objects,\\n305–306\\nTimeSeries class, 293–297\\nduplicate indices with, 296–297\\nindexes for, 294–296\\nselecting data in, 294–296\\ntimestamps\\nconverting to periods, 311\\ndefined, 289\\nusing periods instead of, 333–334\\ntiming code, 67–68\\ntitle in matplotlib, 226–227\\ntop method, 267, 282\\nto_csv method, 162, 163\\nto_datetime method, 292\\nto_panel method, 154\\nto_period method, 311\\ntrace function, 106\\ntracebacks, 53–54\\ntransform method, 264–266\\ntransforming data, 194–205\\ndiscretization, 199–201\\ndummy variables, 203–205\\nfiltering outliers, 201–202\\nmapping, 195–196\\npermutation, 202\\nremoving duplicates, 194–195\\nrenaming axis indexes, 197–198\\nreplacing values, 196–197\\ntranspose method, 93, 94\\ntransposing arrays, 93–94\\ntrellis package, 247\\ntrigonometric functions, 96\\ntruncate method, 296\\ntry/except block, 403, 404\\ntuples, 406–407\\nmethods for, 407\\nunpacking, 407\\ntype casting, 399\\ntype command, 156\\nTypeError event, 84, 403\\ntypes, 388\\n450 | Index\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'tz_convert method, 305\\ntz_localize method, 304, 305\\nU\\nU file mode, 431\\nuint16 data type, 84\\nuint32 data type, 84\\nuint64 data type, 84\\nuint8 data type, 84\\nunary functions, 95\\nunderscore (_), 48, 58\\nunicode type, 19, 84, 395\\nuniform function, 107\\nunion method, 103, 122, 204, 417\\nunique method, 102–103, 122, 141–142, 279\\nuniversal functions, 95–96, 367–370\\ncustom, 370\\nin pandas, 132–133\\ninstance methods for, 368–369\\nuniversal newline mode, 431\\nunpacking tuples, 407\\nunstack function, 148\\nupdate method, 337\\nupper method, 207, 212\\nupsampling, 312, 316–317\\nUS baby names 1880-2010 example, 32–43\\nboy names that became girl names, 42–43\\nmeasuring increase in diversity, 37–40\\nrevolution of last letter, 40–41\\nusa.gov data from bit.ly example, 17–26\\nUSDA (US Department of Agriculture) food\\ndatabase example, 212–217\\nuse_index argument, 234\\nUTC (coordinated universal time), 303\\nV\\nValueError event, 402, 403\\nvalues method, 414\\nvalue_counts method, 141–142\\nvar method, 101, 139, 261\\nvariables, 55\\n(see also environment variables)\\ndeleting, 55\\ndisplaying, 55\\nin Python, 389–390\\nVaroquaux, Gaël, 248\\nvectorization, 85\\ndefined, 97\\nvectorize function, 370\\nvectorized string methods, 210–211\\nverbose argument, 160\\nverify_integrity argument, 188\\nviews, 86, 118\\nvisualization tools\\nChaco, 248\\nmayavi, 248\\nvsplit function, 359\\nvstack function, 358\\nW\\nw file mode, 431\\nWattenberg, Laura, 40\\nWeb APIs, file input/output with, 173–174\\nweek of month dates, 301\\nwhen expressions, 394\\nwhere function, 98–100, 188\\nwhile loops, 402\\nwhitespace, structuring code with, 387–388\\nWickham, Hadley, 252\\nWilliams, Ashley, 212\\nWindows, setting up Python on, 7–9\\nworking directory\\nchanging to passed directory, 60\\nof current system, returning, 60\\nwrangling (see data wrangling)\\nwrite method, 431\\nwrite-only mode, 431\\nwritelines method, 431\\nwriter method, 165\\nwriting\\nto databases, 174–176\\nto text files, 162–163\\nX\\nXcode, 9\\nxlim method, 225, 226\\nXML (extensible markup language) files, 169–\\n170\\nxrange function, 404–405\\nxs method, 128\\nxticklabels method, 225\\nY\\nyield keyword, 428\\nylim argument, 234\\nyticks argument, 234\\nIndex | 451\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': 'Z\\nzeros function, 82\\nzip function, 412–413\\n452 | Index\\nwww.it-ebooks.info'},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf',\n",
       "  'page_content': \"About the Author\\nWes McKinney is a New York −based data hacker and entrepreneur. After finishing\\nhis undergraduate degree in mathematics at MIT in 2007, he went on to do quantitative\\nfinance work at AQR Capital Management in Greenwich, CT. Frustrated by cumber-\\nsome data analysis tools, he learned Python and in 2008, started building what would\\nlater become the pandas project. He's now an active member of the scientific Python\\ncommunity and is an advocate for the use of Python in data analysis, finance, and\\nstatistical computing applications.\\nColophon\\nThe animal on the cover of Python for Data Analysis is a golden-tailed, or pen-tailed,\\ntree shrew (Ptilocercus lowii). The golden-tailed tree shrew is the only one of its species\\nin the genus Ptilocercus and family Ptilocercidae; all the other tree shrews are of the\\nfamily Tupaiidae. Tree shrews are identified by their long tails and soft red-brown fur.\\nAs nicknamed, the golden-tailed tree shrew has a tail that resembles the feather on a\\nquill pen. Tree shrews are omnivores, feeding primarily on insects, fruit, seeds, and\\nsmall vertebrates.\\nFound predominantly in Indonesia, Malaysia, and Thailand, these wild mammals are\\nknown for their chronic consumption of alcohol. Malaysian tree shrews were found to\\nspend several hours consuming the naturally fermented nectar of the bertam palm,\\nequalling about 10 to 12 glasses of wine with 3.8% alcohol content. Despite this, no\\ngolden-tailed tree shrew has ever been intoxicated, thanks largely to their impressive\\nethanol breakdown, which includes metabolizing the alcohol in a way not used by\\nhumans. Also more impressive than any of their mammal counterparts, including hu-\\nmans? Brain to body mass ratio.\\nDespite these mammals’ name, the golden-tailed shrew is not a true shrew, instead\\nmore closely related to primates. Because of their close relation, tree shrews have be-\\ncome an alternative to primates in medical experimentation for myopia, psychosocial\\nstress, and hepatitis.\\nThe cover image is from Cassel’s Natural History. The cover font is Adobe ITC Gara-\\nmond. The text font is Linotype Birka; the heading font is Adobe Myriad Condensed;\\nand the code font is LucasFont’s TheSansMonoCondensed.\\nwww.it-ebooks.info\"},\n",
       " {'source': 'data/DATA_ANALYSIS.pdf', 'page_content': 'www.it-ebooks.info'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trimmed_extracted_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a979cf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating chunks of data from the extracted text\n",
    "def create_chunks(trimmed_extracted_doc):\n",
    "    text_splitter=RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200\n",
    "    )\n",
    "    docs=text_splitter.split_documents(trimmed_extracted_doc)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7966b863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='www.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='www.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Python for Data Analysis\\nWes McKinney\\nBeijing • Cambridge • Farnham • Köln • Sebastopol • Tokyo\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Python for Data Analysis\\nby Wes McKinney\\nCopyright © 2013 Wes McKinney. All rights reserved.\\nPrinted in the United States of America.\\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions\\nare also available for most titles ( http://my.safaribooksonline.com). For more information, contact our\\ncorporate/institutional sales department: 800-998-9938 or corporate@oreilly.com.\\nEditors: Julie Steele and Meghan Blanchette\\nProduction Editor: Melanie Yarbrough\\nCopyeditor: Teresa Exley\\nProofreader: BIM Publishing Services\\nIndexer: BIM Publishing Services\\nCover Designer: Karen Montgomery\\nInterior Designer: David Futato\\nIllustrator: Rebecca Demarest\\nOctober 2012: First Edition. \\nRevision History for the First Edition:\\n2012-10-05 First release\\nSee http://oreilly.com/catalog/errata.csp?isbn=9781449319793 for release details.'),\n",
       " Document(metadata={}, page_content='October 2012: First Edition. \\nRevision History for the First Edition:\\n2012-10-05 First release\\nSee http://oreilly.com/catalog/errata.csp?isbn=9781449319793 for release details.\\nNutshell Handbook, the Nutshell Handbook logo, and the O’Reilly logo are registered trademarks of\\nO’Reilly Media, Inc. Python for Data Analysis, the cover image of a golden-tailed tree shrew, and related\\ntrade dress are trademarks of O’Reilly Media, Inc.\\nMany of the designations used by manufacturers and sellers to distinguish their products are claimed as\\ntrademarks. Where those designations appear in this book, and O’Reilly Media, Inc., was aware of a\\ntrademark claim, the designations have been printed in caps or initial caps.\\nWhile every precaution has been taken in the preparation of this book, the publisher and author assume\\nno responsibility for errors or omissions, or for damages resulting from the use of the information con-\\ntained herein.\\nISBN: 978-1-449-31979-3\\n[LSI]\\n1349356084\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Table of Contents\\nPreface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xi\\n1. Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1\\nWhat Is This Book About? 1\\nWhy Python for Data Analysis? 2\\nPython as Glue 2\\nSolving the “Two-Language” Problem 2\\nWhy Not Python? 3\\nEssential Python Libraries 3\\nNumPy 4\\npandas 4\\nmatplotlib 5\\nIPython 5\\nSciPy 6\\nInstallation and Setup 6\\nWindows 7\\nApple OS X 9\\nGNU/Linux 10\\nPython 2 and Python 3 11\\nIntegrated Development Environments (IDEs) 11\\nCommunity and Conferences 12\\nNavigating This Book 12\\nCode Examples 13\\nData for Examples 13\\nImport Conventions 13\\nJargon 13\\nAcknowledgements 14\\n2. Introductory Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  17\\n1.usa.gov data from bit.ly 17\\nCounting Time Zones in Pure Python 19\\niii'),\n",
       " Document(metadata={}, page_content='1.usa.gov data from bit.ly 17\\nCounting Time Zones in Pure Python 19\\niii\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Counting Time Zones with pandas 21\\nMovieLens 1M Data Set 26\\nMeasuring rating disagreement 30\\nUS Baby Names 1880-2010 32\\nAnalyzing Naming Trends 36\\nConclusions and The Path Ahead 43\\n3. IPython: An Interactive Computing and Development Environment . . . . . . . . . . . .  45\\nIPython Basics 46\\nTab Completion 47\\nIntrospection 48\\nThe %run Command 49\\nExecuting Code from the Clipboard 50\\nKeyboard Shortcuts 52\\nExceptions and Tracebacks 53\\nMagic Commands 54\\nQt-based Rich GUI Console 55\\nMatplotlib Integration and Pylab Mode 56\\nUsing the Command History 58\\nSearching and Reusing the Command History 58\\nInput and Output Variables 58\\nLogging the Input and Output 59\\nInteracting with the Operating System 60\\nShell Commands and Aliases 60\\nDirectory Bookmark System 62\\nSoftware Development Tools 62\\nInteractive Debugger 62\\nTiming Code: %time and %timeit 67\\nBasic Profiling: %prun and %run -p 68\\nProfiling a Function Line-by-Line 70\\nIPython HTML Notebook 72\\nTips for Productive Code Development Using IPython 72'),\n",
       " Document(metadata={}, page_content='Timing Code: %time and %timeit 67\\nBasic Profiling: %prun and %run -p 68\\nProfiling a Function Line-by-Line 70\\nIPython HTML Notebook 72\\nTips for Productive Code Development Using IPython 72\\nReloading Module Dependencies 74\\nCode Design Tips 74\\nAdvanced IPython Features 76\\nMaking Your Own Classes IPython-friendly 76\\nProfiles and Configuration 77\\nCredits 78\\n4. NumPy Basics: Arrays and Vectorized Computation . . . . . . . . . . . . . . . . . . . . . . . . . .  79\\nThe NumPy ndarray: A Multidimensional Array Object 80\\nCreating ndarrays 81\\nData Types for ndarrays 83\\niv | Table of Contents\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Operations between Arrays and Scalars 85\\nBasic Indexing and Slicing 86\\nBoolean Indexing 89\\nFancy Indexing 92\\nTransposing Arrays and Swapping Axes 93\\nUniversal Functions: Fast Element-wise Array Functions 95\\nData Processing Using Arrays 97\\nExpressing Conditional Logic as Array Operations 98\\nMathematical and Statistical Methods 100\\nMethods for Boolean Arrays 101\\nSorting 101\\nUnique and Other Set Logic 102\\nFile Input and Output with Arrays 103\\nStoring Arrays on Disk in Binary Format 103\\nSaving and Loading Text Files 104\\nLinear Algebra 105\\nRandom Number Generation 106\\nExample: Random Walks 108\\nSimulating Many Random Walks at Once 109\\n5. Getting Started with pandas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  111\\nIntroduction to pandas Data Structures 112\\nSeries 112\\nDataFrame 115\\nIndex Objects 120\\nEssential Functionality 122\\nReindexing 122\\nDropping entries from an axis 125\\nIndexing, selection, and filtering 125\\nArithmetic and data alignment 128'),\n",
       " Document(metadata={}, page_content='Series 112\\nDataFrame 115\\nIndex Objects 120\\nEssential Functionality 122\\nReindexing 122\\nDropping entries from an axis 125\\nIndexing, selection, and filtering 125\\nArithmetic and data alignment 128\\nFunction application and mapping 132\\nSorting and ranking 133\\nAxis indexes with duplicate values 136\\nSummarizing and Computing Descriptive Statistics 137\\nCorrelation and Covariance 139\\nUnique Values, Value Counts, and Membership 141\\nHandling Missing Data 142\\nFiltering Out Missing Data 143\\nFilling in Missing Data 145\\nHierarchical Indexing 147\\nReordering and Sorting Levels 149\\nSummary Statistics by Level 150\\nUsing a DataFrame’s Columns 150\\nTable of Contents | v\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Other pandas Topics 151\\nInteger Indexing 151\\nPanel Data 152\\n6. Data Loading, Storage, and File Formats . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  155\\nReading and Writing Data in Text Format 155\\nReading Text Files in Pieces 160\\nWriting Data Out to Text Format 162\\nManually Working with Delimited Formats 163\\nJSON Data 165\\nXML and HTML: Web Scraping 166\\nBinary Data Formats 171\\nUsing HDF5 Format 171\\nReading Microsoft Excel Files 172\\nInteracting with HTML and Web APIs 173\\nInteracting with Databases 174\\nStoring and Loading Data in MongoDB 176\\n7. Data Wrangling: Clean, Transform, Merge, Reshape . . . . . . . . . . . . . . . . . . . . . . . .  177\\nCombining and Merging Data Sets 177\\nDatabase-style DataFrame Merges 178\\nMerging on Index 182\\nConcatenating Along an Axis 185\\nCombining Data with Overlap 188\\nReshaping and Pivoting 189\\nReshaping with Hierarchical Indexing 190\\nPivoting “long” to “wide” Format 192\\nData Transformation 194\\nRemoving Duplicates 194'),\n",
       " Document(metadata={}, page_content='Combining Data with Overlap 188\\nReshaping and Pivoting 189\\nReshaping with Hierarchical Indexing 190\\nPivoting “long” to “wide” Format 192\\nData Transformation 194\\nRemoving Duplicates 194\\nTransforming Data Using a Function or Mapping 195\\nReplacing Values 196\\nRenaming Axis Indexes 197\\nDiscretization and Binning 199\\nDetecting and Filtering Outliers 201\\nPermutation and Random Sampling 202\\nComputing Indicator/Dummy Variables 203\\nString Manipulation 205\\nString Object Methods 206\\nRegular expressions 207\\nVectorized string functions in pandas 210\\nExample: USDA Food Database 212\\nvi | Table of Contents\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='8. Plotting and Visualization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  219\\nA Brief matplotlib API Primer 219\\nFigures and Subplots 220\\nColors, Markers, and Line Styles 224\\nTicks, Labels, and Legends 225\\nAnnotations and Drawing on a Subplot 228\\nSaving Plots to File 231\\nmatplotlib Configuration 231\\nPlotting Functions in pandas 232\\nLine Plots 232\\nBar Plots 235\\nHistograms and Density Plots 238\\nScatter Plots 239\\nPlotting Maps: Visualizing Haiti Earthquake Crisis Data 241\\nPython Visualization Tool Ecosystem 247\\nChaco 248\\nmayavi 248\\nOther Packages 248\\nThe Future of Visualization Tools? 249\\n9. Data Aggregation and Group Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  251\\nGroupBy Mechanics 252\\nIterating Over Groups 255\\nSelecting a Column or Subset of Columns 256\\nGrouping with Dicts and Series 257\\nGrouping with Functions 258\\nGrouping by Index Levels 259\\nData Aggregation 259'),\n",
       " Document(metadata={}, page_content='Iterating Over Groups 255\\nSelecting a Column or Subset of Columns 256\\nGrouping with Dicts and Series 257\\nGrouping with Functions 258\\nGrouping by Index Levels 259\\nData Aggregation 259\\nColumn-wise and Multiple Function Application 262\\nReturning Aggregated Data in “unindexed” Form 264\\nGroup-wise Operations and Transformations 264\\nApply: General split-apply-combine 266\\nQuantile and Bucket Analysis 268\\nExample: Filling Missing Values with Group-specific Values 270\\nExample: Random Sampling and Permutation 271\\nExample: Group Weighted Average and Correlation 273\\nExample: Group-wise Linear Regression 274\\nPivot Tables and Cross-Tabulation 275\\nCross-Tabulations: Crosstab 277\\nExample: 2012 Federal Election Commission Database 278\\nDonation Statistics by Occupation and Employer 280\\nBucketing Donation Amounts 283\\nDonation Statistics by State 285\\nTable of Contents | vii\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='10. Time Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  289\\nDate and Time Data Types and Tools 290\\nConverting between string and datetime 291\\nTime Series Basics 293\\nIndexing, Selection, Subsetting 294\\nTime Series with Duplicate Indices 296\\nDate Ranges, Frequencies, and Shifting 297\\nGenerating Date Ranges 298\\nFrequencies and Date Offsets 299\\nShifting (Leading and Lagging) Data 301\\nTime Zone Handling 303\\nLocalization and Conversion 304\\nOperations with Time Zone−aware Timestamp Objects 305\\nOperations between Different Time Zones 306\\nPeriods and Period Arithmetic 307\\nPeriod Frequency Conversion 308\\nQuarterly Period Frequencies 309\\nConverting Timestamps to Periods (and Back) 311\\nCreating a PeriodIndex from Arrays 312\\nResampling and Frequency Conversion 312\\nDownsampling 314\\nUpsampling and Interpolation 316\\nResampling with Periods 318\\nTime Series Plotting 319\\nMoving Window Functions 320'),\n",
       " Document(metadata={}, page_content='Resampling and Frequency Conversion 312\\nDownsampling 314\\nUpsampling and Interpolation 316\\nResampling with Periods 318\\nTime Series Plotting 319\\nMoving Window Functions 320\\nExponentially-weighted functions 324\\nBinary Moving Window Functions 324\\nUser-Defined Moving Window Functions 326\\nPerformance and Memory Usage Notes 327\\n11. Financial and Economic Data Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  329\\nData Munging Topics 329\\nTime Series and Cross-Section Alignment 330\\nOperations with Time Series of Different Frequencies 332\\nTime of Day and “as of” Data Selection 334\\nSplicing Together Data Sources 336\\nReturn Indexes and Cumulative Returns 338\\nGroup Transforms and Analysis 340\\nGroup Factor Exposures 342\\nDecile and Quartile Analysis 343\\nMore Example Applications 345\\nSignal Frontier Analysis 345\\nFuture Contract Rolling 347\\nviii | Table of Contents\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Rolling Correlation and Linear Regression 350\\n12. Advanced NumPy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  353\\nndarray Object Internals 353\\nNumPy dtype Hierarchy 354\\nAdvanced Array Manipulation 355\\nReshaping Arrays 355\\nC versus Fortran Order 356\\nConcatenating and Splitting Arrays 357\\nRepeating Elements: Tile and Repeat 360\\nFancy Indexing Equivalents: Take and Put 361\\nBroadcasting 362\\nBroadcasting Over Other Axes 364\\nSetting Array Values by Broadcasting 367\\nAdvanced ufunc Usage 367\\nufunc Instance Methods 368\\nCustom ufuncs 370\\nStructured and Record Arrays 370\\nNested dtypes and Multidimensional Fields 371\\nWhy Use Structured Arrays? 372\\nStructured Array Manipulations: numpy.lib.recfunctions 372\\nMore About Sorting 373\\nIndirect Sorts: argsort and lexsort 374\\nAlternate Sort Algorithms 375\\nnumpy.searchsorted: Finding elements in a Sorted Array 376\\nNumPy Matrix Class 377\\nAdvanced Array Input and Output 379\\nMemory-mapped Files 379'),\n",
       " Document(metadata={}, page_content='Alternate Sort Algorithms 375\\nnumpy.searchsorted: Finding elements in a Sorted Array 376\\nNumPy Matrix Class 377\\nAdvanced Array Input and Output 379\\nMemory-mapped Files 379\\nHDF5 and Other Array Storage Options 380\\nPerformance Tips 380\\nThe Importance of Contiguous Memory 381\\nOther Speed Options: Cython, f2py, C 382\\nAppendix: Python Language Essentials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  385\\nIndex . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  433\\nTable of Contents | ix\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='www.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Preface\\nThe scientific Python ecosystem of open source libraries has grown substantially over\\nthe last 10 years. By late 2011, I had long felt that the lack of centralized learning\\nresources for data analysis and statistical applications was a stumbling block for new\\nPython programmers engaged in such work. Key projects for data analysis (especially\\nNumPy, IPython, matplotlib, and pandas) had also matured enough that a book written\\nabout them would likely not go out-of-date very quickly. Thus, I mustered the nerve\\nto embark on this writing project. This is the book that I wish existed when I started\\nusing Python for data analysis in 2007. I hope you find it useful and are able to apply\\nthese tools productively in your work.\\nConventions Used in This Book\\nThe following typographical conventions are used in this book:\\nItalic\\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\\nConstant width'),\n",
       " Document(metadata={}, page_content='Conventions Used in This Book\\nThe following typographical conventions are used in this book:\\nItalic\\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\\nConstant width\\nUsed for program listings, as well as within paragraphs to refer to program elements\\nsuch as variable or function names, databases, data types, environment variables,\\nstatements, and keywords.\\nConstant width bold\\nShows commands or other text that should be typed literally by the user.\\nConstant width italic\\nShows text that should be replaced with user-supplied values or by values deter-\\nmined by context.\\nThis icon signifies a tip, suggestion, or general note.\\nxi\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='This icon indicates a warning or caution.\\nUsing Code Examples\\nThis book is here to help you get your job done. In general, you may use the code in\\nthis book in your programs and documentation. You do not need to contact us for\\npermission unless you’re reproducing a significant portion of the code. For example,\\nwriting a program that uses several chunks of code from this book does not require\\npermission. Selling or distributing a CD-ROM of examples from O’Reilly books does\\nrequire permission. Answering a question by citing this book and quoting example\\ncode does not require permission. Incorporating a significant amount of example code\\nfrom this book into your product’s documentation does require permission.\\nWe appreciate, but do not require, attribution. An attribution usually includes the title,\\nauthor, publisher, and ISBN. For example: “Python for Data Analysis by William Wes-\\nley McKinney (O’Reilly). Copyright 2012 William McKinney, 978-1-449-31979-3.”'),\n",
       " Document(metadata={}, page_content='author, publisher, and ISBN. For example: “Python for Data Analysis by William Wes-\\nley McKinney (O’Reilly). Copyright 2012 William McKinney, 978-1-449-31979-3.”\\nIf you feel your use of code examples falls outside fair use or the permission given above,\\nfeel free to contact us at permissions@oreilly.com.\\nSafari® Books Online\\nSafari Books Online (www.safaribooksonline.com) is an on-demand digital\\nlibrary that delivers expert content in both book and video form from the\\nworld’s leading authors in technology and business.\\nTechnology professionals, software developers, web designers, and business and cre-\\native professionals use Safari Books Online as their primary resource for research,\\nproblem solving, learning, and certification training.\\nSafari Books Online offers a range of product mixes and pricing programs for organi-\\nzations, government agencies, and individuals. Subscribers have access to thousands'),\n",
       " Document(metadata={}, page_content='Safari Books Online offers a range of product mixes and pricing programs for organi-\\nzations, government agencies, and individuals. Subscribers have access to thousands\\nof books, training videos, and prepublication manuscripts in one fully searchable da-\\ntabase from publishers like O’Reilly Media, Prentice Hall Professional, Addison-Wesley\\nProfessional, Microsoft Press, Sams, Que, Peachpit Press, Focal Press, Cisco Press, John\\nWiley & Sons, Syngress, Morgan Kaufmann, IBM Redbooks, Packt, Adobe Press, FT\\nPress, Apress, Manning, New Riders, McGraw-Hill, Jones & Bartlett, Course Tech-\\nnology, and dozens more. For more information about Safari Books Online, please visit\\nus online.\\nxii | Preface\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='How to Contact Us\\nPlease address comments and questions concerning this book to the publisher:\\nO’Reilly Media, Inc.\\n1005 Gravenstein Highway North\\nSebastopol, CA 95472\\n800-998-9938 (in the United States or Canada)\\n707-829-0515 (international or local)\\n707-829-0104 (fax)\\nWe have a web page for this book, where we list errata, examples, and any additional\\ninformation. You can access this page at http://oreil.ly/python_for_data_analysis.\\nTo comment or ask technical questions about this book, send email to\\nbookquestions@oreilly.com.\\nFor more information about our books, courses, conferences, and news, see our website\\nat http://www.oreilly.com.\\nFind us on Facebook: http://facebook.com/oreilly\\nFollow us on Twitter: http://twitter.com/oreillymedia\\nWatch us on YouTube: http://www.youtube.com/oreillymedia\\nPreface | xiii\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='www.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='CHAPTER 1\\nPreliminaries\\nWhat Is This Book About?\\nThis book is concerned with the nuts and bolts of manipulating, processing, cleaning,\\nand crunching data in Python. It is also a practical, modern introduction to scientific\\ncomputing in Python, tailored for data-intensive applications. This is a book about the\\nparts of the Python language and libraries you’ll need to effectively solve a broad set of\\ndata analysis problems. This book is not an exposition on analytical methods using\\nPython as the implementation language.\\nWhen I say “data”, what am I referring to exactly? The primary focus is on structured\\ndata, a deliberately vague term that encompasses many different common forms of\\ndata, such as\\n• Multidimensional arrays (matrices)\\n• Tabular or spreadsheet-like data in which each column may be a different type\\n(string, numeric, date, or otherwise). This includes most kinds of data commonly\\nstored in relational databases or tab- or comma-delimited text files'),\n",
       " Document(metadata={}, page_content='(string, numeric, date, or otherwise). This includes most kinds of data commonly\\nstored in relational databases or tab- or comma-delimited text files\\n• Multiple tables of data interrelated by key columns (what would be primary or\\nforeign keys for a SQL user)\\n• Evenly or unevenly spaced time series\\nThis is by no means a complete list. Even though it may not always be obvious, a large\\npercentage of data sets can be transformed into a structured form that is more suitable\\nfor analysis and modeling. If not, it may be possible to extract features from a data set\\ninto a structured form. As an example, a collection of news articles could be processed\\ninto a word frequency table which could then be used to perform sentiment analysis.\\nMost users of spreadsheet programs like Microsoft Excel, perhaps the most widely used\\ndata analysis tool in the world, will not be strangers to these kinds of data.\\n1\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Why Python for Data Analysis?\\nFor many people (myself among them), the Python language is easy to fall in love with.\\nSince its first appearance in 1991, Python has become one of the most popular dynamic,\\nprogramming languages, along with Perl, Ruby, and others. Python and Ruby have\\nbecome especially popular in recent years for building websites using their numerous\\nweb frameworks, like Rails (Ruby) and Django (Python). Such languages are often\\ncalled scripting languages as they can be used to write quick-and-dirty small programs,\\nor scripts. I don’t like the term “scripting language” as it carries a connotation that they\\ncannot be used for building mission-critical software. Among interpreted languages\\nPython is distinguished by its large and active scientific computing community. Adop-\\ntion of Python for scientific computing in both industry applications and academic\\nresearch has increased significantly since the early 2000s.'),\n",
       " Document(metadata={}, page_content='tion of Python for scientific computing in both industry applications and academic\\nresearch has increased significantly since the early 2000s.\\nFor data analysis and interactive, exploratory computing and data visualization, Python\\nwill inevitably draw comparisons with the many other domain-specific open source\\nand commercial programming languages and tools in wide use, such as R, MATLAB,\\nSAS, Stata, and others. In recent years, Python’s improved library support (primarily\\npandas) has made it a strong alternative for data manipulation tasks. Combined with\\nPython’s strength in general purpose programming, it is an excellent choice as a single\\nlanguage for building data-centric applications.\\nPython as Glue\\nPart of Python’s success as a scientific computing platform is the ease of integrating C,\\nC++, and FORTRAN code. Most modern computing environments share a similar set\\nof legacy FORTRAN and C libraries for doing linear algebra, optimization, integration,'),\n",
       " Document(metadata={}, page_content='C++, and FORTRAN code. Most modern computing environments share a similar set\\nof legacy FORTRAN and C libraries for doing linear algebra, optimization, integration,\\nfast fourier transforms, and other such algorithms. The same story has held true for\\nmany companies and national labs that have used Python to glue together 30 years’\\nworth of legacy software.\\nMost programs consist of small portions of code where most of the time is spent, with\\nlarge amounts of “glue code” that doesn’t run often. In many cases, the execution time\\nof the glue code is insignificant; effort is most fruitfully invested in optimizing the\\ncomputational bottlenecks, sometimes by moving the code to a lower-level language\\nlike C.\\nIn the last few years, the Cython project ( http://cython.org) has become one of the\\npreferred ways of both creating fast compiled extensions for Python and also interfacing\\nwith C and C++ code.\\nSolving the “Two-Language” Problem'),\n",
       " Document(metadata={}, page_content='preferred ways of both creating fast compiled extensions for Python and also interfacing\\nwith C and C++ code.\\nSolving the “Two-Language” Problem\\nIn many organizations, it is common to research, prototype, and test new ideas using\\na more domain-specific computing language like MATLAB or R then later port those\\n2 | Chapter 1: \\u2002Preliminaries\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='ideas to be part of a larger production system written in, say, Java, C#, or C++. What\\npeople are increasingly finding is that Python is a suitable language not only for doing\\nresearch and prototyping but also building the production systems, too. I believe that\\nmore and more companies will go down this path as there are often significant organ-\\nizational benefits to having both scientists and technologists using the same set of pro-\\ngrammatic tools.\\nWhy Not Python?\\nWhile Python is an excellent environment for building computationally-intensive sci-\\nentific applications and building most kinds of general purpose systems, there are a\\nnumber of uses for which Python may be less suitable.\\nAs Python is an interpreted programming language, in general most Python code will\\nrun substantially slower than code written in a compiled language like Java or C++. As\\nprogrammer time is typically more valuable than CPU time, many are happy to make'),\n",
       " Document(metadata={}, page_content='run substantially slower than code written in a compiled language like Java or C++. As\\nprogrammer time is typically more valuable than CPU time, many are happy to make\\nthis tradeoff. However, in an application with very low latency requirements (for ex-\\nample, a high frequency trading system), the time spent programming in a lower-level,\\nlower-productivity language like C++ to achieve the maximum possible performance\\nmight be time well spent.\\nPython is not an ideal language for highly concurrent, multithreaded applications, par-\\nticularly applications with many CPU-bound threads. The reason for this is that it has\\nwhat is known as the global interpreter lock  (GIL), a mechanism which prevents the\\ninterpreter from executing more than one Python bytecode instruction at a time. The\\ntechnical reasons for why the GIL exists are beyond the scope of this book, but as of\\nthis writing it does not seem likely that the GIL will disappear anytime soon. While it'),\n",
       " Document(metadata={}, page_content='technical reasons for why the GIL exists are beyond the scope of this book, but as of\\nthis writing it does not seem likely that the GIL will disappear anytime soon. While it\\nis true that in many big data processing applications, a cluster of computers may be\\nrequired to process a data set in a reasonable amount of time, there are still situations\\nwhere a single-process, multithreaded system is desirable.\\nThis is not to say that Python cannot execute truly multithreaded, parallel code; that\\ncode just cannot be executed in a single Python process. As an example, the Cython\\nproject features easy integration with OpenMP, a C framework for parallel computing,\\nin order to to parallelize loops and thus significantly speed up numerical algorithms.\\nEssential Python Libraries\\nFor those who are less familiar with the scientific Python ecosystem and the libraries\\nused throughout the book, I present the following overview of each library.\\nEssential Python Libraries | 3\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='NumPy\\nNumPy, short for Numerical Python, is the foundational package for scientific com-\\nputing in Python. The majority of this book will be based on NumPy and libraries built\\non top of NumPy. It provides, among other things:\\n• A fast and efficient multidimensional array object ndarray\\n• Functions for performing element-wise computations with arrays or mathematical\\noperations between arrays\\n• Tools for reading and writing array-based data sets to disk\\n• Linear algebra operations, Fourier transform, and random number generation\\n• Tools for integrating connecting C, C++, and Fortran code to Python\\nBeyond the fast array-processing capabilities that NumPy adds to Python, one of its\\nprimary purposes with regards to data analysis is as the primary container for data to\\nbe passed between algorithms. For numerical data, NumPy arrays are a much more\\nefficient way of storing and manipulating data than the other built-in Python data'),\n",
       " Document(metadata={}, page_content='be passed between algorithms. For numerical data, NumPy arrays are a much more\\nefficient way of storing and manipulating data than the other built-in Python data\\nstructures. Also, libraries written in a lower-level language, such as C or Fortran, can\\noperate on the data stored in a NumPy array without copying any data.\\npandas\\npandas provides rich data structures and functions designed to make working with\\nstructured data fast, easy, and expressive. It is, as you will see, one of the critical in-\\ngredients enabling Python to be a powerful and productive data analysis environment.\\nThe primary object in pandas that will be used in this book is the DataFrame, a two-\\ndimensional tabular, column-oriented data structure with both row and column labels:\\n>>> frame\\n    total_bill  tip   sex     smoker  day  time    size\\n1   16.99       1.01  Female  No      Sun  Dinner  2\\n2   10.34       1.66  Male    No      Sun  Dinner  3\\n3   21.01       3.5   Male    No      Sun  Dinner  3'),\n",
       " Document(metadata={}, page_content='1   16.99       1.01  Female  No      Sun  Dinner  2\\n2   10.34       1.66  Male    No      Sun  Dinner  3\\n3   21.01       3.5   Male    No      Sun  Dinner  3\\n4   23.68       3.31  Male    No      Sun  Dinner  2\\n5   24.59       3.61  Female  No      Sun  Dinner  4\\n6   25.29       4.71  Male    No      Sun  Dinner  4\\n7   8.77        2     Male    No      Sun  Dinner  2\\n8   26.88       3.12  Male    No      Sun  Dinner  4\\n9   15.04       1.96  Male    No      Sun  Dinner  2\\n10  14.78       3.23  Male    No      Sun  Dinner  2\\npandas combines the high performance array-computing features of NumPy with the\\nflexible data manipulation capabilities of spreadsheets and relational databases (such\\nas SQL). It provides sophisticated indexing functionality to make it easy to reshape,\\nslice and dice, perform aggregations, and select subsets of data. pandas is the primary\\ntool that we will use in this book.\\n4 | Chapter 1: \\u2002Preliminaries\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='For financial users, pandas features rich, high-performance time series functionality\\nand tools well-suited for working with financial data. In fact, I initially designed pandas\\nas an ideal tool for financial data analysis applications.\\nFor users of the R language for statistical computing, the DataFrame name will be\\nfamiliar, as the object was named after the similar R data.frame object. They are not\\nthe same, however; the functionality provided by data.frame in R is essentially a strict\\nsubset of that provided by the pandas DataFrame. While this is a book about Python, I\\nwill occasionally draw comparisons with R as it is one of the most widely-used open\\nsource data analysis environments and will be familiar to many readers.\\nThe pandas name itself is derived from panel data, an econometrics term for multidi-\\nmensional structured data sets, and Python data analysis itself.\\nmatplotlib\\nmatplotlib is the most popular Python library for producing plots and other 2D data'),\n",
       " Document(metadata={}, page_content='mensional structured data sets, and Python data analysis itself.\\nmatplotlib\\nmatplotlib is the most popular Python library for producing plots and other 2D data\\nvisualizations. It was originally created by John D. Hunter (JDH) and is now maintained\\nby a large team of developers. It is well-suited for creating plots suitable for publication.\\nIt integrates well with IPython (see below), thus providing a comfortable interactive\\nenvironment for plotting and exploring data. The plots are also interactive; you can\\nzoom in on a section of the plot and pan around the plot using the toolbar in the plot\\nwindow.\\nIPython\\nIPython is the component in the standard scientific Python toolset that ties everything\\ntogether. It provides a robust and productive environment for interactive and explor-\\natory computing. It is an enhanced Python shell designed to accelerate the writing,\\ntesting, and debugging of Python code. It is particularly useful for interactively working'),\n",
       " Document(metadata={}, page_content='atory computing. It is an enhanced Python shell designed to accelerate the writing,\\ntesting, and debugging of Python code. It is particularly useful for interactively working\\nwith data and visualizing data with matplotlib. IPython is usually involved with the\\nmajority of my Python work, including running, debugging, and testing code.\\nAside from the standard terminal-based IPython shell, the project also provides\\n• A Mathematica-like HTML notebook for connecting to IPython through a web\\nbrowser (more on this later).\\n• A Qt framework-based GUI console with inline plotting, multiline editing, and\\nsyntax highlighting\\n• An infrastructure for interactive parallel and distributed computing\\nI will devote a chapter to IPython and how to get the most out of its features. I strongly\\nrecommend using it while working through this book.\\nEssential Python Libraries | 5\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='SciPy\\nSciPy is a collection of packages addressing a number of different standard problem\\ndomains in scientific computing. Here is a sampling of the packages included:\\n• scipy.integrate: numerical integration routines and differential equation solvers\\n• scipy.linalg: linear algebra routines and matrix decompositions extending be-\\nyond those provided in numpy.linalg.\\n• scipy.optimize: function optimizers (minimizers) and root finding algorithms\\n• scipy.signal: signal processing tools\\n• scipy.sparse: sparse matrices and sparse linear system solvers\\n• scipy.special: wrapper around SPECFUN, a Fortran library implementing many\\ncommon mathematical functions, such as the gamma function\\n• scipy.stats: standard continuous and discrete probability distributions (density\\nfunctions, samplers, continuous distribution functions), various statistical tests,\\nand more descriptive statistics\\n• scipy.weave: tool for using inline C++ code to accelerate array computations'),\n",
       " Document(metadata={}, page_content='functions, samplers, continuous distribution functions), various statistical tests,\\nand more descriptive statistics\\n• scipy.weave: tool for using inline C++ code to accelerate array computations\\nTogether NumPy and SciPy form a reasonably complete computational replacement\\nfor much of MATLAB along with some of its add-on toolboxes.\\nInstallation and Setup\\nSince everyone uses Python for different applications, there is no single solution for\\nsetting up Python and required add-on packages. Many readers will not have a complete\\nscientific Python environment suitable for following along with this book, so here I will\\ngive detailed instructions to get set up on each operating system. I recommend using\\none of the following base Python distributions:\\n• Enthought Python Distribution: a scientific-oriented Python distribution from En-\\nthought (http://www.enthought.com). This includes EPDFree, a free base scientific\\ndistribution (with NumPy, SciPy, matplotlib, Chaco, and IPython) and EPD Full,'),\n",
       " Document(metadata={}, page_content='thought (http://www.enthought.com). This includes EPDFree, a free base scientific\\ndistribution (with NumPy, SciPy, matplotlib, Chaco, and IPython) and EPD Full,\\na comprehensive suite of more than 100 scientific packages across many domains.\\nEPD Full is free for academic use but has an annual subscription for non-academic\\nusers.\\n• Python(x,y) ( http://pythonxy.googlecode.com): A free scientific-oriented Python\\ndistribution for Windows.\\nI will be using EPDFree for the installation guides, though you are welcome to take\\nanother approach depending on your needs. At the time of this writing, EPD includes\\nPython 2.7, though this might change at some point in the future. After installing, you\\nwill have the following packages installed and importable:\\n6 | Chapter 1: \\u2002Preliminaries\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='• Scientific Python base: NumPy, SciPy, matplotlib, and IPython. These are all in-\\ncluded in EPDFree.\\n• IPython Notebook dependencies: tornado and pyzmq. These are included in EPD-\\nFree.\\n• pandas (version 0.8.2 or higher).\\nAt some point while reading you may wish to install one or more of the following\\npackages: statsmodels, PyTables, PyQt (or equivalently, PySide), xlrd, lxml, basemap,\\npymongo, and requests. These are used in various examples. Installing these optional\\nlibraries is not necessary, and I would would suggest waiting until you need them. For\\nexample, installing PyQt or PyTables from source on OS X or Linux can be rather\\narduous. For now, it’s most important to get up and running with the bare minimum:\\nEPDFree and pandas.\\nFor information on each Python package and links to binary installers or other help,\\nsee the Python Package Index (PyPI, http://pypi.python.org). This is also an excellent\\nresource for finding new Python packages.'),\n",
       " Document(metadata={}, page_content='see the Python Package Index (PyPI, http://pypi.python.org). This is also an excellent\\nresource for finding new Python packages.\\nTo avoid confusion and to keep things simple, I am avoiding discussion\\nof more complex environment management tools like pip and virtua-\\nlenv. There are many excellent guides available for these tools on the\\nInternet.\\nSome users may be interested in alternate Python implementations, such\\nas IronPython, Jython, or PyPy. To make use of the tools presented in\\nthis book, it is (currently) necessary to use the standard C-based Python\\ninterpreter, known as CPython.\\nWindows\\nTo get started on Windows, download the EPDFree installer from http://www.en\\nthought.com, which should be an MSI installer named like epd_free-7.3-1-win-\\nx86.msi. Run the installer and accept the default installation location C:\\\\Python27. If\\nyou had previously installed Python in this location, you may want to delete it manually\\nfirst (or using Add/Remove Programs).'),\n",
       " Document(metadata={}, page_content='you had previously installed Python in this location, you may want to delete it manually\\nfirst (or using Add/Remove Programs).\\nNext, you need to verify that Python has been successfully added to the system path\\nand that there are no conflicts with any prior-installed Python versions. First, open a\\ncommand prompt by going to the Start Menu and starting the Command Prompt ap-\\nplication, also known as cmd.exe. Try starting the Python interpreter by typing\\npython. You should see a message that matches the version of EPDFree you installed:\\nC:\\\\Users\\\\Wes>python\\nPython 2.7.3 |EPD_free 7.3-1 (32-bit)| (default, Apr 12 2012, 14:30:37) on win32\\nType \"credits\", \"demo\" or \"enthought\" for more information.\\n>>>\\nInstallation and Setup | 7\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='If you see a message for a different version of EPD or it doesn’t work at all, you will\\nneed to clean up your Windows environment variables. On Windows 7 you can start\\ntyping “environment variables” in the programs search field and select Edit environ\\nment variables for your account . On Windows XP, you will have to go to Control\\nPanel > System > Advanced > Environment Variables . On the window that pops up,\\nyou are looking for the Path variable. It needs to contain the following two directory\\npaths, separated by semicolons:\\nC:\\\\Python27;C:\\\\Python27\\\\Scripts\\nIf you installed other versions of Python, be sure to delete any other Python-related\\ndirectories from both the system and user Path variables. After making a path alterna-\\ntion, you have to restart the command prompt for the changes to take effect.\\nOnce you can launch Python successfully from the command prompt, you need to\\ninstall pandas. The easiest way is to download the appropriate binary installer from'),\n",
       " Document(metadata={}, page_content='Once you can launch Python successfully from the command prompt, you need to\\ninstall pandas. The easiest way is to download the appropriate binary installer from\\nhttp://pypi.python.org/pypi/pandas. For EPDFree, this should be pandas-0.9.0.win32-\\npy2.7.exe. After you run this, let’s launch IPython and check that things are installed\\ncorrectly by importing pandas and making a simple matplotlib plot:\\nC:\\\\Users\\\\Wes>ipython --pylab\\nPython 2.7.3 |EPD_free 7.3-1 (32-bit)|\\nType \"copyright\", \"credits\" or \"license\" for more information.\\nIPython 0.12.1 -- An enhanced Interactive Python.\\n?         -> Introduction and overview of IPython\\'s features.\\n%quickref -> Quick reference.\\nhelp      -> Python\\'s own help system.\\nobject?   -> Details about \\'object\\', use \\'object??\\' for extra details.\\nWelcome to pylab, a matplotlib-based Python environment [backend: WXAgg].\\nFor more information, type \\'help(pylab)\\'.\\nIn [1]: import pandas\\nIn [2]: plot(arange(10))'),\n",
       " Document(metadata={}, page_content=\"Welcome to pylab, a matplotlib-based Python environment [backend: WXAgg].\\nFor more information, type 'help(pylab)'.\\nIn [1]: import pandas\\nIn [2]: plot(arange(10))\\nIf successful, there should be no error messages and a plot window will appear. You\\ncan also check that the IPython HTML notebook can be successfully run by typing:\\n$ ipython notebook --pylab=inline\\nIf you use the IPython notebook application on Windows and normally\\nuse Internet Explorer, you will likely need to install and run Mozilla\\nFirefox or Google Chrome instead.\\nEPDFree on Windows contains only 32-bit executables. If you want or need a 64-bit\\nsetup on Windows, using EPD Full is the most painless way to accomplish that. If you\\nwould rather install from scratch and not pay for an EPD subscription, Christoph\\nGohlke at the University of California, Irvine, publishes unofficial binary installers for\\n8 | Chapter 1: \\u2002Preliminaries\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content='all of the book’s necessary packages (http://www.lfd.uci.edu/~gohlke/pythonlibs/) for 32-\\nand 64-bit Windows.\\nApple OS X\\nTo get started on OS X, you must first install Xcode, which includes Apple’s suite of\\nsoftware development tools. The necessary component for our purposes is the gcc C\\nand C++ compiler suite. The Xcode installer can be found on the OS X install DVD\\nthat came with your computer or downloaded from Apple directly.\\nOnce you’ve installed Xcode, launch the terminal (Terminal.app) by navigating to\\nApplications > Utilities. Type gcc and press enter. You should hopefully see some-\\nthing like:\\n$ gcc\\ni686-apple-darwin10-gcc-4.2.1: no input files\\nNow you need to install EPDFree. Download the installer which should be a disk image\\nnamed something like epd_free-7.3-1-macosx-i386.dmg. Double-click the .dmg file to\\nmount it, then double-click the .mpkg file inside to run the installer.\\nWhen the installer runs, it automatically appends the EPDFree executable path to'),\n",
       " Document(metadata={}, page_content='mount it, then double-click the .mpkg file inside to run the installer.\\nWhen the installer runs, it automatically appends the EPDFree executable path to\\nyour .bash_profile file. This is located at /Users/your_uname/.bash_profile:\\n# Setting PATH for EPD_free-7.3-1\\nPATH=\"/Library/Frameworks/Python.framework/Versions/Current/bin:${PATH}\"\\nexport PATH\\nShould you encounter any problems in the following steps, you’ll want to inspect\\nyour .bash_profile and potentially add the above directory to your path.\\nNow, it’s time to install pandas. Execute this command in the terminal:\\n$ sudo easy_install pandas\\nSearching for pandas\\nReading http://pypi.python.org/simple/pandas/\\nReading http://pandas.pydata.org\\nReading http://pandas.sourceforge.net\\nBest match: pandas 0.9.0\\nDownloading http://pypi.python.org/packages/source/p/pandas/pandas-0.9.0.zip\\nProcessing pandas-0.9.0.zip\\nWriting /tmp/easy_install-H5mIX6/pandas-0.9.0/setup.cfg'),\n",
       " Document(metadata={}, page_content='Best match: pandas 0.9.0\\nDownloading http://pypi.python.org/packages/source/p/pandas/pandas-0.9.0.zip\\nProcessing pandas-0.9.0.zip\\nWriting /tmp/easy_install-H5mIX6/pandas-0.9.0/setup.cfg\\nRunning pandas-0.9.0/setup.py -q bdist_egg --dist-dir /tmp/easy_install-H5mIX6/\\npandas-0.9.0/egg-dist-tmp-RhLG0z\\nAdding pandas 0.9.0 to easy-install.pth file\\nInstalled /Library/Frameworks/Python.framework/Versions/7.3/lib/python2.7/\\nsite-packages/pandas-0.9.0-py2.7-macosx-10.5-i386.egg\\nProcessing dependencies for pandas\\nFinished processing dependencies for pandas\\nTo verify everything is working, launch IPython in Pylab mode and test importing pan-\\ndas then making a plot interactively:\\nInstallation and Setup | 9\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='$ ipython --pylab\\n22:29 ~/VirtualBox VMs/WindowsXP $ ipython\\nPython 2.7.3 |EPD_free 7.3-1 (32-bit)| (default, Apr 12 2012, 11:28:34)\\nType \"copyright\", \"credits\" or \"license\" for more information.\\nIPython 0.12.1 -- An enhanced Interactive Python.\\n?         -> Introduction and overview of IPython\\'s features.\\n%quickref -> Quick reference.\\nhelp      -> Python\\'s own help system.\\nobject?   -> Details about \\'object\\', use \\'object??\\' for extra details.\\nWelcome to pylab, a matplotlib-based Python environment [backend: WXAgg].\\nFor more information, type \\'help(pylab)\\'.\\nIn [1]: import pandas\\nIn [2]: plot(arange(10))\\nIf this succeeds, a plot window with a straight line should pop up.\\nGNU/Linux\\nSome, but not all, Linux distributions include sufficiently up-to-date\\nversions of all the required Python packages and can be installed using\\nthe built-in package management tool like apt. I detail setup using EPD-\\nFree as it\\'s easily reproducible across distributions.'),\n",
       " Document(metadata={}, page_content=\"versions of all the required Python packages and can be installed using\\nthe built-in package management tool like apt. I detail setup using EPD-\\nFree as it's easily reproducible across distributions.\\nLinux details will vary a bit depending on your Linux flavor, but here I give details for\\nDebian-based GNU/Linux systems like Ubuntu and Mint. Setup is similar to OS X with\\nthe exception of how EPDFree is installed. The installer is a shell script that must be\\nexecuted in the terminal. Depending on whether you have a 32-bit or 64-bit system,\\nyou will either need to install the x86 (32-bit) or x86_64 (64-bit) installer. You will then\\nhave a file named something similar to epd_free-7.3-1-rh5-x86_64.sh. To install it,\\nexecute this script with bash:\\n$ bash epd_free-7.3-1-rh5-x86_64.sh\\nAfter accepting the license, you will be presented with a choice of where to put the\\nEPDFree files. I recommend installing the files in your home directory, say /home/wesm/\"),\n",
       " Document(metadata={}, page_content='After accepting the license, you will be presented with a choice of where to put the\\nEPDFree files. I recommend installing the files in your home directory, say /home/wesm/\\nepd (substituting your own username for wesm).\\nOnce the installer has finished, you need to add EPDFree’s bin directory to your \\n$PATH variable. If you are using the bash shell (the default in Ubuntu, for example), this\\nmeans adding the following path addition in your .bashrc:\\nexport PATH=/home/wesm/epd/bin:$PATH\\nObviously, substitute the installation directory you used for /home/wesm/epd/. After\\ndoing this you can either start a new terminal process or execute your .bashrc again\\nwith source ~/.bashrc.\\n10 | Chapter 1: \\u2002Preliminaries\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='You need a C compiler such as gcc to move forward; many Linux distributions include\\ngcc, but others may not. On Debian systems, you can install gcc by executing:\\nsudo apt-get install gcc\\nIf you type gcc on the command line it should say something like:\\n$ gcc\\ngcc: no input files\\nNow, time to install pandas:\\n$ easy_install pandas\\nIf you installed EPDFree as root, you may need to add sudo to the command and enter\\nthe sudo or root password. To verify things are working, perform the same checks as\\nin the OS X section.\\nPython 2 and Python 3\\nThe Python community is currently undergoing a drawn-out transition from the Python\\n2 series of interpreters to the Python 3 series. Until the appearance of Python 3.0, all\\nPython code was backwards compatible. The community decided that in order to move\\nthe language forward, certain backwards incompatible changes were necessary.\\nI am writing this book with Python 2.7 as its basis, as the majority of the scientific'),\n",
       " Document(metadata={}, page_content='the language forward, certain backwards incompatible changes were necessary.\\nI am writing this book with Python 2.7 as its basis, as the majority of the scientific\\nPython community has not yet transitioned to Python 3. The good news is that, with\\na few exceptions, you should have no trouble following along with the book if you\\nhappen to be using Python 3.2.\\nIntegrated Development Environments (IDEs)\\nWhen asked about my standard development environment, I almost always say “IPy-\\nthon plus a text editor”. I typically write a program and iteratively test and debug each\\npiece of it in IPython. It is also useful to be able to play around with data interactively\\nand visually verify that a particular set of data manipulations are doing the right thing.\\nLibraries like pandas and NumPy are designed to be easy-to-use in the shell.\\nHowever, some will still prefer to work in an IDE instead of a text editor. They do'),\n",
       " Document(metadata={}, page_content='Libraries like pandas and NumPy are designed to be easy-to-use in the shell.\\nHowever, some will still prefer to work in an IDE instead of a text editor. They do\\nprovide many nice “code intelligence” features like completion or quickly pulling up\\nthe documentation associated with functions and classes. Here are some that you can\\nexplore:\\n• Eclipse with PyDev Plugin\\n• Python Tools for Visual Studio (for Windows users)\\n• PyCharm\\n• Spyder\\n• Komodo IDE\\nInstallation and Setup | 11\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Community and Conferences\\nOutside of an Internet search, the scientific Python mailing lists are generally helpful\\nand responsive to questions. Some ones to take a look at are:\\n• pydata: a Google Group list for questions related to Python for data analysis and\\npandas\\n• pystatsmodels: for statsmodels or pandas-related questions\\n• numpy-discussion: for NumPy-related questions\\n• scipy-user: for general SciPy or scientific Python questions\\nI deliberately did not post URLs for these in case they change. They can be easily located\\nvia Internet search.\\nEach year many conferences are held all over the world for Python programmers. PyCon\\nand EuroPython are the two main general Python conferences in the United States and\\nEurope, respectively. SciPy and EuroSciPy are scientific-oriented Python conferences\\nwhere you will likely find many “birds of a feather” if you become more involved with\\nusing Python for data analysis after reading this book.\\nNavigating This Book'),\n",
       " Document(metadata={}, page_content='where you will likely find many “birds of a feather” if you become more involved with\\nusing Python for data analysis after reading this book.\\nNavigating This Book\\nIf you have never programmed in Python before, you may actually want to start at the\\nend of the book, where I have placed a condensed tutorial on Python syntax, language\\nfeatures, and built-in data structures like tuples, lists, and dicts. These things are con-\\nsidered prerequisite knowledge for the remainder of the book.\\nThe book starts by introducing you to the IPython environment. Next, I give a short\\nintroduction to the key features of NumPy, leaving more advanced NumPy use for\\nanother chapter at the end of the book. Then, I introduce pandas and devote the rest\\nof the book to data analysis topics applying pandas, NumPy, and matplotlib (for vis-\\nualization). I have structured the material in the most incremental way possible, though\\nthere is occasionally some minor cross-over between chapters.'),\n",
       " Document(metadata={}, page_content=\"ualization). I have structured the material in the most incremental way possible, though\\nthere is occasionally some minor cross-over between chapters.\\nData files and related material for each chapter are hosted as a git repository on GitHub:\\nhttp://github.com/pydata/pydata-book\\nI encourage you to download the data and use it to replicate the book’s code examples\\nand experiment with the tools presented in each chapter. I will happily accept contri-\\nbutions, scripts, IPython notebooks, or any other materials you wish to contribute to\\nthe book's repository for all to enjoy.\\n12 | Chapter 1: \\u2002Preliminaries\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content='Code Examples\\nMost of the code examples in the book are shown with input and output as it would\\nappear executed in the IPython shell.\\nIn [5]: code\\nOut[5]: output\\nAt times, for clarity, multiple code examples will be shown side by side. These should\\nbe read left to right and executed separately.\\nIn [5]: code         In [6]: code2\\nOut[5]: output       Out[6]: output2\\nData for Examples\\nData sets for the examples in each chapter are hosted in a repository on GitHub: http:\\n//github.com/pydata/pydata-book. You can download this data either by using the git\\nrevision control command-line program or by downloading a zip file of the repository\\nfrom the website.\\nI have made every effort to ensure that it contains everything necessary to reproduce\\nthe examples, but I may have made some mistakes or omissions. If so, please send me\\nan e-mail: wesmckinn@gmail.com.\\nImport Conventions\\nThe Python community has adopted a number of naming conventions for commonly-\\nused modules:\\nimport numpy as np'),\n",
       " Document(metadata={}, page_content='an e-mail: wesmckinn@gmail.com.\\nImport Conventions\\nThe Python community has adopted a number of naming conventions for commonly-\\nused modules:\\nimport numpy as np\\nimport pandas as pd\\nimport matplotlib.pyplot as plt\\nThis means that when you see np.arange, this is a reference to the arange function in\\nNumPy. This is done as it’s considered bad practice in Python software development\\nto import everything (from numpy import *) from a large package like NumPy.\\nJargon\\nI’ll use some terms common both to programming and data science that you may not\\nbe familiar with. Thus, here are some brief definitions:\\nMunge/Munging/Wrangling\\nDescribes the overall process of manipulating unstructured and/or messy data into\\na structured or clean form. The word has snuck its way into the jargon of many\\nmodern day data hackers. Munge rhymes with “lunge”.\\nNavigating This Book | 13\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Pseudocode\\nA description of an algorithm or process that takes a code-like form while likely\\nnot being actual valid source code.\\nSyntactic sugar\\nProgramming syntax which does not add new features, but makes something more\\nconvenient or easier to type.\\nAcknowledgements\\nIt would have been difficult for me to write this book without the support of a large\\nnumber of people.\\nOn the O’Reilly staff, I’m very grateful for my editors Meghan Blanchette and Julie\\nSteele who guided me through the process. Mike Loukides also worked with me in the\\nproposal stages and helped make the book a reality.\\nI received a wealth of technical review from a large cast of characters. In particular,\\nMartin Blais and Hugh White were incredibly helpful in improving the book’s exam-\\nples, clarity, and organization from cover to cover. James Long, Drew Conway, Fer-\\nnando Pérez, Brian Granger, Thomas Kluyver, Adam Klein, Josh Klein, Chang She, and'),\n",
       " Document(metadata={}, page_content='ples, clarity, and organization from cover to cover. James Long, Drew Conway, Fer-\\nnando Pérez, Brian Granger, Thomas Kluyver, Adam Klein, Josh Klein, Chang She, and\\nStéfan van der Walt each reviewed one or more chapters, providing pointed feedback\\nfrom many different perspectives.\\nI got many great ideas for examples and data sets from friends and colleagues in the\\ndata community, among them: Mike Dewar, Jeff Hammerbacher, James Johndrow,\\nKristian Lum, Adam Klein, Hilary Mason, Chang She, and Ashley Williams.\\nI am of course indebted to the many leaders in the open source scientific Python com-\\nmunity who’ve built the foundation for my development work and gave encouragement\\nwhile I was writing this book: the IPython core team (Fernando Pérez, Brian Granger,\\nMin Ragan-Kelly, Thomas Kluyver, and others), John Hunter, Skipper Seabold, Travis\\nOliphant, Peter Wang, Eric Jones, Robert Kern, Josef Perktold, Francesc Alted, Chris'),\n",
       " Document(metadata={}, page_content='Min Ragan-Kelly, Thomas Kluyver, and others), John Hunter, Skipper Seabold, Travis\\nOliphant, Peter Wang, Eric Jones, Robert Kern, Josef Perktold, Francesc Alted, Chris\\nFonnesbeck, and too many others to mention. Several other people provided a great\\ndeal of support, ideas, and encouragement along the way: Drew Conway, Sean Taylor,\\nGiuseppe Paleologo, Jared Lander, David Epstein, John Krowas, Joshua Bloom, Den\\nPilsworth, John Myles-White, and many others I’ve forgotten.\\nI’d also like to thank a number of people from my formative years. First, my former\\nAQR colleagues who’ve cheered me on in my pandas work over the years: Alex Reyf-\\nman, Michael Wong, Tim Sargen, Oktay Kurbanov, Matthew Tschantz, Roni Israelov,\\nMichael Katz, Chris Uga, Prasad Ramanan, Ted Square, and Hoon Kim. Lastly, my\\nacademic advisors Haynes Miller (MIT) and Mike West (Duke).\\nOn the personal side, Casey Dinkin provided invaluable day-to-day support during the'),\n",
       " Document(metadata={}, page_content='academic advisors Haynes Miller (MIT) and Mike West (Duke).\\nOn the personal side, Casey Dinkin provided invaluable day-to-day support during the\\nwriting process, tolerating my highs and lows as I hacked together the final draft on\\n14 | Chapter 1: \\u2002Preliminaries\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='top of an already overcommitted schedule. Lastly, my parents, Bill and Kim, taught me\\nto always follow my dreams and to never settle for less.\\nAcknowledgements | 15\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='www.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='CHAPTER 2\\nIntroductory Examples\\nThis book teaches you the Python tools to work productively with data. While readers\\nmay have many different end goals for their work, the tasks required generally fall into\\na number of different broad groups:\\nInteracting with the outside world\\nReading and writing with a variety of file formats and databases.\\nPreparation\\nCleaning, munging, combining, normalizing, reshaping, slicing and dicing, and\\ntransforming data for analysis.\\nTransformation\\nApplying mathematical and statistical operations to groups of data sets to derive\\nnew data sets. For example, aggregating a large table by group variables.\\nModeling and computation\\nConnecting your data to statistical models, machine learning algorithms, or other\\ncomputational tools\\nPresentation\\nCreating interactive or static graphical visualizations or textual summaries\\nIn this chapter I will show you a few data sets and some things we can do with them.'),\n",
       " Document(metadata={}, page_content='computational tools\\nPresentation\\nCreating interactive or static graphical visualizations or textual summaries\\nIn this chapter I will show you a few data sets and some things we can do with them.\\nThese examples are just intended to pique your interest and thus will only be explained\\nat a high level. Don’t worry if you have no experience with any of these tools; they will\\nbe discussed in great detail throughout the rest of the book. In the code examples you’ll\\nsee input and output prompts like In [15]:; these are from the IPython shell.\\n1.usa.gov data from bit.ly\\nIn 2011, URL shortening service bit.ly partnered with the United States government\\nwebsite usa.gov to provide a feed of anonymous data gathered from users who shorten\\nlinks ending with .gov or .mil. As of this writing, in addition to providing a live feed,\\nhourly snapshots are available as downloadable text files.1\\n17\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='In the case of the hourly snapshots, each line in each file contains a common form of\\nweb data known as JSON, which stands for JavaScript Object Notation. For example,\\nif we read just the first line of a file you may see something like\\nIn [15]: path = \\'ch02/usagov_bitly_data2012-03-16-1331923249.txt\\'\\nIn [16]: open(path).readline()\\nOut[16]: \\'{ \"a\": \"Mozilla\\\\\\\\/5.0 (Windows NT 6.1; WOW64) AppleWebKit\\\\\\\\/535.11\\n(KHTML, like Gecko) Chrome\\\\\\\\/17.0.963.78 Safari\\\\\\\\/535.11\", \"c\": \"US\", \"nk\": 1,\\n\"tz\": \"America\\\\\\\\/New_York\", \"gr\": \"MA\", \"g\": \"A6qOVH\", \"h\": \"wfLQtf\", \"l\":\\n\"orofrog\", \"al\": \"en-US,en;q=0.8\", \"hh\": \"1.usa.gov\", \"r\":\\n\"http:\\\\\\\\/\\\\\\\\/www.facebook.com\\\\\\\\/l\\\\\\\\/7AQEFzjSi\\\\\\\\/1.usa.gov\\\\\\\\/wfLQtf\", \"u\":\\n\"http:\\\\\\\\/\\\\\\\\/www.ncbi.nlm.nih.gov\\\\\\\\/pubmed\\\\\\\\/22415991\", \"t\": 1331923247, \"hc\":\\n1331822918, \"cy\": \"Danvers\", \"ll\": [ 42.576698, -70.954903 ] }\\\\n\\'\\nPython has numerous built-in and 3rd party modules for converting a JSON string into'),\n",
       " Document(metadata={}, page_content='1331822918, \"cy\": \"Danvers\", \"ll\": [ 42.576698, -70.954903 ] }\\\\n\\'\\nPython has numerous built-in and 3rd party modules for converting a JSON string into\\na Python dictionary object. Here I’ll use the json module and its loads function invoked\\non each line in the sample file I downloaded:\\nimport json\\npath = \\'ch02/usagov_bitly_data2012-03-16-1331923249.txt\\'\\nrecords = [json.loads(line) for line in open(path)]\\nIf you’ve never programmed in Python before, the last expression here is called a list\\ncomprehension, which is a concise way of applying an operation (like json.loads) to a\\ncollection of strings or other objects. Conveniently, iterating over an open file handle\\ngives you a sequence of its lines. The resulting object records is now a list of Python\\ndicts:\\nIn [18]: records[0]\\nOut[18]:\\n{u\\'a\\': u\\'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like\\nGecko) Chrome/17.0.963.78 Safari/535.11\\',\\n u\\'al\\': u\\'en-US,en;q=0.8\\',\\n u\\'c\\': u\\'US\\',\\n u\\'cy\\': u\\'Danvers\\',\\n u\\'g\\': u\\'A6qOVH\\','),\n",
       " Document(metadata={}, page_content=\"{u'a': u'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like\\nGecko) Chrome/17.0.963.78 Safari/535.11',\\n u'al': u'en-US,en;q=0.8',\\n u'c': u'US',\\n u'cy': u'Danvers',\\n u'g': u'A6qOVH',\\n u'gr': u'MA',\\n u'h': u'wfLQtf',\\n u'hc': 1331822918,\\n u'hh': u'1.usa.gov',\\n u'l': u'orofrog',\\n u'll': [42.576698, -70.954903],\\n u'nk': 1,\\n u'r': u'http://www.facebook.com/l/7AQEFzjSi/1.usa.gov/wfLQtf',\\n u't': 1331923247,\\n u'tz': u'America/New_York',\\n u'u': u'http://www.ncbi.nlm.nih.gov/pubmed/22415991'}\\n1. http://www.usa.gov/About/developer-resources/1usagov.shtml\\n18 | Chapter 2: \\u2002Introductory Examples\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"Note that Python indices start at 0 and not 1 like some other languages (like R). It’s\\nnow easy to access individual values within records by passing a string for the key you\\nwish to access:\\nIn [19]: records[0]['tz']\\nOut[19]: u'America/New_York'\\nThe u here in front of the quotation stands for unicode, a standard form of string en-\\ncoding. Note that IPython shows the time zone string object representation here rather\\nthan its print equivalent:\\nIn [20]: print records[0]['tz']\\nAmerica/New_York\\nCounting Time Zones in Pure Python\\nSuppose we were interested in the most often-occurring time zones in the data set (the\\ntz field). There are many ways we could do this. First, let’s extract a list of time zones\\nagain using a list comprehension:\\nIn [25]: time_zones = [rec['tz'] for rec in records]\\n---------------------------------------------------------------------------\\nKeyError                                  Traceback (most recent call last)\"),\n",
       " Document(metadata={}, page_content=\"---------------------------------------------------------------------------\\nKeyError                                  Traceback (most recent call last)\\n/home/wesm/book_scripts/whetting/<ipython> in <module>()\\n----> 1 time_zones = [rec['tz'] for rec in records]\\nKeyError: 'tz'\\nOops! Turns out that not all of the records have a time zone field. This is easy to handle\\nas we can add the check if 'tz' in rec at the end of the list comprehension:\\nIn [26]: time_zones = [rec['tz'] for rec in records if 'tz' in rec]\\nIn [27]: time_zones[:10]\\nOut[27]:\\n[u'America/New_York',\\n u'America/Denver',\\n u'America/New_York',\\n u'America/Sao_Paulo',\\n u'America/New_York',\\n u'America/New_York',\\n u'Europe/Warsaw',\\n u'',\\n u'',\\n u'']\\nJust looking at the first 10 time zones we see that some of them are unknown (empty).\\nYou can filter these out also but I’ll leave them in for now. Now, to produce counts by\\ntime zone I’ll show two approaches: the harder way (using just the Python standard\"),\n",
       " Document(metadata={}, page_content='You can filter these out also but I’ll leave them in for now. Now, to produce counts by\\ntime zone I’ll show two approaches: the harder way (using just the Python standard\\nlibrary) and the easier way (using pandas). One way to do the counting is to use a dict\\nto store counts while we iterate through the time zones:\\ndef get_counts(sequence):\\n    counts = {}\\n1.usa.gov data from bit.ly | 19\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"for x in sequence:\\n        if x in counts:\\n            counts[x] += 1\\n        else:\\n            counts[x] = 1\\n    return counts\\nIf you know a bit more about the Python standard library, you might prefer to write\\nthe same thing more briefly:\\nfrom collections import defaultdict\\ndef get_counts2(sequence):\\n    counts = defaultdict(int) # values will initialize to 0\\n    for x in sequence:\\n        counts[x] += 1\\n    return counts\\nI put this logic in a function just to make it more reusable. To use it on the time zones,\\njust pass the time_zones list:\\nIn [31]: counts = get_counts(time_zones)\\nIn [32]: counts['America/New_York']\\nOut[32]: 1251\\nIn [33]: len(time_zones)\\nOut[33]: 3440\\nIf we wanted the top 10 time zones and their counts, we have to do a little bit of dic-\\ntionary acrobatics:\\ndef top_counts(count_dict, n=10):\\n    value_key_pairs = [(count, tz) for tz, count in count_dict.items()]\\n    value_key_pairs.sort()\\n    return value_key_pairs[-n:]\\nWe have then:\\nIn [35]: top_counts(counts)\"),\n",
       " Document(metadata={}, page_content=\"value_key_pairs = [(count, tz) for tz, count in count_dict.items()]\\n    value_key_pairs.sort()\\n    return value_key_pairs[-n:]\\nWe have then:\\nIn [35]: top_counts(counts)\\nOut[35]:\\n[(33, u'America/Sao_Paulo'),\\n (35, u'Europe/Madrid'),\\n (36, u'Pacific/Honolulu'),\\n (37, u'Asia/Tokyo'),\\n (74, u'Europe/London'),\\n (191, u'America/Denver'),\\n (382, u'America/Los_Angeles'),\\n (400, u'America/Chicago'),\\n (521, u''),\\n (1251, u'America/New_York')]\\n20 | Chapter 2: \\u2002Introductory Examples\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"If you search the Python standard library, you may find the collections.Counter class,\\nwhich makes this task a lot easier:\\nIn [49]: from collections import Counter\\nIn [50]: counts = Counter(time_zones)\\nIn [51]: counts.most_common(10)\\nOut[51]:\\n[(u'America/New_York', 1251),\\n (u'', 521),\\n (u'America/Chicago', 400),\\n (u'America/Los_Angeles', 382),\\n (u'America/Denver', 191),\\n (u'Europe/London', 74),\\n (u'Asia/Tokyo', 37),\\n (u'Pacific/Honolulu', 36),\\n (u'Europe/Madrid', 35),\\n (u'America/Sao_Paulo', 33)]\\nCounting Time Zones with pandas\\nThe main pandas data structure is the DataFrame, which you can think of as repre-\\nsenting a table or spreadsheet of data. Creating a DataFrame from the original set of\\nrecords is simple:\\nIn [289]: from pandas import DataFrame, Series\\nIn [290]: import pandas as pd\\nIn [291]: frame = DataFrame(records)\\nIn [292]: frame\\nOut[292]: \\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 3560 entries, 0 to 3559\\nData columns:\\n_heartbeat_    120  non-null values\"),\n",
       " Document(metadata={}, page_content=\"In [291]: frame = DataFrame(records)\\nIn [292]: frame\\nOut[292]: \\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 3560 entries, 0 to 3559\\nData columns:\\n_heartbeat_    120  non-null values\\na              3440  non-null values\\nal             3094  non-null values\\nc              2919  non-null values\\ncy             2919  non-null values\\ng              3440  non-null values\\ngr             2919  non-null values\\nh              3440  non-null values\\nhc             3440  non-null values\\nhh             3440  non-null values\\nkw             93  non-null values\\nl              3440  non-null values\\nll             2919  non-null values\\nnk             3440  non-null values\\nr              3440  non-null values\\nt              3440  non-null values\\ntz             3440  non-null values\\n1.usa.gov data from bit.ly | 21\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"u              3440  non-null values\\ndtypes: float64(4), object(14)\\nIn [293]: frame['tz'][:10]\\nOut[293]: \\n0     America/New_York\\n1       America/Denver\\n2     America/New_York\\n3    America/Sao_Paulo\\n4     America/New_York\\n5     America/New_York\\n6        Europe/Warsaw\\n7                     \\n8                     \\n9                     \\nName: tz\\nThe output shown for the frame is the summary view, shown for large DataFrame ob-\\njects. The Series object returned by frame['tz'] has a method value_counts that gives\\nus what we’re looking for:\\nIn [294]: tz_counts = frame['tz'].value_counts()\\nIn [295]: tz_counts[:10]\\nOut[295]: \\nAmerica/New_York       1251\\n                        521\\nAmerica/Chicago         400\\nAmerica/Los_Angeles     382\\nAmerica/Denver          191\\nEurope/London            74\\nAsia/Tokyo               37\\nPacific/Honolulu         36\\nEurope/Madrid            35\\nAmerica/Sao_Paulo        33\\nThen, we might want to make a plot of this data using plotting library, matplotlib. You\"),\n",
       " Document(metadata={}, page_content=\"Asia/Tokyo               37\\nPacific/Honolulu         36\\nEurope/Madrid            35\\nAmerica/Sao_Paulo        33\\nThen, we might want to make a plot of this data using plotting library, matplotlib. You\\ncan do a bit of munging to fill in a substitute value for unknown and missing time zone\\ndata in the records. The fillna function can replace missing (NA) values and unknown\\n(empty strings) values can be replaced by boolean array indexing:\\nIn [296]: clean_tz = frame['tz'].fillna('Missing')\\nIn [297]: clean_tz[clean_tz == ''] = 'Unknown'\\nIn [298]: tz_counts = clean_tz.value_counts()\\nIn [299]: tz_counts[:10]\\nOut[299]: \\nAmerica/New_York       1251\\nUnknown                 521\\nAmerica/Chicago         400\\nAmerica/Los_Angeles     382\\nAmerica/Denver          191\\nMissing                 120\\n22 | Chapter 2: \\u2002Introductory Examples\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"Europe/London            74\\nAsia/Tokyo               37\\nPacific/Honolulu         36\\nEurope/Madrid            35\\nMaking a horizontal bar plot can be accomplished using the plot method on the\\ncounts objects:\\nIn [301]: tz_counts[:10].plot(kind='barh', rot=0)\\nSee Figure 2-1 for the resulting figure. We’ll explore more tools for working with this\\nkind of data. For example, the a field contains information about the browser, device,\\nor application used to perform the URL shortening:\\nIn [302]: frame['a'][1]\\nOut[302]: u'GoogleMaps/RochesterNY'\\nIn [303]: frame['a'][50]\\nOut[303]: u'Mozilla/5.0 (Windows NT 5.1; rv:10.0.2) Gecko/20100101 Firefox/10.0.2'\\nIn [304]: frame['a'][51]\\nOut[304]: u'Mozilla/5.0 (Linux; U; Android 2.2.2; en-us; LG-P925/V10e Build/FRG83G) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1'\\nFigure 2-1. Top time zones in the 1.usa.gov sample data\\nParsing all of the interesting information in these “agent” strings may seem like a\"),\n",
       " Document(metadata={}, page_content='Figure 2-1. Top time zones in the 1.usa.gov sample data\\nParsing all of the interesting information in these “agent” strings may seem like a\\ndaunting task. Luckily, once you have mastered Python’s built-in string functions and\\nregular expression capabilities, it is really not so bad. For example, we could split off\\nthe first token in the string (corresponding roughly to the browser capability) and make\\nanother summary of the user behavior:\\nIn [305]: results = Series([x.split()[0] for x in frame.a.dropna()])\\nIn [306]: results[:5]\\nOut[306]: \\n0               Mozilla/5.0\\n1    GoogleMaps/RochesterNY\\n2               Mozilla/4.0\\n3               Mozilla/5.0\\n4               Mozilla/5.0\\n1.usa.gov data from bit.ly | 23\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"In [307]: results.value_counts()[:8]\\nOut[307]: \\nMozilla/5.0                 2594\\nMozilla/4.0                  601\\nGoogleMaps/RochesterNY       121\\nOpera/9.80                    34\\nTEST_INTERNET_AGENT           24\\nGoogleProducer                21\\nMozilla/6.0                    5\\nBlackBerry8520/5.0.0.681       4\\nNow, suppose you wanted to decompose the top time zones into Windows and non-\\nWindows users. As a simplification, let’s say that a user is on Windows if the string\\n'Windows' is in the agent string. Since some of the agents are missing, I’ll exclude these\\nfrom the data:\\nIn [308]: cframe = frame[frame.a.notnull()]\\nWe want to then compute a value whether each row is Windows or not:\\nIn [309]: operating_system = np.where(cframe['a'].str.contains('Windows'),\\n   .....:                             'Windows', 'Not Windows')\\nIn [310]: operating_system[:5]\\nOut[310]: \\n0        Windows\\n1    Not Windows\\n2        Windows\\n3    Not Windows\\n4        Windows\\nName: a\"),\n",
       " Document(metadata={}, page_content=\".....:                             'Windows', 'Not Windows')\\nIn [310]: operating_system[:5]\\nOut[310]: \\n0        Windows\\n1    Not Windows\\n2        Windows\\n3    Not Windows\\n4        Windows\\nName: a\\nThen, you can group the data by its time zone column and this new list of operating\\nsystems:\\nIn [311]: by_tz_os = cframe.groupby(['tz', operating_system])\\nThe group counts, analogous to the value_counts function above, can be computed\\nusing size. This result is then reshaped into a table with unstack:\\nIn [312]: agg_counts = by_tz_os.size().unstack().fillna(0)\\nIn [313]: agg_counts[:10]\\nOut[313]: \\na                               Not Windows  Windows\\ntz                                                  \\n                                        245      276\\nAfrica/Cairo                              0        3\\nAfrica/Casablanca                         0        1\\nAfrica/Ceuta                              0        2\\nAfrica/Johannesburg                       0        1\"),\n",
       " Document(metadata={}, page_content='Africa/Casablanca                         0        1\\nAfrica/Ceuta                              0        2\\nAfrica/Johannesburg                       0        1\\nAfrica/Lusaka                             0        1\\nAmerica/Anchorage                         4        1\\nAmerica/Argentina/Buenos_Aires            1        0\\n24 | Chapter 2: \\u2002Introductory Examples\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='America/Argentina/Cordoba                 0        1\\nAmerica/Argentina/Mendoza                 0        1\\nFinally, let’s select the top overall time zones. To do so, I construct an indirect index\\narray from the row counts in agg_counts:\\n# Use to sort in ascending order\\nIn [314]: indexer = agg_counts.sum(1).argsort()\\nIn [315]: indexer[:10]\\nOut[315]: \\ntz\\n                                  24\\nAfrica/Cairo                      20\\nAfrica/Casablanca                 21\\nAfrica/Ceuta                      92\\nAfrica/Johannesburg               87\\nAfrica/Lusaka                     53\\nAmerica/Anchorage                 54\\nAmerica/Argentina/Buenos_Aires    57\\nAmerica/Argentina/Cordoba         26\\nAmerica/Argentina/Mendoza         55\\nI then use take to select the rows in that order, then slice off the last 10 rows:\\nIn [316]: count_subset = agg_counts.take(indexer)[-10:]\\nIn [317]: count_subset\\nOut[317]: \\na                    Not Windows  Windows\\ntz'),\n",
       " Document(metadata={}, page_content=\"In [316]: count_subset = agg_counts.take(indexer)[-10:]\\nIn [317]: count_subset\\nOut[317]: \\na                    Not Windows  Windows\\ntz                                       \\nAmerica/Sao_Paulo             13       20\\nEurope/Madrid                 16       19\\nPacific/Honolulu               0       36\\nAsia/Tokyo                     2       35\\nEurope/London                 43       31\\nAmerica/Denver               132       59\\nAmerica/Los_Angeles          130      252\\nAmerica/Chicago              115      285\\n                             245      276\\nAmerica/New_York             339      912\\nThen, as shown in the preceding code block, this can be plotted in a bar plot; I’ll make\\nit a stacked bar plot by passing stacked=True (see Figure 2-2) :\\nIn [319]: count_subset.plot(kind='barh', stacked=True)\\nThe plot doesn’t make it easy to see the relative percentage of Windows users in the\\nsmaller groups, but the rows can easily be normalized to sum to 1 then plotted again\\n(see Figure 2-3):\"),\n",
       " Document(metadata={}, page_content=\"The plot doesn’t make it easy to see the relative percentage of Windows users in the\\nsmaller groups, but the rows can easily be normalized to sum to 1 then plotted again\\n(see Figure 2-3):\\nIn [321]: normed_subset = count_subset.div(count_subset.sum(1), axis=0)\\nIn [322]: normed_subset.plot(kind='barh', stacked=True)\\n1.usa.gov data from bit.ly | 25\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content='All of the methods employed here will be examined in great detail throughout the rest\\nof the book.\\nMovieLens 1M Data Set\\nGroupLens Research (http://www.grouplens.org/node/73) provides a number of collec-\\ntions of movie ratings data collected from users of MovieLens in the late 1990s and\\nFigure 2-2. Top time zones by Windows and non-Windows users\\nFigure 2-3. Percentage Windows and non-Windows users in top-occurring time zones\\n26 | Chapter 2: \\u2002Introductory Examples\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"early 2000s. The data provide movie ratings, movie metadata (genres and year), and\\ndemographic data about the users (age, zip code, gender, and occupation). Such data\\nis often of interest in the development of recommendation systems based on machine\\nlearning algorithms. While I will not be exploring machine learning techniques in great\\ndetail in this book, I will show you how to slice and dice data sets like these into the\\nexact form you need.\\nThe MovieLens 1M data set contains 1 million ratings collected from 6000 users on\\n4000 movies. It’s spread across 3 tables: ratings, user information, and movie infor-\\nmation. After extracting the data from the zip file, each table can be loaded into a pandas\\nDataFrame object using pandas.read_table:\\nimport pandas as pd\\nunames = ['user_id', 'gender', 'age', 'occupation', 'zip']\\nusers = pd.read_table('ml-1m/users.dat', sep='::', header=None,\\n                      names=unames)\\nrnames = ['user_id', 'movie_id', 'rating', 'timestamp']\"),\n",
       " Document(metadata={}, page_content=\"users = pd.read_table('ml-1m/users.dat', sep='::', header=None,\\n                      names=unames)\\nrnames = ['user_id', 'movie_id', 'rating', 'timestamp']\\nratings = pd.read_table('ml-1m/ratings.dat', sep='::', header=None,\\n                        names=rnames)\\nmnames = ['movie_id', 'title', 'genres']\\nmovies = pd.read_table('ml-1m/movies.dat', sep='::', header=None,\\n                        names=mnames)\\nYou can verify that everything succeeded by looking at the first few rows of each Da-\\ntaFrame with Python's slice syntax:\\nIn [334]: users[:5]\\nOut[334]: \\n   user_id gender  age  occupation    zip\\n0        1      F    1          10  48067\\n1        2      M   56          16  70072\\n2        3      M   25          15  55117\\n3        4      M   45           7  02460\\n4        5      M   25          20  55455\\nIn [335]: ratings[:5]\\nOut[335]: \\n   user_id  movie_id  rating  timestamp\\n0        1      1193       5  978300760\\n1        1       661       3  978302109\"),\n",
       " Document(metadata={}, page_content=\"4        5      M   25          20  55455\\nIn [335]: ratings[:5]\\nOut[335]: \\n   user_id  movie_id  rating  timestamp\\n0        1      1193       5  978300760\\n1        1       661       3  978302109\\n2        1       914       3  978301968\\n3        1      3408       4  978300275\\n4        1      2355       5  978824291\\nIn [336]: movies[:5]\\nOut[336]: \\n   movie_id                               title                        genres\\n0         1                    Toy Story (1995)   Animation|Children's|Comedy\\n1         2                      Jumanji (1995)  Adventure|Children's|Fantasy\\n2         3             Grumpier Old Men (1995)                Comedy|Romance\\n3         4            Waiting to Exhale (1995)                  Comedy|Drama\\nMovieLens 1M Data Set | 27\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"4         5  Father of the Bride Part II (1995)                        Comedy\\nIn [337]: ratings\\nOut[337]: \\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 1000209 entries, 0 to 1000208\\nData columns:\\nuser_id      1000209  non-null values\\nmovie_id     1000209  non-null values\\nrating       1000209  non-null values\\ntimestamp    1000209  non-null values\\ndtypes: int64(4)\\nNote that ages and occupations are coded as integers indicating groups described in\\nthe data set’s README file. Analyzing the data spread across three tables is not a simple\\ntask; for example, suppose you wanted to compute mean ratings for a particular movie\\nby sex and age. As you will see, this is much easier to do with all of the data merged\\ntogether into a single table. Using pandas’s merge function, we first merge ratings with\\nusers then merging that result with the movies data. pandas infers which columns to\\nuse as the merge (or join) keys based on overlapping names:\"),\n",
       " Document(metadata={}, page_content=\"users then merging that result with the movies data. pandas infers which columns to\\nuse as the merge (or join) keys based on overlapping names:\\nIn [338]: data = pd.merge(pd.merge(ratings, users), movies)\\nIn [339]: data\\nOut[339]: \\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 1000209 entries, 0 to 1000208\\nData columns:\\nuser_id       1000209  non-null values\\nmovie_id      1000209  non-null values\\nrating        1000209  non-null values\\ntimestamp     1000209  non-null values\\ngender        1000209  non-null values\\nage           1000209  non-null values\\noccupation    1000209  non-null values\\nzip           1000209  non-null values\\ntitle         1000209  non-null values\\ngenres        1000209  non-null values\\ndtypes: int64(6), object(4)\\nIn [340]: data.ix[0]\\nOut[340]: \\nuser_id                                 1\\nmovie_id                                1\\nrating                                  5\\ntimestamp                       978824268\\ngender                                  F\"),\n",
       " Document(metadata={}, page_content=\"movie_id                                1\\nrating                                  5\\ntimestamp                       978824268\\ngender                                  F\\nage                                     1\\noccupation                             10\\nzip                                 48067\\ntitle                    Toy Story (1995)\\ngenres        Animation|Children's|Comedy\\nName: 0\\n28 | Chapter 2: \\u2002Introductory Examples\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"In this form, aggregating the ratings grouped by one or more user or movie attributes\\nis straightforward once you build some familiarity with pandas. To get mean movie\\nratings for each film grouped by gender, we can use the pivot_table method:\\nIn [341]: mean_ratings = data.pivot_table('rating', rows='title',\\n   .....:                                 cols='gender', aggfunc='mean')\\nIn [342]: mean_ratings[:5]\\nOut[342]: \\ngender                                F         M\\ntitle                                            \\n$1,000,000 Duck (1971)         3.375000  2.761905\\n'Night Mother (1986)           3.388889  3.352941\\n'Til There Was You (1997)      2.675676  2.733333\\n'burbs, The (1989)             2.793478  2.962085\\n...And Justice for All (1979)  3.828571  3.689024\\nThis produced another DataFrame containing mean ratings with movie totals as row\\nlabels and gender as column labels. First, I’m going to filter down to movies that re-\"),\n",
       " Document(metadata={}, page_content=\"This produced another DataFrame containing mean ratings with movie totals as row\\nlabels and gender as column labels. First, I’m going to filter down to movies that re-\\nceived at least 250 ratings (a completely arbitrary number); to do this, I group the data\\nby title and use size() to get a Series of group sizes for each title:\\nIn [343]: ratings_by_title = data.groupby('title').size()\\nIn [344]: ratings_by_title[:10]\\nOut[344]: \\ntitle\\n$1,000,000 Duck (1971)                37\\n'Night Mother (1986)                  70\\n'Til There Was You (1997)             52\\n'burbs, The (1989)                   303\\n...And Justice for All (1979)        199\\n1-900 (1994)                           2\\n10 Things I Hate About You (1999)    700\\n101 Dalmatians (1961)                565\\n101 Dalmatians (1996)                364\\n12 Angry Men (1957)                  616\\nIn [345]: active_titles = ratings_by_title.index[ratings_by_title >= 250]\\nIn [346]: active_titles\\nOut[346]:\"),\n",
       " Document(metadata={}, page_content=\"101 Dalmatians (1996)                364\\n12 Angry Men (1957)                  616\\nIn [345]: active_titles = ratings_by_title.index[ratings_by_title >= 250]\\nIn [346]: active_titles\\nOut[346]: \\nIndex(['burbs, The (1989), 10 Things I Hate About You (1999),\\n       101 Dalmatians (1961), ..., Young Sherlock Holmes (1985),\\n       Zero Effect (1998), eXistenZ (1999)], dtype=object)\\nThe index of titles receiving at least 250 ratings can then be used to select rows from\\nmean_ratings above:\\nIn [347]: mean_ratings = mean_ratings.ix[active_titles]\\nIn [348]: mean_ratings\\nOut[348]: \\n<class 'pandas.core.frame.DataFrame'>\\nIndex: 1216 entries, 'burbs, The (1989) to eXistenZ (1999)\\nMovieLens 1M Data Set | 29\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"Data columns:\\nF    1216  non-null values\\nM    1216  non-null values\\ndtypes: float64(2)\\nTo see the top films among female viewers, we can sort by the F column in descending\\norder:\\nIn [350]: top_female_ratings = mean_ratings.sort_index(by='F', ascending=False)\\nIn [351]: top_female_ratings[:10]\\nOut[351]: \\ngender                                                         F         M\\nClose Shave, A (1995)                                   4.644444  4.473795\\nWrong Trousers, The (1993)                              4.588235  4.478261\\nSunset Blvd. (a.k.a. Sunset Boulevard) (1950)           4.572650  4.464589\\nWallace & Gromit: The Best of Aardman Animation (1996)  4.563107  4.385075\\nSchindler's List (1993)                                 4.562602  4.491415\\nShawshank Redemption, The (1994)                        4.539075  4.560625\\nGrand Day Out, A (1992)                                 4.537879  4.293255\\nTo Kill a Mockingbird (1962)                            4.536667  4.372611\"),\n",
       " Document(metadata={}, page_content=\"Grand Day Out, A (1992)                                 4.537879  4.293255\\nTo Kill a Mockingbird (1962)                            4.536667  4.372611\\nCreature Comforts (1990)                                4.513889  4.272277\\nUsual Suspects, The (1995)                              4.513317  4.518248\\nMeasuring rating disagreement\\nSuppose you wanted to find the movies that are most divisive between male and female\\nviewers. One way is to add a column to mean_ratings containing the difference in\\nmeans, then sort by that:\\nIn [352]: mean_ratings['diff'] = mean_ratings['M'] - mean_ratings['F']\\nSorting by 'diff' gives us the movies with the greatest rating difference and which were\\npreferred by women:\\nIn [353]: sorted_by_diff = mean_ratings.sort_index(by='diff')\\nIn [354]: sorted_by_diff[:15]\\nOut[354]: \\ngender                                        F         M      diff\\nDirty Dancing (1987)                   3.790378  2.959596 -0.830782\"),\n",
       " Document(metadata={}, page_content=\"In [354]: sorted_by_diff[:15]\\nOut[354]: \\ngender                                        F         M      diff\\nDirty Dancing (1987)                   3.790378  2.959596 -0.830782\\nJumpin' Jack Flash (1986)              3.254717  2.578358 -0.676359\\nGrease (1978)                          3.975265  3.367041 -0.608224\\nLittle Women (1994)                    3.870588  3.321739 -0.548849\\nSteel Magnolias (1989)                 3.901734  3.365957 -0.535777\\nAnastasia (1997)                       3.800000  3.281609 -0.518391\\nRocky Horror Picture Show, The (1975)  3.673016  3.160131 -0.512885\\nColor Purple, The (1985)               4.158192  3.659341 -0.498851\\nAge of Innocence, The (1993)           3.827068  3.339506 -0.487561\\nFree Willy (1993)                      2.921348  2.438776 -0.482573\\nFrench Kiss (1995)                     3.535714  3.056962 -0.478752\\nLittle Shop of Horrors, The (1960)     3.650000  3.179688 -0.470312\\nGuys and Dolls (1955)                  4.051724  3.583333 -0.468391\"),\n",
       " Document(metadata={}, page_content='Little Shop of Horrors, The (1960)     3.650000  3.179688 -0.470312\\nGuys and Dolls (1955)                  4.051724  3.583333 -0.468391\\nMary Poppins (1964)                    4.197740  3.730594 -0.467147\\nPatch Adams (1998)                     3.473282  3.008746 -0.464536\\n30 | Chapter 2: \\u2002Introductory Examples\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Reversing the order of the rows and again slicing off the top 15 rows, we get the movies\\npreferred by men that women didn’t rate as highly:\\n# Reverse order of rows, take first 15 rows\\nIn [355]: sorted_by_diff[::-1][:15]\\nOut[355]: \\ngender                                         F         M      diff\\nGood, The Bad and The Ugly, The (1966)  3.494949  4.221300  0.726351\\nKentucky Fried Movie, The (1977)        2.878788  3.555147  0.676359\\nDumb & Dumber (1994)                    2.697987  3.336595  0.638608\\nLongest Day, The (1962)                 3.411765  4.031447  0.619682\\nCable Guy, The (1996)                   2.250000  2.863787  0.613787\\nEvil Dead II (Dead By Dawn) (1987)      3.297297  3.909283  0.611985\\nHidden, The (1987)                      3.137931  3.745098  0.607167\\nRocky III (1982)                        2.361702  2.943503  0.581801\\nCaddyshack (1980)                       3.396135  3.969737  0.573602\\nFor a Few Dollars More (1965)           3.409091  3.953795  0.544704'),\n",
       " Document(metadata={}, page_content=\"Caddyshack (1980)                       3.396135  3.969737  0.573602\\nFor a Few Dollars More (1965)           3.409091  3.953795  0.544704\\nPorky's (1981)                          2.296875  2.836364  0.539489\\nAnimal House (1978)                     3.628906  4.167192  0.538286\\nExorcist, The (1973)                    3.537634  4.067239  0.529605\\nFright Night (1985)                     2.973684  3.500000  0.526316\\nBarb Wire (1996)                        1.585366  2.100386  0.515020\\nSuppose instead you wanted the movies that elicited the most disagreement among\\nviewers, independent of gender. Disagreement can be measured by the variance or\\nstandard deviation of the ratings:\\n# Standard deviation of rating grouped by title\\nIn [356]: rating_std_by_title = data.groupby('title')['rating'].std()\\n# Filter down to active_titles\\nIn [357]: rating_std_by_title = rating_std_by_title.ix[active_titles]\\n# Order Series by value in descending order\\nIn [358]: rating_std_by_title.order(ascending=False)[:10]\"),\n",
       " Document(metadata={}, page_content='In [357]: rating_std_by_title = rating_std_by_title.ix[active_titles]\\n# Order Series by value in descending order\\nIn [358]: rating_std_by_title.order(ascending=False)[:10]\\nOut[358]: \\ntitle\\nDumb & Dumber (1994)                     1.321333\\nBlair Witch Project, The (1999)          1.316368\\nNatural Born Killers (1994)              1.307198\\nTank Girl (1995)                         1.277695\\nRocky Horror Picture Show, The (1975)    1.260177\\nEyes Wide Shut (1999)                    1.259624\\nEvita (1996)                             1.253631\\nBilly Madison (1995)                     1.249970\\nFear and Loathing in Las Vegas (1998)    1.246408\\nBicentennial Man (1999)                  1.245533\\nName: rating\\nYou may have noticed that movie genres are given as a pipe-separated (|) string. If you\\nwanted to do some analysis by genre, more work would be required to transform the\\ngenre information into a more usable form. I will revisit this data later in the book to\\nillustrate such a transformation.'),\n",
       " Document(metadata={}, page_content='genre information into a more usable form. I will revisit this data later in the book to\\nillustrate such a transformation.\\nMovieLens 1M Data Set | 31\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='US Baby Names 1880-2010\\nThe United States Social Security Administration (SSA) has made available data on the\\nfrequency of baby names from 1880 through the present. Hadley Wickham, an author\\nof several popular R packages, has often made use of this data set in illustrating data\\nmanipulation in R.\\nIn [4]: names.head(10)\\nOut[4]:\\n        name sex  births  year\\n0       Mary   F    7065  1880\\n1       Anna   F    2604  1880\\n2       Emma   F    2003  1880\\n3  Elizabeth   F    1939  1880\\n4     Minnie   F    1746  1880\\n5   Margaret   F    1578  1880\\n6        Ida   F    1472  1880\\n7      Alice   F    1414  1880\\n8     Bertha   F    1320  1880\\n9      Sarah   F    1288  1880\\nThere are many things you might want to do with the data set:\\n• Visualize the proportion of babies given a particular name (your own, or another\\nname) over time.\\n• Determine the relative rank of a name.\\n• Determine the most popular names in each year or the names with largest increases\\nor decreases.'),\n",
       " Document(metadata={}, page_content='name) over time.\\n• Determine the relative rank of a name.\\n• Determine the most popular names in each year or the names with largest increases\\nor decreases.\\n• Analyze trends in names: vowels, consonants, length, overall diversity, changes in\\nspelling, first and last letters\\n• Analyze external sources of trends: biblical names, celebrities, demographic\\nchanges\\nUsing the tools we’ve looked at so far, most of these kinds of analyses are very straight-\\nforward, so I will walk you through many of them. I encourage you to download and\\nexplore the data yourself. If you find an interesting pattern in the data, I would love to\\nhear about it.\\nAs of this writing, the US Social Security Administration makes available data files, one\\nper year, containing the total number of births for each sex/name combination. The\\nraw archive of these files can be obtained here:\\nhttp://www.ssa.gov/oact/babynames/limits.html\\nIn the event that this page has been moved by the time you’re reading this, it can most'),\n",
       " Document(metadata={}, page_content='raw archive of these files can be obtained here:\\nhttp://www.ssa.gov/oact/babynames/limits.html\\nIn the event that this page has been moved by the time you’re reading this, it can most\\nlikely be located again by Internet search. After downloading the “National data” file\\nnames.zip and unzipping it, you will have a directory containing a series of files like\\nyob1880.txt. I use the UNIX head command to look at the first 10 lines of one of the\\nfiles (on Windows, you can use the more command or open it in a text editor):\\n32 | Chapter 2: \\u2002Introductory Examples\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"In [367]: !head -n 10 names/yob1880.txt\\nMary,F,7065\\nAnna,F,2604\\nEmma,F,2003\\nElizabeth,F,1939\\nMinnie,F,1746\\nMargaret,F,1578\\nIda,F,1472\\nAlice,F,1414\\nBertha,F,1320\\nSarah,F,1288\\nAs this is a nicely comma-separated form, it can be loaded into a DataFrame with\\npandas.read_csv:\\nIn [368]: import pandas as pd\\nIn [369]: names1880 = pd.read_csv('names/yob1880.txt', names=['name', 'sex', 'births'])\\nIn [370]: names1880\\nOut[370]: \\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 2000 entries, 0 to 1999\\nData columns:\\nname      2000  non-null values\\nsex       2000  non-null values\\nbirths    2000  non-null values\\ndtypes: int64(1), object(2)\\nThese files only contain names with at least 5 occurrences in each year, so for simplic-\\nity’s sake we can use the sum of the births column by sex as the total number of births\\nin that year:\\nIn [371]: names1880.groupby('sex').births.sum()\\nOut[371]: \\nsex\\nF       90993\\nM      110493\\nName: births\"),\n",
       " Document(metadata={}, page_content=\"in that year:\\nIn [371]: names1880.groupby('sex').births.sum()\\nOut[371]: \\nsex\\nF       90993\\nM      110493\\nName: births\\nSince the data set is split into files by year, one of the first things to do is to assemble\\nall of the data into a single DataFrame and further to add a year field. This is easy to\\ndo using pandas.concat:\\n# 2010 is the last available year right now\\nyears = range(1880, 2011)\\npieces = []\\ncolumns = ['name', 'sex', 'births']\\nfor year in years:\\n    path = 'names/yob%d.txt' % year\\n    frame = pd.read_csv(path, names=columns)\\n    frame['year'] = year\\n    pieces.append(frame)\\nUS Baby Names 1880-2010 | 33\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"# Concatenate everything into a single DataFrame\\nnames = pd.concat(pieces, ignore_index=True)\\nThere are a couple things to note here. First, remember that concat glues the DataFrame\\nobjects together row-wise by default. Secondly, you have to pass ignore_index=True\\nbecause we’re not interested in preserving the original row numbers returned from\\nread_csv. So we now have a very large DataFrame containing all of the names data:\\nNow the names DataFrame looks like:\\nIn [373]: names\\nOut[373]: \\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 1690784 entries, 0 to 1690783\\nData columns:\\nname      1690784  non-null values\\nsex       1690784  non-null values\\nbirths    1690784  non-null values\\nyear      1690784  non-null values\\ndtypes: int64(2), object(2)\\nWith this data in hand, we can already start aggregating the data at the year and sex\\nlevel using groupby or pivot_table, see Figure 2-4:\\nIn [374]: total_births = names.pivot_table('births', rows='year',\"),\n",
       " Document(metadata={}, page_content=\"level using groupby or pivot_table, see Figure 2-4:\\nIn [374]: total_births = names.pivot_table('births', rows='year',\\n   .....:                                  cols='sex', aggfunc=sum)\\nIn [375]: total_births.tail()\\nOut[375]: \\nsex         F        M\\nyear                  \\n2006  1896468  2050234\\n2007  1916888  2069242\\n2008  1883645  2032310\\n2009  1827643  1973359\\n2010  1759010  1898382\\nIn [376]: total_births.plot(title='Total births by sex and year')\\nNext, let’s insert a column prop with the fraction of babies given each name relative to\\nthe total number of births. A prop value of 0.02 would indicate that 2 out of every 100\\nbabies was given a particular name. Thus, we group the data by year and sex, then add\\nthe new column to each group:\\ndef add_prop(group):\\n    # Integer division floors\\n    births = group.births.astype(float)\\n    group['prop'] = births / births.sum()\\n    return group\\nnames = names.groupby(['year', 'sex']).apply(add_prop)\\n34 | Chapter 2: \\u2002Introductory Examples\"),\n",
       " Document(metadata={}, page_content=\"births = group.births.astype(float)\\n    group['prop'] = births / births.sum()\\n    return group\\nnames = names.groupby(['year', 'sex']).apply(add_prop)\\n34 | Chapter 2: \\u2002Introductory Examples\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"Remember that because births is of integer type, we have to cast either\\nthe numerator or denominator to floating point to compute a fraction\\n(unless you are using Python 3!).\\nThe resulting complete data set now has the following columns:\\nIn [378]: names\\nOut[378]: \\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 1690784 entries, 0 to 1690783\\nData columns:\\nname      1690784  non-null values\\nsex       1690784  non-null values\\nbirths    1690784  non-null values\\nyear      1690784  non-null values\\nprop      1690784  non-null values\\ndtypes: float64(1), int64(2), object(2)\\nWhen performing a group operation like this, it's often valuable to do a sanity check,\\nlike verifying that the prop column sums to 1 within all the groups. Since this is floating\\npoint data, use np.allclose to check that the group sums are sufficiently close to (but\\nperhaps not exactly equal to) 1:\\nIn [379]: np.allclose(names.groupby(['year', 'sex']).prop.sum(), 1)\\nOut[379]: True\"),\n",
       " Document(metadata={}, page_content=\"perhaps not exactly equal to) 1:\\nIn [379]: np.allclose(names.groupby(['year', 'sex']).prop.sum(), 1)\\nOut[379]: True\\nNow that this is done, I’m going to extract a subset of the data to facilitate further\\nanalysis: the top 1000 names for each sex/year combination. This is yet another group\\noperation:\\ndef get_top1000(group):\\n    return group.sort_index(by='births', ascending=False)[:1000]\\nFigure 2-4. Total births by sex and year\\nUS Baby Names 1880-2010 | 35\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"grouped = names.groupby(['year', 'sex'])\\ntop1000 = grouped.apply(get_top1000)\\nIf you prefer a do-it-yourself approach, you could also do:\\npieces = []\\nfor year, group in names.groupby(['year', 'sex']):\\n    pieces.append(group.sort_index(by='births', ascending=False)[:1000])\\ntop1000 = pd.concat(pieces, ignore_index=True)\\nThe resulting data set is now quite a bit smaller:\\nIn [382]: top1000\\nOut[382]: \\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 261877 entries, 0 to 261876\\nData columns:\\nname      261877  non-null values\\nsex       261877  non-null values\\nbirths    261877  non-null values\\nyear      261877  non-null values\\nprop      261877  non-null values\\ndtypes: float64(1), int64(2), object(2)\\nWe’ll use this Top 1,000 data set in the following investigations into the data.\\nAnalyzing Naming Trends\\nWith the full data set and Top 1,000 data set in hand, we can start analyzing various\\nnaming trends of interest. Splitting the Top 1,000 names into the boy and girl portions\"),\n",
       " Document(metadata={}, page_content=\"Analyzing Naming Trends\\nWith the full data set and Top 1,000 data set in hand, we can start analyzing various\\nnaming trends of interest. Splitting the Top 1,000 names into the boy and girl portions\\nis easy to do first:\\nIn [383]: boys = top1000[top1000.sex == 'M']\\nIn [384]: girls = top1000[top1000.sex == 'F']\\nSimple time series, like the number of Johns or Marys for each year can be plotted but\\nrequire a bit of munging to be a bit more useful. Let’s form a pivot table of the total\\nnumber of births by year and name:\\nIn [385]: total_births = top1000.pivot_table('births', rows='year', cols='name',\\n   .....:                                    aggfunc=sum)\\nNow, this can be plotted for a handful of names using DataFrame’s plot method:\\nIn [386]: total_births\\nOut[386]: \\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 131 entries, 1880 to 2010\\nColumns: 6865 entries, Aaden to Zuri\\ndtypes: float64(6865)\\nIn [387]: subset = total_births[['John', 'Harry', 'Mary', 'Marilyn']]\"),\n",
       " Document(metadata={}, page_content='Int64Index: 131 entries, 1880 to 2010\\nColumns: 6865 entries, Aaden to Zuri\\ndtypes: float64(6865)\\nIn [387]: subset = total_births[[\\'John\\', \\'Harry\\', \\'Mary\\', \\'Marilyn\\']]\\nIn [388]: subset.plot(subplots=True, figsize=(12, 10), grid=False,\\n   .....:             title=\"Number of births per year\")\\n36 | Chapter 2: \\u2002Introductory Examples\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"See Figure 2-5 for the result. On looking at this, you might conclude that these names\\nhave grown out of favor with the American population. But the story is actually more\\ncomplicated than that, as will be explored in the next section.\\nFigure 2-5. A few boy and girl names over time\\nMeasuring the increase in naming diversity\\nOne explanation for the decrease in plots above is that fewer parents are choosing\\ncommon names for their children. This hypothesis can be explored and confirmed in\\nthe data. One measure is the proportion of births represented by the top 1000 most\\npopular names, which I aggregate and plot by year and sex:\\nIn [390]: table = top1000.pivot_table('prop', rows='year',\\n   .....:                             cols='sex', aggfunc=sum)\\nIn [391]: table.plot(title='Sum of table1000.prop by year and sex',\\n   .....:            yticks=np.linspace(0, 1.2, 13), xticks=range(1880, 2020, 10))\\nSee Figure 2-6 for this plot. So you can see that, indeed, there appears to be increasing\"),\n",
       " Document(metadata={}, page_content=\".....:            yticks=np.linspace(0, 1.2, 13), xticks=range(1880, 2020, 10))\\nSee Figure 2-6 for this plot. So you can see that, indeed, there appears to be increasing\\nname diversity (decreasing total proportion in the top 1,000). Another interesting met-\\nric is the number of distinct names, taken in order of popularity from highest to lowest,\\nin the top 50% of births. This number is a bit more tricky to compute. Let’s consider\\njust the boy names from 2010:\\nIn [392]: df = boys[boys.year == 2010]\\nIn [393]: df\\nOut[393]: \\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 1000 entries, 260877 to 261876\\nData columns:\\nUS Baby Names 1880-2010 | 37\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"name      1000  non-null values\\nsex       1000  non-null values\\nbirths    1000  non-null values\\nyear      1000  non-null values\\nprop      1000  non-null values\\ndtypes: float64(1), int64(2), object(2)\\nFigure 2-6. Proportion of births represented in top 1000 names by sex\\nAfter sorting prop in descending order, we want to know how many of the most popular\\nnames it takes to reach 50%. You could write a for loop to do this, but a vectorized\\nNumPy way is a bit more clever. Taking the cumulative sum, cumsum, of prop then calling\\nthe method searchsorted returns the position in the cumulative sum at which 0.5 would\\nneed to be inserted to keep it in sorted order:\\nIn [394]: prop_cumsum = df.sort_index(by='prop', ascending=False).prop.cumsum()\\nIn [395]: prop_cumsum[:10]\\nOut[395]: \\n260877    0.011523\\n260878    0.020934\\n260879    0.029959\\n260880    0.038930\\n260881    0.047817\\n260882    0.056579\\n260883    0.065155\\n260884    0.073414\\n260885    0.081528\\n260886    0.089621\"),\n",
       " Document(metadata={}, page_content='260877    0.011523\\n260878    0.020934\\n260879    0.029959\\n260880    0.038930\\n260881    0.047817\\n260882    0.056579\\n260883    0.065155\\n260884    0.073414\\n260885    0.081528\\n260886    0.089621\\nIn [396]: prop_cumsum.searchsorted(0.5)\\nOut[396]: 116\\n38 | Chapter 2: \\u2002Introductory Examples\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"Since arrays are zero-indexed, adding 1 to this result gives you a result of 117. By con-\\ntrast, in 1900 this number was much smaller:\\nIn [397]: df = boys[boys.year == 1900]\\nIn [398]: in1900 = df.sort_index(by='prop', ascending=False).prop.cumsum()\\nIn [399]: in1900.searchsorted(0.5) + 1\\nOut[399]: 25\\nIt should now be fairly straightforward to apply this operation to each year/sex com-\\nbination; groupby those fields and apply a function returning the count for each group:\\ndef get_quantile_count(group, q=0.5):\\n    group = group.sort_index(by='prop', ascending=False)\\n    return group.prop.cumsum().searchsorted(q) + 1\\ndiversity = top1000.groupby(['year', 'sex']).apply(get_quantile_count)\\ndiversity = diversity.unstack('sex')\\nThis resulting DataFrame diversity now has two time series, one for each sex, indexed\\nby year. This can be inspected in IPython and plotted as before (see Figure 2-7):\\nIn [401]: diversity.head()\\nOut[401]: \\nsex    F   M\\nyear        \\n1880  38  14\\n1881  38  14\\n1882  38  15\"),\n",
       " Document(metadata={}, page_content='by year. This can be inspected in IPython and plotted as before (see Figure 2-7):\\nIn [401]: diversity.head()\\nOut[401]: \\nsex    F   M\\nyear        \\n1880  38  14\\n1881  38  14\\n1882  38  15\\n1883  39  15\\n1884  39  16\\nIn [402]: diversity.plot(title=\"Number of popular names in top 50%\")\\nFigure 2-7. Plot of diversity metric by year\\nUS Baby Names 1880-2010 | 39\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"As you can see, girl names have always been more diverse than boy names, and they\\nhave only become more so over time. Further analysis of what exactly is driving the\\ndiversity, like the increase of alternate spellings, is left to the reader.\\nThe “Last letter” Revolution\\nIn 2007, a baby name researcher Laura Wattenberg pointed out on her website ( http:\\n//www.babynamewizard.com) that the distribution of boy names by final letter has\\nchanged significantly over the last 100 years. To see this, I first aggregate all of the births\\nin the full data set by year, sex, and final letter:\\n# extract last letter from name column\\nget_last_letter = lambda x: x[-1]\\nlast_letters = names.name.map(get_last_letter)\\nlast_letters.name = 'last_letter'\\ntable = names.pivot_table('births', rows=last_letters,\\n                          cols=['sex', 'year'], aggfunc=sum)\\nThen, I select out three representative years spanning the history and print the first few\\nrows:\"),\n",
       " Document(metadata={}, page_content=\"cols=['sex', 'year'], aggfunc=sum)\\nThen, I select out three representative years spanning the history and print the first few\\nrows:\\nIn [404]: subtable = table.reindex(columns=[1910, 1960, 2010], level='year')\\nIn [405]: subtable.head()\\nOut[405]: \\nsex               F                      M                \\nyear           1910    1960    2010   1910    1960    2010\\nlast_letter                                               \\na            108376  691247  670605    977    5204   28438\\nb               NaN     694     450    411    3912   38859\\nc                 5      49     946    482   15476   23125\\nd              6750    3729    2607  22111  262112   44398\\ne            133569  435013  313833  28655  178823  129012\\nNext, normalize the table by total births to compute a new table containing proportion\\nof total births for each sex ending in each letter:\\nIn [406]: subtable.sum()\\nOut[406]: \\nsex  year\\nF    1910     396416\\n     1960    2022062\\n     2010    1759010\"),\n",
       " Document(metadata={}, page_content='of total births for each sex ending in each letter:\\nIn [406]: subtable.sum()\\nOut[406]: \\nsex  year\\nF    1910     396416\\n     1960    2022062\\n     2010    1759010\\nM    1910     194198\\n     1960    2132588\\n     2010    1898382\\nIn [407]: letter_prop = subtable / subtable.sum().astype(float)\\nWith the letter proportions now in hand, I can make bar plots for each sex broken\\ndown by year. See Figure 2-8:\\nimport matplotlib.pyplot as plt\\n40 | Chapter 2: \\u2002Introductory Examples\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"fig, axes = plt.subplots(2, 1, figsize=(10, 8))\\nletter_prop['M'].plot(kind='bar', rot=0, ax=axes[0], title='Male')\\nletter_prop['F'].plot(kind='bar', rot=0, ax=axes[1], title='Female',\\n                      legend=False)\\nFigure 2-8. Proportion of boy and girl names ending in each letter\\nAs you can see, boy names ending in “n” have experienced significant growth since the\\n1960s. Going back to the full table created above, I again normalize by year and sex\\nand select a subset of letters for the boy names, finally transposing to make each column\\na time series:\\nIn [410]: letter_prop = table / table.sum().astype(float)\\nIn [411]: dny_ts = letter_prop.ix[['d', 'n', 'y'], 'M'].T\\nIn [412]: dny_ts.head()\\nOut[412]: \\n             d         n         y\\nyear                              \\n1880  0.083055  0.153213  0.075760\\n1881  0.083247  0.153214  0.077451\\n1882  0.085340  0.149560  0.077537\\n1883  0.084066  0.151646  0.079144\\n1884  0.086120  0.149915  0.080405\"),\n",
       " Document(metadata={}, page_content='1880  0.083055  0.153213  0.075760\\n1881  0.083247  0.153214  0.077451\\n1882  0.085340  0.149560  0.077537\\n1883  0.084066  0.151646  0.079144\\n1884  0.086120  0.149915  0.080405\\nWith this DataFrame of time series in hand, I can make a plot of the trends over time\\nagain with its plot method (see Figure 2-9):\\nIn [414]: dny_ts.plot()\\nUS Baby Names 1880-2010 | 41\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"Figure 2-9. Proportion of boys born with names ending in d/n/y over time\\nBoy names that became girl names (and vice versa)\\nAnother fun trend is looking at boy names that were more popular with one sex earlier\\nin the sample but have “changed sexes” in the present. One example is the name Lesley\\nor Leslie. Going back to the top1000 dataset, I compute a list of names occurring in the\\ndataset starting with 'lesl':\\nIn [415]: all_names = top1000.name.unique()\\nIn [416]: mask = np.array(['lesl' in x.lower() for x in all_names])\\nIn [417]: lesley_like = all_names[mask]\\nIn [418]: lesley_like\\nOut[418]: array([Leslie, Lesley, Leslee, Lesli, Lesly], dtype=object)\\nFrom there, we can filter down to just those names and sum births grouped by name\\nto see the relative frequencies:\\nIn [419]: filtered = top1000[top1000.name.isin(lesley_like)]\\nIn [420]: filtered.groupby('name').births.sum()\\nOut[420]: \\nname\\nLeslee      1082\\nLesley     35022\\nLesli        929\\nLeslie    370429\\nLesly      10067\\nName: births\"),\n",
       " Document(metadata={}, page_content=\"In [420]: filtered.groupby('name').births.sum()\\nOut[420]: \\nname\\nLeslee      1082\\nLesley     35022\\nLesli        929\\nLeslie    370429\\nLesly      10067\\nName: births\\nNext, let’s aggregate by sex and year and normalize within year:\\n42 | Chapter 2: \\u2002Introductory Examples\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"In [421]: table = filtered.pivot_table('births', rows='year',\\n   .....:                              cols='sex', aggfunc='sum')\\nIn [422]: table = table.div(table.sum(1), axis=0)\\nIn [423]: table.tail()\\nOut[423]: \\nsex   F   M\\nyear       \\n2006  1 NaN\\n2007  1 NaN\\n2008  1 NaN\\n2009  1 NaN\\n2010  1 NaN\\nLastly, it’s now easy to make a plot of the breakdown by sex over time (Figure 2-10):\\nIn [425]: table.plot(style={'M': 'k-', 'F': 'k--'})\\nFigure 2-10. Proportion of male/female Lesley-like names over time\\nConclusions and The Path Ahead\\nThe examples in this chapter are rather simple, but they’re here to give you a bit of a\\nflavor of what sorts of things you can expect in the upcoming chapters. The focus of\\nthis book is on tools as opposed to presenting more sophisticated analytical methods.\\nMastering the techniques in this book will enable you to implement your own analyses\\n(assuming you know what you want to do!) in short order.\\nConclusions and The Path Ahead | 43\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content='www.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='CHAPTER 3\\nIPython: An Interactive Computing and\\nDevelopment Environment\\nAct without doing; work without effort. Think of the small as large and the few as many.\\nConfront the difficult while it is still easy; accomplish the great task by a series of small\\nacts.\\n—Laozi\\nPeople often ask me, “What is your Python development environment?” My answer is\\nalmost always the same, “IPython and a text editor”. You may choose to substitute an\\nIntegrated Development Environment (IDE) for a text editor in order to take advantage\\nof more advanced graphical tools and code completion capabilities. Even if so, I strongly\\nrecommend making IPython an important part of your workflow. Some IDEs even\\nprovide IPython integration, so it’s possible to get the best of both worlds.\\nThe IPython project began in 2001 as Fernando Pérez’s side project to make a better\\ninteractive Python interpreter. In the subsequent 11 years it has grown into what’s'),\n",
       " Document(metadata={}, page_content='The IPython project began in 2001 as Fernando Pérez’s side project to make a better\\ninteractive Python interpreter. In the subsequent 11 years it has grown into what’s\\nwidely considered one of the most important tools in the modern scientific Python\\ncomputing stack. While it does not provide any computational or data analytical tools\\nby itself, IPython is designed from the ground up to maximize your productivity in both\\ninteractive computing and software development. It encourages an execute-explore\\nworkflow instead of the typical edit-compile-run workflow of many other programming\\nlanguages. It also provides very tight integration with the operating system’s shell and\\nfile system. Since much of data analysis coding involves exploration, trial and error,\\nand iteration, IPython will, in almost all cases, help you get the job done faster.\\nOf course, the IPython project now encompasses a great deal more than just an en-'),\n",
       " Document(metadata={}, page_content='and iteration, IPython will, in almost all cases, help you get the job done faster.\\nOf course, the IPython project now encompasses a great deal more than just an en-\\nhanced, interactive Python shell. It also includes a rich GUI console with inline plotting,\\na web-based interactive notebook format, and a lightweight, fast parallel computing\\nengine. And, as with so many other tools designed for and by programmers, it is highly\\ncustomizable. I’ll discuss some of these features later in the chapter.\\n45\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Since IPython has interactivity at its core, some of the features in this chapter are dif-\\nficult to fully illustrate without a live console. If this is your first time learning about\\nIPython, I recommend that you follow along with the examples to get a feel for how\\nthings work. As with any keyboard-driven console-like environment, developing mus-\\ncle-memory for the common commands is part of the learning curve.\\nMany parts of this chapter (for example: profiling and debugging) can\\nbe safely omitted on a first reading as they are not necessary for under-\\nstanding the rest of the book. This chapter is intended to provide a\\nstandalone, rich overview of the functionality provided by IPython.\\nIPython Basics\\nYou can launch IPython on the command line just like launching the regular Python\\ninterpreter except with the ipython command:\\n$ ipython\\nPython 2.7.2 (default, May 27 2012, 21:26:12)\\nType \"copyright\", \"credits\" or \"license\" for more information.'),\n",
       " Document(metadata={}, page_content='interpreter except with the ipython command:\\n$ ipython\\nPython 2.7.2 (default, May 27 2012, 21:26:12)\\nType \"copyright\", \"credits\" or \"license\" for more information.\\nIPython 0.12 -- An enhanced Interactive Python.\\n?         -> Introduction and overview of IPython\\'s features.\\n%quickref -> Quick reference.\\nhelp      -> Python\\'s own help system.\\nobject?   -> Details about \\'object\\', use \\'object??\\' for extra details.\\nIn [1]: a = 5\\nIn [2]: a\\nOut[2]: 5\\nYou can execute arbitrary Python statements by typing them in and pressing\\n<return>. When typing just a variable into IPython, it renders a string representation\\nof the object:\\nIn [542]: data = {i : randn() for i in range(7)}\\nIn [543]: data\\nOut[543]: \\n{0: 0.6900018528091594,\\n 1: 1.0015434424937888,\\n 2: -0.5030873913603446,\\n 3: -0.6222742250596455,\\n 4: -0.9211686080130108,\\n 5: -0.726213492660829,\\n 6: 0.2228955458351768}\\n46 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Many kinds of Python objects are formatted to be more readable, or pretty-printed,\\nwhich is distinct from normal printing with print. If you printed a dict like the above\\nin the standard Python interpreter, it would be much less readable:\\n>>> from numpy.random import randn\\n>>> data = {i : randn() for i in range(7)}\\n>>> print data\\n{0: -1.5948255432744511, 1: 0.10569006472787983, 2: 1.972367135977295,\\n3: 0.15455217573074576, 4: -0.24058577449429575, 5: -1.2904897053651216,\\n6: 0.3308507317325902}\\nIPython also provides facilities to make it easy to execute arbitrary blocks of code (via\\nsomewhat glorified copy-and-pasting) and whole Python scripts. These will be dis-\\ncussed shortly.\\nTab Completion\\nOn the surface, the IPython shell looks like a cosmetically slightly-different interactive\\nPython interpreter. Users of Mathematica may find the enumerated input and output\\nprompts familiar. One of the major improvements over the standard Python shell is'),\n",
       " Document(metadata={}, page_content='Python interpreter. Users of Mathematica may find the enumerated input and output\\nprompts familiar. One of the major improvements over the standard Python shell is\\ntab completion , a feature common to most interactive data analysis environments.\\nWhile entering expressions in the shell, pressing <Tab> will search the namespace for\\nany variables (objects, functions, etc.) matching the characters you have typed so far:\\nIn [1]: an_apple = 27\\nIn [2]: an_example = 42\\nIn [3]: an<Tab>\\nan_apple    and         an_example  any\\nIn this example, note that IPython displayed both the two variables I defined as well as\\nthe Python keyword and and built-in function any. Naturally, you can also complete\\nmethods and attributes on any object after typing a period:\\nIn [3]: b = [1, 2, 3]\\nIn [4]: b.<Tab>\\nb.append   b.extend   b.insert   b.remove   b.sort\\nb.count    b.index    b.pop      b.reverse\\nThe same goes for modules:\\nIn [1]: import datetime\\nIn [2]: datetime.<Tab>'),\n",
       " Document(metadata={}, page_content='In [4]: b.<Tab>\\nb.append   b.extend   b.insert   b.remove   b.sort\\nb.count    b.index    b.pop      b.reverse\\nThe same goes for modules:\\nIn [1]: import datetime\\nIn [2]: datetime.<Tab>\\ndatetime.date           datetime.MAXYEAR        datetime.timedelta\\ndatetime.datetime       datetime.MINYEAR        datetime.tzinfo\\ndatetime.datetime_CAPI  datetime.time\\nIPython Basics | 47\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"Note that IPython by default hides methods and attributes starting with\\nunderscores, such as magic methods and internal “private” methods\\nand attributes, in order to avoid cluttering the display (and confusing\\nnew Python users!). These, too, can be tab-completed but you must first\\ntype an underscore to see them. If you prefer to always see such methods\\nin tab completion, you can change this setting in the IPython configu-\\nration.\\nTab completion works in many contexts outside of searching the interactive namespace\\nand completing object or module attributes.When typing anything that looks like a file\\npath (even in a Python string), pressing <Tab> will complete anything on your com-\\nputer’s file system matching what you’ve typed:\\nIn [3]: book_scripts/<Tab>\\nbook_scripts/cprof_example.py        book_scripts/ipython_script_test.py\\nbook_scripts/ipython_bug.py          book_scripts/prof_mod.py\\nIn [3]: path = 'book_scripts/<Tab>\"),\n",
       " Document(metadata={}, page_content=\"In [3]: book_scripts/<Tab>\\nbook_scripts/cprof_example.py        book_scripts/ipython_script_test.py\\nbook_scripts/ipython_bug.py          book_scripts/prof_mod.py\\nIn [3]: path = 'book_scripts/<Tab>\\nbook_scripts/cprof_example.py        book_scripts/ipython_script_test.py\\nbook_scripts/ipython_bug.py          book_scripts/prof_mod.py\\nCombined with the %run command (see later section), this functionality will undoubt-\\nedly save you many keystrokes.\\nAnother area where tab completion saves time is in the completion of function keyword\\narguments (including the = sign!).\\nIntrospection\\nUsing a question mark (?) before or after a variable will display some general informa-\\ntion about the object:\\nIn [545]: b?\\nType:       list\\nString Form:[1, 2, 3]\\nLength:     3\\nDocstring:\\nlist() -> new empty list\\nlist(iterable) -> new list initialized from iterable's items\\nThis is referred to as object introspection. If the object is a function or instance method,\"),\n",
       " Document(metadata={}, page_content='Docstring:\\nlist() -> new empty list\\nlist(iterable) -> new list initialized from iterable\\'s items\\nThis is referred to as object introspection. If the object is a function or instance method,\\nthe docstring, if defined, will also be shown. Suppose we’d written the following func-\\ntion:\\ndef add_numbers(a, b):\\n    \"\"\"\\n    Add two numbers together\\n    Returns\\n    -------\\n    the_sum : type of arguments\\n48 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='\"\"\"\\n    return a + b\\nThen using ? shows us the docstring:\\nIn [547]: add_numbers?\\nType:       function\\nString Form:<function add_numbers at 0x5fad848>\\nFile:       book_scripts/<ipython-input-546-5473012eeb65>\\nDefinition: add_numbers(a, b)\\nDocstring:\\nAdd two numbers together\\nReturns\\n-------\\nthe_sum : type of arguments\\nUsing ?? will also show the function’s source code if possible:\\nIn [548]: add_numbers??\\nType:       function\\nString Form:<function add_numbers at 0x5fad848>\\nFile:       book_scripts/<ipython-input-546-5473012eeb65>\\nDefinition: add_numbers(a, b)\\nSource:\\ndef add_numbers(a, b):\\n    \"\"\"\\n    Add two numbers together\\n    Returns\\n    -------\\n    the_sum : type of arguments\\n    \"\"\"\\n    return a + b\\n? has a final usage, which is for searching the IPython namespace in a manner similar\\nto the standard UNIX or Windows command line. A number of characters combined\\nwith the wildcard ( *) will show all names matching the wildcard expression. For ex-'),\n",
       " Document(metadata={}, page_content='to the standard UNIX or Windows command line. A number of characters combined\\nwith the wildcard ( *) will show all names matching the wildcard expression. For ex-\\nample, we could get a list of all functions in the top level NumPy namespace containing\\nload:\\nIn [549]: np.*load*?\\nnp.load\\nnp.loads\\nnp.loadtxt\\nnp.pkgload\\nThe %run Command\\nAny file can be run as a Python program inside the environment of your IPython session\\nusing the %run command. Suppose you had the following simple script stored in ipy\\nthon_script_test.py:\\ndef f(x, y, z):\\n    return (x + y) / z\\na = 5\\nIPython Basics | 49\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='b = 6\\nc = 7.5\\nresult = f(a, b, c)\\nThis can be executed by passing the file name to %run:\\nIn [550]: %run ipython_script_test.py\\nThe script is run in an empty namespace (with no imports or other variables defined)\\nso that the behavior should be identical to running the program on the command line\\nusing python script.py. All of the variables (imports, functions, and globals) defined\\nin the file (up until an exception, if any, is raised) will then be accessible in the IPython\\nshell:\\nIn [551]: c\\nOut[551]: 7.5\\nIn [552]: result\\nOut[552]: 1.4666666666666666\\nIf a Python script expects command line arguments (to be found in sys.argv), these\\ncan be passed after the file path as though run on the command line.\\nShould you wish to give a script access to variables already defined in\\nthe interactive IPython namespace, use %run -i instead of plain %run.\\nInterrupting running code\\nPressing <Ctrl-C> while any code is running, whether a script through %run or a long-'),\n",
       " Document(metadata={}, page_content='the interactive IPython namespace, use %run -i instead of plain %run.\\nInterrupting running code\\nPressing <Ctrl-C> while any code is running, whether a script through %run or a long-\\nrunning command, will cause a KeyboardInterrupt to be raised. This will cause nearly\\nall Python programs to stop immediately except in very exceptional cases.\\nWhen a piece of Python code has called into some compiled extension\\nmodules, pressing <Ctrl-C> will not cause the program execution to stop\\nimmediately in all cases. In such cases, you will have to either wait until\\ncontrol is returned to the Python interpreter, or, in more dire circum-\\nstances, forcibly terminate the Python process via the OS task manager.\\nExecuting Code from the Clipboard\\nA quick-and-dirty way to execute code in IPython is via pasting from the clipboard.\\nThis might seem fairly crude, but in practice it is very useful. For example, while de-\\nveloping a complex or time-consuming application, you may wish to execute a script'),\n",
       " Document(metadata={}, page_content='This might seem fairly crude, but in practice it is very useful. For example, while de-\\nveloping a complex or time-consuming application, you may wish to execute a script\\npiece by piece, pausing at each stage to examine the currently loaded data and results.\\nOr, you might find a code snippet on the Internet that you want to run and play around\\nwith, but you’d rather not create a new .py file for it.\\n50 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Code snippets can be pasted from the clipboard in many cases by pressing <Ctrl-Shift-\\nV>. Note that it is not completely robust as this mode of pasting mimics typing each\\nline into IPython, and line breaks are treated as <return>. This means that if you paste\\ncode with an indented block and there is a blank line, IPython will think that the in-\\ndented block is over. Once the next line in the block is executed, an IndentationEr\\nror will be raised. For example the following code:\\nx = 5\\ny = 7\\nif x > 5:\\n    x += 1\\n    y = 8\\nwill not work if simply pasted:\\nIn [1]: x = 5\\nIn [2]: y = 7\\nIn [3]: if x > 5:\\n   ...:         x += 1\\n   ...:\\nIn [4]:     y = 8\\nIndentationError: unexpected indent\\nIf you want to paste code into IPython, try the %paste and %cpaste\\nmagic functions.\\nAs the error message suggests, we should instead use the %paste and %cpaste magic\\nfunctions. %paste takes whatever text is in the clipboard and executes it as a single block\\nin the shell:\\nIn [6]: %paste\\nx = 5\\ny = 7\\nif x > 5:'),\n",
       " Document(metadata={}, page_content=\"functions. %paste takes whatever text is in the clipboard and executes it as a single block\\nin the shell:\\nIn [6]: %paste\\nx = 5\\ny = 7\\nif x > 5:\\n    x += 1\\n    y = 8\\n## -- End pasted text --\\nDepending on your platform and how you installed Python, there’s a\\nsmall chance that %paste will not work. Packaged distributions like\\nEPDFree (as described in in the intro) should not be a problem.\\n%cpaste is similar, except that it gives you a special prompt for pasting code into:\\nIn [7]: %cpaste\\nPasting code; enter '--' alone on the line to stop or use Ctrl-D.\\n:x = 5\\n:y = 7\\n:if x > 5:\\nIPython Basics | 51\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=':    x += 1\\n:\\n:    y = 8\\n:--\\nWith the %cpaste block, you have the freedom to paste as much code as you like before\\nexecuting it. You might decide to use %cpaste in order to look at the pasted code before\\nexecuting it. If you accidentally paste the wrong code, you can break out of the \\n%cpaste prompt by pressing <Ctrl-C>.\\nLater, I’ll introduce the IPython HTML Notebook which brings a new level of sophis-\\ntication for developing analyses block-by-block in a browser-based notebook format\\nwith executable code cells.\\nIPython interaction with editors and IDEs\\nSome text editors, such as Emacs and vim, have 3rd party extensions enabling blocks\\nof code to be sent directly from the editor to a running IPython shell. Refer to the\\nIPython website or do an Internet search to find out more.\\nSome IDEs, such as the PyDev plugin for Eclipse and Python Tools for Visual Studio\\nfrom Microsoft (and possibly others), have integration with the IPython terminal ap-'),\n",
       " Document(metadata={}, page_content='Some IDEs, such as the PyDev plugin for Eclipse and Python Tools for Visual Studio\\nfrom Microsoft (and possibly others), have integration with the IPython terminal ap-\\nplication. If you want to work in an IDE but don’t want to give up the IPython console\\nfeatures, this may be a good option for you.\\nKeyboard Shortcuts\\nIPython has many keyboard shortcuts for navigating the prompt (which will be familiar\\nto users of the Emacs text editor or the UNIX bash shell) and interacting with the shell’s\\ncommand history (see later section). Table 3-1 summarizes some of the most commonly\\nused shortcuts. See Figure 3-1 for an illustration of a few of these, such as cursor move-\\nment.\\nFigure 3-1. Illustration of some of IPython’s keyboard shortcuts\\n52 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Table 3-1. Standard IPython Keyboard Shortcuts\\nCommand Description\\nCtrl-P or up-arrow Search backward in command history for commands starting with currently-entered text\\nCtrl-N or down-arrow Search forward in command history for commands starting with currently-entered text\\nCtrl-R Readline-style reverse history search (partial matching)\\nCtrl-Shift-V Paste text from clipboard\\nCtrl-C Interrupt currently-executing code\\nCtrl-A Move cursor to beginning of line\\nCtrl-E Move cursor to end of line\\nCtrl-K Delete text from cursor until end of line\\nCtrl-U Discard all text on current line\\nCtrl-F Move cursor forward one character\\nCtrl-B Move cursor back one character\\nCtrl-L Clear screen\\nExceptions and Tracebacks\\nIf an exception is raised while %run-ing a script or executing any statement, IPython will\\nby default print a full call stack trace (traceback) with a few lines of context around the\\nposition at each point in the stack.\\nIn [553]: %run ch03/ipython_bug.py'),\n",
       " Document(metadata={}, page_content='by default print a full call stack trace (traceback) with a few lines of context around the\\nposition at each point in the stack.\\nIn [553]: %run ch03/ipython_bug.py\\n---------------------------------------------------------------------------\\nAssertionError                            Traceback (most recent call last)\\n/home/wesm/code/ipython/IPython/utils/py3compat.pyc in execfile(fname, *where)\\n    176             else:\\n    177                 filename = fname\\n--> 178             __builtin__.execfile(filename, *where)\\nbook_scripts/ch03/ipython_bug.py in <module>()\\n     13     throws_an_exception()\\n     14 \\n---> 15 calling_things()\\nbook_scripts/ch03/ipython_bug.py in calling_things()\\n     11 def calling_things():\\n     12     works_fine()\\n---> 13     throws_an_exception()\\n     14 \\n     15 calling_things()\\nbook_scripts/ch03/ipython_bug.py in throws_an_exception()\\n      7     a = 5\\n      8     b = 6\\n----> 9     assert(a + b == 10)\\n     10 \\n     11 def calling_things():\\nAssertionError:'),\n",
       " Document(metadata={}, page_content='book_scripts/ch03/ipython_bug.py in throws_an_exception()\\n      7     a = 5\\n      8     b = 6\\n----> 9     assert(a + b == 10)\\n     10 \\n     11 def calling_things():\\nAssertionError:\\nIPython Basics | 53\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Having additional context by itself is a big advantage over the standard Python inter-\\npreter (which does not provide any additional context). The amount of context shown\\ncan be controlled using the %xmode magic command, from minimal (same as the stan-\\ndard Python interpreter) to verbose (which inlines function argument values and more).\\nAs you will see later in the chapter, you can step into the stack (using the %debug or \\n%pdb magics) after an error has occurred for interactive post-mortem debugging.\\nMagic Commands\\nIPython has many special commands, known as “magic” commands, which are de-\\nsigned to faciliate common tasks and enable you to easily control the behavior of the\\nIPython system. A magic command is any command prefixed by the the percent symbol\\n%. For example, you can check the execution time of any Python statement, such as a\\nmatrix multiplication, using the %timeit magic function (which will be discussed in\\nmore detail later):\\nIn [554]: a = np.random.randn(100, 100)'),\n",
       " Document(metadata={}, page_content=\"matrix multiplication, using the %timeit magic function (which will be discussed in\\nmore detail later):\\nIn [554]: a = np.random.randn(100, 100)\\nIn [555]: %timeit np.dot(a, a)\\n10000 loops, best of 3: 69.1 us per loop\\nMagic commands can be viewed as command line programs to be run within the IPy-\\nthon system. Many of them have additional “command line” options, which can all be\\nviewed (as you might expect) using ?:\\nIn [1]: %reset?\\nResets the namespace by removing all names defined by the user.\\nParameters\\n----------\\n  -f : force reset without asking for confirmation.\\n  -s : 'Soft' reset: Only clears your namespace, leaving history intact.\\n  References to objects may be kept. By default (without this option),\\n  we do a 'hard' reset, giving you a new session and removing all\\n  references to objects from the current session.\\nExamples\\n--------\\nIn [6]: a = 1\\nIn [7]: a\\nOut[7]: 1\\nIn [8]: 'a' in _ip.user_ns\\nOut[8]: True\\nIn [9]: %reset -f\\nIn [1]: 'a' in _ip.user_ns\\nOut[1]: False\"),\n",
       " Document(metadata={}, page_content=\"Examples\\n--------\\nIn [6]: a = 1\\nIn [7]: a\\nOut[7]: 1\\nIn [8]: 'a' in _ip.user_ns\\nOut[8]: True\\nIn [9]: %reset -f\\nIn [1]: 'a' in _ip.user_ns\\nOut[1]: False\\n54 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content='Magic functions can be used by default without the percent sign, as long as no variable\\nis defined with the same name as the magic function in question. This feature is called\\nautomagic and can be enabled or disabled using %automagic.\\nSince IPython’s documentation is easily accessible from within the system, I encourage\\nyou to explore all of the special commands available by typing %quickref or %magic. I\\nwill highlight a few more of the most critical ones for being productive in interactive\\ncomputing and Python development in IPython.\\nTable 3-2. Frequently-used IPython Magic Commands\\nCommand Description\\n%quickref Display the IPython Quick Reference Card\\n%magic Display detailed documentation for all of the available magic commands\\n%debug Enter the interactive debugger at the bottom of the last exception traceback\\n%hist Print command input (and optionally output) history\\n%pdb Automatically enter debugger after any exception\\n%paste Execute pre-formatted Python code from clipboard'),\n",
       " Document(metadata={}, page_content='%hist Print command input (and optionally output) history\\n%pdb Automatically enter debugger after any exception\\n%paste Execute pre-formatted Python code from clipboard\\n%cpaste Open a special prompt for manually pasting Python code to be executed\\n%reset Delete all variables / names defined in interactive namespace\\n%page OBJECT Pretty print the object and display it through a pager\\n%run script.py Run a Python script inside IPython\\n%prun statement Execute statement with cProfile and report the profiler output\\n%time statement Report the execution time of single statement\\n%timeit statement Run a statement multiple times to compute an emsemble average execution time. Useful for\\ntiming code with very short execution time\\n%who, %who_ls, %whos Display variables defined in interactive namespace, with varying levels of information / verbosity\\n%xdel variable Delete a variable and attempt to clear any references to the object in the IPython internals\\nQt-based Rich GUI Console'),\n",
       " Document(metadata={}, page_content='%xdel variable Delete a variable and attempt to clear any references to the object in the IPython internals\\nQt-based Rich GUI Console\\nThe IPython team has developed a Qt framework-based GUI console, designed to wed\\nthe features of the terminal-only applications with the features provided by a rich text\\nwidget, like embedded images, multiline editing, and syntax highlighting. If you have\\neither PyQt or PySide installed, the application can be launched with inline plotting by\\nrunning this on the command line:\\nipython qtconsole --pylab=inline\\nThe Qt console can launch multiple IPython processes in tabs, enabling you to switch\\nbetween tasks. It can also share a process with the IPython HTML Notebook applica-\\ntion, which I’ll highlight later.\\nIPython Basics | 55\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Matplotlib Integration and Pylab Mode\\nPart of why IPython is so widely used in scientific computing is that it is designed as a\\ncompanion to libraries like matplotlib and other GUI toolkits. Don’t worry if you have\\nnever used matplotlib before; it will be discussed in much more detail later in this book.\\nIf you create a matplotlib plot window in the regular Python shell, you’ll be sad to find\\nthat the GUI event loop “takes control” of the Python session until the plot window is\\nclosed. That won’t work for interactive data analysis and visualization, so IPython has\\nFigure 3-2. IPython Qt Console\\n56 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='implemented special handling for each GUI framework so that it will work seamlessly\\nwith the shell.\\nThe typical way to launch IPython with matplotlib integration is by adding the --\\npylab flag (two dashes).\\n$ ipython --pylab\\nThis will cause several things to happen. First IPython will launch with the default GUI\\nbackend integration enabled so that matplotlib plot windows can be created with no\\nissues. Secondly, most of NumPy and matplotlib will be imported into the top level\\ninteractive namespace to produce an interactive computing environment reminiscent\\nof MATLAB and other domain-specific scientific computing environments. It’s possi-\\nble to do this setup by hand by using %gui, too (try running %gui? to find out how).\\nFigure 3-3. Pylab mode: IPython with matplotlib windows\\nIPython Basics | 57\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Using the Command History\\nIPython maintains a small on-disk database containing the text of each command that\\nyou execute. This serves various purposes:\\n• Searching, completing, and executing previously-executed commands with mini-\\nmal typing\\n• Persisting the command history between sessions.\\n• Logging the input/output history to a file\\nSearching and Reusing the Command History\\nBeing able to search and execute previous commands is, for many people, the most\\nuseful feature. Since IPython encourages an iterative, interactive code development\\nworkflow, you may often find yourself repeating the same commands, such as a %run\\ncommand or some other code snippet. Suppose you had run:\\nIn[7]: %run first/second/third/data_script.py\\nand then explored the results of the script (assuming it ran successfully), only to find\\nthat you made an incorrect calculation. After figuring out the problem and modifying\\ndata_script.py, you can start typing a few letters of the %run command then press either'),\n",
       " Document(metadata={}, page_content='that you made an incorrect calculation. After figuring out the problem and modifying\\ndata_script.py, you can start typing a few letters of the %run command then press either\\nthe <Ctrl-P> key combination or the <up arrow> key. This will search the command\\nhistory for the first prior command matching the letters you typed. Pressing either\\n<Ctrl-P> or <up arrow> multiple times will continue to search through the history. If\\nyou pass over the command you wish to execute, fear not. You can move forward\\nthrough the command history by pressing either <Ctrl-N> or <down arrow>. After doing\\nthis a few times you may start pressing these keys without thinking!\\nUsing <Ctrl-R> gives you the same partial incremental searching capability provided\\nby the readline used in UNIX-style shells, such as the bash shell. On Windows, read\\nline functionality is emulated by IPython. To use this, press <Ctrl-R> then type a few\\ncharacters contained in the input line you want to search for:'),\n",
       " Document(metadata={}, page_content=\"line functionality is emulated by IPython. To use this, press <Ctrl-R> then type a few\\ncharacters contained in the input line you want to search for:\\nIn [1]: a_command = foo(x, y, z)\\n(reverse-i-search)`com': a_command = foo(x, y, z)\\nPressing <Ctrl-R> will cycle through the history for each line matching the characters\\nyou’ve typed.\\nInput and Output Variables\\nForgetting to assign the result of a function call to a variable can be very annoying.\\nFortunately, IPython stores references to both the input (the text that you type) and\\noutput (the object that is returned) in special variables. The previous two outputs are\\nstored in the _ (one underscore) and __ (two underscores) variables, respectively:\\n58 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"In [556]: 2 ** 27\\nOut[556]: 134217728\\nIn [557]: _\\nOut[557]: 134217728\\nInput variables are stored in variables named like _iX, where X is the input line number.\\nFor each such input variables there is a corresponding output variable _X. So after input\\nline 27, say, there will be two new variables _27 (for the output) and _i27 for the input.\\nIn [26]: foo = 'bar'\\nIn [27]: foo\\nOut[27]: 'bar'\\nIn [28]: _i27\\nOut[28]: u'foo'\\nIn [29]: _27\\nOut[29]: 'bar'\\nSince the input variables are strings, that can be executed again using the Python \\nexec keyword:\\nIn [30]: exec _i27\\nSeveral magic functions allow you to work with the input and output history. %hist is\\ncapable of printing all or part of the input history, with or without line numbers. \\n%reset is for clearing the interactive namespace and optionally the input and output\\ncaches. The %xdel magic function is intended for removing all references to a particu-\\nlar object from the IPython machinery. See the documentation for both of these magics\"),\n",
       " Document(metadata={}, page_content='caches. The %xdel magic function is intended for removing all references to a particu-\\nlar object from the IPython machinery. See the documentation for both of these magics\\nfor more details.\\nWhen working with very large data sets, keep in mind that IPython’s\\ninput and output history causes any object referenced there to not be\\ngarbage collected (freeing up the memory), even if you delete the vari-\\nables from the interactive namespace using the del keyword. In such\\ncases, careful usage of %xdel and %reset can help you avoid running into\\nmemory problems.\\nLogging the Input and Output\\nIPython is capable of logging the entire console session including input and output.\\nLogging is turned on by typing %logstart:\\nIn [3]: %logstart\\nActivating auto-logging. Current session state plus future input saved.\\nFilename       : ipython_log.py\\nMode           : rotate\\nOutput logging : False\\nRaw input log  : False\\nUsing the Command History | 59\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Timestamping   : False\\nState          : active\\nIPython logging can be enabled at any time and it will record your entire session (in-\\ncluding previous commands). Thus, if you are working on something and you decide\\nyou want to save everything you did, you can simply enable logging. See the docstring\\nof %logstart for more options (including changing the output file path), as well as the\\ncompanion functions %logoff, %logon, %logstate, and %logstop.\\nInteracting with the Operating System\\nAnother important feature of IPython is that it provides very strong integration with\\nthe operating system shell. This means, among other things, that you can perform most\\nstandard command line actions as you would in the Windows or UNIX (Linux, OS X)\\nshell without having to exit IPython. This includes executing shell commands, changing\\ndirectories, and storing the results of a command in a Python object (list or string).\\nThere are also simple shell command aliasing and directory bookmarking features.'),\n",
       " Document(metadata={}, page_content='directories, and storing the results of a command in a Python object (list or string).\\nThere are also simple shell command aliasing and directory bookmarking features.\\nSee Table 3-3 for a summary of magic functions and syntax for calling shell commands.\\nI’ll briefly visit these features in the next few sections.\\nTable 3-3. IPython system-related commands\\nCommand Description\\n!cmd Execute cmd in the system shell\\noutput = !cmd args Run cmd and store the stdout in output\\n%alias alias_name cmd Define an alias for a system (shell) command\\n%bookmark Utilize IPython’s directory bookmarking system\\n%cd directory Change system working directory to passed directory\\n%pwd Return the current system working directory\\n%pushd directory Place current directory on stack and change to target directory\\n%popd Change to directory popped off the top of the stack\\n%dirs Return a list containing the current directory stack\\n%dhist Print the history of visited directories'),\n",
       " Document(metadata={}, page_content='%popd Change to directory popped off the top of the stack\\n%dirs Return a list containing the current directory stack\\n%dhist Print the history of visited directories\\n%env Return the system environment variables as a dict\\nShell Commands and Aliases\\nStarting a line in IPython with an exclamation point !, or bang, tells IPython to execute\\neverything after the bang in the system shell. This means that you can delete files (using\\nrm or del, depending on your OS), change directories, or execute any other process. It’s\\neven possible to start processes that take control away from IPython, even another\\nPython interpreter:\\n60 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='In [2]: !python\\nPython 2.7.2 |EPD 7.1-2 (64-bit)| (default, Jul  3 2011, 15:17:51)\\n[GCC 4.1.2 20080704 (Red Hat 4.1.2-44)] on linux2\\nType \"packages\", \"demo\" or \"enthought\" for more information.\\n>>>\\nThe console output of a shell command can be stored in a variable by assigning the !-\\nescaped expression to a variable. For example, on my Linux-based machine connected\\nto the Internet via ethernet, I can get my IP address as a Python variable:\\nIn [1]: ip_info = !ifconfig eth0 | grep \"inet \"\\nIn [2]: ip_info[0].strip()\\nOut[2]: \\'inet addr:192.168.1.137  Bcast:192.168.1.255  Mask:255.255.255.0\\'\\nThe returned Python object ip_info is actually a custom list type containing various\\nversions of the console output.\\nIPython can also substitute in Python values defined in the current environment when\\nusing !. To do this, preface the variable name by the dollar sign $:\\nIn [3]: foo = \\'test*\\'\\nIn [4]: !ls $foo\\ntest4.py  test.py  test.xml'),\n",
       " Document(metadata={}, page_content=\"using !. To do this, preface the variable name by the dollar sign $:\\nIn [3]: foo = 'test*'\\nIn [4]: !ls $foo\\ntest4.py  test.py  test.xml\\nThe %alias magic function can define custom shortcuts for shell commands. As a simple\\nexample:\\nIn [1]: %alias ll ls -l\\nIn [2]: ll /usr\\ntotal 332\\ndrwxr-xr-x   2 root root  69632 2012-01-29 20:36 bin/\\ndrwxr-xr-x   2 root root   4096 2010-08-23 12:05 games/\\ndrwxr-xr-x 123 root root  20480 2011-12-26 18:08 include/\\ndrwxr-xr-x 265 root root 126976 2012-01-29 20:36 lib/\\ndrwxr-xr-x  44 root root  69632 2011-12-26 18:08 lib32/\\nlrwxrwxrwx   1 root root      3 2010-08-23 16:02 lib64 -> lib/\\ndrwxr-xr-x  15 root root   4096 2011-10-13 19:03 local/\\ndrwxr-xr-x   2 root root  12288 2012-01-12 09:32 sbin/\\ndrwxr-xr-x 387 root root  12288 2011-11-04 22:53 share/\\ndrwxrwsr-x  24 root src    4096 2011-07-17 18:38 src/\\nMultiple commands can be executed just as on the command line by separating them\\nwith semicolons:\\nIn [558]: %alias test_alias (cd ch08; ls; cd ..)\"),\n",
       " Document(metadata={}, page_content='Multiple commands can be executed just as on the command line by separating them\\nwith semicolons:\\nIn [558]: %alias test_alias (cd ch08; ls; cd ..)\\nIn [559]: test_alias\\nmacrodata.csv  spx.csv    tips.csv\\nYou’ll notice that IPython “forgets” any aliases you define interactively as soon as the\\nsession is closed. To create permanent aliases, you will need to use the configuration\\nsystem. See later in the chapter.\\nInteracting with the Operating System | 61\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Directory Bookmark System\\nIPython has a simple directory bookmarking system to enable you to save aliases for\\ncommon directories so that you can jump around very easily. For example, I’m an avid\\nuser of Dropbox, so I can define a bookmark to make it easy to change directories to\\nmy Dropbox:\\nIn [6]: %bookmark db /home/wesm/Dropbox/\\nOnce I’ve done this, when I use the %cd magic, I can use any bookmarks I’ve defined\\nIn [7]: cd db\\n(bookmark:db) -> /home/wesm/Dropbox/\\n/home/wesm/Dropbox\\nIf a bookmark name conflicts with a directory name in your current working directory,\\nyou can use the -b flag to override and use the bookmark location. Using the -l option\\nwith %bookmark lists all of your bookmarks:\\nIn [8]: %bookmark -l\\nCurrent bookmarks:\\ndb -> /home/wesm/Dropbox/\\nBookmarks, unlike aliases, are automatically persisted between IPython sessions.\\nSoftware Development Tools\\nIn addition to being a comfortable environment for interactive computing and data'),\n",
       " Document(metadata={}, page_content='Bookmarks, unlike aliases, are automatically persisted between IPython sessions.\\nSoftware Development Tools\\nIn addition to being a comfortable environment for interactive computing and data\\nexploration, IPython is well suited as a software development environment. In data\\nanalysis applications, it’s important first to have correct code. Fortunately, IPython has\\nclosely integrated and enhanced the built-in Python pdb debugger. Secondly you want\\nyour code to be fast. For this IPython has easy-to-use code timing and profiling tools.\\nI will give an overview of these tools in detail here.\\nInteractive Debugger\\nIPython’s debugger enhances pdb with tab completion, syntax highlighting, and context\\nfor each line in exception tracebacks. One of the best times to debug code is right after\\nan error has occurred. The %debug command, when entered immediately after an ex-\\nception, invokes the “post-mortem” debugger and drops you into the stack frame where\\nthe exception was raised:'),\n",
       " Document(metadata={}, page_content='an error has occurred. The %debug command, when entered immediately after an ex-\\nception, invokes the “post-mortem” debugger and drops you into the stack frame where\\nthe exception was raised:\\nIn [2]: run ch03/ipython_bug.py\\n---------------------------------------------------------------------------\\nAssertionError                            Traceback (most recent call last)\\n/home/wesm/book_scripts/ch03/ipython_bug.py in <module>()\\n     13     throws_an_exception()\\n     14\\n---> 15 calling_things()\\n/home/wesm/book_scripts/ch03/ipython_bug.py in calling_things()\\n62 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='11 def calling_things():\\n     12     works_fine()\\n---> 13     throws_an_exception()\\n     14\\n     15 calling_things()\\n/home/wesm/book_scripts/ch03/ipython_bug.py in throws_an_exception()\\n      7     a = 5\\n      8     b = 6\\n----> 9     assert(a + b == 10)\\n     10\\n     11 def calling_things():\\nAssertionError:\\nIn [3]: %debug\\n> /home/wesm/book_scripts/ch03/ipython_bug.py(9)throws_an_exception()\\n      8     b = 6\\n----> 9     assert(a + b == 10)\\n     10\\nipdb>\\nOnce inside the debugger, you can execute arbitrary Python code and explore all of the\\nobjects and data (which have been “kept alive” by the interpreter) inside each stack\\nframe. By default you start in the lowest level, where the error occurred. By pressing\\nu (up) and d (down), you can switch between the levels of the stack trace:\\nipdb> u\\n> /home/wesm/book_scripts/ch03/ipython_bug.py(13)calling_things()\\n     12     works_fine()\\n---> 13     throws_an_exception()\\n     14'),\n",
       " Document(metadata={}, page_content=\"ipdb> u\\n> /home/wesm/book_scripts/ch03/ipython_bug.py(13)calling_things()\\n     12     works_fine()\\n---> 13     throws_an_exception()\\n     14\\nExecuting the %pdb command makes it so that IPython automatically invokes the de-\\nbugger after any exception, a mode that many users will find especially useful.\\nIt’s also easy to use the debugger to help develop code, especially when you wish to set\\nbreakpoints or step through the execution of a function or script to examine the state\\nat each stage. There are several ways to accomplish this. The first is by using %run with\\nthe -d flag, which invokes the debugger before executing any code in the passed script.\\nYou must immediately press s (step) to enter the script:\\nIn [5]: run -d ch03/ipython_bug.py\\nBreakpoint 1 at /home/wesm/book_scripts/ch03/ipython_bug.py:1\\nNOTE: Enter 'c' at the ipdb>  prompt to start your script.\\n> <string>(1)<module>()\\nipdb> s\\n--Call--\\n> /home/wesm/book_scripts/ch03/ipython_bug.py(1)<module>()\\n1---> 1 def works_fine():\"),\n",
       " Document(metadata={}, page_content=\"NOTE: Enter 'c' at the ipdb>  prompt to start your script.\\n> <string>(1)<module>()\\nipdb> s\\n--Call--\\n> /home/wesm/book_scripts/ch03/ipython_bug.py(1)<module>()\\n1---> 1 def works_fine():\\n      2     a = 5\\n      3     b = 6\\nSoftware Development Tools | 63\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content='After this point, it’s up to you how you want to work your way through the file. For\\nexample, in the above exception, we could set a breakpoint right before calling the\\nworks_fine method and run the script until we reach the breakpoint by pressing c\\n(continue):\\nipdb> b 12\\nipdb> c\\n> /home/wesm/book_scripts/ch03/ipython_bug.py(12)calling_things()\\n     11 def calling_things():\\n2--> 12     works_fine()\\n     13     throws_an_exception()\\nAt this point, you can step into works_fine() or execute works_fine() by pressing n\\n(next) to advance to the next line:\\nipdb> n\\n> /home/wesm/book_scripts/ch03/ipython_bug.py(13)calling_things()\\n2    12     works_fine()\\n---> 13     throws_an_exception()\\n     14\\nThen, we could step into throws_an_exception and advance to the line where the error\\noccurs and look at the variables in the scope. Note that debugger commands take\\nprecedence over variable names; in such cases preface the variables with ! to examine\\ntheir contents.\\nipdb> s\\n--Call--'),\n",
       " Document(metadata={}, page_content='occurs and look at the variables in the scope. Note that debugger commands take\\nprecedence over variable names; in such cases preface the variables with ! to examine\\ntheir contents.\\nipdb> s\\n--Call--\\n> /home/wesm/book_scripts/ch03/ipython_bug.py(6)throws_an_exception()\\n      5\\n----> 6 def throws_an_exception():\\n      7     a = 5\\nipdb> n\\n> /home/wesm/book_scripts/ch03/ipython_bug.py(7)throws_an_exception()\\n      6 def throws_an_exception():\\n----> 7     a = 5\\n      8     b = 6\\nipdb> n\\n> /home/wesm/book_scripts/ch03/ipython_bug.py(8)throws_an_exception()\\n      7     a = 5\\n----> 8     b = 6\\n      9     assert(a + b == 10)\\nipdb> n\\n> /home/wesm/book_scripts/ch03/ipython_bug.py(9)throws_an_exception()\\n      8     b = 6\\n----> 9     assert(a + b == 10)\\n     10\\nipdb> !a\\n5\\nipdb> !b\\n6\\n64 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Becoming proficient in the interactive debugger is largely a matter of practice and ex-\\nperience. See Table 3-3 for a full catalogue of the debugger commands. If you are used\\nto an IDE, you might find the terminal-driven debugger to be a bit bewildering at first,\\nbut that will improve in time. Most of the Python IDEs have excellent GUI debuggers,\\nbut it is usually a significant productivity gain to remain in IPython for your debugging.\\nTable 3-4. (I)Python debugger commands\\nCommand Action\\nh(elp) Display command list\\nhelp command Show documentation for command\\nc(ontinue) Resume program execution\\nq(uit) Exit debugger without executing any more code\\nb(reak) number Set breakpoint at number in current file\\nb path/to/file.py:number Set breakpoint at line number in specified file\\ns(tep) Step into function call\\nn(ext) Execute current line and advance to next line at current level\\nu(p) / d(own) Move up/down in function call stack\\na(rgs) Show arguments for current function'),\n",
       " Document(metadata={}, page_content=\"s(tep) Step into function call\\nn(ext) Execute current line and advance to next line at current level\\nu(p) / d(own) Move up/down in function call stack\\na(rgs) Show arguments for current function\\ndebug statement Invoke statement statement in new (recursive) debugger\\nl(ist) statement Show current position and context at current level of stack\\nw(here) Print full stack trace with context at current position\\nOther ways to make use of the debugger\\nThere are a couple of other useful ways to invoke the debugger. The first is by using a\\nspecial set_trace function (named after pdb.set_trace), which is basically a “poor\\nman’s breakpoint”. Here are two small recipes you might want to put somewhere for\\nyour general use (potentially adding them to your IPython profile as I do):\\ndef set_trace():\\n    from IPython.core.debugger import Pdb\\n    Pdb(color_scheme='Linux').set_trace(sys._getframe().f_back)\\ndef debug(f, *args, **kwargs):\\n    from IPython.core.debugger import Pdb\"),\n",
       " Document(metadata={}, page_content=\"def set_trace():\\n    from IPython.core.debugger import Pdb\\n    Pdb(color_scheme='Linux').set_trace(sys._getframe().f_back)\\ndef debug(f, *args, **kwargs):\\n    from IPython.core.debugger import Pdb\\n    pdb = Pdb(color_scheme='Linux')\\n    return pdb.runcall(f, *args, **kwargs)\\nThe first function, set_trace, is very simple. Put set_trace() anywhere in your code\\nthat you want to stop and take a look around (for example, right before an exception\\noccurs):\\nIn [7]: run ch03/ipython_bug.py\\n> /home/wesm/book_scripts/ch03/ipython_bug.py(16)calling_things()\\n     15     set_trace()\\nSoftware Development Tools | 65\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content='---> 16     throws_an_exception()\\n     17\\nPressing c (continue) will cause the code to resume normally with no harm done.\\nThe debug function above enables you to invoke the interactive debugger easily on an\\narbitrary function call. Suppose we had written a function like\\ndef f(x, y, z=1):\\n    tmp = x + y\\n    return tmp / z\\nand we wished to step through its logic. Ordinarily using f would look like f(1, 2,\\nz=3). To instead step into f, pass f as the first argument to debug followed by the po-\\nsitional and keyword arguments to be passed to f:\\nIn [6]: debug(f, 1, 2, z=3)\\n> <ipython-input>(2)f()\\n      1 def f(x, y, z):\\n----> 2     tmp = x + y\\n      3     return tmp / z\\nipdb>\\nI find that these two simple recipes save me a lot of time on a day-to-day basis.\\nLastly, the debugger can be used in conjunction with %run. By running a script with\\n%run -d, you will be dropped directly into the debugger, ready to set any breakpoints\\nand start the script:\\nIn [1]: %run -d ch03/ipython_bug.py'),\n",
       " Document(metadata={}, page_content=\"%run -d, you will be dropped directly into the debugger, ready to set any breakpoints\\nand start the script:\\nIn [1]: %run -d ch03/ipython_bug.py\\nBreakpoint 1 at /home/wesm/book_scripts/ch03/ipython_bug.py:1\\nNOTE: Enter 'c' at the ipdb>  prompt to start your script.\\n> <string>(1)<module>()\\nipdb>\\nAdding -b with a line number starts the debugger with a breakpoint set already:\\nIn [2]: %run -d -b2 ch03/ipython_bug.py\\nBreakpoint 1 at /home/wesm/book_scripts/ch03/ipython_bug.py:2\\nNOTE: Enter 'c' at the ipdb>  prompt to start your script.\\n> <string>(1)<module>()\\nipdb> c\\n> /home/wesm/book_scripts/ch03/ipython_bug.py(2)works_fine()\\n      1 def works_fine():\\n1---> 2     a = 5\\n      3     b = 6\\nipdb>\\n66 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content='Timing Code: %time and %timeit\\nFor larger-scale or longer-running data analysis applications, you may wish to measure\\nthe execution time of various components or of individual statements or function calls.\\nYou may want a report of which functions are taking up the most time in a complex\\nprocess. Fortunately, IPython enables you to get this information very easily while you\\nare developing and testing your code.\\nTiming code by hand using the built-in time module and its functions time.clock and\\ntime.time is often tedious and repetitive, as you must write the same uninteresting\\nboilerplate code:\\nimport time\\nstart = time.time()\\nfor i in range(iterations):\\n    # some code to run here\\nelapsed_per = (time.time() - start) / iterations\\nSince this is such a common operation, IPython has two magic functions %time and \\n%timeit to automate this process for you. %time runs a statement once, reporting the\\ntotal execution time. Suppose we had a large list of strings and we wanted to compare'),\n",
       " Document(metadata={}, page_content=\"%timeit to automate this process for you. %time runs a statement once, reporting the\\ntotal execution time. Suppose we had a large list of strings and we wanted to compare\\ndifferent methods of selecting all strings starting with a particular prefix. Here is a\\nsimple list of 700,000 strings and two identical methods of selecting only the ones that\\nstart with 'foo':\\n# a very large list of strings\\nstrings = ['foo', 'foobar', 'baz', 'qux',\\n           'python', 'Guido Van Rossum'] * 100000\\nmethod1 = [x for x in strings if x.startswith('foo')]\\nmethod2 = [x for x in strings if x[:3] == 'foo']\\nIt looks like they should be about the same performance-wise, right? We can check for\\nsure using %time:\\nIn [561]: %time method1 = [x for x in strings if x.startswith('foo')]\\nCPU times: user 0.19 s, sys: 0.00 s, total: 0.19 s\\nWall time: 0.19 s\\nIn [562]: %time method2 = [x for x in strings if x[:3] == 'foo']\\nCPU times: user 0.09 s, sys: 0.00 s, total: 0.09 s\\nWall time: 0.09 s\"),\n",
       " Document(metadata={}, page_content=\"Wall time: 0.19 s\\nIn [562]: %time method2 = [x for x in strings if x[:3] == 'foo']\\nCPU times: user 0.09 s, sys: 0.00 s, total: 0.09 s\\nWall time: 0.09 s\\nThe Wall time is the main number of interest. So, it looks like the first method takes\\nmore than twice as long, but it’s not a very precise measurement. If you try %time-ing\\nthose statements multiple times yourself, you’ll find that the results are somewhat\\nvariable. To get a more precise measurement, use the %timeit magic function. Given\\nan arbitrary statement, it has a heuristic to run a statement multiple times to produce\\na fairly accurate average runtime.\\nIn [563]: %timeit [x for x in strings if x.startswith('foo')]\\n10 loops, best of 3: 159 ms per loop\\nSoftware Development Tools | 67\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"In [564]: %timeit [x for x in strings if x[:3] == 'foo']\\n10 loops, best of 3: 59.3 ms per loop\\nThis seemingly innocuous example illustrates that it is worth understanding the per-\\nformance characteristics of the Python standard library, NumPy, pandas, and other\\nlibraries used in this book. In larger-scale data analysis applications, those milliseconds\\nwill start to add up!\\n%timeit is especially useful for analyzing statements and functions with very short ex-\\necution times, even at the level of microseconds (1e-6 seconds) or nanoseconds (1e-9\\nseconds). These may seem like insignificant amounts of time, but of course a 20 mi-\\ncrosecond function invoked 1 million times takes 15 seconds longer than a 5 micro-\\nsecond function. In the above example, we could very directly compare the two string\\noperations to understand their performance characteristics:\\nIn [565]: x = 'foobar'\\nIn [566]: y = 'foo'\\nIn [567]: %timeit x.startswith(y)\\n1000000 loops, best of 3: 267 ns per loop\"),\n",
       " Document(metadata={}, page_content=\"operations to understand their performance characteristics:\\nIn [565]: x = 'foobar'\\nIn [566]: y = 'foo'\\nIn [567]: %timeit x.startswith(y)\\n1000000 loops, best of 3: 267 ns per loop\\nIn [568]: %timeit x[:3] == y\\n10000000 loops, best of 3: 147 ns per loop\\nBasic Profiling: %prun and %run -p\\nProfiling code is closely related to timing code, except it is concerned with determining\\nwhere time is spent. The main Python profiling tool is the cProfile module, which is\\nnot specific to IPython at all. cProfile executes a program or any arbitrary block of\\ncode while keeping track of how much time is spent in each function.\\nA common way to use cProfile is on the command line, running an entire program\\nand outputting the aggregated time per function. Suppose we had a simple script which\\ndoes some linear algebra in a loop (computing the maximum absolute eigenvalues of\\na series of 100 x 100 matrices):\\nimport numpy as np\\nfrom numpy.linalg import eigvals\\ndef run_experiment(niter=100):\\n    K = 100\"),\n",
       " Document(metadata={}, page_content=\"a series of 100 x 100 matrices):\\nimport numpy as np\\nfrom numpy.linalg import eigvals\\ndef run_experiment(niter=100):\\n    K = 100\\n    results = []\\n    for _ in xrange(niter):\\n        mat = np.random.randn(K, K)\\n        max_eigenvalue = np.abs(eigvals(mat)).max()\\n        results.append(max_eigenvalue)\\n    return results\\nsome_results = run_experiment()\\nprint 'Largest one we saw: %s' % np.max(some_results)\\n68 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"Don’t worry if you are not familiar with NumPy. You can run this script through\\ncProfile by running the following in the command line:\\npython -m cProfile cprof_example.py\\nIf you try that, you’ll find that the results are outputted sorted by function name. This\\nmakes it a bit hard to get an idea of where the most time is spent, so it’s very common\\nto specify a sort order using the -s flag:\\n$ python -m cProfile -s cumulative cprof_example.py\\nLargest one we saw: 11.923204422\\n    15116 function calls (14927 primitive calls) in 0.720 seconds\\nOrdered by: cumulative time\\nncalls  tottime  percall  cumtime  percall filename:lineno(function)\\n     1    0.001    0.001    0.721    0.721 cprof_example.py:1(<module>)\\n   100    0.003    0.000    0.586    0.006 linalg.py:702(eigvals)\\n   200    0.572    0.003    0.572    0.003 {numpy.linalg.lapack_lite.dgeev}\\n     1    0.002    0.002    0.075    0.075 __init__.py:106(<module>)\\n   100    0.059    0.001    0.059    0.001 {method 'randn')\"),\n",
       " Document(metadata={}, page_content=\"1    0.002    0.002    0.075    0.075 __init__.py:106(<module>)\\n   100    0.059    0.001    0.059    0.001 {method 'randn')\\n     1    0.000    0.000    0.044    0.044 add_newdocs.py:9(<module>)\\n     2    0.001    0.001    0.037    0.019 __init__.py:1(<module>)\\n     2    0.003    0.002    0.030    0.015 __init__.py:2(<module>)\\n     1    0.000    0.000    0.030    0.030 type_check.py:3(<module>)\\n     1    0.001    0.001    0.021    0.021 __init__.py:15(<module>)\\n     1    0.013    0.013    0.013    0.013 numeric.py:1(<module>)\\n     1    0.000    0.000    0.009    0.009 __init__.py:6(<module>)\\n     1    0.001    0.001    0.008    0.008 __init__.py:45(<module>)\\n   262    0.005    0.000    0.007    0.000 function_base.py:3178(add_newdoc)\\n   100    0.003    0.000    0.005    0.000 linalg.py:162(_assertFinite)\\n   ...\\nOnly the first 15 rows of the output are shown. It’s easiest to read by scanning down\\nthe cumtime column to see how much total time was spent inside each function. Note\"),\n",
       " Document(metadata={}, page_content='...\\nOnly the first 15 rows of the output are shown. It’s easiest to read by scanning down\\nthe cumtime column to see how much total time was spent inside each function. Note\\nthat if a function calls some other function, the clock does not stop running . cProfile\\nrecords the start and end time of each function call and uses that to produce the timing.\\nIn addition to the above command-line usage, cProfile can also be used programmat-\\nically to profile arbitrary blocks of code without having to run a new process. IPython\\nhas a convenient interface to this capability using the %prun command and the -p option\\nto %run. %prun takes the same “command line options” as cProfile but will profile an\\narbitrary Python statement instead of a while .py file:\\nIn [4]: %prun -l 7 -s cumulative run_experiment()\\n         4203 function calls in 0.643 seconds\\nOrdered by: cumulative time\\nList reduced from 32 to 7 due to restriction <7>\\nncalls  tottime  percall  cumtime  percall filename:lineno(function)'),\n",
       " Document(metadata={}, page_content='4203 function calls in 0.643 seconds\\nOrdered by: cumulative time\\nList reduced from 32 to 7 due to restriction <7>\\nncalls  tottime  percall  cumtime  percall filename:lineno(function)\\n     1    0.000    0.000    0.643    0.643 <string>:1(<module>)\\n     1    0.001    0.001    0.643    0.643 cprof_example.py:4(run_experiment)\\n   100    0.003    0.000    0.583    0.006 linalg.py:702(eigvals)\\nSoftware Development Tools | 69\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"200    0.569    0.003    0.569    0.003 {numpy.linalg.lapack_lite.dgeev}\\n   100    0.058    0.001    0.058    0.001 {method 'randn'}\\n   100    0.003    0.000    0.005    0.000 linalg.py:162(_assertFinite)\\n   200    0.002    0.000    0.002    0.000 {method 'all' of 'numpy.ndarray' objects}\\nSimilarly, calling %run -p -s cumulative cprof_example.py has the same effect as the\\ncommand-line approach above, except you never have to leave IPython.\\nProfiling a Function Line-by-Line\\nIn some cases the information you obtain from %prun (or another cProfile-based profile\\nmethod) may not tell the whole story about a function’s execution time, or it may be\\nso complex that the results, aggregated by function name, are hard to interpret. For\\nthis case, there is a small library called line_profiler (obtainable via PyPI or one of the\\npackage management tools). It contains an IPython extension enabling a new magic\\nfunction %lprun that computes a line-by-line-profiling of one or more functions. You\"),\n",
       " Document(metadata={}, page_content=\"package management tools). It contains an IPython extension enabling a new magic\\nfunction %lprun that computes a line-by-line-profiling of one or more functions. You\\ncan enable this extension by modifying your IPython configuration (see the IPython\\ndocumentation or the section on configuration later in this chapter) to include the\\nfollowing line:\\n# A list of dotted module names of IPython extensions to load.\\nc.TerminalIPythonApp.extensions = ['line_profiler']\\nline_profiler can be used programmatically (see the full documentation), but it is\\nperhaps most powerful when used interactively in IPython. Suppose you had a module\\nprof_mod with the following code doing some NumPy array operations:\\nfrom numpy.random import randn\\ndef add_and_sum(x, y):\\n    added = x + y\\n    summed = added.sum(axis=1)\\n    return summed\\ndef call_function():\\n    x = randn(1000, 1000)\\n    y = randn(1000, 1000)\\n    return add_and_sum(x, y)\"),\n",
       " Document(metadata={}, page_content='def add_and_sum(x, y):\\n    added = x + y\\n    summed = added.sum(axis=1)\\n    return summed\\ndef call_function():\\n    x = randn(1000, 1000)\\n    y = randn(1000, 1000)\\n    return add_and_sum(x, y)\\nIf we wanted to understand the performance of the add_and_sum function, %prun gives\\nus the following:\\nIn [569]: %run prof_mod\\nIn [570]: x = randn(3000, 3000)\\nIn [571]: y = randn(3000, 3000)\\nIn [572]: %prun add_and_sum(x, y)\\n         4 function calls in 0.049 seconds\\n   Ordered by: internal time\\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\\n        1    0.036    0.036    0.046    0.046 prof_mod.py:3(add_and_sum)\\n70 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"1    0.009    0.009    0.009    0.009 {method 'sum' of 'numpy.ndarray' objects}\\n        1    0.003    0.003    0.049    0.049 <string>:1(<module>)\\n        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\\nThis is not especially enlightening. With the line_profiler IPython extension activa-\\nted, a new command %lprun is available. The only difference in usage is that we must\\ninstruct %lprun which function or functions we wish to profile. The general syntax is:\\n%lprun -f func1 -f func2 statement_to_profile\\nIn this case, we want to profile add_and_sum, so we run:\\nIn [573]: %lprun -f add_and_sum add_and_sum(x, y)\\nTimer unit: 1e-06 s\\nFile: book_scripts/prof_mod.py\\nFunction: add_and_sum at line 3\\nTotal time: 0.045936 s\\nLine #      Hits         Time  Per Hit   % Time  Line Contents\\n==============================================================\\n     3                                           def add_and_sum(x, y):\"),\n",
       " Document(metadata={}, page_content='Line #      Hits         Time  Per Hit   % Time  Line Contents\\n==============================================================\\n     3                                           def add_and_sum(x, y):\\n     4         1        36510  36510.0     79.5      added = x + y\\n     5         1         9425   9425.0     20.5      summed = added.sum(axis=1)\\n     6         1            1      1.0      0.0      return summed\\nYou’ll probably agree this is much easier to interpret. In this case we profiled the same\\nfunction we used in the statement. Looking at the module code above, we could call\\ncall_function and profile that as well as add_and_sum, thus getting a full picture of the\\nperformance of the code:\\nIn [574]: %lprun -f add_and_sum -f call_function call_function()\\nTimer unit: 1e-06 s\\nFile: book_scripts/prof_mod.py\\nFunction: add_and_sum at line 3\\nTotal time: 0.005526 s\\nLine #      Hits         Time  Per Hit   % Time  Line Contents\\n=============================================================='),\n",
       " Document(metadata={}, page_content='Function: add_and_sum at line 3\\nTotal time: 0.005526 s\\nLine #      Hits         Time  Per Hit   % Time  Line Contents\\n==============================================================\\n     3                                           def add_and_sum(x, y):\\n     4         1         4375   4375.0     79.2      added = x + y\\n     5         1         1149   1149.0     20.8      summed = added.sum(axis=1)\\n     6         1            2      2.0      0.0      return summed\\nFile: book_scripts/prof_mod.py\\nFunction: call_function at line 8\\nTotal time: 0.121016 s\\nLine #      Hits         Time  Per Hit   % Time  Line Contents\\n==============================================================\\n     8                                           def call_function():\\n     9         1        57169  57169.0     47.2      x = randn(1000, 1000)\\n    10         1        58304  58304.0     48.2      y = randn(1000, 1000)\\n    11         1         5543   5543.0      4.6      return add_and_sum(x, y)'),\n",
       " Document(metadata={}, page_content='10         1        58304  58304.0     48.2      y = randn(1000, 1000)\\n    11         1         5543   5543.0      4.6      return add_and_sum(x, y)\\nAs a general rule of thumb, I tend to prefer %prun (cProfile) for “macro” profiling and\\n%lprun (line_profiler) for “micro” profiling. It’s worthwhile to have a good under-\\nstanding of both tools.\\nSoftware Development Tools | 71\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='The reason that you have to specify explicitly the names of the functions\\nyou want to profile with %lprun is that the overhead of “tracing” the\\nexecution time of each line is significant. Tracing functions that are not\\nof interest would potentially significantly alter the profile results.\\nIPython HTML Notebook\\nStarting in 2011, the IPython team, led by Brian Granger, built a web technology−based\\ninteractive computational document format that is commonly known as the IPython\\nNotebook. It has grown into a wonderful tool for interactive computing and an ideal\\nmedium for reproducible research and teaching. I’ve used it while writing most of the\\nexamples in the book; I encourage you to make use of it, too.\\nIt has a JSON-based .ipynb document format that enables easy sharing of code, output,\\nand figures. Recently in Python conferences, a popular approach for demonstrations\\nhas been to use the notebook and post the .ipynb files online afterward for everyone\\nto play with.'),\n",
       " Document(metadata={}, page_content=\"and figures. Recently in Python conferences, a popular approach for demonstrations\\nhas been to use the notebook and post the .ipynb files online afterward for everyone\\nto play with.\\nThe notebook application runs as a lightweight server process on the command line.\\nIt can be started by running:\\n$ ipython notebook --pylab=inline\\n[NotebookApp] Using existing profile dir: u'/home/wesm/.config/ipython/profile_default'\\n[NotebookApp] Serving notebooks from /home/wesm/book_scripts\\n[NotebookApp] The IPython Notebook is running at: http://127.0.0.1:8888/\\n[NotebookApp] Use Control-C to stop this server and shut down all kernels.\\nOn most platforms, your primary web browser will automatically open up to the note-\\nbook dashboard. In some cases you may have to navigate to the listed URL. From there,\\nyou can create a new notebook and start exploring.\\nSince you use the notebook inside a web browser, the server process can run anywhere.\"),\n",
       " Document(metadata={}, page_content='you can create a new notebook and start exploring.\\nSince you use the notebook inside a web browser, the server process can run anywhere.\\nYou can even securely connect to notebooks running on cloud service providers like\\nAmazon EC2. As of this writing, a new project NotebookCloud (http://notebookcloud\\n.appspot.com) makes it easy to launch notebooks on EC2.\\nTips for Productive Code Development Using IPython\\nWriting code in a way that makes it easy to develop, debug, and ultimately use inter-\\nactively may be a paradigm shift for many users. There are procedural details like code\\nreloading that may require some adjustment as well as coding style concerns.\\nAs such, most of this section is more of an art than a science and will require some\\nexperimentation on your part to determine a way to write your Python code that is\\neffective and productive for you. Ultimately you want to structure your code in a way'),\n",
       " Document(metadata={}, page_content='experimentation on your part to determine a way to write your Python code that is\\neffective and productive for you. Ultimately you want to structure your code in a way\\nthat makes it easy to use iteratively and to be able to explore the results of running a\\nprogram or function as effortlessly as possible. I have found software designed with\\n72 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='IPython in mind to be easier to work with than code intended only to be run as as\\nstandalone command-line application. This becomes especially important when some-\\nthing goes wrong and you have to diagnose an error in code that you or someone else\\nmight have written months or years beforehand.\\nFigure 3-4. IPython Notebook\\nTips for Productive Code Development Using IPython | 73\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Reloading Module Dependencies\\nIn Python, when you type import some_lib, the code in some_lib is executed and all the\\nvariables, functions, and imports defined within are stored in the newly created\\nsome_lib module namespace. The next time you type import some_lib, you will get a\\nreference to the existing module namespace. The potential difficulty in interactive code\\ndevelopment in IPython comes when you, say, %run a script that depends on some other\\nmodule where you may have made changes. Suppose I had the following code in\\ntest_script.py:\\nimport some_lib\\nx = 5\\ny = [1, 2, 3, 4]\\nresult = some_lib.get_answer(x, y)\\nIf you were to execute %run test_script.py then modify some_lib.py, the next time you\\nexecute %run test_script.py you will still get the old version of some_lib because of\\nPython’s “load-once” module system. This behavior differs from some other data anal-\\nysis environments, like MATLAB, which automatically propagate code changes. 1 To'),\n",
       " Document(metadata={}, page_content=\"Python’s “load-once” module system. This behavior differs from some other data anal-\\nysis environments, like MATLAB, which automatically propagate code changes. 1 To\\ncope with this, you have a couple of options. The first way is to use Python's built-in \\nreload function, altering test_script.py to look like the following:\\nimport some_lib\\nreload(some_lib)\\nx = 5\\ny = [1, 2, 3, 4]\\nresult = some_lib.get_answer(x, y)\\nThis guarantees that you will get a fresh copy of some_lib every time you run\\ntest_script.py. Obviously, if the dependencies go deeper, it might be a bit tricky to be\\ninserting usages of reload all over the place. For this problem, IPython has a special \\ndreload function (not a magic function) for “deep” (recursive) reloading of modules. If\\nI were to run import some_lib then type dreload(some_lib), it will attempt to reload\\nsome_lib as well as all of its dependencies. This will not work in all cases, unfortunately,\\nbut when it does it beats having to restart IPython.\"),\n",
       " Document(metadata={}, page_content='some_lib as well as all of its dependencies. This will not work in all cases, unfortunately,\\nbut when it does it beats having to restart IPython.\\nCode Design Tips\\nThere’s no simple recipe for this, but here are some high-level principles I have found\\neffective in my own work.\\n1. Since a module or package may be imported in many different places in a particular program, Python\\ncaches a module’s code the first time it is imported rather than executing the code in the module every\\ntime. Otherwise, modularity and good code organization could potentially cause inefficiency in an\\napplication.\\n74 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"Keep relevant objects and data alive\\nIt’s not unusual to see a program written for the command line with a structure some-\\nwhat like the following trivial example:\\nfrom my_functions import g\\ndef f(x, y):\\n    return g(x + y)\\ndef main():\\n    x = 6\\n    y = 7.5\\n    result = x + y\\nif __name__ == '__main__':\\n    main()\\nDo you see what might be wrong with this program if we were to run it in IPython?\\nAfter it’s done, none of the results or objects defined in the main function willl be ac-\\ncessible in the IPython shell. A better way is to have whatever code is in main execute\\ndirectly in the module’s global namespace (or in the if __name__ == '__main__': block,\\nif you want the module to also be importable). That way, when you %run the code,\\nyou’ll be able to look at all of the variables defined in main. It’s less meaningful in this\\nsimple example, but in this book we’ll be looking at some complex data analysis prob-\"),\n",
       " Document(metadata={}, page_content='you’ll be able to look at all of the variables defined in main. It’s less meaningful in this\\nsimple example, but in this book we’ll be looking at some complex data analysis prob-\\nlems involving large data sets that you will want to be able to play with in IPython.\\nFlat is better than nested\\nDeeply nested code makes me think about the many layers of an onion. When testing\\nor debugging a function, how many layers of the onion must you peel back in order to\\nreach the code of interest? The idea that “flat is better than nested” is a part of the Zen\\nof Python, and it applies generally to developing code for interactive use as well. Making\\nfunctions and classes as decoupled and modular as possible makes them easier to test\\n(if you are writing unit tests), debug, and use interactively.\\nOvercome a fear of longer files\\nIf you come from a Java (or another such language) background, you may have been\\ntold to keep files short. In many languages, this is sound advice; long length is usually'),\n",
       " Document(metadata={}, page_content='If you come from a Java (or another such language) background, you may have been\\ntold to keep files short. In many languages, this is sound advice; long length is usually\\na bad “code smell”, indicating refactoring or reorganization may be necessary. How-\\never, while developing code using IPython, working with 10 small, but interconnected\\nfiles (under, say, 100 lines each) is likely to cause you more headache in general than a\\nsingle large file or two or three longer files. Fewer files means fewer modules to reload\\nand less jumping between files while editing, too. I have found maintaining larger\\nmodules, each with high internal cohesion, to be much more useful and pythonic. After\\niterating toward a solution, it sometimes will make sense to refactor larger files into\\nsmaller ones.\\nTips for Productive Code Development Using IPython | 75\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Obviously, I don’t support taking this argument to the extreme, which would to be to\\nput all of your code in a single monstrous file. Finding a sensible and intuitive module\\nand package structure for a large codebase often takes a bit of work, but it is especially\\nimportant to get right in teams. Each module should be internally cohesive, and it\\nshould be as obvious as possible where to find functions and classes responsible for\\neach area of functionality.\\nAdvanced IPython Features\\nMaking Your Own Classes IPython-friendly\\nIPython makes every effort to display a console-friendly string representation of any\\nobject that you inspect. For many objects, like dicts, lists, and tuples, the built-in \\npprint module is used to do the nice formatting. In user-defined classes, however, you\\nhave to generate the desired string output yourself. Suppose we had the following sim-\\nple class:\\nclass Message:\\n    def __init__(self, msg):\\n        self.msg = msg'),\n",
       " Document(metadata={}, page_content=\"have to generate the desired string output yourself. Suppose we had the following sim-\\nple class:\\nclass Message:\\n    def __init__(self, msg):\\n        self.msg = msg\\nIf you wrote this, you would be disappointed to discover that the default output for\\nyour class isn’t very nice:\\nIn [576]: x = Message('I have a secret')\\nIn [577]: x\\nOut[577]: <__main__.Message instance at 0x60ebbd8>\\nIPython takes the string returned by the __repr__ magic method (by doing output =\\nrepr(obj)) and prints that to the console. Thus, we can add a simple __repr__ method\\nto the above class to get a more helpful output:\\nclass Message:\\n    def __init__(self, msg):\\n        self.msg = msg\\n    def __repr__(self):\\n        return 'Message: %s' % self.msg\\nIn [579]: x = Message('I have a secret')\\nIn [580]: x\\nOut[580]: Message: I have a secret\\n76 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content='Profiles and Configuration\\nMost aspects of the appearance (colors, prompt, spacing between lines, etc.) and be-\\nhavior of the IPython shell are configurable through an extensive configuration system.\\nHere are some of the things you can do via configuration:\\n• Change the color scheme\\n• Change how the input and output prompts look, or remove the blank line after\\nOut and before the next In prompt\\n• Change how the input and output prompts look\\n• Execute an arbitrary list of Python statements. These could be imports that you\\nuse all the time or anything else you want to happen each time you launch IPython\\n• Enable IPython extensions, like the %lprun magic in line_profiler\\n• Define your own magics or system aliases\\nAll of these configuration options are specified in a special ipython_config.py file which\\nwill be found in the ~/.config/ipython/ directory on UNIX-like systems and %HOME\\n%/.ipython/ directory on Windows. Where your home directory is depends on your'),\n",
       " Document(metadata={}, page_content='will be found in the ~/.config/ipython/ directory on UNIX-like systems and %HOME\\n%/.ipython/ directory on Windows. Where your home directory is depends on your\\nsystem. Configuration is performed based on a particular profile. When you start IPy-\\nthon normally, you load up, by default, the default profile , stored in the pro\\nfile_default directory. Thus, on my Linux OS the full path to my default IPython\\nconfiguration file is:\\n/home/wesm/.config/ipython/profile_default/ipython_config.py\\nI’ll spare you the gory details of what’s in this file. Fortunately it has comments de-\\nscribing what each configuration option is for, so I will leave it to the reader to tinker\\nand customize. One additional useful feature is that it’s possible to have multiple pro-\\nfiles. Suppose you wanted to have an alternate IPython configuration tailored for a\\nparticular application or project. Creating a new profile is as simple is typing something\\nlike\\nipython profile create secret_project'),\n",
       " Document(metadata={}, page_content='particular application or project. Creating a new profile is as simple is typing something\\nlike\\nipython profile create secret_project\\nOnce you’ve done this, edit the config files in the newly-created pro\\nfile_secret_project directory then launch IPython like so\\n$ ipython --profile=secret_project\\nPython 2.7.2 |EPD 7.1-2 (64-bit)| (default, Jul  3 2011, 15:17:51)\\nType \"copyright\", \"credits\" or \"license\" for more information.\\nIPython 0.13 -- An enhanced Interactive Python.\\n?         -> Introduction and overview of IPython\\'s features.\\n%quickref -> Quick reference.\\nhelp      -> Python\\'s own help system.\\nobject?   -> Details about \\'object\\', use \\'object??\\' for extra details.\\nIPython profile: secret_project\\nAdvanced IPython Features | 77\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='In [1]:\\nAs always, the online IPython documentation is an excellent resource for more on\\nprofiles and configuration.\\nCredits\\nParts of this chapter were derived from the wonderful documentation put together by\\nthe IPython Development Team. I can’t thank them enough for all of their work build-\\ning this amazing set of tools.\\n78 | Chapter 3: \\u2002IPython: An Interactive Computing and Development Environment\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='CHAPTER 4\\nNumPy Basics: Arrays and Vectorized\\nComputation\\nNumPy, short for Numerical Python, is the fundamental package required for high\\nperformance scientific computing and data analysis. It is the foundation on which\\nnearly all of the higher-level tools in this book are built. Here are some of the things it\\nprovides:\\n• ndarray, a fast and space-efficient multidimensional array providing vectorized\\narithmetic operations and sophisticated broadcasting capabilities\\n• Standard mathematical functions for fast operations on entire arrays of data\\nwithout having to write loops\\n• Tools for reading / writing array data to disk and working with memory-mapped\\nfiles\\n• Linear algebra, random number generation, and Fourier transform capabilities\\n• Tools for integrating code written in C, C++, and Fortran\\nThe last bullet point is also one of the most important ones from an ecosystem point\\nof view. Because NumPy provides an easy-to-use C API, it is very easy to pass data to'),\n",
       " Document(metadata={}, page_content='The last bullet point is also one of the most important ones from an ecosystem point\\nof view. Because NumPy provides an easy-to-use C API, it is very easy to pass data to\\nexternal libraries written in a low-level language and also for external libraries to return\\ndata to Python as NumPy arrays. This feature has made Python a language of choice\\nfor wrapping legacy C/C++/Fortran codebases and giving them a dynamic and easy-\\nto-use interface.\\nWhile NumPy by itself does not provide very much high-level data analytical func-\\ntionality, having an understanding of NumPy arrays and array-oriented computing will\\nhelp you use tools like pandas much more effectively. If you’re new to Python and just\\nlooking to get your hands dirty working with data using pandas, feel free to give this\\nchapter a skim. For more on advanced NumPy features like broadcasting, see Chap-\\nter 12.\\n79\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='For most data analysis applications, the main areas of functionality I’ll focus on are:\\n• Fast vectorized array operations for data munging and cleaning, subsetting and\\nfiltering, transformation, and any other kinds of computations\\n• Common array algorithms like sorting, unique, and set operations\\n• Efficient descriptive statistics and aggregating/summarizing data\\n• Data alignment and relational data manipulations for merging and joining together\\nheterogeneous data sets\\n• Expressing conditional logic as array expressions instead of loops with if-elif-\\nelse branches\\n• Group-wise data manipulations (aggregation, transformation, function applica-\\ntion). Much more on this in Chapter 5\\nWhile NumPy provides the computational foundation for these operations, you will\\nlikely want to use pandas as your basis for most kinds of data analysis (especially for\\nstructured or tabular data) as it provides a rich, high-level interface making most com-'),\n",
       " Document(metadata={}, page_content='likely want to use pandas as your basis for most kinds of data analysis (especially for\\nstructured or tabular data) as it provides a rich, high-level interface making most com-\\nmon data tasks very concise and simple. pandas also provides some more domain-\\nspecific functionality like time series manipulation, which is not present in NumPy.\\nIn this chapter and throughout the book, I use the standard NumPy\\nconvention of always using import numpy as np . You are, of course,\\nwelcome to put from numpy import * in your code to avoid having to\\nwrite np., but I would caution you against making a habit of this.\\nThe NumPy ndarray: A Multidimensional Array Object\\nOne of the key features of NumPy is its N-dimensional array object, or ndarray, which\\nis a fast, flexible container for large data sets in Python. Arrays enable you to perform\\nmathematical operations on whole blocks of data using similar syntax to the equivalent\\noperations between scalar elements:\\nIn [8]: data\\nOut[8]:'),\n",
       " Document(metadata={}, page_content='mathematical operations on whole blocks of data using similar syntax to the equivalent\\noperations between scalar elements:\\nIn [8]: data\\nOut[8]: \\narray([[ 0.9526, -0.246 , -0.8856],\\n       [ 0.5639,  0.2379,  0.9104]])\\nIn [9]: data * 10                         In [10]: data + data                \\nOut[9]:                                   Out[10]:                            \\narray([[ 9.5256, -2.4601, -8.8565],       array([[ 1.9051, -0.492 , -1.7713], \\n       [ 5.6385,  2.3794,  9.104 ]])             [ 1.1277,  0.4759,  1.8208]])\\nAn ndarray is a generic multidimensional container for homogeneous data; that is, all\\nof the elements must be the same type. Every array has a shape, a tuple indicating the\\nsize of each dimension, and a dtype, an object describing the data type of the array:\\nIn [11]: data.shape\\nOut[11]: (2, 3)\\n80 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"In [12]: data.dtype\\nOut[12]: dtype('float64')\\nThis chapter will introduce you to the basics of using NumPy arrays, and should be\\nsufficient for following along with the rest of the book. While it’s not necessary to have\\na deep understanding of NumPy for many data analytical applications, becoming pro-\\nficient in array-oriented programming and thinking is a key step along the way to be-\\ncoming a scientific Python guru.\\nWhenever you see “array”, “NumPy array”, or “ndarray” in the text,\\nwith few exceptions they all refer to the same thing: the ndarray object.\\nCreating ndarrays\\nThe easiest way to create an array is to use the array function. This accepts any se-\\nquence-like object (including other arrays) and produces a new NumPy array contain-\\ning the passed data. For example, a list is a good candidate for conversion:\\nIn [13]: data1 = [6, 7.5, 8, 0, 1]\\nIn [14]: arr1 = np.array(data1)\\nIn [15]: arr1\\nOut[15]: array([ 6. ,  7.5,  8. ,  0. ,  1. ])\"),\n",
       " Document(metadata={}, page_content=\"In [13]: data1 = [6, 7.5, 8, 0, 1]\\nIn [14]: arr1 = np.array(data1)\\nIn [15]: arr1\\nOut[15]: array([ 6. ,  7.5,  8. ,  0. ,  1. ])\\nNested sequences, like a list of equal-length lists, will be converted into a multidimen-\\nsional array:\\nIn [16]: data2 = [[1, 2, 3, 4], [5, 6, 7, 8]]\\nIn [17]: arr2 = np.array(data2)\\nIn [18]: arr2\\nOut[18]: \\narray([[1, 2, 3, 4],\\n       [5, 6, 7, 8]])\\nIn [19]: arr2.ndim\\nOut[19]: 2\\nIn [20]: arr2.shape\\nOut[20]: (2, 4)\\nUnless explicitly specified (more on this later), np.array tries to infer a good data type\\nfor the array that it creates. The data type is stored in a special dtype object; for example,\\nin the above two examples we have:\\nIn [21]: arr1.dtype\\nOut[21]: dtype('float64')\\nThe NumPy ndarray: A Multidimensional Array Object | 81\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"In [22]: arr2.dtype\\nOut[22]: dtype('int64')\\nIn addition to np.array, there are a number of other functions for creating new arrays.\\nAs examples, zeros and ones create arrays of 0’s or 1’s, respectively, with a given length\\nor shape. empty creates an array without initializing its values to any particular value.\\nTo create a higher dimensional array with these methods, pass a tuple for the shape:\\nIn [23]: np.zeros(10)\\nOut[23]: array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])\\nIn [24]: np.zeros((3, 6))                    \\nOut[24]:                                                         \\narray([[ 0.,  0.,  0.,  0.,  0.,  0.],                \\n       [ 0.,  0.,  0.,  0.,  0.,  0.],                \\n       [ 0.,  0.,  0.,  0.,  0.,  0.]])              \\nIn [25]: np.empty((2, 3, 2))\\nOut[25]:\\narray([[[  4.94065646e-324,   4.94065646e-324],\\n        [  3.87491056e-297,   2.46845796e-130],\\n        [  4.94065646e-324,   4.94065646e-324]],\\n       [[  1.90723115e+083,   5.73293533e-053],\"),\n",
       " Document(metadata={}, page_content='array([[[  4.94065646e-324,   4.94065646e-324],\\n        [  3.87491056e-297,   2.46845796e-130],\\n        [  4.94065646e-324,   4.94065646e-324]],\\n       [[  1.90723115e+083,   5.73293533e-053],\\n        [ -2.33568637e+124,  -6.70608105e-012],\\n        [  4.42786966e+160,   1.27100354e+025]]])\\nIt’s not safe to assume that np.empty will return an array of all zeros. In\\nmany cases, as previously shown, it will return uninitialized garbage\\nvalues.\\narange is an array-valued version of the built-in Python range function:\\nIn [26]: np.arange(15)\\nOut[26]: array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])\\nSee Table 4-1 for a short list of standard array creation functions. Since NumPy is\\nfocused on numerical computing, the data type, if not specified, will in many cases be\\nfloat64 (floating point).\\nTable 4-1. Array creation functions\\nFunction Description\\narray Convert input data (list, tuple, array, or other sequence type) to an ndarray either by'),\n",
       " Document(metadata={}, page_content='float64 (floating point).\\nTable 4-1. Array creation functions\\nFunction Description\\narray Convert input data (list, tuple, array, or other sequence type) to an ndarray either by\\ninferring a dtype or explicitly specifying a dtype. Copies the input data by default.\\nasarray Convert input to ndarray, but do not copy if the input is already an ndarray\\narange Like the built-in range but returns an ndarray instead of a list.\\nones, ones_like Produce an array of all 1’s with the given shape and dtype. ones_like takes another\\narray and produces a ones array of the same shape and dtype.\\nzeros, zeros_like Like ones and ones_like but producing arrays of 0’s instead\\n82 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"Function Description\\nempty, empty_like Create new arrays by allocating new memory, but do not populate with any values like \\nones and zeros\\neye, identity Create a square N x N identity matrix (1’s on the diagonal and 0’s elsewhere)\\nData Types for ndarrays\\nThe data type or dtype is a special object containing the information the ndarray needs\\nto interpret a chunk of memory as a particular type of data:\\nIn [27]: arr1 = np.array([1, 2, 3], dtype=np.float64)\\nIn [28]: arr2 = np.array([1, 2, 3], dtype=np.int32)\\nIn [29]: arr1.dtype            In [30]: arr2.dtype    \\nOut[29]: dtype('float64')      Out[30]: dtype('int32')\\nDtypes are part of what make NumPy so powerful and flexible. In most cases they map\\ndirectly onto an underlying machine representation, which makes it easy to read and\\nwrite binary streams of data to disk and also to connect to code written in a low-level\\nlanguage like C or Fortran. The numerical dtypes are named the same way: a type name,\"),\n",
       " Document(metadata={}, page_content='write binary streams of data to disk and also to connect to code written in a low-level\\nlanguage like C or Fortran. The numerical dtypes are named the same way: a type name,\\nlike float or int, followed by a number indicating the number of bits per element. A\\nstandard double-precision floating point value (what’s used under the hood in Python’s\\nfloat object) takes up 8 bytes or 64 bits. Thus, this type is known in NumPy as\\nfloat64. See Table 4-2 for a full listing of NumPy’s supported data types.\\nDon’t worry about memorizing the NumPy dtypes, especially if you’re\\na new user. It’s often only necessary to care about the general kind of\\ndata you’re dealing with, whether floating point, complex, integer,\\nboolean, string, or general Python object. When you need more control\\nover how data are stored in memory and on disk, especially large data\\nsets, it is good to know that you have control over the storage type.\\nTable 4-2. NumPy data types\\nType Type Code Description'),\n",
       " Document(metadata={}, page_content='over how data are stored in memory and on disk, especially large data\\nsets, it is good to know that you have control over the storage type.\\nTable 4-2. NumPy data types\\nType Type Code Description\\nint8, uint8 i1, u1 Signed and unsigned 8-bit (1 byte) integer types\\nint16, uint16 i2, u2 Signed and unsigned 16-bit integer types\\nint32, uint32 i4, u4 Signed and unsigned 32-bit integer types\\nint64, uint64 i8, u8 Signed and unsigned 32-bit integer types\\nfloat16 f2 Half-precision floating point\\nfloat32 f4 or f Standard single-precision floating point. Compatible with C float\\nfloat64, float128 f8 or d Standard double-precision floating point. Compatible with C double\\nand Python float object\\nThe NumPy ndarray: A Multidimensional Array Object | 83\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"Type Type Code Description\\nfloat128 f16 or g Extended-precision floating point\\ncomplex64, complex128,\\ncomplex256\\nc8, c16,\\nc32\\nComplex numbers represented by two 32, 64, or 128 floats, respectively\\nbool ? Boolean type storing True and False values\\nobject O Python object type\\nstring_ S Fixed-length string type (1 byte per character). For example, to create\\na string dtype with length 10, use 'S10'.\\nunicode_ U Fixed-length unicode type (number of bytes platform specific). Same\\nspecification semantics as string_ (e.g. 'U10').\\nYou can explicitly convert or cast an array from one dtype to another using ndarray’s \\nastype method:\\nIn [31]: arr = np.array([1, 2, 3, 4, 5])\\nIn [32]: arr.dtype\\nOut[32]: dtype('int64')\\nIn [33]: float_arr = arr.astype(np.float64)\\nIn [34]: float_arr.dtype\\nOut[34]: dtype('float64')\\nIn this example, integers were cast to floating point. If I cast some floating point num-\\nbers to be of integer dtype, the decimal part will be truncated:\"),\n",
       " Document(metadata={}, page_content=\"Out[34]: dtype('float64')\\nIn this example, integers were cast to floating point. If I cast some floating point num-\\nbers to be of integer dtype, the decimal part will be truncated:\\nIn [35]: arr = np.array([3.7, -1.2, -2.6, 0.5, 12.9, 10.1])\\nIn [36]: arr\\nOut[36]: array([  3.7,  -1.2,  -2.6,   0.5,  12.9,  10.1])\\nIn [37]: arr.astype(np.int32)\\nOut[37]: array([ 3, -1, -2,  0, 12, 10], dtype=int32)\\nShould you have an array of strings representing numbers, you can use astype to convert\\nthem to numeric form:\\nIn [38]: numeric_strings = np.array(['1.25', '-9.6', '42'], dtype=np.string_)\\nIn [39]: numeric_strings.astype(float)\\nOut[39]: array([  1.25,  -9.6 ,  42.  ])\\nIf casting were to fail for some reason (like a string that cannot be converted to\\nfloat64), a TypeError will be raised. See that I was a bit lazy and wrote float instead of\\nnp.float64; NumPy is smart enough to alias the Python types to the equivalent dtypes.\\nYou can also use another array’s dtype attribute:\"),\n",
       " Document(metadata={}, page_content='np.float64; NumPy is smart enough to alias the Python types to the equivalent dtypes.\\nYou can also use another array’s dtype attribute:\\nIn [40]: int_array = np.arange(10)\\n84 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"In [41]: calibers = np.array([.22, .270, .357, .380, .44, .50], dtype=np.float64)\\nIn [42]: int_array.astype(calibers.dtype)\\nOut[42]: array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9.])\\nThere are shorthand type code strings you can also use to refer to a dtype:\\nIn [43]: empty_uint32 = np.empty(8, dtype='u4')\\nIn [44]: empty_uint32\\nOut[44]: \\narray([       0,        0, 65904672,        0, 64856792,        0,\\n       39438163,        0], dtype=uint32)\\nCalling astype always creates a new array (a copy of the data), even if\\nthe new dtype is the same as the old dtype.\\nIt’s worth keeping in mind that floating point numbers, such as those\\nin float64 and float32 arrays, are only capable of approximating frac-\\ntional quantities. In complex computations, you may accrue some\\nfloating point error, making comparisons only valid up to a certain num-\\nber of decimal places.\\nOperations between Arrays and Scalars\\nArrays are important because they enable you to express batch operations on data\"),\n",
       " Document(metadata={}, page_content='ber of decimal places.\\nOperations between Arrays and Scalars\\nArrays are important because they enable you to express batch operations on data\\nwithout writing any for loops. This is usually called vectorization. Any arithmetic op-\\nerations between equal-size arrays applies the operation elementwise:\\nIn [45]: arr = np.array([[1., 2., 3.], [4., 5., 6.]])\\nIn [46]: arr\\nOut[46]: \\narray([[ 1.,  2.,  3.],\\n       [ 4.,  5.,  6.]])\\nIn [47]: arr * arr                 In [48]: arr - arr      \\nOut[47]:                           Out[48]:                \\narray([[  1.,   4.,   9.],         array([[ 0.,  0.,  0.], \\n       [ 16.,  25.,  36.]])               [ 0.,  0.,  0.]])\\nArithmetic operations with scalars are as you would expect, propagating the value to\\neach element:\\nIn [49]: 1 / arr                            In [50]: arr ** 0.5                 \\nOut[49]:                                    Out[50]:'),\n",
       " Document(metadata={}, page_content='each element:\\nIn [49]: 1 / arr                            In [50]: arr ** 0.5                 \\nOut[49]:                                    Out[50]:                            \\narray([[ 1.    ,  0.5   ,  0.3333],         array([[ 1.    ,  1.4142,  1.7321], \\n       [ 0.25  ,  0.2   ,  0.1667]])               [ 2.    ,  2.2361,  2.4495]])\\nThe NumPy ndarray: A Multidimensional Array Object | 85\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Operations between differently sized arrays is called broadcasting and will be discussed\\nin more detail in Chapter 12. Having a deep understanding of broadcasting is not nec-\\nessary for most of this book.\\nBasic Indexing and Slicing\\nNumPy array indexing is a rich topic, as there are many ways you may want to select\\na subset of your data or individual elements. One-dimensional arrays are simple; on\\nthe surface they act similarly to Python lists:\\nIn [51]: arr = np.arange(10)\\nIn [52]: arr\\nOut[52]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\\nIn [53]: arr[5]\\nOut[53]: 5\\nIn [54]: arr[5:8]\\nOut[54]: array([5, 6, 7])\\nIn [55]: arr[5:8] = 12\\nIn [56]: arr\\nOut[56]: array([ 0,  1,  2,  3,  4, 12, 12, 12,  8,  9])\\nAs you can see, if you assign a scalar value to a slice, as in arr[5:8] = 12, the value is\\npropagated (or broadcasted henceforth) to the entire selection. An important first dis-\\ntinction from lists is that array slices are views on the original array. This means that'),\n",
       " Document(metadata={}, page_content='propagated (or broadcasted henceforth) to the entire selection. An important first dis-\\ntinction from lists is that array slices are views on the original array. This means that\\nthe data is not copied, and any modifications to the view will be reflected in the source\\narray:\\nIn [57]: arr_slice = arr[5:8]\\nIn [58]: arr_slice[1] = 12345\\nIn [59]: arr\\nOut[59]: array([    0,     1,     2,     3,     4,    12, 12345,    12,     8,     9])\\nIn [60]: arr_slice[:] = 64\\nIn [61]: arr\\nOut[61]: array([ 0,  1,  2,  3,  4, 64, 64, 64,  8,  9])\\nIf you are new to NumPy, you might be surprised by this, especially if they have used\\nother array programming languages which copy data more zealously. As NumPy has\\nbeen designed with large data use cases in mind, you could imagine performance and\\nmemory problems if NumPy insisted on copying data left and right.\\n86 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='If you want a copy of a slice of an ndarray instead of a view, you will\\nneed to explicitly copy the array; for example arr[5:8].copy().\\nWith higher dimensional arrays, you have many more options. In a two-dimensional\\narray, the elements at each index are no longer scalars but rather one-dimensional\\narrays:\\nIn [62]: arr2d = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\\nIn [63]: arr2d[2]\\nOut[63]: array([7, 8, 9])\\nThus, individual elements can be accessed recursively. But that is a bit too much work,\\nso you can pass a comma-separated list of indices to select individual elements. So these\\nare equivalent:\\nIn [64]: arr2d[0][2]\\nOut[64]: 3\\nIn [65]: arr2d[0, 2]\\nOut[65]: 3\\nSee Figure 4-1 for an illustration of indexing on a 2D array.\\nFigure 4-1. Indexing elements in a NumPy array\\nIn multidimensional arrays, if you omit later indices, the returned object will be a lower-\\ndimensional ndarray consisting of all the data along the higher dimensions. So in the\\n2 × 2 × 3 array arr3d'),\n",
       " Document(metadata={}, page_content='In multidimensional arrays, if you omit later indices, the returned object will be a lower-\\ndimensional ndarray consisting of all the data along the higher dimensions. So in the\\n2 × 2 × 3 array arr3d\\nIn [66]: arr3d = np.array([[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]])\\nIn [67]: arr3d\\nOut[67]: \\narray([[[ 1,  2,  3],\\nThe NumPy ndarray: A Multidimensional Array Object | 87\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='[ 4,  5,  6]],\\n       [[ 7,  8,  9],\\n        [10, 11, 12]]])\\narr3d[0] is a 2 × 3 array:\\nIn [68]: arr3d[0]\\nOut[68]: \\narray([[1, 2, 3],\\n       [4, 5, 6]])\\nBoth scalar values and arrays can be assigned to arr3d[0]:\\nIn [69]: old_values = arr3d[0].copy()\\nIn [70]: arr3d[0] = 42\\nIn [71]: arr3d\\nOut[71]: \\narray([[[42, 42, 42],\\n        [42, 42, 42]],\\n       [[ 7,  8,  9],\\n        [10, 11, 12]]])\\nIn [72]: arr3d[0] = old_values\\nIn [73]: arr3d\\nOut[73]: \\narray([[[ 1,  2,  3],\\n        [ 4,  5,  6]],\\n       [[ 7,  8,  9],\\n        [10, 11, 12]]])\\nSimilarly, arr3d[1, 0] gives you all of the values whose indices start with (1, 0), form-\\ning a 1-dimensional array:\\nIn [74]: arr3d[1, 0]\\nOut[74]: array([7, 8, 9])\\nNote that in all of these cases where subsections of the array have been selected, the\\nreturned arrays are views.\\nIndexing with slices\\nLike one-dimensional objects such as Python lists, ndarrays can be sliced using the\\nfamiliar syntax:\\nIn [75]: arr[1:6]\\nOut[75]: array([ 1,  2,  3,  4, 64])'),\n",
       " Document(metadata={}, page_content='Indexing with slices\\nLike one-dimensional objects such as Python lists, ndarrays can be sliced using the\\nfamiliar syntax:\\nIn [75]: arr[1:6]\\nOut[75]: array([ 1,  2,  3,  4, 64])\\nHigher dimensional objects give you more options as you can slice one or more axes\\nand also mix integers. Consider the 2D array above, arr2d. Slicing this array is a bit\\ndifferent:\\nIn [76]: arr2d            In [77]: arr2d[:2]\\nOut[76]:                  Out[77]:          \\n88 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='array([[1, 2, 3],         array([[1, 2, 3], \\n       [4, 5, 6],                [4, 5, 6]])\\n       [7, 8, 9]])\\nAs you can see, it has sliced along axis 0, the first axis. A slice, therefore, selects a range\\nof elements along an axis. You can pass multiple slices just like you can pass multiple\\nindexes:\\nIn [78]: arr2d[:2, 1:]\\nOut[78]: \\narray([[2, 3],\\n       [5, 6]])\\nWhen slicing like this, you always obtain array views of the same number of dimensions.\\nBy mixing integer indexes and slices, you get lower dimensional slices:\\nIn [79]: arr2d[1, :2]         In [80]: arr2d[2, :1]\\nOut[79]: array([4, 5])        Out[80]: array([7])\\nSee Figure 4-2 for an illustration. Note that a colon by itself means to take the entire\\naxis, so you can slice only higher dimensional axes by doing:\\nIn [81]: arr2d[:, :1]\\nOut[81]: \\narray([[1],\\n       [4],\\n       [7]])\\nOf course, assigning to a slice expression assigns to the whole selection:\\nIn [82]: arr2d[:2, 1:] = 0\\nBoolean Indexing'),\n",
       " Document(metadata={}, page_content=\"In [81]: arr2d[:, :1]\\nOut[81]: \\narray([[1],\\n       [4],\\n       [7]])\\nOf course, assigning to a slice expression assigns to the whole selection:\\nIn [82]: arr2d[:2, 1:] = 0\\nBoolean Indexing\\nLet’s consider an example where we have some data in an array and an array of names\\nwith duplicates. I’m going to use here the randn function in numpy.random to generate\\nsome random normally distributed data:\\nIn [83]: names = np.array(['Bob', 'Joe', 'Will', 'Bob', 'Will', 'Joe', 'Joe'])\\nIn [84]: data = randn(7, 4)\\nIn [85]: names\\nOut[85]: \\narray(['Bob', 'Joe', 'Will', 'Bob', 'Will', 'Joe', 'Joe'], \\n      dtype='|S4')\\nIn [86]: data\\nOut[86]: \\narray([[-0.048 ,  0.5433, -0.2349,  1.2792],\\n       [-0.268 ,  0.5465,  0.0939, -2.0445],\\n       [-0.047 , -2.026 ,  0.7719,  0.3103],\\n       [ 2.1452,  0.8799, -0.0523,  0.0672],\\n       [-1.0023, -0.1698,  1.1503,  1.7289],\\nThe NumPy ndarray: A Multidimensional Array Object | 89\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"[ 0.1913,  0.4544,  0.4519,  0.5535],\\n       [ 0.5994,  0.8174, -0.9297, -1.2564]])\\nFigure 4-2. Two-dimensional array slicing\\nSuppose each name corresponds to a row in the data array. If we wanted to select all\\nthe rows with corresponding name 'Bob'. Like arithmetic operations, comparisons\\n(such as ==) with arrays are also vectorized. Thus, comparing names with the string\\n'Bob' yields a boolean array:\\nIn [87]: names == 'Bob'\\nOut[87]: array([ True, False, False, True, False, False, False], dtype=bool)\\nThis boolean array can be passed when indexing the array:\\nIn [88]: data[names == 'Bob']\\nOut[88]: \\narray([[-0.048 ,  0.5433, -0.2349,  1.2792],\\n       [ 2.1452,  0.8799, -0.0523,  0.0672]])\\nThe boolean array must be of the same length as the axis it’s indexing. You can even\\nmix and match boolean arrays with slices or integers (or sequences of integers, more\\non this later):\\nIn [89]: data[names == 'Bob', 2:]\\nOut[89]: \\narray([[-0.2349,  1.2792],\"),\n",
       " Document(metadata={}, page_content=\"mix and match boolean arrays with slices or integers (or sequences of integers, more\\non this later):\\nIn [89]: data[names == 'Bob', 2:]\\nOut[89]: \\narray([[-0.2349,  1.2792],\\n90 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"[-0.0523,  0.0672]])\\nIn [90]: data[names == 'Bob', 3]\\nOut[90]: array([ 1.2792,  0.0672])\\nTo select everything but 'Bob', you can either use != or negate the condition using -:\\nIn [91]: names != 'Bob'\\nOut[91]: array([False, True, True, False, True, True, True], dtype=bool)\\nIn [92]: data[-(names == 'Bob')]\\nOut[92]: \\narray([[-0.268 ,  0.5465,  0.0939, -2.0445],\\n       [-0.047 , -2.026 ,  0.7719,  0.3103],\\n       [-1.0023, -0.1698,  1.1503,  1.7289],\\n       [ 0.1913,  0.4544,  0.4519,  0.5535],\\n       [ 0.5994,  0.8174, -0.9297, -1.2564]])\\nSelecting two of the three names to combine multiple boolean conditions, use boolean\\narithmetic operators like & (and) and | (or):\\nIn [93]: mask = (names == 'Bob') | (names == 'Will')\\nIn [94]: mask\\nOut[94]: array([True, False, True, True, True, False, False], dtype=bool)\\nIn [95]: data[mask]\\nOut[95]: \\narray([[-0.048 ,  0.5433, -0.2349,  1.2792],\\n       [-0.047 , -2.026 ,  0.7719,  0.3103],\\n       [ 2.1452,  0.8799, -0.0523,  0.0672],\"),\n",
       " Document(metadata={}, page_content='In [95]: data[mask]\\nOut[95]: \\narray([[-0.048 ,  0.5433, -0.2349,  1.2792],\\n       [-0.047 , -2.026 ,  0.7719,  0.3103],\\n       [ 2.1452,  0.8799, -0.0523,  0.0672],\\n       [-1.0023, -0.1698,  1.1503,  1.7289]])\\nSelecting data from an array by boolean indexing always creates a copy of the data,\\neven if the returned array is unchanged.\\nThe Python keywords and and or do not work with boolean arrays.\\nSetting values with boolean arrays works in a common-sense way. To set all of the\\nnegative values in data to 0 we need only do:\\nIn [96]: data[data < 0] = 0\\nIn [97]: data\\nOut[97]: \\narray([[ 0.    ,  0.5433,  0.    ,  1.2792],\\n       [ 0.    ,  0.5465,  0.0939,  0.    ],\\n       [ 0.    ,  0.    ,  0.7719,  0.3103],\\n       [ 2.1452,  0.8799,  0.    ,  0.0672],\\n       [ 0.    ,  0.    ,  1.1503,  1.7289],\\n       [ 0.1913,  0.4544,  0.4519,  0.5535],\\n       [ 0.5994,  0.8174,  0.    ,  0.    ]])\\nThe NumPy ndarray: A Multidimensional Array Object | 91\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"Setting whole rows or columns using a 1D boolean array is also easy:\\nIn [98]: data[names != 'Joe'] = 7\\nIn [99]: data\\nOut[99]: \\narray([[ 7.    ,  7.    ,  7.    ,  7.    ],\\n       [ 0.    ,  0.5465,  0.0939,  0.    ],\\n       [ 7.    ,  7.    ,  7.    ,  7.    ],\\n       [ 7.    ,  7.    ,  7.    ,  7.    ],\\n       [ 7.    ,  7.    ,  7.    ,  7.    ],\\n       [ 0.1913,  0.4544,  0.4519,  0.5535],\\n       [ 0.5994,  0.8174,  0.    ,  0.    ]])\\nFancy Indexing\\nFancy indexing is a term adopted by NumPy to describe indexing using integer arrays.\\nSuppose we had a 8 × 4 array:\\nIn [100]: arr = np.empty((8, 4))\\nIn [101]: for i in range(8):\\n   .....:     arr[i] = i\\nIn [102]: arr\\nOut[102]: \\narray([[ 0.,  0.,  0.,  0.],\\n       [ 1.,  1.,  1.,  1.],\\n       [ 2.,  2.,  2.,  2.],\\n       [ 3.,  3.,  3.,  3.],\\n       [ 4.,  4.,  4.,  4.],\\n       [ 5.,  5.,  5.,  5.],\\n       [ 6.,  6.,  6.,  6.],\\n       [ 7.,  7.,  7.,  7.]])\"),\n",
       " Document(metadata={}, page_content='[ 2.,  2.,  2.,  2.],\\n       [ 3.,  3.,  3.,  3.],\\n       [ 4.,  4.,  4.,  4.],\\n       [ 5.,  5.,  5.,  5.],\\n       [ 6.,  6.,  6.,  6.],\\n       [ 7.,  7.,  7.,  7.]])\\nTo select out a subset of the rows in a particular order, you can simply pass a list or\\nndarray of integers specifying the desired order:\\nIn [103]: arr[[4, 3, 0, 6]]\\nOut[103]: \\narray([[ 4.,  4.,  4.,  4.],\\n       [ 3.,  3.,  3.,  3.],\\n       [ 0.,  0.,  0.,  0.],\\n       [ 6.,  6.,  6.,  6.]])\\nHopefully this code did what you expected! Using negative indices select rows from\\nthe end:\\nIn [104]: arr[[-3, -5, -7]]\\nOut[104]: \\narray([[ 5.,  5.,  5.,  5.],\\n       [ 3.,  3.,  3.,  3.],\\n       [ 1.,  1.,  1.,  1.]])\\n92 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Passing multiple index arrays does something slightly different; it selects a 1D array of\\nelements corresponding to each tuple of indices:\\n# more on reshape in Chapter 12\\nIn [105]: arr = np.arange(32).reshape((8, 4))\\nIn [106]: arr\\nOut[106]: \\narray([[ 0,  1,  2,  3],\\n       [ 4,  5,  6,  7],\\n       [ 8,  9, 10, 11],\\n       [12, 13, 14, 15],\\n       [16, 17, 18, 19],\\n       [20, 21, 22, 23],\\n       [24, 25, 26, 27],\\n       [28, 29, 30, 31]])\\nIn [107]: arr[[1, 5, 7, 2], [0, 3, 1, 2]]\\nOut[107]: array([ 4, 23, 29, 10])\\nTake a moment to understand what just happened: the elements (1, 0), (5, 3), (7,\\n1), and (2, 2) were selected. The behavior of fancy indexing in this case is a bit different\\nfrom what some users might have expected (myself included), which is the rectangular\\nregion formed by selecting a subset of the matrix’s rows and columns. Here is one way\\nto get that:\\nIn [108]: arr[[1, 5, 7, 2]][:, [0, 3, 1, 2]]\\nOut[108]: \\narray([[ 4,  7,  5,  6],\\n       [20, 23, 21, 22],'),\n",
       " Document(metadata={}, page_content='to get that:\\nIn [108]: arr[[1, 5, 7, 2]][:, [0, 3, 1, 2]]\\nOut[108]: \\narray([[ 4,  7,  5,  6],\\n       [20, 23, 21, 22],\\n       [28, 31, 29, 30],\\n       [ 8, 11,  9, 10]])\\nAnother way is to use the np.ix_ function, which converts two 1D integer arrays to an\\nindexer that selects the square region:\\nIn [109]: arr[np.ix_([1, 5, 7, 2], [0, 3, 1, 2])]\\nOut[109]: \\narray([[ 4,  7,  5,  6],\\n       [20, 23, 21, 22],\\n       [28, 31, 29, 30],\\n       [ 8, 11,  9, 10]])\\nKeep in mind that fancy indexing, unlike slicing, always copies the data into a new array.\\nTransposing Arrays and Swapping Axes\\nTransposing is a special form of reshaping which similarly returns a view on the un-\\nderlying data without copying anything. Arrays have the transpose method and also\\nthe special T attribute:\\nIn [110]: arr = np.arange(15).reshape((3, 5))\\nIn [111]: arr                        In [112]: arr.T      \\nThe NumPy ndarray: A Multidimensional Array Object | 93\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Out[111]:                            Out[112]:            \\narray([[ 0,  1,  2,  3,  4],         array([[ 0,  5, 10], \\n       [ 5,  6,  7,  8,  9],                [ 1,  6, 11], \\n       [10, 11, 12, 13, 14]])               [ 2,  7, 12], \\n                                            [ 3,  8, 13], \\n                                            [ 4,  9, 14]])\\nWhen doing matrix computations, you will do this very often, like for example com-\\nputing the inner matrix product XTX using np.dot:\\nIn [113]: arr = np.random.randn(6, 3)\\nIn [114]: np.dot(arr.T, arr)\\nOut[114]: \\narray([[ 2.584 ,  1.8753,  0.8888],\\n       [ 1.8753,  6.6636,  0.3884],\\n       [ 0.8888,  0.3884,  3.9781]])\\nFor higher dimensional arrays, transpose will accept a tuple of axis numbers to permute\\nthe axes (for extra mind bending):\\nIn [115]: arr = np.arange(16).reshape((2, 2, 4))\\nIn [116]: arr\\nOut[116]: \\narray([[[ 0,  1,  2,  3],\\n        [ 4,  5,  6,  7]],\\n       [[ 8,  9, 10, 11],\\n        [12, 13, 14, 15]]])'),\n",
       " Document(metadata={}, page_content='In [115]: arr = np.arange(16).reshape((2, 2, 4))\\nIn [116]: arr\\nOut[116]: \\narray([[[ 0,  1,  2,  3],\\n        [ 4,  5,  6,  7]],\\n       [[ 8,  9, 10, 11],\\n        [12, 13, 14, 15]]])\\nIn [117]: arr.transpose((1, 0, 2))\\nOut[117]: \\narray([[[ 0,  1,  2,  3],\\n        [ 8,  9, 10, 11]],\\n       [[ 4,  5,  6,  7],\\n        [12, 13, 14, 15]]])\\nSimple transposing with .T is just a special case of swapping axes. ndarray has the\\nmethod swapaxes which takes a pair of axis numbers:\\nIn [118]: arr                      In [119]: arr.swapaxes(1, 2)\\nOut[118]:                          Out[119]:                   \\narray([[[ 0,  1,  2,  3],          array([[[ 0,  4],           \\n        [ 4,  5,  6,  7]],                 [ 1,  5],           \\n                                           [ 2,  6],           \\n       [[ 8,  9, 10, 11],                  [ 3,  7]],          \\n        [12, 13, 14, 15]]])                                    \\n                                          [[ 8, 12],'),\n",
       " Document(metadata={}, page_content='[[ 8,  9, 10, 11],                  [ 3,  7]],          \\n        [12, 13, 14, 15]]])                                    \\n                                          [[ 8, 12],           \\n                                           [ 9, 13],           \\n                                           [10, 14],           \\n                                           [11, 15]]])\\nswapaxes similarly returns a view on the data without making a copy.\\n94 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Universal Functions: Fast Element-wise Array Functions\\nA universal function, or ufunc, is a function that performs elementwise operations on\\ndata in ndarrays. You can think of them as fast vectorized wrappers for simple functions\\nthat take one or more scalar values and produce one or more scalar results.\\nMany ufuncs are simple elementwise transformations, like sqrt or exp:\\nIn [120]: arr = np.arange(10)\\nIn [121]: np.sqrt(arr)\\nOut[121]: \\narray([ 0.    ,  1.    ,  1.4142,  1.7321,  2.    ,  2.2361,  2.4495,\\n        2.6458,  2.8284,  3.    ])\\nIn [122]: np.exp(arr)\\nOut[122]: \\narray([    1.    ,     2.7183,     7.3891,    20.0855,    54.5982,\\n         148.4132,   403.4288,  1096.6332,  2980.958 ,  8103.0839])\\nThese are referred to as unary ufuncs. Others, such as add or maximum, take 2 arrays\\n(thus, binary ufuncs) and return a single array as the result:\\nIn [123]: x = randn(8)\\nIn [124]: y = randn(8)\\nIn [125]: x\\nOut[125]: \\narray([ 0.0749,  0.0974,  0.2002, -0.2551,  0.4655,  0.9222,  0.446 ,'),\n",
       " Document(metadata={}, page_content='In [123]: x = randn(8)\\nIn [124]: y = randn(8)\\nIn [125]: x\\nOut[125]: \\narray([ 0.0749,  0.0974,  0.2002, -0.2551,  0.4655,  0.9222,  0.446 ,\\n       -0.9337])\\nIn [126]: y\\nOut[126]: \\narray([ 0.267 , -1.1131, -0.3361,  0.6117, -1.2323,  0.4788,  0.4315,\\n       -0.7147])\\nIn [127]: np.maximum(x, y) # element-wise maximum\\nOut[127]: \\narray([ 0.267 ,  0.0974,  0.2002,  0.6117,  0.4655,  0.9222,  0.446 ,\\n       -0.7147])\\nWhile not common, a ufunc can return multiple arrays. modf is one example, a vector-\\nized version of the built-in Python divmod: it returns the fractional and integral parts of\\na floating point array:\\nIn [128]: arr = randn(7) * 5\\nIn [129]: np.modf(arr)\\nOut[129]: \\n(array([-0.6808,  0.0636, -0.386 ,  0.1393, -0.8806,  0.9363, -0.883 ]),\\n array([-2.,  4., -3.,  5., -3.,  3., -6.]))\\nUniversal Functions: Fast Element-wise Array Functions | 95\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='See Table 4-3 and Table 4-4 for a listing of available ufuncs.\\nTable 4-3. Unary ufuncs\\nFunction Description\\nabs, fabs Compute the absolute value element-wise for integer, floating point, or complex values.\\nUse fabs as a faster alternative for non-complex-valued data\\nsqrt Compute the square root of each element. Equivalent to arr ** 0.5\\nsquare Compute the square of each element. Equivalent to arr ** 2\\nexp Compute the exponent ex of each element\\nlog, log10, log2, log1p Natural logarithm (base e), log base 10, log base 2, and log(1 + x), respectively\\nsign Compute the sign of each element: 1 (positive), 0 (zero), or -1 (negative)\\nceil Compute the ceiling of each element, i.e. the smallest integer greater than or equal to\\neach element\\nfloor Compute the floor of each element, i.e. the largest integer less than or equal to each\\nelement\\nrint Round elements to the nearest integer, preserving the dtype\\nmodf Return fractional and integral parts of array as separate array'),\n",
       " Document(metadata={}, page_content='element\\nrint Round elements to the nearest integer, preserving the dtype\\nmodf Return fractional and integral parts of array as separate array\\nisnan Return boolean array indicating whether each value is NaN (Not a Number)\\nisfinite, isinf Return boolean array indicating whether each element is finite (non-inf, non-NaN) or\\ninfinite, respectively\\ncos, cosh, sin, sinh,\\ntan, tanh\\nRegular and hyperbolic trigonometric functions\\narccos, arccosh, arcsin,\\narcsinh, arctan, arctanh\\nInverse trigonometric functions\\nlogical_not Compute truth value of not x element-wise. Equivalent to -arr.\\nTable 4-4. Binary universal functions\\nFunction Description\\nadd Add corresponding elements in arrays\\nsubtract Subtract elements in second array from first array\\nmultiply Multiply array elements\\ndivide, floor_divide Divide or floor divide (truncating the remainder)\\npower Raise elements in first array to powers indicated in second array\\nmaximum, fmax Element-wise maximum. fmax ignores NaN'),\n",
       " Document(metadata={}, page_content='divide, floor_divide Divide or floor divide (truncating the remainder)\\npower Raise elements in first array to powers indicated in second array\\nmaximum, fmax Element-wise maximum. fmax ignores NaN\\nminimum, fmin Element-wise minimum. fmin ignores NaN\\nmod Element-wise modulus (remainder of division)\\ncopysign Copy sign of values in second argument to values in first argument\\n96 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Function Description\\ngreater, greater_equal,\\nless, less_equal, equal,\\nnot_equal\\nPerform element-wise comparison, yielding boolean array. Equivalent to infix operators\\n>, >=, <, <=, ==, !=\\nlogical_and,\\nlogical_or, logical_xor\\nCompute element-wise truth value of logical operation. Equivalent to infix operators &\\n|, ^\\nData Processing Using Arrays\\nUsing NumPy arrays enables you to express many kinds of data processing tasks as\\nconcise array expressions that might otherwise require writing loops. This practice of\\nreplacing explicit loops with array expressions is commonly referred to as vectoriza-\\ntion. In general, vectorized array operations will often be one or two (or more) orders\\nof magnitude faster than their pure Python equivalents, with the biggest impact in any\\nkind of numerical computations. Later, in Chapter 12, I will explain broadcasting, a\\npowerful method for vectorizing computations.\\nAs a simple example, suppose we wished to evaluate the function sqrt(x^2 + y^2)'),\n",
       " Document(metadata={}, page_content='powerful method for vectorizing computations.\\nAs a simple example, suppose we wished to evaluate the function sqrt(x^2 + y^2)\\nacross a regular grid of values. The np.meshgrid function takes two 1D arrays and pro-\\nduces two 2D matrices corresponding to all pairs of (x, y) in the two arrays:\\nIn [130]: points = np.arange(-5, 5, 0.01) # 1000 equally spaced points\\nIn [131]: xs, ys = np.meshgrid(points, points)\\nIn [132]: ys\\nOut[132]: \\narray([[-5.  , -5.  , -5.  , ..., -5.  , -5.  , -5.  ],\\n       [-4.99, -4.99, -4.99, ..., -4.99, -4.99, -4.99],\\n       [-4.98, -4.98, -4.98, ..., -4.98, -4.98, -4.98],\\n       ..., \\n       [ 4.97,  4.97,  4.97, ...,  4.97,  4.97,  4.97],\\n       [ 4.98,  4.98,  4.98, ...,  4.98,  4.98,  4.98],\\n       [ 4.99,  4.99,  4.99, ...,  4.99,  4.99,  4.99]])\\nNow, evaluating the function is a simple matter of writing the same expression you\\nwould write with two points:\\nIn [134]: import matplotlib.pyplot as plt\\nIn [135]: z = np.sqrt(xs ** 2 + ys ** 2)\\nIn [136]: z'),\n",
       " Document(metadata={}, page_content='would write with two points:\\nIn [134]: import matplotlib.pyplot as plt\\nIn [135]: z = np.sqrt(xs ** 2 + ys ** 2)\\nIn [136]: z\\nOut[136]: \\narray([[ 7.0711,  7.064 ,  7.0569, ...,  7.0499,  7.0569,  7.064 ],\\n       [ 7.064 ,  7.0569,  7.0499, ...,  7.0428,  7.0499,  7.0569],\\n       [ 7.0569,  7.0499,  7.0428, ...,  7.0357,  7.0428,  7.0499],\\n       ..., \\n       [ 7.0499,  7.0428,  7.0357, ...,  7.0286,  7.0357,  7.0428],\\n       [ 7.0569,  7.0499,  7.0428, ...,  7.0357,  7.0428,  7.0499],\\n       [ 7.064 ,  7.0569,  7.0499, ...,  7.0428,  7.0499,  7.0569]])\\nData Processing Using Arrays | 97\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='In [137]: plt.imshow(z, cmap=plt.cm.gray); plt.colorbar()\\nOut[137]: <matplotlib.colorbar.Colorbar instance at 0x4e46d40>\\nIn [138]: plt.title(\"Image plot of $\\\\sqrt{x^2 + y^2}$ for a grid of values\")\\nOut[138]: <matplotlib.text.Text at 0x4565790>\\nSee Figure 4-3. Here I used the matplotlib function imshow to create an image plot from\\na 2D array of function values.\\nFigure 4-3. Plot of function evaluated on grid\\nExpressing Conditional Logic as Array Operations\\nThe numpy.where function is a vectorized version of the ternary expression x if condi\\ntion else y. Suppose we had a boolean array and two arrays of values:\\nIn [140]: xarr = np.array([1.1, 1.2, 1.3, 1.4, 1.5])\\nIn [141]: yarr = np.array([2.1, 2.2, 2.3, 2.4, 2.5])\\nIn [142]: cond = np.array([True, False, True, True, False])\\nSuppose we wanted to take a value from xarr whenever the corresponding value in\\ncond is True otherwise take the value from yarr. A list comprehension doing this might\\nlook like:\\nIn [143]: result = [(x if c else y)'),\n",
       " Document(metadata={}, page_content='cond is True otherwise take the value from yarr. A list comprehension doing this might\\nlook like:\\nIn [143]: result = [(x if c else y)\\n   .....:           for x, y, c in zip(xarr, yarr, cond)]\\nIn [144]: result\\nOut[144]: [1.1000000000000001, 2.2000000000000002, 1.3, 1.3999999999999999, 2.5]\\n98 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='This has multiple problems. First, it will not be very fast for large arrays (because all\\nthe work is being done in pure Python). Secondly, it will not work with multidimen-\\nsional arrays. With np.where you can write this very concisely:\\nIn [145]: result = np.where(cond, xarr, yarr)\\nIn [146]: result\\nOut[146]: array([ 1.1,  2.2,  1.3,  1.4,  2.5])\\nThe second and third arguments to np.where don’t need to be arrays; one or both of\\nthem can be scalars. A typical use of where in data analysis is to produce a new array of\\nvalues based on another array. Suppose you had a matrix of randomly generated data\\nand you wanted to replace all positive values with 2 and all negative values with -2.\\nThis is very easy to do with np.where:\\nIn [147]: arr = randn(4, 4)\\nIn [148]: arr\\nOut[148]: \\narray([[ 0.6372,  2.2043,  1.7904,  0.0752],\\n       [-1.5926, -1.1536,  0.4413,  0.3483],\\n       [-0.1798,  0.3299,  0.7827, -0.7585],\\n       [ 0.5857,  0.1619,  1.3583, -1.3865]])\\nIn [149]: np.where(arr > 0, 2, -2)'),\n",
       " Document(metadata={}, page_content='[-1.5926, -1.1536,  0.4413,  0.3483],\\n       [-0.1798,  0.3299,  0.7827, -0.7585],\\n       [ 0.5857,  0.1619,  1.3583, -1.3865]])\\nIn [149]: np.where(arr > 0, 2, -2)\\nOut[149]: \\narray([[ 2,  2,  2,  2],\\n       [-2, -2,  2,  2],\\n       [-2,  2,  2, -2],\\n       [ 2,  2,  2, -2]])\\nIn [150]: np.where(arr > 0, 2, arr) # set only positive values to 2\\nOut[150]: \\narray([[ 2.    ,  2.    ,  2.    ,  2.    ],\\n       [-1.5926, -1.1536,  2.    ,  2.    ],\\n       [-0.1798,  2.    ,  2.    , -0.7585],\\n       [ 2.    ,  2.    ,  2.    , -1.3865]])\\nThe arrays passed to where can be more than just equal sizes array or scalers.\\nWith some cleverness you can use where to express more complicated logic; consider\\nthis example where I have two boolean arrays, cond1 and cond2, and wish to assign a\\ndifferent value for each of the 4 possible pairs of boolean values:\\nresult = []\\nfor i in range(n):\\n    if cond1[i] and cond2[i]:\\n        result.append(0)\\n    elif cond1[i]:\\n        result.append(1)'),\n",
       " Document(metadata={}, page_content='different value for each of the 4 possible pairs of boolean values:\\nresult = []\\nfor i in range(n):\\n    if cond1[i] and cond2[i]:\\n        result.append(0)\\n    elif cond1[i]:\\n        result.append(1)\\n    elif cond2[i]:\\n        result.append(2)\\n    else:\\n        result.append(3)\\nData Processing Using Arrays | 99\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='While perhaps not immediately obvious, this for loop can be converted into a nested\\nwhere expression:\\nnp.where(cond1 & cond2, 0,\\n         np.where(cond1, 1,\\n                  np.where(cond2, 2, 3)))\\nIn this particular example, we can also take advantage of the fact that boolean values\\nare treated as 0 or 1 in calculations, so this could alternatively be expressed (though a\\nbit more cryptically) as an arithmetic operation:\\nresult = 1 * cond1 + 2 * cond2 + 3 * -(cond1 | cond2)\\nMathematical and Statistical Methods\\nA set of mathematical functions which compute statistics about an entire array or about\\nthe data along an axis are accessible as array methods. Aggregations (often called\\nreductions) like sum, mean, and standard deviation std can either be used by calling the\\narray instance method or using the top level NumPy function:\\nIn [151]: arr = np.random.randn(5, 4) # normally-distributed data\\nIn [152]: arr.mean()\\nOut[152]: 0.062814911084854597\\nIn [153]: np.mean(arr)'),\n",
       " Document(metadata={}, page_content='In [151]: arr = np.random.randn(5, 4) # normally-distributed data\\nIn [152]: arr.mean()\\nOut[152]: 0.062814911084854597\\nIn [153]: np.mean(arr)\\nOut[153]: 0.062814911084854597\\nIn [154]: arr.sum()\\nOut[154]: 1.2562982216970919\\nFunctions like mean and sum take an optional axis argument which computes the statistic\\nover the given axis, resulting in an array with one fewer dimension:\\nIn [155]: arr.mean(axis=1)\\nOut[155]: array([-1.2833,  0.2844,  0.6574,  0.6743, -0.0187])\\nIn [156]: arr.sum(0)\\nOut[156]: array([-3.1003, -1.6189,  1.4044,  4.5712])\\nOther methods like cumsum and cumprod do not aggregate, instead producing an array\\nof the intermediate results:\\nIn [157]: arr = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8]])\\nIn [158]: arr.cumsum(0)        In [159]: arr.cumprod(1)\\nOut[158]:                      Out[159]:               \\narray([[ 0,  1,  2],           array([[  0,   0,   0], \\n       [ 3,  5,  7],                  [  3,  12,  60], \\n       [ 9, 12, 15]])                 [  6,  42, 336]])'),\n",
       " Document(metadata={}, page_content='array([[ 0,  1,  2],           array([[  0,   0,   0], \\n       [ 3,  5,  7],                  [  3,  12,  60], \\n       [ 9, 12, 15]])                 [  6,  42, 336]])\\nSee Table 4-5 for a full listing. We’ll see many examples of these methods in action in\\nlater chapters.\\n100 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Table 4-5. Basic array statistical methods\\nMethod Description\\nsum Sum of all the elements in the array or along an axis. Zero-length arrays have sum 0.\\nmean Arithmetic mean. Zero-length arrays have NaN mean.\\nstd, var Standard deviation and variance, respectively, with optional degrees of freedom adjust-\\nment (default denominator n).\\nmin, max Minimum and maximum.\\nargmin, argmax Indices of minimum and maximum elements, respectively.\\ncumsum Cumulative sum of elements starting from 0\\ncumprod Cumulative product of elements starting from 1\\nMethods for Boolean Arrays\\nBoolean values are coerced to 1 (True) and 0 (False) in the above methods. Thus, sum\\nis often used as a means of counting True values in a boolean array:\\nIn [160]: arr = randn(100)\\nIn [161]: (arr > 0).sum() # Number of positive values\\nOut[161]: 44\\nThere are two additional methods, any and all, useful especially for boolean arrays. \\nany tests whether one or more values in an array is True, while all checks if every value\\nis True:'),\n",
       " Document(metadata={}, page_content='Out[161]: 44\\nThere are two additional methods, any and all, useful especially for boolean arrays. \\nany tests whether one or more values in an array is True, while all checks if every value\\nis True:\\nIn [162]: bools = np.array([False, False, True, False])\\nIn [163]: bools.any()\\nOut[163]: True\\nIn [164]: bools.all()\\nOut[164]: False\\nThese methods also work with non-boolean arrays, where non-zero elements evaluate\\nto True.\\nSorting\\nLike Python’s built-in list type, NumPy arrays can be sorted in-place using the sort\\nmethod:\\nIn [165]: arr = randn(8)\\nIn [166]: arr\\nOut[166]: \\narray([ 0.6903,  0.4678,  0.0968, -0.1349,  0.9879,  0.0185, -1.3147,\\n       -0.5425])\\nIn [167]: arr.sort()\\nData Processing Using Arrays | 101\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='In [168]: arr\\nOut[168]: \\narray([-1.3147, -0.5425, -0.1349,  0.0185,  0.0968,  0.4678,  0.6903,\\n        0.9879])\\nMultidimensional arrays can have each 1D section of values sorted in-place along an\\naxis by passing the axis number to sort:\\nIn [169]: arr = randn(5, 3)\\nIn [170]: arr\\nOut[170]: \\narray([[-0.7139, -1.6331, -0.4959],\\n       [ 0.8236, -1.3132, -0.1935],\\n       [-1.6748,  3.0336, -0.863 ],\\n       [-0.3161,  0.5362, -2.468 ],\\n       [ 0.9058,  1.1184, -1.0516]])\\nIn [171]: arr.sort(1)\\nIn [172]: arr\\nOut[172]: \\narray([[-1.6331, -0.7139, -0.4959],\\n       [-1.3132, -0.1935,  0.8236],\\n       [-1.6748, -0.863 ,  3.0336],\\n       [-2.468 , -0.3161,  0.5362],\\n       [-1.0516,  0.9058,  1.1184]])\\nThe top level method np.sort returns a sorted copy of an array instead of modifying\\nthe array in place. A quick-and-dirty way to compute the quantiles of an array is to sort\\nit and select the value at a particular rank:\\nIn [173]: large_arr = randn(1000)\\nIn [174]: large_arr.sort()'),\n",
       " Document(metadata={}, page_content=\"the array in place. A quick-and-dirty way to compute the quantiles of an array is to sort\\nit and select the value at a particular rank:\\nIn [173]: large_arr = randn(1000)\\nIn [174]: large_arr.sort()\\nIn [175]: large_arr[int(0.05 * len(large_arr))] # 5% quantile\\nOut[175]: -1.5791023260896004\\nFor more details on using NumPy’s sorting methods, and more advanced techniques\\nlike indirect sorts, see Chapter 12. Several other kinds of data manipulations related to\\nsorting (for example, sorting a table of data by one or more columns) are also to be\\nfound in pandas.\\nUnique and Other Set Logic\\nNumPy has some basic set operations for one-dimensional ndarrays. Probably the most\\ncommonly used one is np.unique, which returns the sorted unique values in an array:\\nIn [176]: names = np.array(['Bob', 'Joe', 'Will', 'Bob', 'Will', 'Joe', 'Joe'])\\nIn [177]: np.unique(names)\\nOut[177]: \\n102 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"array(['Bob', 'Joe', 'Will'], \\n      dtype='|S4')\\nIn [178]: ints = np.array([3, 3, 3, 2, 2, 1, 1, 4, 4])\\nIn [179]: np.unique(ints)\\nOut[179]: array([1, 2, 3, 4])\\nContrast np.unique with the pure Python alternative:\\nIn [180]: sorted(set(names))\\nOut[180]: ['Bob', 'Joe', 'Will']\\nAnother function, np.in1d, tests membership of the values in one array in another,\\nreturning a boolean array:\\nIn [181]: values = np.array([6, 0, 0, 3, 2, 5, 6])\\nIn [182]: np.in1d(values, [2, 3, 6])\\nOut[182]: array([ True, False, False,  True,  True, False,  True], dtype=bool)\\nSee Table 4-6 for a listing of set functions in NumPy.\\nTable 4-6. Array set operations\\nMethod Description\\nunique(x) Compute the sorted, unique elements in x\\nintersect1d(x, y) Compute the sorted, common elements in x and y\\nunion1d(x, y) Compute the sorted union of elements\\nin1d(x, y) Compute a boolean array indicating whether each element of x is contained in y\\nsetdiff1d(x, y) Set difference, elements in x that are not in y\"),\n",
       " Document(metadata={}, page_content=\"in1d(x, y) Compute a boolean array indicating whether each element of x is contained in y\\nsetdiff1d(x, y) Set difference, elements in x that are not in y\\nsetxor1d(x, y) Set symmetric differences; elements that are in either of the arrays, but not both\\nFile Input and Output with Arrays\\nNumPy is able to save and load data to and from disk either in text or binary format.\\nIn later chapters you will learn about tools in pandas for reading tabular data into\\nmemory.\\nStoring Arrays on Disk in Binary Format\\nnp.save and np.load are the two workhorse functions for efficiently saving and loading\\narray data on disk. Arrays are saved by default in an uncompressed raw binary format\\nwith file extension .npy.\\nIn [183]: arr = np.arange(10)\\nIn [184]: np.save('some_array', arr)\\nFile Input and Output with Arrays | 103\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"If the file path does not already end in .npy, the extension will be appended. The array\\non disk can then be loaded using np.load:\\nIn [185]: np.load('some_array.npy')\\nOut[185]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\\nYou save multiple arrays in a zip archive using np.savez and passing the arrays as key-\\nword arguments:\\nIn [186]: np.savez('array_archive.npz', a=arr, b=arr)\\nWhen loading an .npz file, you get back a dict-like object which loads the individual\\narrays lazily:\\nIn [187]: arch = np.load('array_archive.npz')\\nIn [188]: arch['b']\\nOut[188]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\\nSaving and Loading Text Files\\nLoading text from files is a fairly standard task. The landscape of file reading and writing\\nfunctions in Python can be a bit confusing for a newcomer, so I will focus mainly on\\nthe read_csv and read_table functions in pandas. It will at times be useful to load data\\ninto vanilla NumPy arrays using np.loadtxt or the more specialized np.genfromtxt.\"),\n",
       " Document(metadata={}, page_content=\"the read_csv and read_table functions in pandas. It will at times be useful to load data\\ninto vanilla NumPy arrays using np.loadtxt or the more specialized np.genfromtxt.\\nThese functions have many options allowing you to specify different delimiters, con-\\nverter functions for certain columns, skipping rows, and other things. Take a simple\\ncase of a comma-separated file (CSV) like this:\\nIn [191]: !cat array_ex.txt\\n0.580052,0.186730,1.040717,1.134411\\n0.194163,-0.636917,-0.938659,0.124094\\n-0.126410,0.268607,-0.695724,0.047428\\n-1.484413,0.004176,-0.744203,0.005487\\n2.302869,0.200131,1.670238,-1.881090\\n-0.193230,1.047233,0.482803,0.960334\\nThis can be loaded into a 2D array like so:\\nIn [192]: arr = np.loadtxt('array_ex.txt', delimiter=',')\\nIn [193]: arr\\nOut[193]: \\narray([[ 0.5801,  0.1867,  1.0407,  1.1344],\\n       [ 0.1942, -0.6369, -0.9387,  0.1241],\\n       [-0.1264,  0.2686, -0.6957,  0.0474],\\n       [-1.4844,  0.0042, -0.7442,  0.0055],\\n       [ 2.3029,  0.2001,  1.6702, -1.8811],\"),\n",
       " Document(metadata={}, page_content='[ 0.1942, -0.6369, -0.9387,  0.1241],\\n       [-0.1264,  0.2686, -0.6957,  0.0474],\\n       [-1.4844,  0.0042, -0.7442,  0.0055],\\n       [ 2.3029,  0.2001,  1.6702, -1.8811],\\n       [-0.1932,  1.0472,  0.4828,  0.9603]])\\nnp.savetxt performs the inverse operation: writing an array to a delimited text file.\\ngenfromtxt is similar to loadtxt but is geared for structured arrays and missing data\\nhandling; see Chapter 12 for more on structured arrays.\\n104 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='For more on file reading and writing, especially tabular or spreadsheet-\\nlike data, see the later chapters involving pandas and DataFrame objects.\\nLinear Algebra\\nLinear algebra, like matrix multiplication, decompositions, determinants, and other\\nsquare matrix math, is an important part of any array library. Unlike some languages\\nlike MATLAB, multiplying two two-dimensional arrays with * is an element-wise\\nproduct instead of a matrix dot product. As such, there is a function dot, both an array\\nmethod, and a function in the numpy namespace, for matrix multiplication:\\nIn [194]: x = np.array([[1., 2., 3.], [4., 5., 6.]])\\nIn [195]: y = np.array([[6., 23.], [-1, 7], [8, 9]])\\nIn [196]: x                   In [197]: y          \\nOut[196]:                     Out[197]:            \\narray([[ 1.,  2.,  3.],       array([[  6.,  23.], \\n       [ 4.,  5.,  6.]])             [ -1.,   7.], \\n                                     [  8.,   9.]])'),\n",
       " Document(metadata={}, page_content='[ 4.,  5.,  6.]])             [ -1.,   7.], \\n                                     [  8.,   9.]])\\n                                                   \\nIn [198]: x.dot(y)  # equivalently np.dot(x, y)\\nOut[198]: \\narray([[  28.,   64.],\\n       [  67.,  181.]])\\nA matrix product between a 2D array and a suitably sized 1D array results in a 1D array:\\nIn [199]: np.dot(x, np.ones(3))\\nOut[199]: array([  6.,  15.])\\nnumpy.linalg has a standard set of matrix decompositions and things like inverse and\\ndeterminant. These are implemented under the hood using the same industry-standard\\nFortran libraries used in other languages like MATLAB and R, such as like BLAS, LA-\\nPACK, or possibly (depending on your NumPy build) the Intel MKL:\\nIn [201]: from numpy.linalg import inv, qr\\nIn [202]: X = randn(5, 5)\\nIn [203]: mat = X.T.dot(X)\\nIn [204]: inv(mat)\\nOut[204]: \\narray([[ 3.0361, -0.1808, -0.6878, -2.8285, -1.1911],\\n       [-0.1808,  0.5035,  0.1215,  0.6702,  0.0956],'),\n",
       " Document(metadata={}, page_content='In [202]: X = randn(5, 5)\\nIn [203]: mat = X.T.dot(X)\\nIn [204]: inv(mat)\\nOut[204]: \\narray([[ 3.0361, -0.1808, -0.6878, -2.8285, -1.1911],\\n       [-0.1808,  0.5035,  0.1215,  0.6702,  0.0956],\\n       [-0.6878,  0.1215,  0.2904,  0.8081,  0.3049],\\n       [-2.8285,  0.6702,  0.8081,  3.4152,  1.1557],\\n       [-1.1911,  0.0956,  0.3049,  1.1557,  0.6051]])\\nIn [205]: mat.dot(inv(mat))\\nLinear Algebra | 105\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Out[205]: \\narray([[ 1.,  0.,  0.,  0., -0.],\\n       [ 0.,  1., -0.,  0.,  0.],\\n       [ 0., -0.,  1.,  0.,  0.],\\n       [ 0., -0., -0.,  1., -0.],\\n       [ 0.,  0.,  0.,  0.,  1.]])\\nIn [206]: q, r = qr(mat)\\nIn [207]: r\\nOut[207]: \\narray([[ -6.9271,   7.389 ,   6.1227,  -7.1163,  -4.9215],\\n       [  0.    ,  -3.9735,  -0.8671,   2.9747,  -5.7402],\\n       [  0.    ,   0.    , -10.2681,   1.8909,   1.6079],\\n       [  0.    ,   0.    ,   0.    ,  -1.2996,   3.3577],\\n       [  0.    ,   0.    ,   0.    ,   0.    ,   0.5571]])\\nSee Table 4-7 for a list of some of the most commonly-used linear algebra functions.\\nThe scientific Python community is hopeful that there may be a matrix\\nmultiplication infix operator implemented someday, providing syntac-\\ntically nicer alternative to using np.dot. But for now this is the way.\\nTable 4-7. Commonly-used numpy.linalg functions\\nFunction Description'),\n",
       " Document(metadata={}, page_content='tically nicer alternative to using np.dot. But for now this is the way.\\nTable 4-7. Commonly-used numpy.linalg functions\\nFunction Description\\ndiag Return the diagonal (or off-diagonal) elements of a square matrix as a 1D array, or convert a 1D array into a square\\nmatrix with zeros on the off-diagonal\\ndot Matrix multiplication\\ntrace Compute the sum of the diagonal elements\\ndet Compute the matrix determinant\\neig Compute the eigenvalues and eigenvectors of a square matrix\\ninv Compute the inverse of a square matrix\\npinv Compute the Moore-Penrose pseudo-inverse inverse of a square matrix\\nqr Compute the QR decomposition\\nsvd Compute the singular value decomposition (SVD)\\nsolve Solve the linear system Ax = b for x, where A is a square matrix\\nlstsq Compute the least-squares solution to y = Xb\\nRandom Number Generation\\nThe numpy.random module supplements the built-in Python random with functions for\\nefficiently generating whole arrays of sample values from many kinds of probability'),\n",
       " Document(metadata={}, page_content='Random Number Generation\\nThe numpy.random module supplements the built-in Python random with functions for\\nefficiently generating whole arrays of sample values from many kinds of probability\\n106 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='distributions. For example, you can get a 4 by 4 array of samples from the standard\\nnormal distribution using normal:\\nIn [208]: samples = np.random.normal(size=(4, 4))\\nIn [209]: samples\\nOut[209]: \\narray([[ 0.1241,  0.3026,  0.5238,  0.0009],\\n       [ 1.3438, -0.7135, -0.8312, -2.3702],\\n       [-1.8608, -0.8608,  0.5601, -1.2659],\\n       [ 0.1198, -1.0635,  0.3329, -2.3594]])\\nPython’s built-in random module, by contrast, only samples one value at a time. As you\\ncan see from this benchmark, numpy.random is well over an order of magnitude faster\\nfor generating very large samples:\\nIn [210]: from random import normalvariate\\nIn [211]: N = 1000000\\nIn [212]: %timeit samples = [normalvariate(0, 1) for _ in xrange(N)]\\n1 loops, best of 3: 1.33 s per loop\\nIn [213]: %timeit np.random.normal(size=N)\\n10 loops, best of 3: 57.7 ms per loop\\nSee table Table 4-8 for a partial list of functions available in numpy.random. I’ll give some'),\n",
       " Document(metadata={}, page_content='In [213]: %timeit np.random.normal(size=N)\\n10 loops, best of 3: 57.7 ms per loop\\nSee table Table 4-8 for a partial list of functions available in numpy.random. I’ll give some\\nexamples of leveraging these functions’ ability to generate large arrays of samples all at\\nonce in the next section.\\nTable 4-8. Partial list of numpy.random functions\\nFunction Description\\nseed Seed the random number generator\\npermutation Return a random permutation of a sequence, or return a permuted range\\nshuffle Randomly permute a sequence in place\\nrand Draw samples from a uniform distribution\\nrandint Draw random integers from a given low-to-high range\\nrandn Draw samples from a normal distribution with mean 0 and standard deviation 1 (MATLAB-like interface)\\nbinomial Draw samples a binomial distribution\\nnormal Draw samples from a normal (Gaussian) distribution\\nbeta Draw samples from a beta distribution\\nchisquare Draw samples from a chi-square distribution\\ngamma Draw samples from a gamma distribution'),\n",
       " Document(metadata={}, page_content='normal Draw samples from a normal (Gaussian) distribution\\nbeta Draw samples from a beta distribution\\nchisquare Draw samples from a chi-square distribution\\ngamma Draw samples from a gamma distribution\\nuniform Draw samples from a uniform [0, 1) distribution\\nRandom Number Generation | 107\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Example: Random Walks\\nAn illustrative application of utilizing array operations is in the simulation of random\\nwalks. Let’s first consider a simple random walk starting at 0 with steps of 1 and -1\\noccurring with equal probability. A pure Python way to implement a single random\\nwalk with 1,000 steps using the built-in random module:\\nimport random\\nposition = 0\\nwalk = [position]\\nsteps = 1000\\nfor i in xrange(steps):\\n    step = 1 if random.randint(0, 1) else -1\\n    position += step\\n    walk.append(position)\\nSee Figure 4-4 for an example plot of the first 100 values on one of these random walks.\\nFigure 4-4. A simple random walk\\nYou might make the observation that walk is simply the cumulative sum of the random\\nsteps and could be evaluated as an array expression. Thus, I use the np.random module\\nto draw 1,000 coin flips at once, set these to 1 and -1, and compute the cumulative sum:\\nIn [215]: nsteps = 1000\\nIn [216]: draws = np.random.randint(0, 2, size=nsteps)'),\n",
       " Document(metadata={}, page_content='to draw 1,000 coin flips at once, set these to 1 and -1, and compute the cumulative sum:\\nIn [215]: nsteps = 1000\\nIn [216]: draws = np.random.randint(0, 2, size=nsteps)\\nIn [217]: steps = np.where(draws > 0, 1, -1)\\nIn [218]: walk = steps.cumsum()\\n108 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='From this we can begin to extract statistics like the minimum and maximum value along\\nthe walk’s trajectory:\\nIn [219]: walk.min()        In [220]: walk.max()\\nOut[219]: -3                Out[220]: 31\\nA more complicated statistic is the first crossing time , the step at which the random\\nwalk reaches a particular value. Here we might want to know how long it took the\\nrandom walk to get at least 10 steps away from the origin 0 in either direction.\\nnp.abs(walk) >= 10 gives us a boolean array indicating where the walk has reached or\\nexceeded 10, but we want the index of the first 10 or -10. Turns out this can be com-\\nputed using argmax, which returns the first index of the maximum value in the boolean\\narray (True is the maximum value):\\nIn [221]: (np.abs(walk) >= 10).argmax()\\nOut[221]: 37\\nNote that using argmax here is not always efficient because it always makes a full scan\\nof the array. In this special case once a True is observed we know it to be the maximum\\nvalue.'),\n",
       " Document(metadata={}, page_content='Out[221]: 37\\nNote that using argmax here is not always efficient because it always makes a full scan\\nof the array. In this special case once a True is observed we know it to be the maximum\\nvalue.\\nSimulating Many Random Walks at Once\\nIf your goal was to simulate many random walks, say 5,000 of them, you can generate\\nall of the random walks with minor modifications to the above code. The numpy.ran\\ndom functions if passed a 2-tuple will generate a 2D array of draws, and we can compute\\nthe cumulative sum across the rows to compute all 5,000 random walks in one shot:\\nIn [222]: nwalks = 5000\\nIn [223]: nsteps = 1000\\nIn [224]: draws = np.random.randint(0, 2, size=(nwalks, nsteps)) # 0 or 1\\nIn [225]: steps = np.where(draws > 0, 1, -1)\\nIn [226]: walks = steps.cumsum(1)\\nIn [227]: walks\\nOut[227]: \\narray([[  1,   0,   1, ...,   8,   7,   8],\\n       [  1,   0,  -1, ...,  34,  33,  32],\\n       [  1,   0,  -1, ...,   4,   5,   4],\\n       ..., \\n       [  1,   2,   1, ...,  24,  25,  26],'),\n",
       " Document(metadata={}, page_content='Out[227]: \\narray([[  1,   0,   1, ...,   8,   7,   8],\\n       [  1,   0,  -1, ...,  34,  33,  32],\\n       [  1,   0,  -1, ...,   4,   5,   4],\\n       ..., \\n       [  1,   2,   1, ...,  24,  25,  26],\\n       [  1,   2,   3, ...,  14,  13,  14],\\n       [ -1,  -2,  -3, ..., -24, -23, -22]])\\nNow, we can compute the maximum and minimum values obtained over all of the\\nwalks:\\nIn [228]: walks.max()        In [229]: walks.min()\\nOut[228]: 138                Out[229]: -133\\nExample: Random Walks | 109\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Out of these walks, let’s compute the minimum crossing time to 30 or -30. This is\\nslightly tricky because not all 5,000 of them reach 30. We can check this using the \\nany method:\\nIn [230]: hits30 = (np.abs(walks) >= 30).any(1)\\nIn [231]: hits30\\nOut[231]: array([False, True, False, ..., False, True, False], dtype=bool)\\nIn [232]: hits30.sum() # Number that hit 30 or -30\\nOut[232]: 3410\\nWe can use this boolean array to select out the rows of walks that actually cross the\\nabsolute 30 level and call argmax across axis 1 to get the crossing times:\\nIn [233]: crossing_times = (np.abs(walks[hits30]) >= 30).argmax(1)\\nIn [234]: crossing_times.mean()\\nOut[234]: 498.88973607038122\\nFeel free to experiment with other distributions for the steps other than equal sized\\ncoin flips. You need only use a different random number generation function, like \\nnormal to generate normally distributed steps with some mean and standard deviation:\\nIn [235]: steps = np.random.normal(loc=0, scale=0.25,'),\n",
       " Document(metadata={}, page_content='normal to generate normally distributed steps with some mean and standard deviation:\\nIn [235]: steps = np.random.normal(loc=0, scale=0.25,\\n   .....:                          size=(nwalks, nsteps))\\n110 | Chapter 4: \\u2002NumPy Basics: Arrays and Vectorized Computation\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='CHAPTER 5\\nGetting Started with pandas\\npandas will be the primary library of interest throughout much of the rest of the book.\\nIt contains high-level data structures and manipulation tools designed to make data\\nanalysis fast and easy in Python. pandas is built on top of NumPy and makes it easy to\\nuse in NumPy-centric applications.\\nAs a bit of background, I started building pandas in early 2008 during my tenure at\\nAQR, a quantitative investment management firm. At the time, I had a distinct set of\\nrequirements that were not well-addressed by any single tool at my disposal:\\n• Data structures with labeled axes supporting automatic or explicit data alignment.\\nThis prevents common errors resulting from misaligned data and working with\\ndifferently-indexed data coming from different sources.\\n• Integrated time series functionality.\\n• The same data structures handle both time series data and non-time series data.\\n• Arithmetic operations and reductions (like summing across an axis) would pass'),\n",
       " Document(metadata={}, page_content='• The same data structures handle both time series data and non-time series data.\\n• Arithmetic operations and reductions (like summing across an axis) would pass\\non the metadata (axis labels).\\n• Flexible handling of missing data.\\n• Merge and other relational operations found in popular database databases (SQL-\\nbased, for example).\\nI wanted to be able to do all of these things in one place, preferably in a language well-\\nsuited to general purpose software development. Python was a good candidate lan-\\nguage for this, but at that time there was not an integrated set of data structures and\\ntools providing this functionality.\\nOver the last four years, pandas has matured into a quite large library capable of solving\\na much broader set of data handling problems than I ever anticipated, but it has ex-\\npanded in its scope without compromising the simplicity and ease-of-use that I desired\\nfrom the very beginning. I hope that after reading this book, you will find it to be just'),\n",
       " Document(metadata={}, page_content='panded in its scope without compromising the simplicity and ease-of-use that I desired\\nfrom the very beginning. I hope that after reading this book, you will find it to be just\\nas much of an indispensable tool as I do.\\nThroughout the rest of the book, I use the following import conventions for pandas:\\n111\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='In [1]: from pandas import Series, DataFrame\\nIn [2]: import pandas as pd\\nThus, whenever you see pd. in code, it’s referring to pandas. Series and DataFrame are\\nused so much that I find it easier to import them into the local namespace.\\nIntroduction to pandas Data Structures\\nTo get started with pandas, you will need to get comfortable with its two workhorse\\ndata structures: Series and DataFrame. While they are not a universal solution for every\\nproblem, they provide a solid, easy-to-use basis for most applications.\\nSeries\\nA Series is a one-dimensional array-like object containing an array of data (of any\\nNumPy data type) and an associated array of data labels, called its index. The simplest\\nSeries is formed from only an array of data:\\nIn [4]: obj = Series([4, 7, -5, 3])\\nIn [5]: obj\\nOut[5]: \\n0    4\\n1    7\\n2   -5\\n3    3\\nThe string representation of a Series displayed interactively shows the index on the left'),\n",
       " Document(metadata={}, page_content=\"In [4]: obj = Series([4, 7, -5, 3])\\nIn [5]: obj\\nOut[5]: \\n0    4\\n1    7\\n2   -5\\n3    3\\nThe string representation of a Series displayed interactively shows the index on the left\\nand the values on the right. Since we did not specify an index for the data, a default\\none consisting of the integers 0 through N - 1 (where N is the length of the data) is\\ncreated. You can get the array representation and index object of the Series via its values\\nand index attributes, respectively:\\nIn [6]: obj.values\\nOut[6]: array([ 4,  7, -5,  3])\\nIn [7]: obj.index\\nOut[7]: Int64Index([0, 1, 2, 3])\\nOften it will be desirable to create a Series with an index identifying each data point:\\nIn [8]: obj2 = Series([4, 7, -5, 3], index=['d', 'b', 'a', 'c'])\\nIn [9]: obj2\\nOut[9]: \\nd    4\\nb    7\\na   -5\\nc    3\\n112 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"In [10]: obj2.index\\nOut[10]: Index([d, b, a, c], dtype=object)\\nCompared with a regular NumPy array, you can use values in the index when selecting\\nsingle values or a set of values:\\nIn [11]: obj2['a']\\nOut[11]: -5\\nIn [12]: obj2['d'] = 6\\nIn [13]: obj2[['c', 'a', 'd']]\\nOut[13]: \\nc    3\\na   -5\\nd    6\\nNumPy array operations, such as filtering with a boolean array, scalar multiplication,\\nor applying math functions, will preserve the index-value link:\\nIn [14]: obj2\\nOut[14]: \\nd    6\\nb    7\\na   -5\\nc    3\\nIn [15]: obj2[obj2 > 0]      In [16]: obj2 * 2      In [17]: np.exp(obj2)\\nOut[15]:                     Out[16]:               Out[17]:             \\nd    6                       d    12                d     403.428793     \\nb    7                       b    14                b    1096.633158     \\nc    3                       a   -10                a       0.006738     \\n                             c     6                c      20.085537\"),\n",
       " Document(metadata={}, page_content=\"c    3                       a   -10                a       0.006738     \\n                             c     6                c      20.085537\\nAnother way to think about a Series is as a fixed-length, ordered dict, as it is a mapping\\nof index values to data values. It can be substituted into many functions that expect a\\ndict:\\nIn [18]: 'b' in obj2\\nOut[18]: True\\nIn [19]: 'e' in obj2\\nOut[19]: False\\nShould you have data contained in a Python dict, you can create a Series from it by\\npassing the dict:\\nIn [20]: sdata = {'Ohio': 35000, 'Texas': 71000, 'Oregon': 16000, 'Utah': 5000}\\nIn [21]: obj3 = Series(sdata)\\nIn [22]: obj3\\nOut[22]: \\nOhio      35000\\nOregon    16000\\nIntroduction to pandas Data Structures | 113\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"Texas     71000\\nUtah       5000\\nWhen only passing a dict, the index in the resulting Series will have the dict’s keys in\\nsorted order.\\nIn [23]: states = ['California', 'Ohio', 'Oregon', 'Texas']\\nIn [24]: obj4 = Series(sdata, index=states)\\nIn [25]: obj4\\nOut[25]: \\nCalifornia      NaN\\nOhio          35000\\nOregon        16000\\nTexas         71000\\nIn this case, 3 values found in sdata were placed in the appropriate locations, but since\\nno value for 'California' was found, it appears as NaN (not a number) which is con-\\nsidered in pandas to mark missing or NA values. I will use the terms “missing” or “NA”\\nto refer to missing data. The isnull and notnull functions in pandas should be used to\\ndetect missing data:\\nIn [26]: pd.isnull(obj4)      In [27]: pd.notnull(obj4)\\nOut[26]:                      Out[27]:                 \\nCalifornia     True           California    False      \\nOhio          False           Ohio           True      \\nOregon        False           Oregon         True\"),\n",
       " Document(metadata={}, page_content='California     True           California    False      \\nOhio          False           Ohio           True      \\nOregon        False           Oregon         True      \\nTexas         False           Texas          True\\nSeries also has these as instance methods:\\nIn [28]: obj4.isnull()\\nOut[28]: \\nCalifornia     True\\nOhio          False\\nOregon        False\\nTexas         False\\nI discuss working with missing data in more detail later in this chapter.\\nA critical Series feature for many applications is that it automatically aligns differently-\\nindexed data in arithmetic operations:\\nIn [29]: obj3          In [30]: obj4      \\nOut[29]:               Out[30]:           \\nOhio      35000        California      NaN\\nOregon    16000        Ohio          35000\\nTexas     71000        Oregon        16000\\nUtah       5000        Texas         71000\\n                                          \\nIn [31]: obj3 + obj4\\nOut[31]: \\nCalifornia       NaN\\nOhio           70000\\nOregon         32000'),\n",
       " Document(metadata={}, page_content='Utah       5000        Texas         71000\\n                                          \\nIn [31]: obj3 + obj4\\nOut[31]: \\nCalifornia       NaN\\nOhio           70000\\nOregon         32000\\n114 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"Texas         142000\\nUtah             NaN\\nData alignment features are addressed as a separate topic.\\nBoth the Series object itself and its index have a name attribute, which integrates with\\nother key areas of pandas functionality:\\nIn [32]: obj4.name = 'population'\\nIn [33]: obj4.index.name = 'state'\\nIn [34]: obj4\\nOut[34]: \\nstate\\nCalifornia      NaN\\nOhio          35000\\nOregon        16000\\nTexas         71000\\nName: population\\nA Series’s index can be altered in place by assignment:\\nIn [35]: obj.index = ['Bob', 'Steve', 'Jeff', 'Ryan']\\nIn [36]: obj\\nOut[36]: \\nBob      4\\nSteve    7\\nJeff    -5\\nRyan     3\\nDataFrame\\nA DataFrame represents a tabular, spreadsheet-like data structure containing an or-\\ndered collection of columns, each of which can be a different value type (numeric,\\nstring, boolean, etc.). The DataFrame has both a row and column index; it can be\\nthought of as a dict of Series (one for all sharing the same index). Compared with other\"),\n",
       " Document(metadata={}, page_content='string, boolean, etc.). The DataFrame has both a row and column index; it can be\\nthought of as a dict of Series (one for all sharing the same index). Compared with other\\nsuch DataFrame-like structures you may have used before (like R’s data.frame), row-\\noriented and column-oriented operations in DataFrame are treated roughly symmet-\\nrically. Under the hood, the data is stored as one or more two-dimensional blocks rather\\nthan a list, dict, or some other collection of one-dimensional arrays. The exact details\\nof DataFrame’s internals are far outside the scope of this book.\\nWhile DataFrame stores the data internally in a two-dimensional for-\\nmat, you can easily represent much higher-dimensional data in a tabular\\nformat using hierarchical indexing, a subject of a later section and a key\\ningredient in many of the more advanced data-handling features in pan-\\ndas.\\nIntroduction to pandas Data Structures | 115\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"There are numerous ways to construct a DataFrame, though one of the most common\\nis from a dict of equal-length lists or NumPy arrays\\ndata = {'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'],\\n        'year': [2000, 2001, 2002, 2001, 2002],\\n        'pop': [1.5, 1.7, 3.6, 2.4, 2.9]}\\nframe = DataFrame(data)\\nThe resulting DataFrame will have its index assigned automatically as with Series, and\\nthe columns are placed in sorted order:\\nIn [38]: frame\\nOut[38]: \\n   pop   state  year\\n0  1.5    Ohio  2000\\n1  1.7    Ohio  2001\\n2  3.6    Ohio  2002\\n3  2.4  Nevada  2001\\n4  2.9  Nevada  2002\\nIf you specify a sequence of columns, the DataFrame’s columns will be exactly what\\nyou pass:\\nIn [39]: DataFrame(data, columns=['year', 'state', 'pop'])\\nOut[39]: \\n   year   state  pop\\n0  2000    Ohio  1.5\\n1  2001    Ohio  1.7\\n2  2002    Ohio  3.6\\n3  2001  Nevada  2.4\\n4  2002  Nevada  2.9\\nAs with Series, if you pass a column that isn’t contained in data, it will appear with NA\\nvalues in the result:\"),\n",
       " Document(metadata={}, page_content=\"1  2001    Ohio  1.7\\n2  2002    Ohio  3.6\\n3  2001  Nevada  2.4\\n4  2002  Nevada  2.9\\nAs with Series, if you pass a column that isn’t contained in data, it will appear with NA\\nvalues in the result:\\nIn [40]: frame2 = DataFrame(data, columns=['year', 'state', 'pop', 'debt'],\\n   ....:                    index=['one', 'two', 'three', 'four', 'five'])\\nIn [41]: frame2\\nOut[41]: \\n       year   state  pop  debt\\none    2000    Ohio  1.5   NaN\\ntwo    2001    Ohio  1.7   NaN\\nthree  2002    Ohio  3.6   NaN\\nfour   2001  Nevada  2.4   NaN\\nfive   2002  Nevada  2.9   NaN\\nIn [42]: frame2.columns\\nOut[42]: Index([year, state, pop, debt], dtype=object)\\nA column in a DataFrame can be retrieved as a Series either by dict-like notation or by\\nattribute:\\nIn [43]: frame2['state']        In [44]: frame2.year\\nOut[43]:                        Out[44]:            \\none        Ohio                 one      2000       \\n116 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"two        Ohio                 two      2001       \\nthree      Ohio                 three    2002       \\nfour     Nevada                 four     2001       \\nfive     Nevada                 five     2002       \\nName: state                     Name: year\\nNote that the returned Series have the same index as the DataFrame, and their name\\nattribute has been appropriately set.\\nRows can also be retrieved by position or name by a couple of methods, such as the\\nix indexing field (much more on this later):\\nIn [45]: frame2.ix['three']\\nOut[45]: \\nyear     2002\\nstate    Ohio\\npop       3.6\\ndebt      NaN\\nName: three\\nColumns can be modified by assignment. For example, the empty 'debt' column could\\nbe assigned a scalar value or an array of values:\\nIn [46]: frame2['debt'] = 16.5\\nIn [47]: frame2\\nOut[47]: \\n       year   state  pop  debt\\none    2000    Ohio  1.5  16.5\\ntwo    2001    Ohio  1.7  16.5\\nthree  2002    Ohio  3.6  16.5\\nfour   2001  Nevada  2.4  16.5\\nfive   2002  Nevada  2.9  16.5\"),\n",
       " Document(metadata={}, page_content=\"Out[47]: \\n       year   state  pop  debt\\none    2000    Ohio  1.5  16.5\\ntwo    2001    Ohio  1.7  16.5\\nthree  2002    Ohio  3.6  16.5\\nfour   2001  Nevada  2.4  16.5\\nfive   2002  Nevada  2.9  16.5\\nIn [48]: frame2['debt'] = np.arange(5.)\\nIn [49]: frame2\\nOut[49]: \\n       year   state  pop  debt\\none    2000    Ohio  1.5     0\\ntwo    2001    Ohio  1.7     1\\nthree  2002    Ohio  3.6     2\\nfour   2001  Nevada  2.4     3\\nfive   2002  Nevada  2.9     4\\nWhen assigning lists or arrays to a column, the value’s length must match the length\\nof the DataFrame. If you assign a Series, it will be instead conformed exactly to the\\nDataFrame’s index, inserting missing values in any holes:\\nIn [50]: val = Series([-1.2, -1.5, -1.7], index=['two', 'four', 'five'])\\nIn [51]: frame2['debt'] = val\\nIn [52]: frame2\\nOut[52]: \\n       year   state  pop  debt\\nIntroduction to pandas Data Structures | 117\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"one    2000    Ohio  1.5   NaN\\ntwo    2001    Ohio  1.7  -1.2\\nthree  2002    Ohio  3.6   NaN\\nfour   2001  Nevada  2.4  -1.5\\nfive   2002  Nevada  2.9  -1.7\\nAssigning a column that doesn’t exist will create a new column. The del keyword will\\ndelete columns as with a dict:\\nIn [53]: frame2['eastern'] = frame2.state == 'Ohio'\\nIn [54]: frame2\\nOut[54]: \\n       year   state  pop  debt eastern\\none    2000    Ohio  1.5   NaN    True\\ntwo    2001    Ohio  1.7  -1.2    True\\nthree  2002    Ohio  3.6   NaN    True\\nfour   2001  Nevada  2.4  -1.5   False\\nfive   2002  Nevada  2.9  -1.7   False\\nIn [55]: del frame2['eastern']\\nIn [56]: frame2.columns\\nOut[56]: Index([year, state, pop, debt], dtype=object)\\nThe column returned when indexing a DataFrame is a view on the un-\\nderlying data, not a copy. Thus, any in-place modifications to the Series\\nwill be reflected in the DataFrame. The column can be explicitly copied\\nusing the Series’s copy method.\\nAnother common form of data is a nested dict of dicts format:\"),\n",
       " Document(metadata={}, page_content=\"will be reflected in the DataFrame. The column can be explicitly copied\\nusing the Series’s copy method.\\nAnother common form of data is a nested dict of dicts format:\\nIn [57]: pop = {'Nevada': {2001: 2.4, 2002: 2.9},\\n   ....:        'Ohio': {2000: 1.5, 2001: 1.7, 2002: 3.6}}\\nIf passed to DataFrame, it will interpret the outer dict keys as the columns and the inner\\nkeys as the row indices:\\nIn [58]: frame3 = DataFrame(pop)\\nIn [59]: frame3\\nOut[59]: \\n      Nevada  Ohio\\n2000     NaN   1.5\\n2001     2.4   1.7\\n2002     2.9   3.6\\nOf course you can always transpose the result:\\nIn [60]: frame3.T\\nOut[60]: \\n        2000  2001  2002\\nNevada   NaN   2.4   2.9\\nOhio     1.5   1.7   3.6\\n118 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"The keys in the inner dicts are unioned and sorted to form the index in the result. This\\nisn’t true if an explicit index is specified:\\nIn [61]: DataFrame(pop, index=[2001, 2002, 2003])\\nOut[61]: \\n      Nevada  Ohio\\n2001     2.4   1.7\\n2002     2.9   3.6\\n2003     NaN   NaN\\nDicts of Series are treated much in the same way:\\nIn [62]: pdata = {'Ohio': frame3['Ohio'][:-1],\\n   ....:          'Nevada': frame3['Nevada'][:2]}\\nIn [63]: DataFrame(pdata)\\nOut[63]: \\n      Nevada  Ohio\\n2000     NaN   1.5\\n2001     2.4   1.7\\nFor a complete list of things you can pass the DataFrame constructor, see Table 5-1.\\nIf a DataFrame’s index and columns have their name attributes set, these will also be\\ndisplayed:\\nIn [64]: frame3.index.name = 'year'; frame3.columns.name = 'state'\\nIn [65]: frame3\\nOut[65]: \\nstate  Nevada  Ohio\\nyear               \\n2000      NaN   1.5\\n2001      2.4   1.7\\n2002      2.9   3.6\\nLike Series, the values attribute returns the data contained in the DataFrame as a 2D\\nndarray:\"),\n",
       " Document(metadata={}, page_content='state  Nevada  Ohio\\nyear               \\n2000      NaN   1.5\\n2001      2.4   1.7\\n2002      2.9   3.6\\nLike Series, the values attribute returns the data contained in the DataFrame as a 2D\\nndarray:\\nIn [66]: frame3.values\\nOut[66]: \\narray([[ nan,  1.5],\\n       [ 2.4,  1.7],\\n       [ 2.9,  3.6]])\\nIf the DataFrame’s columns are different dtypes, the dtype of the values array will be\\nchosen to accomodate all of the columns:\\nIn [67]: frame2.values\\nOut[67]: \\narray([[2000, Ohio, 1.5, nan],\\n       [2001, Ohio, 1.7, -1.2],\\n       [2002, Ohio, 3.6, nan],\\n       [2001, Nevada, 2.4, -1.5],\\n       [2002, Nevada, 2.9, -1.7]], dtype=object)\\nIntroduction to pandas Data Structures | 119\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Table 5-1. Possible data inputs to DataFrame constructor\\nType Notes\\n2D ndarray A matrix of data, passing optional row and column labels\\ndict of arrays, lists, or tuples Each sequence becomes a column in the DataFrame. All sequences must be the same length.\\nNumPy structured/record array Treated as the “dict of arrays” case\\ndict of Series Each value becomes a column. Indexes from each Series are unioned together to form the\\nresult’s row index if no explicit index is passed.\\ndict of dicts Each inner dict becomes a column. Keys are unioned to form the row index as in the “dict of\\nSeries” case.\\nlist of dicts or Series Each item becomes a row in the DataFrame. Union of dict keys or Series indexes become the\\nDataFrame’s column labels\\nList of lists or tuples Treated as the “2D ndarray” case\\nAnother DataFrame The DataFrame’s indexes are used unless different ones are passed\\nNumPy MaskedArray\\nLike the “2D ndarray” case except masked values become NA/missing in the DataFrame result\\nIndex Objects'),\n",
       " Document(metadata={}, page_content=\"NumPy MaskedArray\\nLike the “2D ndarray” case except masked values become NA/missing in the DataFrame result\\nIndex Objects\\npandas’s \\nIndex objects are responsible for holding the axis labels and other metadata\\n(like the axis name or names). Any array or other sequence of labels used when con-\\nstructing a Series or DataFrame is internally converted to an Index:\\nIn [68]: obj = Series(range(3), index=['a', 'b', 'c'])\\nIn [69]: index = obj.index\\nIn [70]: index\\nOut[70]: Index([a, b, c], dtype=object)\\nIn [71]: index[1:]\\nOut[71]: Index([b, c], dtype=object)\\nIndex objects are immutable and thus can’t be modified by the user:\\nIn [72]: index[1] = 'd'\\n---------------------------------------------------------------------------\\nException                                 Traceback (most recent call last)\\n<ipython-input-72-676fdeb26a68> in <module>()\\n----> 1 index[1] = 'd'\\n/Users/wesm/code/pandas/pandas/core/index.pyc in __setitem__(self, key, value)\\n    302     def __setitem__(self, key, value):\"),\n",
       " Document(metadata={}, page_content='<ipython-input-72-676fdeb26a68> in <module>()\\n----> 1 index[1] = \\'d\\'\\n/Users/wesm/code/pandas/pandas/core/index.pyc in __setitem__(self, key, value)\\n    302     def __setitem__(self, key, value):\\n    303         \"\"\"Disable the setting of values.\"\"\"\\n--> 304         raise Exception(str(self.__class__) + \\' object is immutable\\')\\n    305 \\n    306     def __getitem__(self, key):\\nException: <class \\'pandas.core.index.Index\\'> object is immutable\\n120 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Immutability is important so that Index objects can be safely shared among data\\nstructures:\\nIn [73]: index = pd.Index(np.arange(3))\\nIn [74]: obj2 = Series([1.5, -2.5, 0], index=index)\\nIn [75]: obj2.index is index\\nOut[75]: True\\nTable 5-2 has a list of built-in Index classes in the library. With some development\\neffort, Index can even be subclassed to implement specialized axis indexing function-\\nality.\\nMany users will not need to know much about Index objects, but they’re\\nnonetheless an important part of pandas’s data model.\\nTable 5-2. Main Index objects in pandas\\nClass Description\\nIndex The most general Index object, representing axis labels in a NumPy array of Python objects.\\nInt64Index Specialized Index for integer values.\\nMultiIndex “Hierarchical” index object representing multiple levels of indexing on a single axis. Can be thought of\\nas similar to an array of tuples.\\nDatetimeIndex Stores nanosecond timestamps (represented using NumPy’s datetime64 dtype).'),\n",
       " Document(metadata={}, page_content=\"as similar to an array of tuples.\\nDatetimeIndex Stores nanosecond timestamps (represented using NumPy’s datetime64 dtype).\\nPeriodIndex Specialized Index for Period data (timespans).\\nIn addition to being array-like, an Index also functions as a fixed-size set:\\nIn [76]: frame3\\nOut[76]: \\nstate  Nevada  Ohio\\nyear               \\n2000      NaN   1.5\\n2001      2.4   1.7\\n2002      2.9   3.6\\nIn [77]: 'Ohio' in frame3.columns\\nOut[77]: True\\nIn [78]: 2003 in frame3.index\\nOut[78]: False\\nEach Index has a number of methods and properties for set logic and answering other\\ncommon questions about the data it contains. These are summarized in Table 5-3.\\nIntroduction to pandas Data Structures | 121\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content='Table 5-3. Index methods and properties\\nMethod Description\\nappend Concatenate with additional Index objects, producing a new Index\\ndiff Compute set difference as an Index\\nintersection Compute set intersection\\nunion Compute set union\\nisin Compute boolean array indicating whether each value is contained in the passed collection\\ndelete Compute new Index with element at index i deleted\\ndrop Compute new index by deleting passed values\\ninsert Compute new Index by inserting element at index i\\nis_monotonic Returns True if each element is greater than or equal to the previous element\\nis_unique Returns True if the Index has no duplicate values\\nunique Compute the array of unique values in the Index\\nEssential Functionality\\nIn this section, I’ll walk you through the fundamental mechanics of interacting with\\nthe data contained in a Series or DataFrame. Upcoming chapters will delve more deeply\\ninto data analysis and manipulation topics using pandas. This book is not intended to'),\n",
       " Document(metadata={}, page_content=\"the data contained in a Series or DataFrame. Upcoming chapters will delve more deeply\\ninto data analysis and manipulation topics using pandas. This book is not intended to\\nserve as exhaustive documentation for the pandas library; I instead focus on the most\\nimportant features, leaving the less common (that is, more esoteric) things for you to\\nexplore on your own.\\nReindexing\\nA critical method on pandas objects is reindex, which means to create a new object\\nwith the data conformed to a new index. Consider a simple example from above:\\nIn [79]: obj = Series([4.5, 7.2, -5.3, 3.6], index=['d', 'b', 'a', 'c'])\\nIn [80]: obj\\nOut[80]: \\nd    4.5\\nb    7.2\\na   -5.3\\nc    3.6\\nCalling reindex on this Series rearranges the data according to the new index, intro-\\nducing missing values if any index values were not already present:\\nIn [81]: obj2 = obj.reindex(['a', 'b', 'c', 'd', 'e'])\\nIn [82]: obj2\\nOut[82]: \\na   -5.3\\n122 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"b    7.2\\nc    3.6\\nd    4.5\\ne    NaN\\nIn [83]: obj.reindex(['a', 'b', 'c', 'd', 'e'], fill_value=0)\\nOut[83]: \\na   -5.3\\nb    7.2\\nc    3.6\\nd    4.5\\ne    0.0\\nFor ordered data like time series, it may be desirable to do some interpolation or filling\\nof values when reindexing. The method option allows us to do this, using a method such\\nas ffill which forward fills the values:\\nIn [84]: obj3 = Series(['blue', 'purple', 'yellow'], index=[0, 2, 4])\\nIn [85]: obj3.reindex(range(6), method='ffill')\\nOut[85]: \\n0      blue\\n1      blue\\n2    purple\\n3    purple\\n4    yellow\\n5    yellow\\nTable 5-4 lists available method options. At this time, interpolation more sophisticated\\nthan forward- and backfilling would need to be applied after the fact.\\nTable 5-4. reindex method (interpolation) options\\nArgument Description\\nffill or pad Fill (or carry) values forward\\nbfill or backfill Fill (or carry) values backward\\nWith DataFrame, reindex can alter either the (row) index, columns, or both. When\"),\n",
       " Document(metadata={}, page_content=\"Argument Description\\nffill or pad Fill (or carry) values forward\\nbfill or backfill Fill (or carry) values backward\\nWith DataFrame, reindex can alter either the (row) index, columns, or both. When\\npassed just a sequence, the rows are reindexed in the result:\\nIn [86]: frame = DataFrame(np.arange(9).reshape((3, 3)), index=['a', 'c', 'd'],\\n   ....:                   columns=['Ohio', 'Texas', 'California'])\\nIn [87]: frame\\nOut[87]: \\n   Ohio  Texas  California\\na     0      1           2\\nc     3      4           5\\nd     6      7           8\\nIn [88]: frame2 = frame.reindex(['a', 'b', 'c', 'd'])\\nIn [89]: frame2\\nOut[89]: \\nEssential Functionality | 123\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"Ohio  Texas  California\\na     0      1           2\\nb   NaN    NaN         NaN\\nc     3      4           5\\nd     6      7           8\\nThe columns can be reindexed using the columns keyword:\\nIn [90]: states = ['Texas', 'Utah', 'California']\\nIn [91]: frame.reindex(columns=states)\\nOut[91]: \\n   Texas  Utah  California\\na      1   NaN           2\\nc      4   NaN           5\\nd      7   NaN           8\\nBoth can be reindexed in one shot, though interpolation will only apply row-wise (axis\\n0):\\nIn [92]: frame.reindex(index=['a', 'b', 'c', 'd'], method='ffill',\\n   ....:               columns=states)\\nOut[92]: \\n   Texas  Utah  California\\na      1   NaN           2\\nb      1   NaN           2\\nc      4   NaN           5\\nd      7   NaN           8\\nAs you’ll see soon, reindexing can be done more succinctly by label-indexing with ix:\\nIn [93]: frame.ix[['a', 'b', 'c', 'd'], states]\\nOut[93]: \\n   Texas  Utah  California\\na      1   NaN           2\\nb    NaN   NaN         NaN\\nc      4   NaN           5\"),\n",
       " Document(metadata={}, page_content=\"In [93]: frame.ix[['a', 'b', 'c', 'd'], states]\\nOut[93]: \\n   Texas  Utah  California\\na      1   NaN           2\\nb    NaN   NaN         NaN\\nc      4   NaN           5\\nd      7   NaN           8\\nTable 5-5. reindex function arguments\\nArgument Description\\nindex New sequence to use as index. Can be Index instance or any other sequence-like Python data structure. An\\nIndex will be used exactly as is without any copying\\nmethod Interpolation (fill) method, see Table 5-4 for options.\\nfill_value Substitute value to use when introducing missing data by reindexing\\nlimit When forward- or backfilling, maximum size gap to fill\\nlevel Match simple Index on level of MultiIndex, otherwise select subset of\\ncopy Do not copy underlying data if new index is equivalent to old index. True by default (i.e. always copy data).\\n124 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"Dropping entries from an axis\\nDropping one or more entries from an axis is easy if you have an index array or list\\nwithout those entries. As that can require a bit of munging and set logic, the drop\\nmethod will return a new object with the indicated value or values deleted from an axis:\\nIn [94]: obj = Series(np.arange(5.), index=['a', 'b', 'c', 'd', 'e'])\\nIn [95]: new_obj = obj.drop('c')\\nIn [96]: new_obj\\nOut[96]: \\na    0\\nb    1\\nd    3\\ne    4\\nIn [97]: obj.drop(['d', 'c'])\\nOut[97]: \\na    0\\nb    1\\ne    4\\nWith DataFrame, index values can be deleted from either axis:\\nIn [98]: data = DataFrame(np.arange(16).reshape((4, 4)),\\n   ....:                  index=['Ohio', 'Colorado', 'Utah', 'New York'],\\n   ....:                  columns=['one', 'two', 'three', 'four'])\\nIn [99]: data.drop(['Colorado', 'Ohio'])\\nOut[99]: \\n          one  two  three  four\\nUtah        8    9     10    11\\nNew York   12   13     14    15\\nIn [100]: data.drop('two', axis=1)      In [101]: data.drop(['two', 'four'], axis=1)\"),\n",
       " Document(metadata={}, page_content=\"Out[99]: \\n          one  two  three  four\\nUtah        8    9     10    11\\nNew York   12   13     14    15\\nIn [100]: data.drop('two', axis=1)      In [101]: data.drop(['two', 'four'], axis=1)\\nOut[100]:                               Out[101]:                                   \\n          one  three  four                        one  three                        \\nOhio        0      2     3              Ohio        0      2                        \\nColorado    4      6     7              Colorado    4      6                        \\nUtah        8     10    11              Utah        8     10                        \\nNew York   12     14    15              New York   12     14\\nIndexing, selection, and filtering\\nSeries indexing (obj[...]) works analogously to NumPy array indexing, except you can\\nuse the Series’s index values instead of only integers. Here are some examples this:\\nIn [102]: obj = Series(np.arange(4.), index=['a', 'b', 'c', 'd'])\\nIn [103]: obj['b']          In [104]: obj[1]\"),\n",
       " Document(metadata={}, page_content=\"use the Series’s index values instead of only integers. Here are some examples this:\\nIn [102]: obj = Series(np.arange(4.), index=['a', 'b', 'c', 'd'])\\nIn [103]: obj['b']          In [104]: obj[1]\\nOut[103]: 1.0               Out[104]: 1.0   \\n                                            \\nIn [105]: obj[2:4]          In [106]: obj[['b', 'a', 'd']]\\nOut[105]:                   Out[106]:                     \\nEssential Functionality | 125\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"c    2                      b    1                        \\nd    3                      a    0                        \\n                            d    3                        \\n                                                          \\nIn [107]: obj[[1, 3]]       In [108]: obj[obj < 2]\\nOut[107]:                   Out[108]:             \\nb    1                      a    0                \\nd    3                      b    1\\nSlicing with labels behaves differently than normal Python slicing in that the endpoint\\nis inclusive:\\nIn [109]: obj['b':'c']\\nOut[109]: \\nb    1\\nc    2\\nSetting using these methods works just as you would expect:\\nIn [110]: obj['b':'c'] = 5\\nIn [111]: obj\\nOut[111]: \\na    0\\nb    5\\nc    5\\nd    3\\nAs you’ve seen above, indexing into a DataFrame is for retrieving one or more columns\\neither with a single value or sequence:\\nIn [112]: data = DataFrame(np.arange(16).reshape((4, 4)),\\n   .....:                  index=['Ohio', 'Colorado', 'Utah', 'New York'],\"),\n",
       " Document(metadata={}, page_content=\"either with a single value or sequence:\\nIn [112]: data = DataFrame(np.arange(16).reshape((4, 4)),\\n   .....:                  index=['Ohio', 'Colorado', 'Utah', 'New York'],\\n   .....:                  columns=['one', 'two', 'three', 'four'])\\nIn [113]: data\\nOut[113]: \\n          one  two  three  four\\nOhio        0    1      2     3\\nColorado    4    5      6     7\\nUtah        8    9     10    11\\nNew York   12   13     14    15\\nIn [114]: data['two']        In [115]: data[['three', 'one']]\\nOut[114]:                    Out[115]:                       \\nOhio         1                         three  one            \\nColorado     5               Ohio          2    0            \\nUtah         9               Colorado      6    4            \\nNew York    13               Utah         10    8            \\nName: two                    New York     14   12\\nIndexing like this has a few special cases. First selecting rows by slicing or a boolean\\narray:\"),\n",
       " Document(metadata={}, page_content=\"Name: two                    New York     14   12\\nIndexing like this has a few special cases. First selecting rows by slicing or a boolean\\narray:\\nIn [116]: data[:2]                     In [117]: data[data['three'] > 5]\\nOut[116]:                              Out[117]:                        \\n          one  two  three  four                  one  two  three  four  \\n126 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content='Ohio        0    1      2     3        Colorado    4    5      6     7  \\nColorado    4    5      6     7        Utah        8    9     10    11  \\n                                       New York   12   13     14    15\\nThis might seem inconsistent to some readers, but this syntax arose out of practicality\\nand nothing more. Another use case is in indexing with a boolean DataFrame, such as\\none produced by a scalar comparison:\\nIn [118]: data < 5\\nOut[118]: \\n            one    two  three   four\\nOhio       True   True   True   True\\nColorado   True  False  False  False\\nUtah      False  False  False  False\\nNew York  False  False  False  False\\nIn [119]: data[data < 5] = 0\\nIn [120]: data\\nOut[120]: \\n          one  two  three  four\\nOhio        0    0      0     0\\nColorado    0    5      6     7\\nUtah        8    9     10    11\\nNew York   12   13     14    15\\nThis is intended to make DataFrame syntactically more like an ndarray in this case.'),\n",
       " Document(metadata={}, page_content=\"Colorado    0    5      6     7\\nUtah        8    9     10    11\\nNew York   12   13     14    15\\nThis is intended to make DataFrame syntactically more like an ndarray in this case.\\nFor DataFrame label-indexing on the rows, I introduce the special indexing field ix. It\\nenables you to select a subset of the rows and columns from a DataFrame with NumPy-\\nlike notation plus axis labels. As I mentioned earlier, this is also a less verbose way to\\ndo reindexing:\\nIn [121]: data.ix['Colorado', ['two', 'three']]\\nOut[121]: \\ntwo      5\\nthree    6\\nName: Colorado\\nIn [122]: data.ix[['Colorado', 'Utah'], [3, 0, 1]]\\nOut[122]: \\n          four  one  two\\nColorado     7    0    5\\nUtah        11    8    9\\nIn [123]: data.ix[2]        In [124]: data.ix[:'Utah', 'two']\\nOut[123]:                   Out[124]:                        \\none       8                 Ohio        0                    \\ntwo       9                 Colorado    5\"),\n",
       " Document(metadata={}, page_content='Out[123]:                   Out[124]:                        \\none       8                 Ohio        0                    \\ntwo       9                 Colorado    5                    \\nthree    10                 Utah        9                    \\nfour     11                 Name: two                        \\nName: Utah                                                   \\n                                                             \\nIn [125]: data.ix[data.three > 5, :3]\\nOut[125]: \\nEssential Functionality | 127\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='one  two  three\\nColorado    0    5      6\\nUtah        8    9     10\\nNew York   12   13     14\\nSo there are many ways to select and rearrange the data contained in a pandas object.\\nFor DataFrame, there is a short summary of many of them in Table 5-6. You have a\\nnumber of additional options when working with hierarchical indexes as you’ll later\\nsee.\\nWhen designing pandas, I felt that having to type frame[:, col] to select\\na column was too verbose (and error-prone), since column selection is\\none of the most common operations. Thus I made the design trade-off\\nto push all of the rich label-indexing into ix.\\nTable 5-6. Indexing options with DataFrame\\nType Notes\\nobj[val] Select single column or sequence of columns from the DataFrame. Special case con-\\nveniences: boolean array (filter rows), slice (slice rows), or boolean DataFrame (set\\nvalues based on some criterion).\\nobj.ix[val] Selects single row of subset of rows from the DataFrame.'),\n",
       " Document(metadata={}, page_content=\"veniences: boolean array (filter rows), slice (slice rows), or boolean DataFrame (set\\nvalues based on some criterion).\\nobj.ix[val] Selects single row of subset of rows from the DataFrame.\\nobj.ix[:, val] Selects single column of subset of columns.\\nobj.ix[val1, val2] Select both rows and columns.\\nreindex method Conform one or more axes to new indexes.\\nxs method Select single row or column as a Series by label.\\nicol, irow methods Select single column or row, respectively, as a Series by integer location.\\nget_value, set_value methods Select single value by row and column label.\\nArithmetic and data alignment\\nOne of the most important pandas features is the behavior of arithmetic between ob-\\njects with different indexes. When adding together objects, if any index pairs are not\\nthe same, the respective index in the result will be the union of the index pairs. Let’s\\nlook at a simple example:\\nIn [126]: s1 = Series([7.3, -2.5, 3.4, 1.5], index=['a', 'c', 'd', 'e'])\"),\n",
       " Document(metadata={}, page_content=\"the same, the respective index in the result will be the union of the index pairs. Let’s\\nlook at a simple example:\\nIn [126]: s1 = Series([7.3, -2.5, 3.4, 1.5], index=['a', 'c', 'd', 'e'])\\nIn [127]: s2 = Series([-2.1, 3.6, -1.5, 4, 3.1], index=['a', 'c', 'e', 'f', 'g'])\\nIn [128]: s1        In [129]: s2\\nOut[128]:           Out[129]:   \\na    7.3            a   -2.1    \\nc   -2.5            c    3.6    \\nd    3.4            e   -1.5    \\n128 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"e    1.5            f    4.0    \\n                    g    3.1\\nAdding these together yields:\\nIn [130]: s1 + s2\\nOut[130]: \\na    5.2\\nc    1.1\\nd    NaN\\ne    0.0\\nf    NaN\\ng    NaN\\nThe internal data alignment introduces NA values in the indices that don’t overlap.\\nMissing values propagate in arithmetic computations.\\nIn the case of DataFrame, alignment is performed on both the rows and the columns:\\nIn [131]: df1 = DataFrame(np.arange(9.).reshape((3, 3)), columns=list('bcd'),\\n   .....:                 index=['Ohio', 'Texas', 'Colorado'])\\nIn [132]: df2 = DataFrame(np.arange(12.).reshape((4, 3)), columns=list('bde'),\\n   .....:                 index=['Utah', 'Ohio', 'Texas', 'Oregon'])\\nIn [133]: df1            In [134]: df2    \\nOut[133]:                Out[134]:        \\n          b  c  d                b   d   e\\nOhio      0  1  2        Utah    0   1   2\\nTexas     3  4  5        Ohio    3   4   5\\nColorado  6  7  8        Texas   6   7   8\\n                         Oregon  9  10  11\"),\n",
       " Document(metadata={}, page_content=\"Ohio      0  1  2        Utah    0   1   2\\nTexas     3  4  5        Ohio    3   4   5\\nColorado  6  7  8        Texas   6   7   8\\n                         Oregon  9  10  11\\nAdding these together returns a DataFrame whose index and columns are the unions\\nof the ones in each DataFrame:\\nIn [135]: df1 + df2\\nOut[135]: \\n           b   c   d   e\\nColorado NaN NaN NaN NaN\\nOhio       3 NaN   6 NaN\\nOregon   NaN NaN NaN NaN\\nTexas      9 NaN  12 NaN\\nUtah     NaN NaN NaN NaN\\nArithmetic methods with fill values\\nIn arithmetic operations between differently-indexed objects, you might want to fill\\nwith a special value, like 0, when an axis label is found in one object but not the other:\\nIn [136]: df1 = DataFrame(np.arange(12.).reshape((3, 4)), columns=list('abcd'))\\nIn [137]: df2 = DataFrame(np.arange(20.).reshape((4, 5)), columns=list('abcde'))\\nIn [138]: df1          In [139]: df2        \\nOut[138]:              Out[139]:            \\n   a  b   c   d            a   b   c   d   e\"),\n",
       " Document(metadata={}, page_content='In [138]: df1          In [139]: df2        \\nOut[138]:              Out[139]:            \\n   a  b   c   d            a   b   c   d   e\\nEssential Functionality | 129\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='0  0  1   2   3        0   0   1   2   3   4\\n1  4  5   6   7        1   5   6   7   8   9\\n2  8  9  10  11        2  10  11  12  13  14\\n                       3  15  16  17  18  19\\nAdding these together results in NA values in the locations that don’t overlap:\\nIn [140]: df1 + df2\\nOut[140]: \\n    a   b   c   d   e\\n0   0   2   4   6 NaN\\n1   9  11  13  15 NaN\\n2  18  20  22  24 NaN\\n3 NaN NaN NaN NaN NaN\\nUsing the add method on df1, I pass df2 and an argument to fill_value:\\nIn [141]: df1.add(df2, fill_value=0)\\nOut[141]: \\n    a   b   c   d   e\\n0   0   2   4   6   4\\n1   9  11  13  15   9\\n2  18  20  22  24  14\\n3  15  16  17  18  19\\nRelatedly, when reindexing a Series or DataFrame, you can also specify a different fill\\nvalue:\\nIn [142]: df1.reindex(columns=df2.columns, fill_value=0)\\nOut[142]: \\n   a  b   c   d  e\\n0  0  1   2   3  0\\n1  4  5   6   7  0\\n2  8  9  10  11  0\\nTable 5-7. Flexible arithmetic methods\\nMethod Description\\nadd Method for addition (+)\\nsub Method for subtraction (-)'),\n",
       " Document(metadata={}, page_content='a  b   c   d  e\\n0  0  1   2   3  0\\n1  4  5   6   7  0\\n2  8  9  10  11  0\\nTable 5-7. Flexible arithmetic methods\\nMethod Description\\nadd Method for addition (+)\\nsub Method for subtraction (-)\\ndiv Method for division (/)\\nmul Method for multiplication (*)\\nOperations between DataFrame and Series\\nAs with NumPy arrays, arithmetic between DataFrame and Series is well-defined. First,\\nas a motivating example, consider the difference between a 2D array and one of its rows:\\nIn [143]: arr = np.arange(12.).reshape((3, 4))\\nIn [144]: arr\\nOut[144]: \\narray([[  0.,   1.,   2.,   3.],\\n       [  4.,   5.,   6.,   7.],\\n130 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"[  8.,   9.,  10.,  11.]])\\nIn [145]: arr[0]\\nOut[145]: array([ 0.,  1.,  2.,  3.])\\nIn [146]: arr - arr[0]\\nOut[146]: \\narray([[ 0.,  0.,  0.,  0.],\\n       [ 4.,  4.,  4.,  4.],\\n       [ 8.,  8.,  8.,  8.]])\\nThis is referred to as broadcasting and is explained in more detail in Chapter 12. Op-\\nerations between a DataFrame and a Series are similar:\\nIn [147]: frame = DataFrame(np.arange(12.).reshape((4, 3)), columns=list('bde'),\\n   .....:                   index=['Utah', 'Ohio', 'Texas', 'Oregon'])\\nIn [148]: series = frame.ix[0]\\nIn [149]: frame          In [150]: series\\nOut[149]:                Out[150]:       \\n        b   d   e        b    0          \\nUtah    0   1   2        d    1          \\nOhio    3   4   5        e    2          \\nTexas   6   7   8        Name: Utah      \\nOregon  9  10  11\\nBy default, arithmetic between DataFrame and Series matches the index of the Series\\non the DataFrame's columns, broadcasting down the rows:\\nIn [151]: frame - series\\nOut[151]: \\n        b  d  e\"),\n",
       " Document(metadata={}, page_content=\"By default, arithmetic between DataFrame and Series matches the index of the Series\\non the DataFrame's columns, broadcasting down the rows:\\nIn [151]: frame - series\\nOut[151]: \\n        b  d  e\\nUtah    0  0  0\\nOhio    3  3  3\\nTexas   6  6  6\\nOregon  9  9  9\\nIf an index value is not found in either the DataFrame’s columns or the Series’s index,\\nthe objects will be reindexed to form the union:\\nIn [152]: series2 = Series(range(3), index=['b', 'e', 'f'])\\nIn [153]: frame + series2\\nOut[153]: \\n        b   d   e   f\\nUtah    0 NaN   3 NaN\\nOhio    3 NaN   6 NaN\\nTexas   6 NaN   9 NaN\\nOregon  9 NaN  12 NaN\\nIf you want to instead broadcast over the columns, matching on the rows, you have to\\nuse one of the arithmetic methods. For example:\\nIn [154]: series3 = frame['d']\\nIn [155]: frame      In [156]: series3\\nEssential Functionality | 131\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"Out[155]:            Out[156]:        \\n        b   d   e    Utah       1     \\nUtah    0   1   2    Ohio       4     \\nOhio    3   4   5    Texas      7     \\nTexas   6   7   8    Oregon    10     \\nOregon  9  10  11    Name: d          \\n                                      \\nIn [157]: frame.sub(series3, axis=0)\\nOut[157]: \\n        b  d  e\\nUtah   -1  0  1\\nOhio   -1  0  1\\nTexas  -1  0  1\\nOregon -1  0  1\\nThe axis number that you pass is the axis to match on. In this case we mean to match\\non the DataFrame’s row index and broadcast across.\\nFunction application and mapping\\nNumPy ufuncs (element-wise array methods) work fine with pandas objects:\\nIn [158]: frame = DataFrame(np.random.randn(4, 3), columns=list('bde'),\\n   .....:                   index=['Utah', 'Ohio', 'Texas', 'Oregon'])\\nIn [159]: frame                           In [160]: np.abs(frame)             \\nOut[159]:                                 Out[160]:\"),\n",
       " Document(metadata={}, page_content='In [159]: frame                           In [160]: np.abs(frame)             \\nOut[159]:                                 Out[160]:                           \\n               b         d         e                     b         d         e\\nUtah   -0.204708  0.478943 -0.519439      Utah    0.204708  0.478943  0.519439\\nOhio   -0.555730  1.965781  1.393406      Ohio    0.555730  1.965781  1.393406\\nTexas   0.092908  0.281746  0.769023      Texas   0.092908  0.281746  0.769023\\nOregon  1.246435  1.007189 -1.296221      Oregon  1.246435  1.007189  1.296221\\nAnother frequent operation is applying a function on 1D arrays to each column or row.\\nDataFrame’s apply method does exactly this:\\nIn [161]: f = lambda x: x.max() - x.min()\\nIn [162]: frame.apply(f)        In [163]: frame.apply(f, axis=1)\\nOut[162]:                       Out[163]:                       \\nb    1.802165                   Utah      0.998382              \\nd    1.684034                   Ohio      2.521511'),\n",
       " Document(metadata={}, page_content=\"Out[162]:                       Out[163]:                       \\nb    1.802165                   Utah      0.998382              \\nd    1.684034                   Ohio      2.521511              \\ne    2.689627                   Texas     0.676115              \\n                                Oregon    2.542656\\nMany of the most common array statistics (like sum and mean) are DataFrame methods,\\nso using apply is not necessary.\\nThe function passed to apply need not return a scalar value, it can also return a Series\\nwith multiple values:\\nIn [164]: def f(x):\\n   .....:     return Series([x.min(), x.max()], index=['min', 'max'])\\nIn [165]: frame.apply(f)\\n132 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"Out[165]: \\n            b         d         e\\nmin -0.555730  0.281746 -1.296221\\nmax  1.246435  1.965781  1.393406\\nElement-wise Python functions can be used, too. Suppose you wanted to compute a\\nformatted string from each floating point value in frame. You can do this with applymap:\\nIn [166]: format = lambda x: '%.2f' % x\\nIn [167]: frame.applymap(format)\\nOut[167]: \\n            b     d      e\\nUtah    -0.20  0.48  -0.52\\nOhio    -0.56  1.97   1.39\\nTexas    0.09  0.28   0.77\\nOregon   1.25  1.01  -1.30\\nThe reason for the name applymap is that Series has a map method for applying an ele-\\nment-wise function:\\nIn [168]: frame['e'].map(format)\\nOut[168]: \\nUtah      -0.52\\nOhio       1.39\\nTexas      0.77\\nOregon    -1.30\\nName: e\\nSorting and ranking\\nSorting a data set by some criterion is another important built-in operation. To sort\\nlexicographically by row or column index, use the sort_index method, which returns\\na new, sorted object:\\nIn [169]: obj = Series(range(4), index=['d', 'a', 'b', 'c'])\"),\n",
       " Document(metadata={}, page_content=\"lexicographically by row or column index, use the sort_index method, which returns\\na new, sorted object:\\nIn [169]: obj = Series(range(4), index=['d', 'a', 'b', 'c'])\\nIn [170]: obj.sort_index()\\nOut[170]: \\na    1\\nb    2\\nc    3\\nd    0\\nWith a DataFrame, you can sort by index on either axis:\\nIn [171]: frame = DataFrame(np.arange(8).reshape((2, 4)), index=['three', 'one'],\\n   .....:                   columns=['d', 'a', 'b', 'c'])\\nIn [172]: frame.sort_index()        In [173]: frame.sort_index(axis=1)\\nOut[172]:                           Out[173]:                         \\n       d  a  b  c                          a  b  c  d                 \\none    4  5  6  7                   three  1  2  3  0                 \\nthree  0  1  2  3                   one    5  6  7  4\\nEssential Functionality | 133\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"The data is sorted in ascending order by default, but can be sorted in descending order,\\ntoo:\\nIn [174]: frame.sort_index(axis=1, ascending=False)\\nOut[174]: \\n       d  c  b  a\\nthree  0  3  2  1\\none    4  7  6  5\\nTo sort a Series by its values, use its order method:\\nIn [175]: obj = Series([4, 7, -3, 2])\\nIn [176]: obj.order()\\nOut[176]: \\n2   -3\\n3    2\\n0    4\\n1    7\\nAny missing values are sorted to the end of the Series by default:\\nIn [177]: obj = Series([4, np.nan, 7, np.nan, -3, 2])\\nIn [178]: obj.order()\\nOut[178]: \\n4    -3\\n5     2\\n0     4\\n2     7\\n1   NaN\\n3   NaN\\nOn DataFrame, you may want to sort by the values in one or more columns. To do so,\\npass one or more column names to the by option:\\nIn [179]: frame = DataFrame({'b': [4, 7, -3, 2], 'a': [0, 1, 0, 1]})\\nIn [180]: frame        In [181]: frame.sort_index(by='b')\\nOut[180]:              Out[181]:                         \\n   a  b                   a  b                           \\n0  0  4                2  0 -3\"),\n",
       " Document(metadata={}, page_content=\"Out[180]:              Out[181]:                         \\n   a  b                   a  b                           \\n0  0  4                2  0 -3                           \\n1  1  7                3  1  2                           \\n2  0 -3                0  0  4                           \\n3  1  2                1  1  7\\nTo sort by multiple columns, pass a list of names:\\nIn [182]: frame.sort_index(by=['a', 'b'])\\nOut[182]: \\n   a  b\\n2  0 -3\\n0  0  4\\n3  1  2\\n1  1  7\\n134 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"Ranking is closely related to sorting, assigning ranks from one through the number of\\nvalid data points in an array. It is similar to the indirect sort indices produced by \\nnumpy.argsort, except that ties are broken according to a rule. The rank methods for\\nSeries and DataFrame are the place to look; by default rank breaks ties by assigning\\neach group the mean rank:\\nIn [183]: obj = Series([7, -5, 7, 4, 2, 0, 4])\\nIn [184]: obj.rank()\\nOut[184]: \\n0    6.5\\n1    1.0\\n2    6.5\\n3    4.5\\n4    3.0\\n5    2.0\\n6    4.5\\nRanks can also be assigned according to the order they’re observed in the data:\\nIn [185]: obj.rank(method='first')\\nOut[185]: \\n0    6\\n1    1\\n2    7\\n3    4\\n4    3\\n5    2\\n6    5\\nNaturally, you can rank in descending order, too:\\nIn [186]: obj.rank(ascending=False, method='max')\\nOut[186]: \\n0    2\\n1    7\\n2    2\\n3    4\\n4    5\\n5    6\\n6    4\\nSee Table 5-8 for a list of tie-breaking methods available. DataFrame can compute ranks\\nover the rows or the columns:\"),\n",
       " Document(metadata={}, page_content=\"Out[186]: \\n0    2\\n1    7\\n2    2\\n3    4\\n4    5\\n5    6\\n6    4\\nSee Table 5-8 for a list of tie-breaking methods available. DataFrame can compute ranks\\nover the rows or the columns:\\nIn [187]: frame = DataFrame({'b': [4.3, 7, -3, 2], 'a': [0, 1, 0, 1],\\n   .....:                    'c': [-2, 5, 8, -2.5]})\\nIn [188]: frame        In [189]: frame.rank(axis=1)\\nOut[188]:              Out[189]:                   \\n   a    b    c            a  b  c                  \\n0  0  4.3 -2.0         0  2  3  1                  \\n1  1  7.0  5.0         1  1  3  2                  \\n2  0 -3.0  8.0         2  2  1  3                  \\n3  1  2.0 -2.5         3  2  3  1\\nEssential Functionality | 135\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"Table 5-8. Tie-breaking methods with rank\\nMethod Description\\n'average' Default: assign the average rank to each entry in the equal group.\\n'min' Use the minimum rank for the whole group.\\n'max' Use the maximum rank for the whole group.\\n'first' Assign ranks in the order the values appear in the data.\\nAxis indexes with duplicate values\\nUp until now all of the examples I’ve showed you have had unique axis labels (index\\nvalues). While many pandas functions (like reindex) require that the labels be unique,\\nit’s not mandatory. Let’s consider a small Series with duplicate indices:\\nIn [190]: obj = Series(range(5), index=['a', 'a', 'b', 'b', 'c'])\\nIn [191]: obj\\nOut[191]: \\na    0\\na    1\\nb    2\\nb    3\\nc    4\\nThe index’s is_unique property can tell you whether its values are unique or not:\\nIn [192]: obj.index.is_unique\\nOut[192]: False\\nData selection is one of the main things that behaves differently with duplicates. In-\"),\n",
       " Document(metadata={}, page_content=\"In [192]: obj.index.is_unique\\nOut[192]: False\\nData selection is one of the main things that behaves differently with duplicates. In-\\ndexing a value with multiple entries returns a Series while single entries return a scalar\\nvalue:\\nIn [193]: obj['a']    In [194]: obj['c']\\nOut[193]:             Out[194]: 4       \\na    0                                  \\na    1\\nThe same logic extends to indexing rows in a DataFrame:\\nIn [195]: df = DataFrame(np.random.randn(4, 3), index=['a', 'a', 'b', 'b'])\\nIn [196]: df\\nOut[196]: \\n          0         1         2\\na  0.274992  0.228913  1.352917\\na  0.886429 -2.001637 -0.371843\\nb  1.669025 -0.438570 -0.539741\\nb  0.476985  3.248944 -1.021228\\nIn [197]: df.ix['b']\\nOut[197]: \\n          0         1         2\\n136 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"b  1.669025 -0.438570 -0.539741\\nb  0.476985  3.248944 -1.021228\\nSummarizing and Computing Descriptive Statistics\\npandas objects are equipped with a set of common mathematical and statistical meth-\\nods. Most of these fall into the category of reductions or summary statistics, methods\\nthat extract a single value (like the sum or mean) from a Series or a Series of values from\\nthe rows or columns of a DataFrame. Compared with the equivalent methods of vanilla\\nNumPy arrays, they are all built from the ground up to exclude missing data. Consider\\na small DataFrame:\\nIn [198]: df = DataFrame([[1.4, np.nan], [7.1, -4.5],\\n   .....:                 [np.nan, np.nan], [0.75, -1.3]],\\n   .....:                index=['a', 'b', 'c', 'd'],\\n   .....:                columns=['one', 'two'])\\nIn [199]: df\\nOut[199]: \\n    one  two\\na  1.40  NaN\\nb  7.10 -4.5\\nc   NaN  NaN\\nd  0.75 -1.3\\nCalling DataFrame’s sum method returns a Series containing column sums:\\nIn [200]: df.sum()\\nOut[200]: \\none    9.25\\ntwo   -5.80\"),\n",
       " Document(metadata={}, page_content='one  two\\na  1.40  NaN\\nb  7.10 -4.5\\nc   NaN  NaN\\nd  0.75 -1.3\\nCalling DataFrame’s sum method returns a Series containing column sums:\\nIn [200]: df.sum()\\nOut[200]: \\none    9.25\\ntwo   -5.80\\nPassing axis=1 sums over the rows instead:\\nIn [201]: df.sum(axis=1)\\nOut[201]: \\na    1.40\\nb    2.60\\nc     NaN\\nd   -0.55\\nNA values are excluded unless the entire slice (row or column in this case) is NA. This\\ncan be disabled using the skipna option:\\nIn [202]: df.mean(axis=1, skipna=False)\\nOut[202]: \\na      NaN\\nb    1.300\\nc      NaN\\nd   -0.275\\nSee Table 5-9 for a list of common options for each reduction method options.\\nSummarizing and Computing Descriptive Statistics | 137\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Table 5-9. Options for reduction methods\\nMethod Description\\naxis Axis to reduce over. 0 for DataFrame’s rows and 1 for columns.\\nskipna Exclude missing values, True by default.\\nlevel Reduce grouped by level if the axis is hierarchically-indexed (MultiIndex).\\nSome methods, like idxmin and idxmax, return indirect statistics like the index value\\nwhere the minimum or maximum values are attained:\\nIn [203]: df.idxmax()\\nOut[203]: \\none    b\\ntwo    d\\nOther methods are accumulations:\\nIn [204]: df.cumsum()\\nOut[204]: \\n    one  two\\na  1.40  NaN\\nb  8.50 -4.5\\nc   NaN  NaN\\nd  9.25 -5.8\\nAnother type of method is neither a reduction nor an accumulation. describe is one\\nsuch example, producing multiple summary statistics in one shot:\\nIn [205]: df.describe()\\nOut[205]: \\n            one       two\\ncount  3.000000  2.000000\\nmean   3.083333 -2.900000\\nstd    3.493685  2.262742\\nmin    0.750000 -4.500000\\n25%    1.075000 -3.700000\\n50%    1.400000 -2.900000\\n75%    4.250000 -2.100000\\nmax    7.100000 -1.300000'),\n",
       " Document(metadata={}, page_content=\"mean   3.083333 -2.900000\\nstd    3.493685  2.262742\\nmin    0.750000 -4.500000\\n25%    1.075000 -3.700000\\n50%    1.400000 -2.900000\\n75%    4.250000 -2.100000\\nmax    7.100000 -1.300000\\nOn non-numeric data, describe produces alternate summary statistics:\\nIn [206]: obj = Series(['a', 'a', 'b', 'c'] * 4)\\nIn [207]: obj.describe()\\nOut[207]: \\ncount     16\\nunique     3\\ntop        a\\nfreq       8\\nSee Table 5-10 for a full list of summary statistics and related methods.\\n138 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content='Table 5-10. Descriptive and summary statistics\\nMethod Description\\ncount Number of non-NA values\\ndescribe Compute set of summary statistics for Series or each DataFrame column\\nmin, max Compute minimum and maximum values\\nargmin, argmax Compute index locations (integers) at which minimum or maximum value obtained, respectively\\nidxmin, idxmax Compute index values at which minimum or maximum value obtained, respectively\\nquantile Compute sample quantile ranging from 0 to 1\\nsum Sum of values\\nmean Mean of values\\nmedian Arithmetic median (50% quantile) of values\\nmad Mean absolute deviation from mean value\\nvar Sample variance of values\\nstd Sample standard deviation of values\\nskew Sample skewness (3rd moment) of values\\nkurt Sample kurtosis (4th moment) of values\\ncumsum Cumulative sum of values\\ncummin, cummax Cumulative minimum or maximum of values, respectively\\ncumprod Cumulative product of values\\ndiff Compute 1st arithmetic difference (useful for time series)\\npct_change Compute percent changes'),\n",
       " Document(metadata={}, page_content=\"cumprod Cumulative product of values\\ndiff Compute 1st arithmetic difference (useful for time series)\\npct_change Compute percent changes\\nCorrelation and Covariance\\nSome summary statistics, like correlation and covariance, are computed from pairs of\\narguments. Let’s consider some DataFrames of stock prices and volumes obtained from\\nYahoo! Finance:\\nimport pandas.io.data as web\\nall_data = {}\\nfor ticker in ['AAPL', 'IBM', 'MSFT', 'GOOG']:\\n    all_data[ticker] = web.get_data_yahoo(ticker, '1/1/2000', '1/1/2010')\\nprice = DataFrame({tic: data['Adj Close']\\n                   for tic, data in all_data.iteritems()})\\nvolume = DataFrame({tic: data['Volume']\\n                    for tic, data in all_data.iteritems()})\\nI now compute percent changes of the prices:\\nIn [209]: returns = price.pct_change()\\nIn [210]: returns.tail()\\nSummarizing and Computing Descriptive Statistics | 139\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content='Out[210]: \\n                AAPL      GOOG       IBM      MSFT\\nDate                                              \\n2009-12-24  0.034339  0.011117  0.004420  0.002747\\n2009-12-28  0.012294  0.007098  0.013282  0.005479\\n2009-12-29 -0.011861 -0.005571 -0.003474  0.006812\\n2009-12-30  0.012147  0.005376  0.005468 -0.013532\\n2009-12-31 -0.004300 -0.004416 -0.012609 -0.015432\\nThe corr method of Series computes the correlation of the overlapping, non-NA,\\naligned-by-index values in two Series. Relatedly, cov computes the covariance:\\nIn [211]: returns.MSFT.corr(returns.IBM)\\nOut[211]: 0.49609291822168838\\nIn [212]: returns.MSFT.cov(returns.IBM)\\nOut[212]: 0.00021600332437329015\\nDataFrame’s corr and cov methods, on the other hand, return a full correlation or\\ncovariance matrix as a DataFrame, respectively:\\nIn [213]: returns.corr()\\nOut[213]: \\n          AAPL      GOOG       IBM      MSFT\\nAAPL  1.000000  0.470660  0.410648  0.424550\\nGOOG  0.470660  1.000000  0.390692  0.443334'),\n",
       " Document(metadata={}, page_content='In [213]: returns.corr()\\nOut[213]: \\n          AAPL      GOOG       IBM      MSFT\\nAAPL  1.000000  0.470660  0.410648  0.424550\\nGOOG  0.470660  1.000000  0.390692  0.443334\\nIBM   0.410648  0.390692  1.000000  0.496093\\nMSFT  0.424550  0.443334  0.496093  1.000000\\nIn [214]: returns.cov()\\nOut[214]: \\n          AAPL      GOOG       IBM      MSFT\\nAAPL  0.001028  0.000303  0.000252  0.000309\\nGOOG  0.000303  0.000580  0.000142  0.000205\\nIBM   0.000252  0.000142  0.000367  0.000216\\nMSFT  0.000309  0.000205  0.000216  0.000516\\nUsing DataFrame’s corrwith method, you can compute pairwise correlations between\\na DataFrame’s columns or rows with another Series or DataFrame. Passing a Series\\nreturns a Series with the correlation value computed for each column:\\nIn [215]: returns.corrwith(returns.IBM)\\nOut[215]: \\nAAPL    0.410648\\nGOOG    0.390692\\nIBM     1.000000\\nMSFT    0.496093\\nPassing a DataFrame computes the correlations of matching column names. Here I'),\n",
       " Document(metadata={}, page_content='In [215]: returns.corrwith(returns.IBM)\\nOut[215]: \\nAAPL    0.410648\\nGOOG    0.390692\\nIBM     1.000000\\nMSFT    0.496093\\nPassing a DataFrame computes the correlations of matching column names. Here I\\ncompute correlations of percent changes with volume:\\nIn [216]: returns.corrwith(volume)\\nOut[216]: \\nAAPL   -0.057461\\nGOOG    0.062644\\n140 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"IBM    -0.007900\\nMSFT   -0.014175\\nPassing axis=1 does things row-wise instead. In all cases, the data points are aligned by\\nlabel before computing the correlation.\\nUnique Values, Value Counts, and Membership\\nAnother class of related methods extracts information about the values contained in a\\none-dimensional Series. To illustrate these, consider this example:\\nIn [217]: obj = Series(['c', 'a', 'd', 'a', 'a', 'b', 'b', 'c', 'c'])\\nThe first function is unique, which gives you an array of the unique values in a Series:\\nIn [218]: uniques = obj.unique()\\nIn [219]: uniques\\nOut[219]: array([c, a, d, b], dtype=object)\\nThe unique values are not necessarily returned in sorted order, but could be sorted after\\nthe fact if needed ( uniques.sort()). Relatedly, value_counts computes a Series con-\\ntaining value frequencies:\\nIn [220]: obj.value_counts()\\nOut[220]: \\nc    3\\na    3\\nb    2\\nd    1\\nThe Series is sorted by value in descending order as a convenience. value_counts is also\"),\n",
       " Document(metadata={}, page_content=\"taining value frequencies:\\nIn [220]: obj.value_counts()\\nOut[220]: \\nc    3\\na    3\\nb    2\\nd    1\\nThe Series is sorted by value in descending order as a convenience. value_counts is also\\navailable as a top-level pandas method that can be used with any array or sequence:\\nIn [221]: pd.value_counts(obj.values, sort=False)\\nOut[221]: \\na    3\\nb    2\\nc    3\\nd    1\\nLastly, isin is responsible for vectorized set membership and can be very useful in\\nfiltering a data set down to a subset of values in a Series or column in a DataFrame:\\nIn [222]: mask = obj.isin(['b', 'c'])\\nIn [223]: mask        In [224]: obj[mask]\\nOut[223]:             Out[224]:          \\n0     True            0    c             \\n1    False            5    b             \\n2    False            6    b             \\n3    False            7    c             \\n4    False            8    c             \\n5     True                               \\n6     True                               \\nSummarizing and Computing Descriptive Statistics | 141\"),\n",
       " Document(metadata={}, page_content='4    False            8    c             \\n5     True                               \\n6     True                               \\nSummarizing and Computing Descriptive Statistics | 141\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"7     True                               \\n8     True\\nSee Table 5-11 for a reference on these methods.\\nTable 5-11. Unique, value counts, and binning methods\\nMethod Description\\nisin Compute boolean array indicating whether each Series value is contained in the passed sequence of values.\\nunique Compute array of unique values in a Series, returned in the order observed.\\nvalue_counts Return a Series containing unique values as its index and frequencies as its values, ordered count in\\ndescending order.\\nIn some cases, you may want to compute a histogram on multiple related columns in\\na DataFrame. Here’s an example:\\nIn [225]: data = DataFrame({'Qu1': [1, 3, 4, 3, 4],\\n   .....:                   'Qu2': [2, 3, 1, 2, 3],\\n   .....:                   'Qu3': [1, 5, 2, 4, 4]})\\nIn [226]: data\\nOut[226]: \\n   Qu1  Qu2  Qu3\\n0    1    2    1\\n1    3    3    5\\n2    4    1    2\\n3    3    2    4\\n4    4    3    4\\nPassing pandas.value_counts to this DataFrame’s apply function gives:\"),\n",
       " Document(metadata={}, page_content='In [226]: data\\nOut[226]: \\n   Qu1  Qu2  Qu3\\n0    1    2    1\\n1    3    3    5\\n2    4    1    2\\n3    3    2    4\\n4    4    3    4\\nPassing pandas.value_counts to this DataFrame’s apply function gives:\\nIn [227]: result = data.apply(pd.value_counts).fillna(0)\\nIn [228]: result\\nOut[228]: \\n   Qu1  Qu2  Qu3\\n1    1    1    1\\n2    0    2    1\\n3    2    2    0\\n4    2    0    2\\n5    0    0    1\\nHandling Missing Data\\nMissing data is common in most data analysis applications. One of the goals in de-\\nsigning pandas was to make working with missing data as painless as possible. For\\nexample, all of the descriptive statistics on pandas objects exclude missing data as\\nyou’ve seen earlier in the chapter.\\n142 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"pandas uses the floating point value NaN (Not a Number) to represent missing data in\\nboth floating as well as in non-floating point arrays. It is just used as a sentinel that can\\nbe easily detected:\\nIn [229]: string_data = Series(['aardvark', 'artichoke', np.nan, 'avocado'])\\nIn [230]: string_data        In [231]: string_data.isnull()\\nOut[230]:                    Out[231]:                     \\n0     aardvark               0    False                    \\n1    artichoke               1    False                    \\n2          NaN               2     True                    \\n3      avocado               3    False\\nThe built-in Python None value is also treated as NA in object arrays:\\nIn [232]: string_data[0] = None\\nIn [233]: string_data.isnull()\\nOut[233]: \\n0     True\\n1    False\\n2     True\\n3    False\\nI do not claim that pandas’s NA representation is optimal, but it is simple and reason-\\nably consistent. It’s the best solution, with good all-around performance characteristics\"),\n",
       " Document(metadata={}, page_content=\"2     True\\n3    False\\nI do not claim that pandas’s NA representation is optimal, but it is simple and reason-\\nably consistent. It’s the best solution, with good all-around performance characteristics\\nand a simple API, that I could concoct in the absence of a true NA data type or bit\\npattern in NumPy’s data types. Ongoing development work in NumPy may change this\\nin the future.\\nTable 5-12. NA handling methods\\nArgument Description\\ndropna Filter axis labels based on whether values for each label have missing data, with varying thresholds for how much\\nmissing data to tolerate.\\nfillna Fill in missing data with some value or using an interpolation method such as 'ffill' or 'bfill'.\\nisnull Return like-type object containing boolean values indicating which values are missing / NA.\\nnotnull Negation of isnull.\\nFiltering Out Missing Data\\nYou have a number of options for filtering out missing data. While doing it by hand is\"),\n",
       " Document(metadata={}, page_content='notnull Negation of isnull.\\nFiltering Out Missing Data\\nYou have a number of options for filtering out missing data. While doing it by hand is\\nalways an option, dropna can be very helpful. On a Series, it returns the Series with only\\nthe non-null data and index values:\\nIn [234]: from numpy import nan as NA\\nIn [235]: data = Series([1, NA, 3.5, NA, 7])\\nIn [236]: data.dropna()\\nOut[236]: \\nHandling Missing Data | 143\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"0    1.0\\n2    3.5\\n4    7.0\\nNaturally, you could have computed this yourself by boolean indexing:\\nIn [237]: data[data.notnull()]\\nOut[237]: \\n0    1.0\\n2    3.5\\n4    7.0\\nWith DataFrame objects, these are a bit more complex. You may want to drop rows\\nor columns which are all NA or just those containing any NAs. dropna by default drops\\nany row containing a missing value:\\nIn [238]: data = DataFrame([[1., 6.5, 3.], [1., NA, NA],\\n   .....:                   [NA, NA, NA], [NA, 6.5, 3.]])\\nIn [239]: cleaned = data.dropna()\\nIn [240]: data        In [241]: cleaned\\nOut[240]:             Out[241]:        \\n    0    1   2           0    1  2     \\n0   1  6.5   3        0  1  6.5  3     \\n1   1  NaN NaN                         \\n2 NaN  NaN NaN                         \\n3 NaN  6.5   3\\nPassing how='all' will only drop rows that are all NA:\\nIn [242]: data.dropna(how='all')\\nOut[242]: \\n    0    1   2\\n0   1  6.5   3\\n1   1  NaN NaN\\n3 NaN  6.5   3\\nDropping columns in the same way is only a matter of passing axis=1:\"),\n",
       " Document(metadata={}, page_content=\"In [242]: data.dropna(how='all')\\nOut[242]: \\n    0    1   2\\n0   1  6.5   3\\n1   1  NaN NaN\\n3 NaN  6.5   3\\nDropping columns in the same way is only a matter of passing axis=1:\\nIn [243]: data[4] = NA\\nIn [244]: data            In [245]: data.dropna(axis=1, how='all')\\nOut[244]:                 Out[245]:                               \\n    0    1   2   4            0    1   2                          \\n0   1  6.5   3 NaN        0   1  6.5   3                          \\n1   1  NaN NaN NaN        1   1  NaN NaN                          \\n2 NaN  NaN NaN NaN        2 NaN  NaN NaN                          \\n3 NaN  6.5   3 NaN        3 NaN  6.5   3\\nA related way to filter out DataFrame rows tends to concern time series data. Suppose\\nyou want to keep only rows containing a certain number of observations. You can\\nindicate this with the thresh argument:\\nIn [246]: df = DataFrame(np.random.randn(7, 3))\\nIn [247]: df.ix[:4, 1] = NA; df.ix[:2, 2] = NA\\n144 | Chapter 5: \\u2002Getting Started with pandas\"),\n",
       " Document(metadata={}, page_content='indicate this with the thresh argument:\\nIn [246]: df = DataFrame(np.random.randn(7, 3))\\nIn [247]: df.ix[:4, 1] = NA; df.ix[:2, 2] = NA\\n144 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='In [248]: df                           In [249]: df.dropna(thresh=3)  \\nOut[248]:                              Out[249]:                      \\n          0         1         2                  0         1         2\\n0 -0.577087       NaN       NaN        5  0.332883 -2.359419 -0.199543\\n1  0.523772       NaN       NaN        6 -1.541996 -0.970736 -1.307030\\n2 -0.713544       NaN       NaN                                       \\n3 -1.860761       NaN  0.560145                                       \\n4 -1.265934       NaN -1.063512                                       \\n5  0.332883 -2.359419 -0.199543                                       \\n6 -1.541996 -0.970736 -1.307030\\nFilling in Missing Data\\nRather than filtering out missing data (and potentially discarding other data along with\\nit), you may want to fill in the “holes” in any number of ways. For most purposes, the \\nfillna method is the workhorse function to use. Calling fillna with a constant replaces\\nmissing values with that value:'),\n",
       " Document(metadata={}, page_content='fillna method is the workhorse function to use. Calling fillna with a constant replaces\\nmissing values with that value:\\nIn [250]: df.fillna(0)\\nOut[250]: \\n          0         1         2\\n0 -0.577087  0.000000  0.000000\\n1  0.523772  0.000000  0.000000\\n2 -0.713544  0.000000  0.000000\\n3 -1.860761  0.000000  0.560145\\n4 -1.265934  0.000000 -1.063512\\n5  0.332883 -2.359419 -0.199543\\n6 -1.541996 -0.970736 -1.307030\\nCalling fillna with a dict you can use a different fill value for each column:\\nIn [251]: df.fillna({1: 0.5, 3: -1})\\nOut[251]: \\n          0         1         2\\n0 -0.577087  0.500000       NaN\\n1  0.523772  0.500000       NaN\\n2 -0.713544  0.500000       NaN\\n3 -1.860761  0.500000  0.560145\\n4 -1.265934  0.500000 -1.063512\\n5  0.332883 -2.359419 -0.199543\\n6 -1.541996 -0.970736 -1.307030\\nfillna returns a new object, but you can modify the existing object in place:\\n# always returns a reference to the filled object\\nIn [252]: _ = df.fillna(0, inplace=True)\\nIn [253]: df\\nOut[253]:'),\n",
       " Document(metadata={}, page_content='fillna returns a new object, but you can modify the existing object in place:\\n# always returns a reference to the filled object\\nIn [252]: _ = df.fillna(0, inplace=True)\\nIn [253]: df\\nOut[253]: \\n          0         1         2\\n0 -0.577087  0.000000  0.000000\\n1  0.523772  0.000000  0.000000\\n2 -0.713544  0.000000  0.000000\\n3 -1.860761  0.000000  0.560145\\nHandling Missing Data | 145\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"4 -1.265934  0.000000 -1.063512\\n5  0.332883 -2.359419 -0.199543\\n6 -1.541996 -0.970736 -1.307030\\nThe same interpolation methods available for reindexing can be used with fillna:\\nIn [254]: df = DataFrame(np.random.randn(6, 3))\\nIn [255]: df.ix[2:, 1] = NA; df.ix[4:, 2] = NA\\nIn [256]: df\\nOut[256]: \\n          0         1         2\\n0  0.286350  0.377984 -0.753887\\n1  0.331286  1.349742  0.069877\\n2  0.246674       NaN  1.004812\\n3  1.327195       NaN -1.549106\\n4  0.022185       NaN       NaN\\n5  0.862580       NaN       NaN\\nIn [257]: df.fillna(method='ffill')      In [258]: df.fillna(method='ffill', limit=2)\\nOut[257]:                                Out[258]:                                   \\n          0         1         2                    0         1         2             \\n0  0.286350  0.377984 -0.753887          0  0.286350  0.377984 -0.753887             \\n1  0.331286  1.349742  0.069877          1  0.331286  1.349742  0.069877\"),\n",
       " Document(metadata={}, page_content='0  0.286350  0.377984 -0.753887          0  0.286350  0.377984 -0.753887             \\n1  0.331286  1.349742  0.069877          1  0.331286  1.349742  0.069877             \\n2  0.246674  1.349742  1.004812          2  0.246674  1.349742  1.004812             \\n3  1.327195  1.349742 -1.549106          3  1.327195  1.349742 -1.549106             \\n4  0.022185  1.349742 -1.549106          4  0.022185       NaN -1.549106             \\n5  0.862580  1.349742 -1.549106          5  0.862580       NaN -1.549106\\nWith fillna you can do lots of other things with a little creativity. For example, you\\nmight pass the mean or median value of a Series:\\nIn [259]: data = Series([1., NA, 3.5, NA, 7])\\nIn [260]: data.fillna(data.mean())\\nOut[260]: \\n0    1.000000\\n1    3.833333\\n2    3.500000\\n3    3.833333\\n4    7.000000\\nSee Table 5-13 for a reference on fillna.\\nTable 5-13. fillna function arguments\\nArgument Description\\nvalue Scalar value or dict-like object to use to fill missing values'),\n",
       " Document(metadata={}, page_content=\"3    3.833333\\n4    7.000000\\nSee Table 5-13 for a reference on fillna.\\nTable 5-13. fillna function arguments\\nArgument Description\\nvalue Scalar value or dict-like object to use to fill missing values\\nmethod Interpolation, by default 'ffill' if function called with no other arguments\\naxis Axis to fill on, default axis=0\\ninplace Modify the calling object without producing a copy\\nlimit For forward and backward filling, maximum number of consecutive periods to fill\\n146 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"Hierarchical Indexing\\nHierarchical indexing is an important feature of pandas enabling you to have multiple\\n(two or more) index levels on an axis. Somewhat abstractly, it provides a way for you\\nto work with higher dimensional data in a lower dimensional form. Let’s start with a\\nsimple example; create a Series with a list of lists or arrays as the index:\\nIn [261]: data = Series(np.random.randn(10),\\n   .....:               index=[['a', 'a', 'a', 'b', 'b', 'b', 'c', 'c', 'd', 'd'],\\n   .....:                      [1, 2, 3, 1, 2, 3, 1, 2, 2, 3]])\\nIn [262]: data\\nOut[262]: \\na  1    0.670216\\n   2    0.852965\\n   3   -0.955869\\nb  1   -0.023493\\n   2   -2.304234\\n   3   -0.652469\\nc  1   -1.218302\\n   2   -1.332610\\nd  2    1.074623\\n   3    0.723642\\nWhat you’re seeing is a prettified view of a Series with a MultiIndex as its index. The\\n“gaps” in the index display mean “use the label directly above”:\\nIn [263]: data.index\\nOut[263]: \\nMultiIndex\"),\n",
       " Document(metadata={}, page_content=\"What you’re seeing is a prettified view of a Series with a MultiIndex as its index. The\\n“gaps” in the index display mean “use the label directly above”:\\nIn [263]: data.index\\nOut[263]: \\nMultiIndex\\n[('a', 1) ('a', 2) ('a', 3) ('b', 1) ('b', 2) ('b', 3) ('c', 1)\\n ('c', 2) ('d', 2) ('d', 3)]\\nWith a hierarchically-indexed object, so-called partial indexing is possible, enabling\\nyou to concisely select subsets of the data:\\nIn [264]: data['b']\\nOut[264]: \\n1   -0.023493\\n2   -2.304234\\n3   -0.652469\\nIn [265]: data['b':'c']        In [266]: data.ix[['b', 'd']]\\nOut[265]:                      Out[266]:                    \\nb  1   -0.023493               b  1   -0.023493             \\n   2   -2.304234                  2   -2.304234             \\n   3   -0.652469                  3   -0.652469             \\nc  1   -1.218302               d  2    1.074623             \\n   2   -1.332610                  3    0.723642\\nSelection is even possible in some cases from an “inner” level:\\nIn [267]: data[:, 2]\"),\n",
       " Document(metadata={}, page_content='c  1   -1.218302               d  2    1.074623             \\n   2   -1.332610                  3    0.723642\\nSelection is even possible in some cases from an “inner” level:\\nIn [267]: data[:, 2]\\nOut[267]: \\na    0.852965\\nHierarchical Indexing | 147\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"b   -2.304234\\nc   -1.332610\\nd    1.074623\\nHierarchical indexing plays a critical role in reshaping data and group-based operations\\nlike forming a pivot table. For example, this data could be rearranged into a DataFrame\\nusing its unstack method:\\nIn [268]: data.unstack()\\nOut[268]: \\n          1         2         3\\na  0.670216  0.852965 -0.955869\\nb -0.023493 -2.304234 -0.652469\\nc -1.218302 -1.332610       NaN\\nd       NaN  1.074623  0.723642\\nThe inverse operation of unstack is stack:\\nIn [269]: data.unstack().stack()\\nOut[269]: \\na  1    0.670216\\n   2    0.852965\\n   3   -0.955869\\nb  1   -0.023493\\n   2   -2.304234\\n   3   -0.652469\\nc  1   -1.218302\\n   2   -1.332610\\nd  2    1.074623\\n   3    0.723642\\nstack and unstack will be explored in more detail in Chapter 7.\\nWith a DataFrame, either axis can have a hierarchical index:\\nIn [270]: frame = DataFrame(np.arange(12).reshape((4, 3)),\\n   .....:                   index=[['a', 'a', 'b', 'b'], [1, 2, 1, 2]],\"),\n",
       " Document(metadata={}, page_content=\"With a DataFrame, either axis can have a hierarchical index:\\nIn [270]: frame = DataFrame(np.arange(12).reshape((4, 3)),\\n   .....:                   index=[['a', 'a', 'b', 'b'], [1, 2, 1, 2]],\\n   .....:                   columns=[['Ohio', 'Ohio', 'Colorado'],\\n   .....:                            ['Green', 'Red', 'Green']])\\nIn [271]: frame\\nOut[271]: \\n      Ohio       Colorado\\n     Green  Red     Green\\na 1      0    1         2\\n  2      3    4         5\\nb 1      6    7         8\\n  2      9   10        11\\nThe hierarchical levels can have names (as strings or any Python objects). If so, these\\nwill show up in the console output (don’t confuse the index names with the axis labels!):\\nIn [272]: frame.index.names = ['key1', 'key2']\\nIn [273]: frame.columns.names = ['state', 'color']\\nIn [274]: frame\\n148 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"Out[274]: \\nstate       Ohio       Colorado\\ncolor      Green  Red     Green\\nkey1 key2                      \\na    1         0    1         2\\n     2         3    4         5\\nb    1         6    7         8\\n     2         9   10        11\\nWith partial column indexing you can similarly select groups of columns:\\nIn [275]: frame['Ohio']\\nOut[275]: \\ncolor      Green  Red\\nkey1 key2            \\na    1         0    1\\n     2         3    4\\nb    1         6    7\\n     2         9   10\\nA MultiIndex can be created by itself and then reused; the columns in the above Data-\\nFrame with level names could be created like this:\\nMultiIndex.from_arrays([['Ohio', 'Ohio', 'Colorado'], ['Green', 'Red', 'Green']],\\n                       names=['state', 'color'])\\nReordering and Sorting Levels\\nAt times you will need to rearrange the order of the levels on an axis or sort the data\\nby the values in one specific level. The swaplevel takes two level numbers or names and\"),\n",
       " Document(metadata={}, page_content=\"At times you will need to rearrange the order of the levels on an axis or sort the data\\nby the values in one specific level. The swaplevel takes two level numbers or names and\\nreturns a new object with the levels interchanged (but the data is otherwise unaltered):\\nIn [276]: frame.swaplevel('key1', 'key2')\\nOut[276]: \\nstate       Ohio       Colorado\\ncolor      Green  Red     Green\\nkey2 key1                      \\n1    a         0    1         2\\n2    a         3    4         5\\n1    b         6    7         8\\n2    b         9   10        11\\nsortlevel, on the other hand, sorts the data (stably) using only the values in a single\\nlevel. When swapping levels, it’s not uncommon to also use sortlevel so that the result\\nis lexicographically sorted:\\nIn [277]: frame.sortlevel(1)           In [278]: frame.swaplevel(0, 1).sortlevel(0)\\nOut[277]:                              Out[278]:                                   \\nstate       Ohio       Colorado        state       Ohio       Colorado\"),\n",
       " Document(metadata={}, page_content='Out[277]:                              Out[278]:                                   \\nstate       Ohio       Colorado        state       Ohio       Colorado             \\ncolor      Green  Red     Green        color      Green  Red     Green             \\nkey1 key2                              key2 key1                                   \\na    1         0    1         2        1    a         0    1         2             \\nb    1         6    7         8             b         6    7         8             \\na    2         3    4         5        2    a         3    4         5             \\nb    2         9   10        11             b         9   10        11\\nHierarchical Indexing | 149\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"Data selection performance is much better on hierarchically indexed\\nobjects if the index is lexicographically sorted starting with the outer-\\nmost level, that is, the result of calling sortlevel(0) or sort_index().\\nSummary Statistics by Level\\nMany descriptive and summary statistics on DataFrame and Series have a level option\\nin which you can specify the level you want to sum by on a particular axis. Consider\\nthe above DataFrame; we can sum by level on either the rows or columns like so:\\nIn [279]: frame.sum(level='key2')\\nOut[279]: \\nstate   Ohio       Colorado\\ncolor  Green  Red     Green\\nkey2                       \\n1          6    8        10\\n2         12   14        16\\nIn [280]: frame.sum(level='color', axis=1)\\nOut[280]: \\ncolor      Green  Red\\nkey1 key2            \\na    1         2    1\\n     2         8    4\\nb    1        14    7\\n     2        20   10\\nUnder the hood, this utilizes pandas’s groupby machinery which will be discussed in\\nmore detail later in the book.\"),\n",
       " Document(metadata={}, page_content=\"2         8    4\\nb    1        14    7\\n     2        20   10\\nUnder the hood, this utilizes pandas’s groupby machinery which will be discussed in\\nmore detail later in the book.\\nUsing a DataFrame’s Columns\\nIt’s not unusual to want to use one or more columns from a DataFrame as the row\\nindex; alternatively, you may wish to move the row index into the DataFrame’s col-\\numns. Here’s an example DataFrame:\\nIn [281]: frame = DataFrame({'a': range(7), 'b': range(7, 0, -1),\\n   .....:                    'c': ['one', 'one', 'one', 'two', 'two', 'two', 'two'],\\n   .....:                    'd': [0, 1, 2, 0, 1, 2, 3]})\\nIn [282]: frame\\nOut[282]: \\n   a  b    c  d\\n0  0  7  one  0\\n1  1  6  one  1\\n2  2  5  one  2\\n3  3  4  two  0\\n4  4  3  two  1\\n5  5  2  two  2\\n6  6  1  two  3\\n150 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"DataFrame’s set_index function will create a new DataFrame using one or more of its\\ncolumns as the index:\\nIn [283]: frame2 = frame.set_index(['c', 'd'])\\nIn [284]: frame2\\nOut[284]: \\n       a  b\\nc   d      \\none 0  0  7\\n    1  1  6\\n    2  2  5\\ntwo 0  3  4\\n    1  4  3\\n    2  5  2\\n    3  6  1\\nBy default the columns are removed from the DataFrame, though you can leave them in:\\nIn [285]: frame.set_index(['c', 'd'], drop=False)\\nOut[285]: \\n       a  b    c  d\\nc   d              \\none 0  0  7  one  0\\n    1  1  6  one  1\\n    2  2  5  one  2\\ntwo 0  3  4  two  0\\n    1  4  3  two  1\\n    2  5  2  two  2\\n    3  6  1  two  3\\nreset_index, on the other hand, does the opposite of set_index; the hierarchical index\\nlevels are are moved into the columns:\\nIn [286]: frame2.reset_index()\\nOut[286]: \\n     c  d  a  b\\n0  one  0  0  7\\n1  one  1  1  6\\n2  one  2  2  5\\n3  two  0  3  4\\n4  two  1  4  3\\n5  two  2  5  2\\n6  two  3  6  1\\nOther pandas Topics\"),\n",
       " Document(metadata={}, page_content='In [286]: frame2.reset_index()\\nOut[286]: \\n     c  d  a  b\\n0  one  0  0  7\\n1  one  1  1  6\\n2  one  2  2  5\\n3  two  0  3  4\\n4  two  1  4  3\\n5  two  2  5  2\\n6  two  3  6  1\\nOther pandas Topics\\nHere are some additional topics that may be of use to you in your data travels.\\nInteger Indexing\\nWorking with pandas objects indexed by integers is something that often trips up new\\nusers due to some differences with indexing semantics on built-in Python data\\nOther pandas Topics | 151\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"structures like lists and tuples. For example, you would not expect the following code\\nto generate an error:\\nser = Series(np.arange(3.))\\nser[-1]\\nIn this case, pandas could “fall back” on integer indexing, but there’s not a safe and\\ngeneral way (that I know of) to do this without introducing subtle bugs. Here we have\\nan index containing 0, 1, 2, but inferring what the user wants (label-based indexing or\\nposition-based) is difficult::\\nIn [288]: ser\\nOut[288]: \\n0    0\\n1    1\\n2    2\\nOn the other hand, with a non-integer index, there is no potential for ambiguity:\\nIn [289]: ser2 = Series(np.arange(3.), index=['a', 'b', 'c'])\\nIn [290]: ser2[-1]\\nOut[290]: 2.0\\nTo keep things consistent, if you have an axis index containing indexers, data selection\\nwith integers will always be label-oriented. This includes slicing with ix, too:\\nIn [291]: ser.ix[:1]\\nOut[291]: \\n0    0\\n1    1\\nIn cases where you need reliable position-based indexing regardless of the index type,\"),\n",
       " Document(metadata={}, page_content='In [291]: ser.ix[:1]\\nOut[291]: \\n0    0\\n1    1\\nIn cases where you need reliable position-based indexing regardless of the index type,\\nyou can use the iget_value method from Series and irow and icol methods from Da-\\ntaFrame:\\nIn [292]: ser3 = Series(range(3), index=[-5, 1, 3])\\nIn [293]: ser3.iget_value(2)\\nOut[293]: 2\\nIn [294]: frame = DataFrame(np.arange(6).reshape(3, 2)), index=[2, 0, 1])\\nIn [295]: frame.irow(0)\\nOut[295]: \\n0    0\\n1    1\\nName: 2\\nPanel Data\\nWhile not a major topic of this book, pandas has a Panel data structure, which you can\\nthink of as a three-dimensional analogue of DataFrame. Much of the development focus\\nof pandas has been in tabular data manipulations as these are easier to reason about,\\n152 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"and hierarchical indexing makes using truly N-dimensional arrays unnecessary in a lot\\nof cases.\\nTo create a Panel, you can use a dict of DataFrame objects or a three-dimensional\\nndarray:\\nimport pandas.io.data as web\\npdata = pd.Panel(dict((stk, web.get_data_yahoo(stk, '1/1/2009', '6/1/2012'))\\n                       for stk in ['AAPL', 'GOOG', 'MSFT', 'DELL']))\\nEach item (the analogue of columns in a DataFrame) in the Panel is a DataFrame:\\nIn [297]: pdata\\nOut[297]: \\n<class 'pandas.core.panel.Panel'>\\nDimensions: 4 (items) x 861 (major) x 6 (minor)\\nItems: AAPL to MSFT\\nMajor axis: 2009-01-02 00:00:00 to 2012-06-01 00:00:00\\nMinor axis: Open to Adj Close\\nIn [298]: pdata = pdata.swapaxes('items', 'minor')\\nIn [299]: pdata['Adj Close']\\nOut[299]: \\n<class 'pandas.core.frame.DataFrame'>\\nDatetimeIndex: 861 entries, 2009-01-02 00:00:00 to 2012-06-01 00:00:00\\nData columns:\\nAAPL    861  non-null values\\nDELL    861  non-null values\\nGOOG    861  non-null values\\nMSFT    861  non-null values\"),\n",
       " Document(metadata={}, page_content=\"Data columns:\\nAAPL    861  non-null values\\nDELL    861  non-null values\\nGOOG    861  non-null values\\nMSFT    861  non-null values\\ndtypes: float64(4)\\nix-based label indexing generalizes to three dimensions, so we can select all data at a\\nparticular date or a range of dates like so:\\nIn [300]: pdata.ix[:, '6/1/2012', :]\\nOut[300]: \\n        Open    High     Low   Close    Volume  Adj Close\\nAAPL  569.16  572.65  560.52  560.99  18606700     560.99\\nDELL   12.15   12.30   12.05   12.07  19396700      12.07\\nGOOG  571.79  572.65  568.35  570.98   3057900     570.98\\nMSFT   28.76   28.96   28.44   28.45  56634300      28.45\\nIn [301]: pdata.ix['Adj Close', '5/22/2012':, :]\\nOut[301]: \\n              AAPL   DELL    GOOG   MSFT\\nDate                                    \\n2012-05-22  556.97  15.08  600.80  29.76\\n2012-05-23  570.56  12.49  609.46  29.11\\n2012-05-24  565.32  12.45  603.66  29.07\\n2012-05-25  562.29  12.46  591.53  29.06\\n2012-05-29  572.27  12.66  594.34  29.56\"),\n",
       " Document(metadata={}, page_content='2012-05-23  570.56  12.49  609.46  29.11\\n2012-05-24  565.32  12.45  603.66  29.07\\n2012-05-25  562.29  12.46  591.53  29.06\\n2012-05-29  572.27  12.66  594.34  29.56\\n2012-05-30  579.17  12.56  588.23  29.34\\nOther pandas Topics | 153\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"2012-05-31  577.73  12.33  580.86  29.19\\n2012-06-01  560.99  12.07  570.98  28.45\\nAn alternate way to represent panel data, especially for fitting statistical models, is in\\n“stacked” DataFrame form:\\nIn [302]: stacked = pdata.ix[:, '5/30/2012':, :].to_frame()\\nIn [303]: stacked\\nOut[303]: \\n                    Open    High     Low   Close    Volume  Adj Close\\nmajor      minor                                                     \\n2012-05-30 AAPL   569.20  579.99  566.56  579.17  18908200     579.17\\n           DELL    12.59   12.70   12.46   12.56  19787800      12.56\\n           GOOG   588.16  591.90  583.53  588.23   1906700     588.23\\n           MSFT    29.35   29.48   29.12   29.34  41585500      29.34\\n2012-05-31 AAPL   580.74  581.50  571.46  577.73  17559800     577.73\\n           DELL    12.53   12.54   12.33   12.33  19955500      12.33\\n           GOOG   588.72  590.00  579.00  580.86   2968300     580.86\\n           MSFT    29.30   29.42   28.94   29.19  39134000      29.19\"),\n",
       " Document(metadata={}, page_content=\"GOOG   588.72  590.00  579.00  580.86   2968300     580.86\\n           MSFT    29.30   29.42   28.94   29.19  39134000      29.19\\n2012-06-01 AAPL   569.16  572.65  560.52  560.99  18606700     560.99\\n           DELL    12.15   12.30   12.05   12.07  19396700      12.07\\n           GOOG   571.79  572.65  568.35  570.98   3057900     570.98\\n           MSFT    28.76   28.96   28.44   28.45  56634300      28.45\\nDataFrame has a related to_panel method, the inverse of to_frame:\\nIn [304]: stacked.to_panel()\\nOut[304]: \\n<class 'pandas.core.panel.Panel'>\\nDimensions: 6 (items) x 3 (major) x 4 (minor)\\nItems: Open to Adj Close\\nMajor axis: 2012-05-30 00:00:00 to 2012-06-01 00:00:00\\nMinor axis: AAPL to MSFT\\n154 | Chapter 5: \\u2002Getting Started with pandas\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content='CHAPTER 6\\nData Loading, Storage, and File\\nFormats\\nThe tools in this book are of little use if you can’t easily import and export data in\\nPython. I’m going to be focused on input and output with pandas objects, though there\\nare of course numerous tools in other libraries to aid in this process. NumPy, for ex-\\nample, features low-level but extremely fast binary data loading and storage, including\\nsupport for memory-mapped array. See Chapter 12 for more on those.\\nInput and output typically falls into a few main categories: reading text files and other\\nmore efficient on-disk formats, loading data from databases, and interacting with net-\\nwork sources like web APIs.\\nReading and Writing Data in Text Format\\nPython has become a beloved language for text and file munging due to its simple syntax\\nfor interacting with files, intuitive data structures, and convenient features like tuple\\npacking and unpacking.\\npandas features a number of functions for reading tabular data as a DataFrame object.'),\n",
       " Document(metadata={}, page_content=\"for interacting with files, intuitive data structures, and convenient features like tuple\\npacking and unpacking.\\npandas features a number of functions for reading tabular data as a DataFrame object.\\nTable 6-1 has a summary of all of them, though read_csv and read_table are likely the\\nones you’ll use the most.\\nTable 6-1. Parsing functions in pandas\\nFunction Description\\nread_csv Load delimited data from a file, URL, or file-like object. Use comma as default delimiter\\nread_table Load delimited data from a file, URL, or file-like object. Use tab ('\\\\t') as default delimiter\\nread_fwf Read data in fixed-width column format (that is, no delimiters)\\nread_clipboard Version of read_table that reads data from the clipboard. Useful for converting tables from web pages\\n155\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content='I’ll give an overview of the mechanics of these functions, which are meant to convert\\ntext data into a DataFrame. The options for these functions fall into a few categories:\\n• Indexing: can treat one or more columns as the returned DataFrame, and whether\\nto get column names from the file, the user, or not at all.\\n• Type inference and data conversion: this includes the user-defined value conver-\\nsions and custom list of missing value markers.\\n• Datetime parsing: includes combining capability, including combining date and\\ntime information spread over multiple columns into a single column in the result.\\n• Iterating: support for iterating over chunks of very large files.\\n• Unclean data issues: skipping rows or a footer, comments, or other minor things\\nlike numeric data with thousands separated by commas.\\nType inference is one of the more important features of these functions; that means you\\ndon’t have to specify which columns are numeric, integer, boolean, or string. Handling'),\n",
       " Document(metadata={}, page_content=\"Type inference is one of the more important features of these functions; that means you\\ndon’t have to specify which columns are numeric, integer, boolean, or string. Handling\\ndates and other custom types requires a bit more effort, though. Let’s start with a small\\ncomma-separated (CSV) text file:\\nIn [846]: !cat ch06/ex1.csv\\na,b,c,d,message\\n1,2,3,4,hello\\n5,6,7,8,world\\n9,10,11,12,foo\\nSince this is comma-delimited, we can use read_csv to read it into a DataFrame:\\nIn [847]: df = pd.read_csv('ch06/ex1.csv')\\nIn [848]: df\\nOut[848]: \\n   a   b   c   d message\\n0  1   2   3   4   hello\\n1  5   6   7   8   world\\n2  9  10  11  12     foo\\nWe could also have used read_table and specifying the delimiter:\\nIn [849]: pd.read_table('ch06/ex1.csv', sep=',')\\nOut[849]: \\n   a   b   c   d message\\n0  1   2   3   4   hello\\n1  5   6   7   8   world\\n2  9  10  11  12     foo\\nHere I used the Unix cat shell command to print the raw contents of\\nthe file to the screen. If you’re on Windows, you can use type instead\"),\n",
       " Document(metadata={}, page_content='1  5   6   7   8   world\\n2  9  10  11  12     foo\\nHere I used the Unix cat shell command to print the raw contents of\\nthe file to the screen. If you’re on Windows, you can use type instead\\nof cat to achieve the same effect.\\n156 | Chapter 6: \\u2002Data Loading, Storage, and File Formats\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"A file will not always have a header row. Consider this file:\\nIn [850]: !cat ch06/ex2.csv\\n1,2,3,4,hello\\n5,6,7,8,world\\n9,10,11,12,foo\\nTo read this in, you have a couple of options. You can allow pandas to assign default\\ncolumn names, or you can specify names yourself:\\nIn [851]: pd.read_csv('ch06/ex2.csv', header=None)\\nOut[851]: \\n   X.1  X.2  X.3  X.4    X.5\\n0    1    2    3    4  hello\\n1    5    6    7    8  world\\n2    9   10   11   12    foo\\nIn [852]: pd.read_csv('ch06/ex2.csv', names=['a', 'b', 'c', 'd', 'message'])\\nOut[852]: \\n   a   b   c   d message\\n0  1   2   3   4   hello\\n1  5   6   7   8   world\\n2  9  10  11  12     foo\\nSuppose you wanted the message column to be the index of the returned DataFrame.\\nYou can either indicate you want the column at index 4 or named 'message' using the\\nindex_col argument:\\nIn [853]: names = ['a', 'b', 'c', 'd', 'message']\\nIn [854]: pd.read_csv('ch06/ex2.csv', names=names, index_col='message')\\nOut[854]: \\n         a   b   c   d\\nmessage\"),\n",
       " Document(metadata={}, page_content=\"index_col argument:\\nIn [853]: names = ['a', 'b', 'c', 'd', 'message']\\nIn [854]: pd.read_csv('ch06/ex2.csv', names=names, index_col='message')\\nOut[854]: \\n         a   b   c   d\\nmessage               \\nhello    1   2   3   4\\nworld    5   6   7   8\\nfoo      9  10  11  12\\nIn the event that you want to form a hierarchical index from multiple columns, just\\npass a list of column numbers or names:\\nIn [855]: !cat ch06/csv_mindex.csv\\nkey1,key2,value1,value2\\none,a,1,2\\none,b,3,4\\none,c,5,6\\none,d,7,8\\ntwo,a,9,10\\ntwo,b,11,12\\ntwo,c,13,14\\ntwo,d,15,16\\nIn [856]: parsed = pd.read_csv('ch06/csv_mindex.csv', index_col=['key1', 'key2'])\\nIn [857]: parsed\\nOut[857]: \\nReading and Writing Data in Text Format | 157\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"value1  value2\\nkey1 key2                \\none  a          1       2\\n     b          3       4\\n     c          5       6\\n     d          7       8\\ntwo  a          9      10\\n     b         11      12\\n     c         13      14\\n     d         15      16\\nIn some cases, a table might not have a fixed delimiter, using whitespace or some other\\npattern to separate fields. In these cases, you can pass a regular expression as a delimiter\\nfor read_table. Consider a text file that looks like this:\\nIn [858]: list(open('ch06/ex3.txt'))\\nOut[858]: \\n['            A         B         C\\\\n',\\n 'aaa -0.264438 -1.026059 -0.619500\\\\n',\\n 'bbb  0.927272  0.302904 -0.032399\\\\n',\\n 'ccc -0.264273 -0.386314 -0.217601\\\\n',\\n 'ddd -0.871858 -0.348382  1.100491\\\\n']\\nWhile you could do some munging by hand, in this case fields are separated by a variable\\namount of whitespace. This can be expressed by the regular expression \\\\s+, so we have\\nthen:\\nIn [859]: result = pd.read_table('ch06/ex3.txt', sep='\\\\s+')\\nIn [860]: result\"),\n",
       " Document(metadata={}, page_content=\"amount of whitespace. This can be expressed by the regular expression \\\\s+, so we have\\nthen:\\nIn [859]: result = pd.read_table('ch06/ex3.txt', sep='\\\\s+')\\nIn [860]: result\\nOut[860]: \\n            A         B         C\\naaa -0.264438 -1.026059 -0.619500\\nbbb  0.927272  0.302904 -0.032399\\nccc -0.264273 -0.386314 -0.217601\\nddd -0.871858 -0.348382  1.100491\\nBecause there was one fewer column name than the number of data rows, read_table\\ninfers that the first column should be the DataFrame’s index in this special case.\\nThe parser functions have many additional arguments to help you handle the wide\\nvariety of exception file formats that occur (see Table 6-2). For example, you can skip\\nthe first, third, and fourth rows of a file with skiprows:\\nIn [861]: !cat ch06/ex4.csv\\n# hey!\\na,b,c,d,message\\n# just wanted to make things more difficult for you\\n# who reads CSV files with computers, anyway?\\n1,2,3,4,hello\\n5,6,7,8,world\\n9,10,11,12,foo\\nIn [862]: pd.read_csv('ch06/ex4.csv', skiprows=[0, 2, 3])\"),\n",
       " Document(metadata={}, page_content=\"# just wanted to make things more difficult for you\\n# who reads CSV files with computers, anyway?\\n1,2,3,4,hello\\n5,6,7,8,world\\n9,10,11,12,foo\\nIn [862]: pd.read_csv('ch06/ex4.csv', skiprows=[0, 2, 3])\\nOut[862]: \\n   a   b   c   d message\\n158 | Chapter 6: \\u2002Data Loading, Storage, and File Formats\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"0  1   2   3   4   hello\\n1  5   6   7   8   world\\n2  9  10  11  12     foo\\nHandling missing values is an important and frequently nuanced part of the file parsing\\nprocess. Missing data is usually either not present (empty string) or marked by some \\nsentinel value. By default, pandas uses a set of commonly occurring sentinels, such as\\nNA, -1.#IND, and NULL:\\nIn [863]: !cat ch06/ex5.csv\\nsomething,a,b,c,d,message\\none,1,2,3,4,NA\\ntwo,5,6,,8,world\\nthree,9,10,11,12,foo\\nIn [864]: result = pd.read_csv('ch06/ex5.csv')\\nIn [865]: result\\nOut[865]: \\n  something  a   b   c   d message\\n0       one  1   2   3   4     NaN\\n1       two  5   6 NaN   8   world\\n2     three  9  10  11  12     foo\\nIn [866]: pd.isnull(result)\\nOut[866]: \\n  something      a      b      c      d message\\n0     False  False  False  False  False    True\\n1     False  False  False   True  False   False\\n2     False  False  False  False  False   False\"),\n",
       " Document(metadata={}, page_content=\"something      a      b      c      d message\\n0     False  False  False  False  False    True\\n1     False  False  False   True  False   False\\n2     False  False  False  False  False   False\\nThe na_values option can take either a list or set of strings to consider missing values:\\nIn [867]: result = pd.read_csv('ch06/ex5.csv', na_values=['NULL'])\\nIn [868]: result\\nOut[868]: \\n  something  a   b   c   d message\\n0       one  1   2   3   4     NaN\\n1       two  5   6 NaN   8   world\\n2     three  9  10  11  12     foo\\nDifferent NA sentinels can be specified for each column in a dict:\\nIn [869]: sentinels = {'message': ['foo', 'NA'], 'something': ['two']}\\nIn [870]: pd.read_csv('ch06/ex5.csv', na_values=sentinels)\\nOut[870]: \\n  something  a   b   c   d message\\n0       one  1   2   3   4     NaN\\n1       NaN  5   6 NaN   8   world\\n2     three  9  10  11  12     NaN\\nReading and Writing Data in Text Format | 159\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content='Table 6-2. read_csv /read_table function arguments\\nArgument Description\\npath String indicating filesystem location, URL, or file-like object\\nsep or delimiter Character sequence or regular expression to use to split fields in each row\\nheader Row number to use as column names. Defaults to 0 (first row), but should be None if there is no header\\nrow\\nindex_col Column numbers or names to use as the row index in the result. Can be a single name/number or a list\\nof them for a hierarchical index\\nnames List of column names for result, combine with header=None\\nskiprows Number of rows at beginning of file to ignore or list of row numbers (starting from 0) to skip\\nna_values Sequence of values to replace with NA\\ncomment Character or characters to split comments off the end of lines\\nparse_dates Attempt to parse data to datetime; False by default. If True, will attempt to parse all columns. Otherwise'),\n",
       " Document(metadata={}, page_content=\"comment Character or characters to split comments off the end of lines\\nparse_dates Attempt to parse data to datetime; False by default. If True, will attempt to parse all columns. Otherwise\\ncan specify a list of column numbers or name to parse. If element of list is tuple or list, will combine\\nmultiple columns together and parse to date (for example if date/time split across two columns)\\nkeep_date_col If joining columns to parse date, drop the joined columns. Default True\\nconverters Dict containing column number of name mapping to functions. For example {'foo': f} would apply\\nthe function f to all values in the 'foo' column\\ndayfirst When parsing potentially ambiguous dates, treat as international format (e.g. 7/6/2012 -> June 7,\\n2012). Default False\\ndate_parser Function to use to parse dates\\nnrows Number of rows to read from beginning of file\\niterator Return a TextParser object for reading file piecemeal\\nchunksize For iteration, size of file chunks\"),\n",
       " Document(metadata={}, page_content=\"nrows Number of rows to read from beginning of file\\niterator Return a TextParser object for reading file piecemeal\\nchunksize For iteration, size of file chunks\\nskip_footer Number of lines to ignore at end of file\\nverbose Print various parser output information, like the number of missing values placed in non-numeric\\ncolumns\\nencoding Text encoding for unicode. For example 'utf-8' for UTF-8 encoded text\\nsqueeze If the parsed data only contains one column return a Series\\nthousands Separator for thousands, e.g. ',' or '.'\\nReading Text Files in Pieces\\nWhen processing very large files or figuring out the right set of arguments to correctly\\nprocess a large file, you may only want to read in a small piece of a file or iterate through\\nsmaller chunks of the file.\\nIn [871]: result = pd.read_csv('ch06/ex6.csv')\\nIn [872]: result\\nOut[872]: \\n160 | Chapter 6: \\u2002Data Loading, Storage, and File Formats\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 10000 entries, 0 to 9999\\nData columns:\\none      10000  non-null values\\ntwo      10000  non-null values\\nthree    10000  non-null values\\nfour     10000  non-null values\\nkey      10000  non-null values\\ndtypes: float64(4), object(1)\\nIf you want to only read out a small number of rows (avoiding reading the entire file),\\nspecify that with nrows:\\nIn [873]: pd.read_csv('ch06/ex6.csv', nrows=5)\\nOut[873]: \\n        one       two     three      four key\\n0  0.467976 -0.038649 -0.295344 -1.824726   L\\n1 -0.358893  1.404453  0.704965 -0.200638   B\\n2 -0.501840  0.659254 -0.421691 -0.057688   G\\n3  0.204886  1.074134  1.388361 -0.982404   R\\n4  0.354628 -0.133116  0.283763 -0.837063   Q\\nTo read out a file in pieces, specify a chunksize as a number of rows:\\nIn [874]: chunker = pd.read_csv('ch06/ex6.csv', chunksize=1000)\\nIn [875]: chunker\\nOut[875]: <pandas.io.parsers.TextParser at 0x8398150>\"),\n",
       " Document(metadata={}, page_content=\"In [874]: chunker = pd.read_csv('ch06/ex6.csv', chunksize=1000)\\nIn [875]: chunker\\nOut[875]: <pandas.io.parsers.TextParser at 0x8398150>\\nThe TextParser object returned by read_csv allows you to iterate over the parts of the\\nfile according to the chunksize. For example, we can iterate over ex6.csv, aggregating\\nthe value counts in the 'key' column like so:\\nchunker = pd.read_csv('ch06/ex6.csv', chunksize=1000)\\ntot = Series([])\\nfor piece in chunker:\\n    tot = tot.add(piece['key'].value_counts(), fill_value=0)\\ntot = tot.order(ascending=False)\\nWe have then:\\nIn [877]: tot[:10]\\nOut[877]: \\nE    368\\nX    364\\nL    346\\nO    343\\nQ    340\\nM    338\\nJ    337\\nF    335\\nK    334\\nH    330\\nReading and Writing Data in Text Format | 161\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"TextParser is also equipped with a get_chunk method which enables you to read pieces\\nof an arbitrary size.\\nWriting Data Out to Text Format\\nData can also be exported to delimited format. Let’s consider one of the CSV files read\\nabove:\\nIn [878]: data = pd.read_csv('ch06/ex5.csv')\\nIn [879]: data\\nOut[879]: \\n  something  a   b   c   d message\\n0       one  1   2   3   4     NaN\\n1       two  5   6 NaN   8   world\\n2     three  9  10  11  12     foo\\nUsing DataFrame’s to_csv method, we can write the data out to a comma-separated file:\\nIn [880]: data.to_csv('ch06/out.csv')\\nIn [881]: !cat ch06/out.csv\\n,something,a,b,c,d,message\\n0,one,1,2,3.0,4,\\n1,two,5,6,,8,world\\n2,three,9,10,11.0,12,foo\\nOther delimiters can be used, of course (writing to sys.stdout so it just prints the text\\nresult):\\nIn [882]: data.to_csv(sys.stdout, sep='|')\\n|something|a|b|c|d|message\\n0|one|1|2|3.0|4|\\n1|two|5|6||8|world\\n2|three|9|10|11.0|12|foo\\nMissing values appear as empty strings in the output. You might want to denote them\"),\n",
       " Document(metadata={}, page_content=\"|something|a|b|c|d|message\\n0|one|1|2|3.0|4|\\n1|two|5|6||8|world\\n2|three|9|10|11.0|12|foo\\nMissing values appear as empty strings in the output. You might want to denote them\\nby some other sentinel value:\\nIn [883]: data.to_csv(sys.stdout, na_rep='NULL')\\n,something,a,b,c,d,message\\n0,one,1,2,3.0,4,NULL\\n1,two,5,6,NULL,8,world\\n2,three,9,10,11.0,12,foo\\nWith no other options specified, both the row and column labels are written. Both of\\nthese can be disabled:\\nIn [884]: data.to_csv(sys.stdout, index=False, header=False)\\none,1,2,3.0,4,\\ntwo,5,6,,8,world\\nthree,9,10,11.0,12,foo\\nYou can also write only a subset of the columns, and in an order of your choosing:\\n162 | Chapter 6: \\u2002Data Loading, Storage, and File Formats\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"In [885]: data.to_csv(sys.stdout, index=False, cols=['a', 'b', 'c'])\\na,b,c\\n1,2,3.0\\n5,6,\\n9,10,11.0\\nSeries also has a to_csv method:\\nIn [886]: dates = pd.date_range('1/1/2000', periods=7)\\nIn [887]: ts = Series(np.arange(7), index=dates)\\nIn [888]: ts.to_csv('ch06/tseries.csv')\\nIn [889]: !cat ch06/tseries.csv\\n2000-01-01 00:00:00,0\\n2000-01-02 00:00:00,1\\n2000-01-03 00:00:00,2\\n2000-01-04 00:00:00,3\\n2000-01-05 00:00:00,4\\n2000-01-06 00:00:00,5\\n2000-01-07 00:00:00,6\\nWith a bit of wrangling (no header, first column as index), you can read a CSV version\\nof a Series with read_csv, but there is also a from_csv convenience method that makes\\nit a bit simpler:\\nIn [890]: Series.from_csv('ch06/tseries.csv', parse_dates=True)\\nOut[890]: \\n2000-01-01    0\\n2000-01-02    1\\n2000-01-03    2\\n2000-01-04    3\\n2000-01-05    4\\n2000-01-06    5\\n2000-01-07    6\\nSee the docstrings for to_csv and from_csv in IPython for more information.\\nManually Working with Delimited Formats\"),\n",
       " Document(metadata={}, page_content='2000-01-03    2\\n2000-01-04    3\\n2000-01-05    4\\n2000-01-06    5\\n2000-01-07    6\\nSee the docstrings for to_csv and from_csv in IPython for more information.\\nManually Working with Delimited Formats\\nMost forms of tabular data can be loaded from disk using functions like pan\\ndas.read_table. In some cases, however, some manual processing may be necessary.\\nIt’s not uncommon to receive a file with one or more malformed lines that trip up \\nread_table. To illustrate the basic tools, consider a small CSV file:\\nIn [891]: !cat ch06/ex7.csv\\n\"a\",\"b\",\"c\"\\n\"1\",\"2\",\"3\"\\n\"1\",\"2\",\"3\",\"4\"\\nFor any file with a single-character delimiter, you can use Python’s built-in csv module.\\nTo use it, pass any open file or file-like object to csv.reader:\\nReading and Writing Data in Text Format | 163\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='import csv\\nf = open(\\'ch06/ex7.csv\\')\\nreader = csv.reader(f)\\nIterating through the reader like a file yields tuples of values in each like with any quote\\ncharacters removed:\\nIn [893]: for line in reader:\\n   .....:     print line\\n[\\'a\\', \\'b\\', \\'c\\']\\n[\\'1\\', \\'2\\', \\'3\\']\\n[\\'1\\', \\'2\\', \\'3\\', \\'4\\']\\nFrom there, it’s up to you to do the wrangling necessary to put the data in the form\\nthat you need it. For example:\\nIn [894]: lines = list(csv.reader(open(\\'ch06/ex7.csv\\')))\\nIn [895]: header, values = lines[0], lines[1:]\\nIn [896]: data_dict = {h: v for h, v in zip(header, zip(*values))}\\nIn [897]: data_dict\\nOut[897]: {\\'a\\': (\\'1\\', \\'1\\'), \\'b\\': (\\'2\\', \\'2\\'), \\'c\\': (\\'3\\', \\'3\\')}\\nCSV files come in many different flavors. Defining a new format with a different de-\\nlimiter, string quoting convention, or line terminator is done by defining a simple sub-\\nclass of csv.Dialect:\\nclass my_dialect(csv.Dialect):\\n    lineterminator = \\'\\\\n\\'\\n    delimiter = \\';\\'\\n    quotechar = \\'\"\\'\\nreader = csv.reader(f, dialect=my_dialect)'),\n",
       " Document(metadata={}, page_content='class of csv.Dialect:\\nclass my_dialect(csv.Dialect):\\n    lineterminator = \\'\\\\n\\'\\n    delimiter = \\';\\'\\n    quotechar = \\'\"\\'\\nreader = csv.reader(f, dialect=my_dialect)\\nIndividual CSV dialect parameters can also be given as keywords to csv.reader without\\nhaving to define a subclass:\\nreader = csv.reader(f, delimiter=\\'|\\')\\nThe possible options (attributes of csv.Dialect) and what they do can be found in\\nTable 6-3.\\nTable 6-3. CSV dialect options\\nArgument Description\\ndelimiter One-character string to separate fields. Defaults to \\',\\'.\\nlineterminator Line terminator for writing, defaults to \\'\\\\r\\\\n\\'. Reader ignores this and recognizes\\ncross-platform line terminators.\\nquotechar Quote character for fields with special characters (like a delimiter). Default is \\'\"\\'.\\nquoting Quoting convention. Options include csv.QUOTE_ALL (quote all fields),\\ncsv.QUOTE_MINIMAL (only fields with special characters like the delimiter),\\n164 | Chapter 6: \\u2002Data Loading, Storage, and File Formats\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"Argument Description\\ncsv.QUOTE_NONNUMERIC, and csv.QUOTE_NON (no quoting). See Python’s\\ndocumentation for full details. Defaults to QUOTE_MINIMAL.\\nskipinitialspace Ignore whitespace after each delimiter. Default False.\\ndoublequote How to handle quoting character inside a field. If True, it is doubled. See online\\ndocumentation for full detail and behavior.\\nescapechar String to escape the delimiter if quoting is set to csv.QUOTE_NONE. Disabled by\\ndefault\\nFor files with more complicated or fixed multicharacter delimiters, you\\nwill not be able to use the csv module. In those cases, you’ll have to do\\nthe line splitting and other cleanup using string’s split method or the\\nregular expression method re.split.\\nTo write delimited files manually, you can use csv.writer. It accepts an open, writable\\nfile object and the same dialect and format options as csv.reader:\\nwith open('mydata.csv', 'w') as f:\\n    writer = csv.writer(f, dialect=my_dialect)\\n    writer.writerow(('one', 'two', 'three'))\"),\n",
       " Document(metadata={}, page_content='file object and the same dialect and format options as csv.reader:\\nwith open(\\'mydata.csv\\', \\'w\\') as f:\\n    writer = csv.writer(f, dialect=my_dialect)\\n    writer.writerow((\\'one\\', \\'two\\', \\'three\\'))\\n    writer.writerow((\\'1\\', \\'2\\', \\'3\\'))\\n    writer.writerow((\\'4\\', \\'5\\', \\'6\\'))\\n    writer.writerow((\\'7\\', \\'8\\', \\'9\\'))\\nJSON Data\\nJSON (short for JavaScript Object Notation) has become one of the standard formats\\nfor sending data by HTTP request between web browsers and other applications. It is\\na much more flexible data format than a tabular text form like CSV. Here is an example:\\nobj = \"\"\"\\n{\"name\": \"Wes\",\\n \"places_lived\": [\"United States\", \"Spain\", \"Germany\"],\\n \"pet\": null,\\n \"siblings\": [{\"name\": \"Scott\", \"age\": 25, \"pet\": \"Zuko\"},\\n              {\"name\": \"Katie\", \"age\": 33, \"pet\": \"Cisco\"}]\\n}\\n\"\"\"\\nJSON is very nearly valid Python code with the exception of its null value null and\\nsome other nuances (such as disallowing trailing commas at the end of lists). The basic'),\n",
       " Document(metadata={}, page_content='}\\n\"\"\"\\nJSON is very nearly valid Python code with the exception of its null value null and\\nsome other nuances (such as disallowing trailing commas at the end of lists). The basic\\ntypes are objects (dicts), arrays (lists), strings, numbers, booleans, and nulls. All of the\\nkeys in an object must be strings. There are several Python libraries for reading and\\nwriting JSON data. I’ll use json here as it is built into the Python standard library. To\\nconvert a JSON string to Python form, use json.loads:\\nIn [899]: import json\\nReading and Writing Data in Text Format | 165\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"In [900]: result = json.loads(obj)\\nIn [901]: result\\nOut[901]: \\n{u'name': u'Wes',\\n u'pet': None,\\n u'places_lived': [u'United States', u'Spain', u'Germany'],\\n u'siblings': [{u'age': 25, u'name': u'Scott', u'pet': u'Zuko'},\\n  {u'age': 33, u'name': u'Katie', u'pet': u'Cisco'}]}\\njson.dumps on the other hand converts a Python object back to JSON:\\nIn [902]: asjson = json.dumps(result)\\nHow you convert a JSON object or list of objects to a DataFrame or some other data\\nstructure for analysis will be up to you. Conveniently, you can pass a list of JSON objects\\nto the DataFrame constructor and select a subset of the data fields:\\nIn [903]: siblings = DataFrame(result['siblings'], columns=['name', 'age'])\\nIn [904]: siblings\\nOut[904]: \\n    name  age\\n0  Scott   25\\n1  Katie   33\\nFor an extended example of reading and manipulating JSON data (including nested\\nrecords), see the USDA Food Database example in the next chapter.\\nAn effort is underway to add fast native JSON export ( to_json) and\"),\n",
       " Document(metadata={}, page_content='records), see the USDA Food Database example in the next chapter.\\nAn effort is underway to add fast native JSON export ( to_json) and\\ndecoding (from_json) to pandas. This was not ready at the time of writ-\\ning.\\nXML and HTML: Web Scraping\\nPython has many libraries for reading and writing data in the ubiquitous HTML and\\nXML formats. lxml (http://lxml.de) is one that has consistently strong performance in\\nparsing very large files. lxml has multiple programmer interfaces; first I’ll show using \\nlxml.html for HTML, then parse some XML using lxml.objectify.\\nMany websites make data available in HTML tables for viewing in a browser, but not\\ndownloadable as an easily machine-readable format like JSON, HTML, or XML. I no-\\nticed that this was the case with Yahoo! Finance’s stock options data. If you aren’t\\nfamiliar with this data; options are derivative contracts giving you the right to buy\\n(call option) or sell ( put option) a company’s stock at some particular price (the'),\n",
       " Document(metadata={}, page_content='familiar with this data; options are derivative contracts giving you the right to buy\\n(call option) or sell ( put option) a company’s stock at some particular price (the\\nstrike) between now and some fixed point in the future (the expiry). People trade both\\ncall and put options across many strikes and expiries; this data can all be found together\\nin tables on Yahoo! Finance.\\n166 | Chapter 6: \\u2002Data Loading, Storage, and File Formats\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"To get started, find the URL you want to extract data from, open it with urllib2 and\\nparse the stream with lxml like so:\\nfrom lxml.html import parse\\nfrom urllib2 import urlopen\\nparsed = parse(urlopen('http://finance.yahoo.com/q/op?s=AAPL+Options'))\\ndoc = parsed.getroot()\\nUsing this object, you can extract all HTML tags of a particular type, such as table tags\\ncontaining the data of interest. As a simple motivating example, suppose you wanted\\nto get a list of every URL linked to in the document; links are a tags in HTML. Using\\nthe document root’s findall method along with an XPath (a means of expressing\\n“queries” on the document):\\nIn [906]: links = doc.findall('.//a')\\nIn [907]: links[15:20]\\nOut[907]: \\n[<Element a at 0x6c488f0>,\\n <Element a at 0x6c48950>,\\n <Element a at 0x6c489b0>,\\n <Element a at 0x6c48a10>,\\n <Element a at 0x6c48a70>]\\nBut these are objects representing HTML elements; to get the URL and link text you\"),\n",
       " Document(metadata={}, page_content=\"<Element a at 0x6c48950>,\\n <Element a at 0x6c489b0>,\\n <Element a at 0x6c48a10>,\\n <Element a at 0x6c48a70>]\\nBut these are objects representing HTML elements; to get the URL and link text you\\nhave to use each element’s get method (for the URL) and text_content method (for\\nthe display text):\\nIn [908]: lnk = links[28]\\nIn [909]: lnk\\nOut[909]: <Element a at 0x6c48dd0>\\nIn [910]: lnk.get('href')\\nOut[910]: 'http://biz.yahoo.com/special.html'\\nIn [911]: lnk.text_content()\\nOut[911]: 'Special Editions'\\nThus, getting a list of all URLs in the document is a matter of writing this list compre-\\nhension:\\nIn [912]: urls = [lnk.get('href') for lnk in doc.findall('.//a')]\\nIn [913]: urls[-10:]\\nOut[913]: \\n['http://info.yahoo.com/privacy/us/yahoo/finance/details.html',\\n 'http://info.yahoo.com/relevantads/',\\n 'http://docs.yahoo.com/info/terms/',\\n 'http://docs.yahoo.com/info/copyright/copyright.html',\\n 'http://help.yahoo.com/l/us/yahoo/finance/forms_index.html',\"),\n",
       " Document(metadata={}, page_content=\"'http://info.yahoo.com/relevantads/',\\n 'http://docs.yahoo.com/info/terms/',\\n 'http://docs.yahoo.com/info/copyright/copyright.html',\\n 'http://help.yahoo.com/l/us/yahoo/finance/forms_index.html',\\n 'http://help.yahoo.com/l/us/yahoo/finance/quotes/fitadelay.html',\\n 'http://help.yahoo.com/l/us/yahoo/finance/quotes/fitadelay.html',\\nReading and Writing Data in Text Format | 167\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"'http://www.capitaliq.com',\\n 'http://www.csidata.com',\\n 'http://www.morningstar.com/']\\nNow, finding the right tables in the document can be a matter of trial and error; some\\nwebsites make it easier by giving a table of interest an id attribute. I determined that\\nthese were the two tables containing the call data and put data, respectively:\\ntables = doc.findall('.//table')\\ncalls = tables[9]\\nputs = tables[13]\\nEach table has a header row followed by each of the data rows:\\nIn [915]: rows = calls.findall('.//tr')\\nFor the header as well as the data rows, we want to extract the text from each cell; in\\nthe case of the header these are th cells and td cells for the data:\\ndef _unpack(row, kind='td'):\\n    elts = row.findall('.//%s' % kind)\\n    return [val.text_content() for val in elts]\\nThus, we obtain:\\nIn [917]: _unpack(rows[0], kind='th')\\nOut[917]: ['Strike', 'Symbol', 'Last', 'Chg', 'Bid', 'Ask', 'Vol', 'Open Int']\\nIn [918]: _unpack(rows[1], kind='td')\\nOut[918]: \\n['295.00',\"),\n",
       " Document(metadata={}, page_content=\"Thus, we obtain:\\nIn [917]: _unpack(rows[0], kind='th')\\nOut[917]: ['Strike', 'Symbol', 'Last', 'Chg', 'Bid', 'Ask', 'Vol', 'Open Int']\\nIn [918]: _unpack(rows[1], kind='td')\\nOut[918]: \\n['295.00',\\n 'AAPL120818C00295000',\\n '310.40',\\n ' 0.00',\\n '289.80',\\n '290.80',\\n '1',\\n '169']\\nNow, it’s a matter of combining all of these steps together to convert this data into a\\nDataFrame. Since the numerical data is still in string format, we want to convert some,\\nbut perhaps not all of the columns to floating point format. You could do this by hand,\\nbut, luckily, pandas has a class TextParser that is used internally in the read_csv and\\nother parsing functions to do the appropriate automatic type conversion:\\nfrom pandas.io.parsers import TextParser\\ndef parse_options_data(table):\\n    rows = table.findall('.//tr')\\n    header = _unpack(rows[0], kind='th')\\n    data = [_unpack(r) for r in rows[1:]]\\n    return TextParser(data, names=header).get_chunk()\"),\n",
       " Document(metadata={}, page_content=\"rows = table.findall('.//tr')\\n    header = _unpack(rows[0], kind='th')\\n    data = [_unpack(r) for r in rows[1:]]\\n    return TextParser(data, names=header).get_chunk()\\nFinally, we invoke this parsing function on the lxml table objects and get DataFrame\\nresults:\\n168 | Chapter 6: \\u2002Data Loading, Storage, and File Formats\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content='In [920]: call_data = parse_options_data(calls)\\nIn [921]: put_data = parse_options_data(puts)\\nIn [922]: call_data[:10]\\nOut[922]: \\n   Strike               Symbol    Last  Chg     Bid     Ask  Vol Open Int\\n0     295  AAPL120818C00295000  310.40  0.0  289.80  290.80    1      169\\n1     300  AAPL120818C00300000  277.10  1.7  284.80  285.60    2      478\\n2     305  AAPL120818C00305000  300.97  0.0  279.80  280.80   10      316\\n3     310  AAPL120818C00310000  267.05  0.0  274.80  275.65    6      239\\n4     315  AAPL120818C00315000  296.54  0.0  269.80  270.80   22       88\\n5     320  AAPL120818C00320000  291.63  0.0  264.80  265.80   96      173\\n6     325  AAPL120818C00325000  261.34  0.0  259.80  260.80  N/A      108\\n7     330  AAPL120818C00330000  230.25  0.0  254.80  255.80  N/A       21\\n8     335  AAPL120818C00335000  266.03  0.0  249.80  250.65    4       46\\n9     340  AAPL120818C00340000  272.58  0.0  244.80  245.80    4       30\\nParsing XML with lxml.objectify'),\n",
       " Document(metadata={}, page_content='8     335  AAPL120818C00335000  266.03  0.0  249.80  250.65    4       46\\n9     340  AAPL120818C00340000  272.58  0.0  244.80  245.80    4       30\\nParsing XML with lxml.objectify\\nXML (extensible markup language) is another common structured data format sup-\\nporting hierarchical, nested data with metadata. The files that generate the book you\\nare reading actually form a series of large XML documents.\\nAbove, I showed the lxml library and its lxml.html interface. Here I show an alternate\\ninterface that’s convenient for XML data, lxml.objectify.\\nThe New York Metropolitan Transportation Authority (MTA) publishes a number of\\ndata series about its bus and train services ( http://www.mta.info/developers/download\\n.html). Here we’ll look at the performance data which is contained in a set of XML files.\\nEach train or bus service has a different file (like Performance_MNR.xml for the Metro-\\nNorth Railroad) containing monthly data as a series of XML records that look like this:\\n<INDICATOR>'),\n",
       " Document(metadata={}, page_content='Each train or bus service has a different file (like Performance_MNR.xml for the Metro-\\nNorth Railroad) containing monthly data as a series of XML records that look like this:\\n<INDICATOR>\\n  <INDICATOR_SEQ>373889</INDICATOR_SEQ>\\n  <PARENT_SEQ></PARENT_SEQ>\\n  <AGENCY_NAME>Metro-North Railroad</AGENCY_NAME>\\n  <INDICATOR_NAME>Escalator Availability</INDICATOR_NAME>\\n  <DESCRIPTION>Percent of the time that escalators are operational\\n  systemwide. The availability rate is based on physical observations performed\\n  the morning of regular business days only. This is a new indicator the agency\\n  began reporting in 2009.</DESCRIPTION>\\n  <PERIOD_YEAR>2011</PERIOD_YEAR>\\n  <PERIOD_MONTH>12</PERIOD_MONTH>\\n  <CATEGORY>Service Indicators</CATEGORY>\\n  <FREQUENCY>M</FREQUENCY>\\n  <DESIRED_CHANGE>U</DESIRED_CHANGE>\\n  <INDICATOR_UNIT>%</INDICATOR_UNIT>\\n  <DECIMAL_PLACES>1</DECIMAL_PLACES>\\n  <YTD_TARGET>97.00</YTD_TARGET>\\n  <YTD_ACTUAL></YTD_ACTUAL>\\n  <MONTHLY_TARGET>97.00</MONTHLY_TARGET>'),\n",
       " Document(metadata={}, page_content='<INDICATOR_UNIT>%</INDICATOR_UNIT>\\n  <DECIMAL_PLACES>1</DECIMAL_PLACES>\\n  <YTD_TARGET>97.00</YTD_TARGET>\\n  <YTD_ACTUAL></YTD_ACTUAL>\\n  <MONTHLY_TARGET>97.00</MONTHLY_TARGET>\\n  <MONTHLY_ACTUAL></MONTHLY_ACTUAL>\\n</INDICATOR>\\nReading and Writing Data in Text Format | 169\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"Using lxml.objectify, we parse the file and get a reference to the root node of the XML\\nfile with getroot:\\nfrom lxml import objectify\\npath = 'Performance_MNR.xml'\\nparsed = objectify.parse(open(path))\\nroot = parsed.getroot()\\nroot.INDICATOR return a generator yielding each <INDICATOR> XML element. For each\\nrecord, we can populate a dict of tag names (like YTD_ACTUAL) to data values (excluding\\na few tags):\\ndata = []\\nskip_fields = ['PARENT_SEQ', 'INDICATOR_SEQ',\\n               'DESIRED_CHANGE', 'DECIMAL_PLACES']\\nfor elt in root.INDICATOR:\\n    el_data = {}\\n    for child in elt.getchildren():\\n        if child.tag in skip_fields:\\n            continue\\n        el_data[child.tag] = child.pyval\\n    data.append(el_data)\\nLastly, convert this list of dicts into a DataFrame:\\nIn [927]: perf = DataFrame(data)\\nIn [928]: perf\\nOut[928]: \\nEmpty DataFrame\\nColumns: array([], dtype=int64)\\nIndex: array([], dtype=int64)\\nXML data can get much more complicated than this example. Each tag can have met-\"),\n",
       " Document(metadata={}, page_content='In [928]: perf\\nOut[928]: \\nEmpty DataFrame\\nColumns: array([], dtype=int64)\\nIndex: array([], dtype=int64)\\nXML data can get much more complicated than this example. Each tag can have met-\\nadata, too. Consider an HTML link tag which is also valid XML:\\nfrom StringIO import StringIO\\ntag = \\'<a href=\"http://www.google.com\">Google</a>\\'\\nroot = objectify.parse(StringIO(tag)).getroot()\\nYou can now access any of the fields (like href) in the tag or the link text:\\nIn [930]: root\\nOut[930]: <Element a at 0x88bd4b0>\\nIn [931]: root.get(\\'href\\')\\nOut[931]: \\'http://www.google.com\\'\\nIn [932]: root.text\\nOut[932]: \\'Google\\'\\n170 | Chapter 6: \\u2002Data Loading, Storage, and File Formats\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"Binary Data Formats\\nOne of the easiest ways to store data efficiently in binary format is using Python’s built-\\nin pickle serialization. Conveniently, pandas objects all have a save method which\\nwrites the data to disk as a pickle:\\nIn [933]: frame = pd.read_csv('ch06/ex1.csv')\\nIn [934]: frame\\nOut[934]: \\n   a   b   c   d message\\n0  1   2   3   4   hello\\n1  5   6   7   8   world\\n2  9  10  11  12     foo\\nIn [935]: frame.save('ch06/frame_pickle')\\nYou read the data back into Python with pandas.load, another pickle convenience\\nfunction:\\nIn [936]: pd.load('ch06/frame_pickle')\\nOut[936]: \\n   a   b   c   d message\\n0  1   2   3   4   hello\\n1  5   6   7   8   world\\n2  9  10  11  12     foo\\npickle is only recommended as a short-term storage format. The prob-\\nlem is that it is hard to guarantee that the format will be stable over time;\\nan object pickled today may not unpickle with a later version of a library.\\nI have made every effort to ensure that this does not occur with pandas,\"),\n",
       " Document(metadata={}, page_content='an object pickled today may not unpickle with a later version of a library.\\nI have made every effort to ensure that this does not occur with pandas,\\nbut at some point in the future it may be necessary to “break” the pickle\\nformat.\\nUsing HDF5 Format\\nThere are a number of tools that facilitate efficiently reading and writing large amounts\\nof scientific data in binary format on disk. A popular industry-grade library for this is\\nHDF5, which is a C library with interfaces in many other languages like Java, Python,\\nand MATLAB. The “HDF” in HDF5 stands for hierarchical data format. Each HDF5\\nfile contains an internal file system-like node structure enabling you to store multiple\\ndatasets and supporting metadata. Compared with simpler formats, HDF5 supports\\non-the-fly compression with a variety of compressors, enabling data with repeated pat-\\nterns to be stored more efficiently. For very large datasets that don’t fit into memory,'),\n",
       " Document(metadata={}, page_content='on-the-fly compression with a variety of compressors, enabling data with repeated pat-\\nterns to be stored more efficiently. For very large datasets that don’t fit into memory,\\nHDF5 is a good choice as you can efficiently read and write small sections of much\\nlarger arrays.\\nThere are not one but two interfaces to the HDF5 library in Python, PyTables and h5py,\\neach of which takes a different approach to the problem. h5py provides a direct, but\\nhigh-level interface to the HDF5 API, while PyTables abstracts many of the details of\\nBinary Data Formats | 171\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"HDF5 to provide multiple flexible data containers, table indexing, querying capability,\\nand some support for out-of-core computations.\\npandas has a minimal dict-like HDFStore class, which uses PyTables to store pandas\\nobjects:\\nIn [937]: store = pd.HDFStore('mydata.h5')\\nIn [938]: store['obj1'] = frame\\nIn [939]: store['obj1_col'] = frame['a']\\nIn [940]: store\\nOut[940]: \\n<class 'pandas.io.pytables.HDFStore'>\\nFile path: mydata.h5\\nobj1         DataFrame\\nobj1_col     Series\\nObjects contained in the HDF5 file can be retrieved in a dict-like fashion:\\nIn [941]: store['obj1']\\nOut[941]: \\n   a   b   c   d message\\n0  1   2   3   4   hello\\n1  5   6   7   8   world\\n2  9  10  11  12     foo\\nIf you work with huge quantities of data, I would encourage you to explore PyTables\\nand h5py to see how they can suit your needs. Since many data analysis problems are\\nIO-bound (rather than CPU-bound), using a tool like HDF5 can massively accelerate\\nyour applications.\"),\n",
       " Document(metadata={}, page_content=\"and h5py to see how they can suit your needs. Since many data analysis problems are\\nIO-bound (rather than CPU-bound), using a tool like HDF5 can massively accelerate\\nyour applications.\\nHDF5 is not a database. It is best suited for write-once, read-many da-\\ntasets. While data can be added to a file at any time, if multiple writers\\ndo so simultaneously, the file can become corrupted.\\nReading Microsoft Excel Files\\npandas also supports reading tabular data stored in Excel 2003 (and higher) files using\\nthe ExcelFile class. Interally ExcelFile uses the xlrd and openpyxl packages, so you\\nmay have to install them first. To use ExcelFile, create an instance by passing a path\\nto an xls or xlsx file:\\nxls_file = pd.ExcelFile('data.xls')\\nData stored in a sheet can then be read into DataFrame using parse:\\ntable = xls_file.parse('Sheet1')\\n172 | Chapter 6: \\u2002Data Loading, Storage, and File Formats\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"Interacting with HTML and Web APIs\\nMany websites have public APIs providing data feeds via JSON or some other format.\\nThere are a number of ways to access these APIs from Python; one easy-to-use method\\nthat I recommend is the requests package ( http://docs.python-requests.org). To search\\nfor the words “python pandas” on Twitter, we can make an HTTP GET request like so:\\nIn [944]: import requests\\nIn [945]: url = 'http://search.twitter.com/search.json?q=python%20pandas'\\nIn [946]: resp = requests.get(url)\\nIn [947]: resp\\nOut[947]: <Response [200]>\\nThe Response object’s text attribute contains the content of the GET query. Many web\\nAPIs will return a JSON string that must be loaded into a Python object:\\nIn [948]: import json\\nIn [949]: data = json.loads(resp.text)\\nIn [950]: data.keys()\\nOut[950]: \\n[u'next_page',\\n u'completed_in',\\n u'max_id_str',\\n u'since_id_str',\\n u'refresh_url',\\n u'results',\\n u'since_id',\\n u'results_per_page',\\n u'query',\\n u'max_id',\\n u'page']\"),\n",
       " Document(metadata={}, page_content='In [950]: data.keys()\\nOut[950]: \\n[u\\'next_page\\',\\n u\\'completed_in\\',\\n u\\'max_id_str\\',\\n u\\'since_id_str\\',\\n u\\'refresh_url\\',\\n u\\'results\\',\\n u\\'since_id\\',\\n u\\'results_per_page\\',\\n u\\'query\\',\\n u\\'max_id\\',\\n u\\'page\\']\\nThe results field in the response contains a list of tweets, each of which is represented\\nas a Python dict that looks like:\\n{u\\'created_at\\': u\\'Mon, 25 Jun 2012 17:50:33 +0000\\',\\n u\\'from_user\\': u\\'wesmckinn\\',\\n u\\'from_user_id\\': 115494880,\\n u\\'from_user_id_str\\': u\\'115494880\\',\\n u\\'from_user_name\\': u\\'Wes McKinney\\',\\n u\\'geo\\': None,\\n u\\'id\\': 217313849177686018,\\n u\\'id_str\\': u\\'217313849177686018\\',\\n u\\'iso_language_code\\': u\\'pt\\',\\n u\\'metadata\\': {u\\'result_type\\': u\\'recent\\'},\\n u\\'source\\': u\\'<a href=\"http://twitter.com/\">web</a>\\',\\n u\\'text\\': u\\'Lunchtime pandas-fu http://t.co/SI70xZZQ #pydata\\',\\n u\\'to_user\\': None,\\n u\\'to_user_id\\': 0,\\nInteracting with HTML and Web APIs | 173\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"u'to_user_id_str': u'0',\\n u'to_user_name': None}\\nWe can then make a list of the tweet fields of interest then pass the results list to Da-\\ntaFrame:\\nIn [951]: tweet_fields = ['created_at', 'from_user', 'id', 'text']\\nIn [952]: tweets = DataFrame(data['results'], columns=tweet_fields)\\nIn [953]: tweets\\nOut[953]: \\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 15 entries, 0 to 14\\nData columns:\\ncreated_at    15  non-null values\\nfrom_user     15  non-null values\\nid            15  non-null values\\ntext          15  non-null values\\ndtypes: int64(1), object(3)\\nEach row in the DataFrame now has the extracted data from each tweet:\\nIn [121]: tweets.ix[7]\\nOut[121]:\\ncreated_at                  Thu, 23 Jul 2012 09:54:00 +0000\\nfrom_user                                           deblike\\nid                                       227419585803059201\\ntext          pandas: powerful Python data analysis toolkit\\nName: 7\\nWith a bit of elbow grease, you can create some higher-level interfaces to common web\"),\n",
       " Document(metadata={}, page_content='text          pandas: powerful Python data analysis toolkit\\nName: 7\\nWith a bit of elbow grease, you can create some higher-level interfaces to common web\\nAPIs that return DataFrame objects for easy analysis.\\nInteracting with Databases\\nIn many applications data rarely comes from text files, that being a fairly inefficient\\nway to store large amounts of data. SQL-based relational databases (such as SQL Server,\\nPostgreSQL, and MySQL) are in wide use, and many alternative non-SQL (so-called\\nNoSQL) databases have become quite popular. The choice of database is usually de-\\npendent on the performance, data integrity, and scalability needs of an application.\\nLoading data from SQL into a DataFrame is fairly straightforward, and pandas has\\nsome functions to simplify the process. As an example, I’ll use an in-memory SQLite\\ndatabase using Python’s built-in sqlite3 driver:\\nimport sqlite3\\nquery = \"\"\"\\nCREATE TABLE test\\n(a VARCHAR(20), b VARCHAR(20),\\n c REAL,        d INTEGER\\n);\"\"\"'),\n",
       " Document(metadata={}, page_content='database using Python’s built-in sqlite3 driver:\\nimport sqlite3\\nquery = \"\"\"\\nCREATE TABLE test\\n(a VARCHAR(20), b VARCHAR(20),\\n c REAL,        d INTEGER\\n);\"\"\"\\n174 | Chapter 6: \\u2002Data Loading, Storage, and File Formats\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='con = sqlite3.connect(\\':memory:\\')\\ncon.execute(query)\\ncon.commit()\\nThen, insert a few rows of data:\\ndata = [(\\'Atlanta\\', \\'Georgia\\', 1.25, 6),\\n        (\\'Tallahassee\\', \\'Florida\\', 2.6, 3),\\n        (\\'Sacramento\\', \\'California\\', 1.7, 5)]\\nstmt = \"INSERT INTO test VALUES(?, ?, ?, ?)\"\\ncon.executemany(stmt, data)\\ncon.commit()\\nMost Python SQL drivers (PyODBC, psycopg2, MySQLdb, pymssql, etc.) return a list\\nof tuples when selecting data from a table:\\nIn [956]: cursor = con.execute(\\'select * from test\\')\\nIn [957]: rows = cursor.fetchall()\\nIn [958]: rows\\nOut[958]: \\n[(u\\'Atlanta\\', u\\'Georgia\\', 1.25, 6),\\n (u\\'Tallahassee\\', u\\'Florida\\', 2.6, 3),\\n (u\\'Sacramento\\', u\\'California\\', 1.7, 5)]\\nYou can pass the list of tuples to the DataFrame constructor, but you also need the\\ncolumn names, contained in the cursor’s description attribute:\\nIn [959]: cursor.description\\nOut[959]: \\n((\\'a\\', None, None, None, None, None, None),\\n (\\'b\\', None, None, None, None, None, None),\\n (\\'c\\', None, None, None, None, None, None),'),\n",
       " Document(metadata={}, page_content=\"In [959]: cursor.description\\nOut[959]: \\n(('a', None, None, None, None, None, None),\\n ('b', None, None, None, None, None, None),\\n ('c', None, None, None, None, None, None),\\n ('d', None, None, None, None, None, None))\\nIn [960]: DataFrame(rows, columns=zip(*cursor.description)[0])\\nOut[960]: \\n             a           b     c  d\\n0      Atlanta     Georgia  1.25  6\\n1  Tallahassee     Florida  2.60  3\\n2   Sacramento  California  1.70  5\\nThis is quite a bit of munging that you’d rather not repeat each time you query the\\ndatabase. pandas has a read_frame function in its pandas.io.sql module that simplifies\\nthe process. Just pass the select statement and the connection object:\\nIn [961]: import pandas.io.sql as sql\\nIn [962]: sql.read_frame('select * from test', con)\\nOut[962]: \\n             a           b     c  d\\n0      Atlanta     Georgia  1.25  6\\n1  Tallahassee     Florida  2.60  3\\n2   Sacramento  California  1.70  5\\nInteracting with Databases | 175\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"Storing and Loading Data in MongoDB\\nNoSQL databases take many different forms. Some are simple dict-like key-value stores\\nlike BerkeleyDB or Tokyo Cabinet, while others are document-based, with a dict-like\\nobject being the basic unit of storage. I've chosen MongoDB ( http://mongodb.org) for\\nmy example. I started a MongoDB instance locally on my machine, and connect to it\\non the default port using pymongo, the official driver for MongoDB:\\nimport pymongo\\ncon = pymongo.Connection('localhost', port=27017)\\nDocuments stored in MongoDB are found in collections inside databases. Each running\\ninstance of the MongoDB server can have multiple databases, and each database can\\nhave multiple collections. Suppose I wanted to store the Twitter API data from earlier\\nin the chapter. First, I can access the (currently empty) tweets collection:\\ntweets = con.db.tweets\\nThen, I load the list of tweets and write each of them to the collection using\\ntweets.save (which writes the Python dict to MongoDB):\"),\n",
       " Document(metadata={}, page_content=\"tweets = con.db.tweets\\nThen, I load the list of tweets and write each of them to the collection using\\ntweets.save (which writes the Python dict to MongoDB):\\nimport requests, json\\nurl = 'http://search.twitter.com/search.json?q=python%20pandas'\\ndata = json.loads(requests.get(url).text)\\nfor tweet in data['results']:\\n    tweets.save(tweet)\\nNow, if I wanted to get all of my tweets (if any) from the collection, I can query the\\ncollection with the following syntax:\\ncursor = tweets.find({'from_user': 'wesmckinn'})\\nThe cursor returned is an iterator that yields each document as a dict. As above I can\\nconvert this into a DataFrame, optionally extracting a subset of the data fields in each\\ntweet:\\ntweet_fields = ['created_at', 'from_user', 'id', 'text']\\nresult = DataFrame(list(cursor), columns=tweet_fields)\\n176 | Chapter 6: \\u2002Data Loading, Storage, and File Formats\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content='CHAPTER 7\\nData Wrangling: Clean, Transform,\\nMerge, Reshape\\nMuch of the programming work in data analysis and modeling is spent on data prep-\\naration: loading, cleaning, transforming, and rearranging. Sometimes the way that data\\nis stored in files or databases is not the way you need it for a data processing application.\\nMany people choose to do ad hoc processing of data from one form to another using\\na general purpose programming, like Python, Perl, R, or Java, or UNIX text processing\\ntools like sed or awk. Fortunately, pandas along with the Python standard library pro-\\nvide you with a high-level, flexible, and high-performance set of core manipulations\\nand algorithms to enable you to wrangle data into the right form without much trouble.\\nIf you identify a type of data manipulation that isn’t anywhere in this book or elsewhere\\nin the pandas library, feel free to suggest it on the mailing list or GitHub site. Indeed,'),\n",
       " Document(metadata={}, page_content='If you identify a type of data manipulation that isn’t anywhere in this book or elsewhere\\nin the pandas library, feel free to suggest it on the mailing list or GitHub site. Indeed,\\nmuch of the design and implementation of pandas has been driven by the needs of real\\nworld applications.\\nCombining and Merging Data Sets\\nData contained in pandas objects can be combined together in a number of built-in\\nways:\\n• pandas.merge connects rows in DataFrames based on one or more keys. This will\\nbe familiar to users of SQL or other relational databases, as it implements database\\njoin operations.\\n• pandas.concat glues or stacks together objects along an axis.\\n• combine_first instance method enables splicing together overlapping data to fill\\nin missing values in one object with values from another.\\nI will address each of these and give a number of examples. They’ll be utilized in ex-\\namples throughout the rest of the book.\\n177\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"Database-style DataFrame Merges\\nMerge or join operations combine data sets by linking rows using one or more keys.\\nThese operations are central to relational databases. The merge function in pandas is\\nthe main entry point for using these algorithms on your data.\\nLet’s start with a simple example:\\nIn [15]: df1 = DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'a', 'b'],\\n   ....:                  'data1': range(7)})\\nIn [16]: df2 = DataFrame({'key': ['a', 'b', 'd'],\\n   ....:                  'data2': range(3)})\\nIn [17]: df1        In [18]: df2\\nOut[17]:            Out[18]:\\n   data1 key           data2 key\\n0      0   b        0      0   a\\n1      1   b        1      1   b\\n2      2   a        2      2   d\\n3      3   c\\n4      4   a\\n5      5   a\\n6      6   b\\nThis is an example of a many-to-one merge situation; the data in df1 has multiple rows\\nlabeled a and b, whereas df2 has only one row for each value in the key column. Calling\\nmerge with these objects we obtain:\\nIn [19]: pd.merge(df1, df2)\"),\n",
       " Document(metadata={}, page_content=\"labeled a and b, whereas df2 has only one row for each value in the key column. Calling\\nmerge with these objects we obtain:\\nIn [19]: pd.merge(df1, df2)\\nOut[19]:\\n   data1 key  data2\\n0      2   a      0\\n1      4   a      0\\n2      5   a      0\\n3      0   b      1\\n4      1   b      1\\n5      6   b      1\\nNote that I didn’t specify which column to join on. If not specified, merge uses the\\noverlapping column names as the keys. It’s a good practice to specify explicitly, though:\\nIn [20]: pd.merge(df1, df2, on='key')\\nOut[20]:\\n   data1 key  data2\\n0      2   a      0\\n1      4   a      0\\n2      5   a      0\\n3      0   b      1\\n4      1   b      1\\n5      6   b      1\\nIf the column names are different in each object, you can specify them separately:\\nIn [21]: df3 = DataFrame({'lkey': ['b', 'b', 'a', 'c', 'a', 'a', 'b'],\\n   ....:                  'data1': range(7)})\\n178 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"In [22]: df4 = DataFrame({'rkey': ['a', 'b', 'd'],\\n   ....:                  'data2': range(3)})\\nIn [23]: pd.merge(df3, df4, left_on='lkey', right_on='rkey')\\nOut[23]:\\n   data1 lkey  data2 rkey\\n0      2    a      0    a\\n1      4    a      0    a\\n2      5    a      0    a\\n3      0    b      1    b\\n4      1    b      1    b\\n5      6    b      1    b\\nYou probably noticed that the 'c' and 'd' values and associated data are missing from\\nthe result. By default merge does an 'inner' join; the keys in the result are the intersec-\\ntion. Other possible options are 'left', 'right', and 'outer'. The outer join takes the\\nunion of the keys, combining the effect of applying both left and right joins:\\nIn [24]: pd.merge(df1, df2, how='outer')\\nOut[24]:\\n   data1 key  data2\\n0      2   a      0\\n1      4   a      0\\n2      5   a      0\\n3      0   b      1\\n4      1   b      1\\n5      6   b      1\\n6      3   c    NaN\\n7    NaN   d      2\"),\n",
       " Document(metadata={}, page_content=\"Out[24]:\\n   data1 key  data2\\n0      2   a      0\\n1      4   a      0\\n2      5   a      0\\n3      0   b      1\\n4      1   b      1\\n5      6   b      1\\n6      3   c    NaN\\n7    NaN   d      2\\nMany-to-many merges have well-defined though not necessarily intuitive behavior.\\nHere’s an example:\\nIn [25]: df1 = DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'b'],\\n   ....:                  'data1': range(6)})\\nIn [26]: df2 = DataFrame({'key': ['a', 'b', 'a', 'b', 'd'],\\n   ....:                  'data2': range(5)})\\nIn [27]: df1        In [28]: df2\\nOut[27]:            Out[28]:\\n   data1 key           data2 key\\n0      0   b        0      0   a\\n1      1   b        1      1   b\\n2      2   a        2      2   a\\n3      3   c        3      3   b\\n4      4   a        4      4   d\\n5      5   b\\nIn [29]: pd.merge(df1, df2, on='key', how='left')\\nOut[29]:\\n    data1 key  data2\\n0       2   a      0\\n1       2   a      2\\nCombining and Merging Data Sets | 179\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"2       4   a      0\\n3       4   a      2\\n4       0   b      1\\n5       0   b      3\\n6       1   b      1\\n7       1   b      3\\n8       5   b      1\\n9       5   b      3\\n10      3   c    NaN\\nMany-to-many joins form the Cartesian product of the rows. Since there were 3 'b'\\nrows in the left DataFrame and 2 in the right one, there are 6 'b' rows in the result.\\nThe join method only affects the distinct key values appearing in the result:\\nIn [30]: pd.merge(df1, df2, how='inner')\\nOut[30]:\\n   data1 key  data2\\n0      2   a      0\\n1      2   a      2\\n2      4   a      0\\n3      4   a      2\\n4      0   b      1\\n5      0   b      3\\n6      1   b      1\\n7      1   b      3\\n8      5   b      1\\n9      5   b      3\\nTo merge with multiple keys, pass a list of column names:\\nIn [31]: left = DataFrame({'key1': ['foo', 'foo', 'bar'],\\n   ....:                   'key2': ['one', 'two', 'one'],\\n   ....:                   'lval': [1, 2, 3]})\\nIn [32]: right = DataFrame({'key1': ['foo', 'foo', 'bar', 'bar'],\"),\n",
       " Document(metadata={}, page_content=\"....:                   'key2': ['one', 'two', 'one'],\\n   ....:                   'lval': [1, 2, 3]})\\nIn [32]: right = DataFrame({'key1': ['foo', 'foo', 'bar', 'bar'],\\n   ....:                    'key2': ['one', 'one', 'one', 'two'],\\n   ....:                    'rval': [4, 5, 6, 7]})\\nIn [33]: pd.merge(left, right, on=['key1', 'key2'], how='outer')\\nOut[33]:\\n  key1 key2  lval  rval\\n0  bar  one     3     6\\n1  bar  two   NaN     7\\n2  foo  one     1     4\\n3  foo  one     1     5\\n4  foo  two     2   NaN\\nTo determine which key combinations will appear in the result depending on the choice\\nof merge method, think of the multiple keys as forming an array of tuples to be used\\nas a single join key (even though it’s not actually implemented that way).\\nWhen joining columns-on-columns, the indexes on the passed Data-\\nFrame objects are discarded.\\n180 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"A last issue to consider in merge operations is the treatment of overlapping column\\nnames. While you can address the overlap manually (see the later section on renaming\\naxis labels), merge has a suffixes option for specifying strings to append to overlapping\\nnames in the left and right DataFrame objects:\\nIn [34]: pd.merge(left, right, on='key1')\\nOut[34]:\\n  key1 key2_x  lval key2_y  rval\\n0  bar    one     3    one     6\\n1  bar    one     3    two     7\\n2  foo    one     1    one     4\\n3  foo    one     1    one     5\\n4  foo    two     2    one     4\\n5  foo    two     2    one     5\\nIn [35]: pd.merge(left, right, on='key1', suffixes=('_left', '_right'))\\nOut[35]:\\n  key1 key2_left  lval key2_right  rval\\n0  bar       one     3        one     6\\n1  bar       one     3        two     7\\n2  foo       one     1        one     4\\n3  foo       one     1        one     5\\n4  foo       two     2        one     4\\n5  foo       two     2        one     5\"),\n",
       " Document(metadata={}, page_content=\"1  bar       one     3        two     7\\n2  foo       one     1        one     4\\n3  foo       one     1        one     5\\n4  foo       two     2        one     4\\n5  foo       two     2        one     5\\nSee Table 7-1 for an argument reference on merge. Joining on index is the subject of the\\nnext section.\\nTable 7-1. merge function arguments\\nArgument Description\\nleft DataFrame to be merged on the left side\\nright DataFrame to be merged on the right side\\nhow One of 'inner', 'outer', 'left' or 'right'. 'inner' by default\\non Column names to join on. Must be found in both DataFrame objects. If not specified and no other join keys\\ngiven, will use the intersection of the column names in left and right as the join keys\\nleft_on Columns in left DataFrame to use as join keys\\nright_on Analogous to left_on for left DataFrame\\nleft_index Use row index in left as its join key (or keys, if a MultiIndex)\\nright_index Analogous to left_index\"),\n",
       " Document(metadata={}, page_content=\"right_on Analogous to left_on for left DataFrame\\nleft_index Use row index in left as its join key (or keys, if a MultiIndex)\\nright_index Analogous to left_index\\nsort Sort merged data lexicographically by join keys; True by default. Disable to get better performance in some\\ncases on large datasets\\nsuffixes Tuple of string values to append to column names in case of overlap; defaults to ('_x', '_y'). For\\nexample, if 'data' in both DataFrame objects, would appear as 'data_x' and 'data_y' in result\\ncopy If False, avoid copying data into resulting data structure in some exceptional cases. By default always copies\\nCombining and Merging Data Sets | 181\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"Merging on Index\\nIn some cases, the merge key or keys in a DataFrame will be found in its index. In this\\ncase, you can pass left_index=True or right_index=True (or both) to indicate that the\\nindex should be used as the merge key:\\nIn [36]: left1 = DataFrame({'key': ['a', 'b', 'a', 'a', 'b', 'c'],\\n   ....:                   'value': range(6)})\\nIn [37]: right1 = DataFrame({'group_val': [3.5, 7]}, index=['a', 'b'])\\nIn [38]: left1        In [39]: right1\\nOut[38]:              Out[39]:\\n  key  value             group_val\\n0   a      0          a        3.5\\n1   b      1          b        7.0\\n2   a      2\\n3   a      3\\n4   b      4\\n5   c      5\\nIn [40]: pd.merge(left1, right1, left_on='key', right_index=True)\\nOut[40]:\\n  key  value  group_val\\n0   a      0        3.5\\n2   a      2        3.5\\n3   a      3        3.5\\n1   b      1        7.0\\n4   b      4        7.0\\nSince the default merge method is to intersect the join keys, you can instead form the\\nunion of them with an outer join:\"),\n",
       " Document(metadata={}, page_content=\"3   a      3        3.5\\n1   b      1        7.0\\n4   b      4        7.0\\nSince the default merge method is to intersect the join keys, you can instead form the\\nunion of them with an outer join:\\nIn [41]: pd.merge(left1, right1, left_on='key', right_index=True, how='outer')\\nOut[41]:\\n  key  value  group_val\\n0   a      0        3.5\\n2   a      2        3.5\\n3   a      3        3.5\\n1   b      1        7.0\\n4   b      4        7.0\\n5   c      5        NaN\\nWith hierarchically-indexed data, things are a bit more complicated:\\nIn [42]: lefth = DataFrame({'key1': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada'],\\n   ....:                    'key2': [2000, 2001, 2002, 2001, 2002],\\n   ....:                    'data': np.arange(5.)})\\nIn [43]: righth = DataFrame(np.arange(12).reshape((6, 2)),\\n   ....:                    index=[['Nevada', 'Nevada', 'Ohio', 'Ohio', 'Ohio', 'Ohio'],\\n   ....:                           [2001, 2000, 2000, 2000, 2001, 2002]],\\n   ....:                    columns=['event1', 'event2'])\"),\n",
       " Document(metadata={}, page_content=\"....:                           [2001, 2000, 2000, 2000, 2001, 2002]],\\n   ....:                    columns=['event1', 'event2'])\\nIn [44]: lefth               In [45]: righth\\nOut[44]:                     Out[45]:\\n182 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"data    key1  key2                     event1  event2\\n0     0    Ohio  2000        Nevada 2001       0       1\\n1     1    Ohio  2001               2000       2       3\\n2     2    Ohio  2002        Ohio   2000       4       5\\n3     3  Nevada  2001               2000       6       7\\n4     4  Nevada  2002               2001       8       9\\n                                    2002      10      11\\nIn this case, you have to indicate multiple columns to merge on as a list (pay attention\\nto the handling of duplicate index values):\\nIn [46]: pd.merge(lefth, righth, left_on=['key1', 'key2'], right_index=True)\\nOut[46]:\\n   data    key1  key2  event1  event2\\n3     3  Nevada  2001       0       1\\n0     0    Ohio  2000       4       5\\n0     0    Ohio  2000       6       7\\n1     1    Ohio  2001       8       9\\n2     2    Ohio  2002      10      11\\nIn [47]: pd.merge(lefth, righth, left_on=['key1', 'key2'],\\n   ....:          right_index=True, how='outer')\\nOut[47]:\\n   data    key1  key2  event1  event2\"),\n",
       " Document(metadata={}, page_content=\"2     2    Ohio  2002      10      11\\nIn [47]: pd.merge(lefth, righth, left_on=['key1', 'key2'],\\n   ....:          right_index=True, how='outer')\\nOut[47]:\\n   data    key1  key2  event1  event2\\n4   NaN  Nevada  2000       2       3\\n3     3  Nevada  2001       0       1\\n4     4  Nevada  2002     NaN     NaN\\n0     0    Ohio  2000       4       5\\n0     0    Ohio  2000       6       7\\n1     1    Ohio  2001       8       9\\n2     2    Ohio  2002      10      11\\nUsing the indexes of both sides of the merge is also not an issue:\\nIn [48]: left2 = DataFrame([[1., 2.], [3., 4.], [5., 6.]], index=['a', 'c', 'e'],\\n   ....:                  columns=['Ohio', 'Nevada'])\\nIn [49]: right2 = DataFrame([[7., 8.], [9., 10.], [11., 12.], [13, 14]],\\n   ....:                    index=['b', 'c', 'd', 'e'], columns=['Missouri', 'Alabama'])\\nIn [50]: left2         In [51]: right2\\nOut[50]:               Out[51]:\\n   Ohio  Nevada           Missouri  Alabama\\na     1       2        b         7        8\"),\n",
       " Document(metadata={}, page_content=\"In [50]: left2         In [51]: right2\\nOut[50]:               Out[51]:\\n   Ohio  Nevada           Missouri  Alabama\\na     1       2        b         7        8\\nc     3       4        c         9       10\\ne     5       6        d        11       12\\n                       e        13       14\\nIn [52]: pd.merge(left2, right2, how='outer', left_index=True, right_index=True)\\nOut[52]:\\n   Ohio  Nevada  Missouri  Alabama\\na     1       2       NaN      NaN\\nb   NaN     NaN         7        8\\nc     3       4         9       10\\nd   NaN     NaN        11       12\\ne     5       6        13       14\\nCombining and Merging Data Sets | 183\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"DataFrame has a more convenient join instance for merging by index. It can also be\\nused to combine together many DataFrame objects having the same or similar indexes\\nbut non-overlapping columns. In the prior example, we could have written:\\nIn [53]: left2.join(right2, how='outer')\\nOut[53]:\\n   Ohio  Nevada  Missouri  Alabama\\na     1       2       NaN      NaN\\nb   NaN     NaN         7        8\\nc     3       4         9       10\\nd   NaN     NaN        11       12\\ne     5       6        13       14\\nIn part for legacy reasons (much earlier versions of pandas), DataFrame’s join method\\nperforms a left join on the join keys. It also supports joining the index of the passed\\nDataFrame on one of the columns of the calling DataFrame:\\nIn [54]: left1.join(right1, on='key')\\nOut[54]:\\n  key  value  group_val\\n0   a      0        3.5\\n1   b      1        7.0\\n2   a      2        3.5\\n3   a      3        3.5\\n4   b      4        7.0\\n5   c      5        NaN\"),\n",
       " Document(metadata={}, page_content=\"Out[54]:\\n  key  value  group_val\\n0   a      0        3.5\\n1   b      1        7.0\\n2   a      2        3.5\\n3   a      3        3.5\\n4   b      4        7.0\\n5   c      5        NaN\\nLastly, for simple index-on-index merges, you can pass a list of DataFrames to join as\\nan alternative to using the more general concat function described below:\\nIn [55]: another = DataFrame([[7., 8.], [9., 10.], [11., 12.], [16., 17.]],\\n   ....:                     index=['a', 'c', 'e', 'f'], columns=['New York', 'Oregon'])\\nIn [56]: left2.join([right2, another])\\nOut[56]:\\n   Ohio  Nevada  Missouri  Alabama  New York  Oregon\\na     1       2       NaN      NaN         7       8\\nc     3       4         9       10         9      10\\ne     5       6        13       14        11      12\\nIn [57]: left2.join([right2, another], how='outer')\\nOut[57]:\\n   Ohio  Nevada  Missouri  Alabama  New York  Oregon\\na     1       2       NaN      NaN         7       8\\nb   NaN     NaN         7        8       NaN     NaN\"),\n",
       " Document(metadata={}, page_content='Out[57]:\\n   Ohio  Nevada  Missouri  Alabama  New York  Oregon\\na     1       2       NaN      NaN         7       8\\nb   NaN     NaN         7        8       NaN     NaN\\nc     3       4         9       10         9      10\\nd   NaN     NaN        11       12       NaN     NaN\\ne     5       6        13       14        11      12\\nf   NaN     NaN       NaN      NaN        16      17\\n184 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Concatenating Along an Axis\\nAnother kind of data combination operation is alternatively referred to as concatena-\\ntion, binding, or stacking. NumPy has a concatenate function for doing this with raw\\nNumPy arrays:\\nIn [58]: arr = np.arange(12).reshape((3, 4))\\nIn [59]: arr\\nOut[59]:\\narray([[ 0,  1,  2,  3],\\n       [ 4,  5,  6,  7],\\n       [ 8,  9, 10, 11]])\\nIn [60]: np.concatenate([arr, arr], axis=1)\\nOut[60]:\\narray([[ 0,  1,  2,  3,  0,  1,  2,  3],\\n       [ 4,  5,  6,  7,  4,  5,  6,  7],\\n       [ 8,  9, 10, 11,  8,  9, 10, 11]])\\nIn the context of pandas objects such as Series and DataFrame, having labeled axes\\nenable you to further generalize array concatenation. In particular, you have a number\\nof additional things to think about:\\n• If the objects are indexed differently on the other axes, should the collection of\\naxes be unioned or intersected?\\n• Do the groups need to be identifiable in the resulting object?\\n• Does the concatenation axis matter at all?'),\n",
       " Document(metadata={}, page_content=\"axes be unioned or intersected?\\n• Do the groups need to be identifiable in the resulting object?\\n• Does the concatenation axis matter at all?\\nThe concat function in pandas provides a consistent way to address each of these con-\\ncerns. I’ll give a number of examples to illustrate how it works. Suppose we have three\\nSeries with no index overlap:\\nIn [61]: s1 = Series([0, 1], index=['a', 'b'])\\nIn [62]: s2 = Series([2, 3, 4], index=['c', 'd', 'e'])\\nIn [63]: s3 = Series([5, 6], index=['f', 'g'])\\nCalling concat with these object in a list glues together the values and indexes:\\nIn [64]: pd.concat([s1, s2, s3])\\nOut[64]:\\na    0\\nb    1\\nc    2\\nd    3\\ne    4\\nf    5\\ng    6\\nCombining and Merging Data Sets | 185\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"By default concat works along axis=0, producing another Series. If you pass axis=1, the\\nresult will instead be a DataFrame (axis=1 is the columns):\\nIn [65]: pd.concat([s1, s2, s3], axis=1)\\nOut[65]:\\n    0   1   2\\na   0 NaN NaN\\nb   1 NaN NaN\\nc NaN   2 NaN\\nd NaN   3 NaN\\ne NaN   4 NaN\\nf NaN NaN   5\\ng NaN NaN   6\\nIn this case there is no overlap on the other axis, which as you can see is the sorted\\nunion (the 'outer' join) of the indexes. You can instead intersect them by passing\\njoin='inner':\\nIn [66]: s4 = pd.concat([s1 * 5, s3])\\nIn [67]: pd.concat([s1, s4], axis=1)      In [68]: pd.concat([s1, s4], axis=1, join='inner')\\nOut[67]:                                  Out[68]:\\n    0  1                                     0  1\\na   0  0                                  a  0  0\\nb   1  5                                  b  1  5\\nf NaN  5\\ng NaN  6\\nYou can even specify the axes to be used on the other axes with join_axes:\\nIn [69]: pd.concat([s1, s4], axis=1, join_axes=[['a', 'c', 'b', 'e']])\\nOut[69]:\"),\n",
       " Document(metadata={}, page_content=\"f NaN  5\\ng NaN  6\\nYou can even specify the axes to be used on the other axes with join_axes:\\nIn [69]: pd.concat([s1, s4], axis=1, join_axes=[['a', 'c', 'b', 'e']])\\nOut[69]:\\n    0   1\\na   0   0\\nc NaN NaN\\nb   1   5\\ne NaN NaN\\nOne issue is that the concatenated pieces are not identifiable in the result. Suppose\\ninstead you wanted to create a hierarchical index on the concatenation axis. To do this,\\nuse the keys argument:\\nIn [70]: result = pd.concat([s1, s1, s3], keys=['one', 'two', 'three'])\\nIn [71]: result\\nOut[71]:\\none    a    0\\n       b    1\\ntwo    a    0\\n       b    1\\nthree  f    5\\n       g    6\\n# Much more on the unstack function later\\nIn [72]: result.unstack()\\nOut[72]:\\n186 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"a   b   f   g\\none     0   1 NaN NaN\\ntwo     0   1 NaN NaN\\nthree NaN NaN   5   6\\nIn the case of combining Series along axis=1, the keys become the DataFrame column\\nheaders:\\nIn [73]: pd.concat([s1, s2, s3], axis=1, keys=['one', 'two', 'three'])\\nOut[73]:\\n   one  two  three\\na    0  NaN    NaN\\nb    1  NaN    NaN\\nc  NaN    2    NaN\\nd  NaN    3    NaN\\ne  NaN    4    NaN\\nf  NaN  NaN      5\\ng  NaN  NaN      6\\nThe same logic extends to DataFrame objects:\\nIn [74]: df1 = DataFrame(np.arange(6).reshape(3, 2), index=['a', 'b', 'c'],\\n   ....:                 columns=['one', 'two'])\\nIn [75]: df2 = DataFrame(5 + np.arange(4).reshape(2, 2), index=['a', 'c'],\\n   ....:                 columns=['three', 'four'])\\nIn [76]: pd.concat([df1, df2], axis=1, keys=['level1', 'level2'])\\nOut[76]:\\n   level1       level2\\n      one  two   three  four\\na       0    1       5     6\\nb       2    3     NaN   NaN\\nc       4    5       7     8\"),\n",
       " Document(metadata={}, page_content=\"Out[76]:\\n   level1       level2\\n      one  two   three  four\\na       0    1       5     6\\nb       2    3     NaN   NaN\\nc       4    5       7     8\\nIf you pass a dict of objects instead of a list, the dict’s keys will be used for the keys\\noption:\\nIn [77]: pd.concat({'level1': df1, 'level2': df2}, axis=1)\\nOut[77]:\\n   level1       level2\\n      one  two   three  four\\na       0    1       5     6\\nb       2    3     NaN   NaN\\nc       4    5       7     8\\nThere are a couple of additional arguments governing how the hierarchical index is\\ncreated (see Table 7-2):\\nIn [78]: pd.concat([df1, df2], axis=1, keys=['level1', 'level2'],\\n   ....:           names=['upper', 'lower'])\\nOut[78]:\\nupper  level1       level2\\nlower     one  two   three  four\\na           0    1       5     6\\nb           2    3     NaN   NaN\\nc           4    5       7     8\\nCombining and Merging Data Sets | 187\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"A last consideration concerns DataFrames in which the row index is not meaningful in\\nthe context of the analysis:\\nIn [79]: df1 = DataFrame(np.random.randn(3, 4), columns=['a', 'b', 'c', 'd'])\\nIn [80]: df2 = DataFrame(np.random.randn(2, 3), columns=['b', 'd', 'a'])\\nIn [81]: df1                                     In [82]: df2\\nOut[81]:                                         Out[82]:\\n          a         b         c         d                  b         d         a\\n0 -0.204708  0.478943 -0.519439 -0.555730        0  0.274992  0.228913  1.352917\\n1  1.965781  1.393406  0.092908  0.281746        1  0.886429 -2.001637 -0.371843\\n2  0.769023  1.246435  1.007189 -1.296221\\nIn this case, you can pass ignore_index=True:\\nIn [83]: pd.concat([df1, df2], ignore_index=True)\\nOut[83]:\\n          a         b         c         d\\n0 -0.204708  0.478943 -0.519439 -0.555730\\n1  1.965781  1.393406  0.092908  0.281746\\n2  0.769023  1.246435  1.007189 -1.296221\\n3  1.352917  0.274992       NaN  0.228913\"),\n",
       " Document(metadata={}, page_content=\"0 -0.204708  0.478943 -0.519439 -0.555730\\n1  1.965781  1.393406  0.092908  0.281746\\n2  0.769023  1.246435  1.007189 -1.296221\\n3  1.352917  0.274992       NaN  0.228913\\n4 -0.371843  0.886429       NaN -2.001637\\nTable 7-2. concat function arguments\\nArgument Description\\nobjs List or dict of pandas objects to be concatenated. The only required argument\\naxis Axis to concatenate along; defaults to 0\\njoin One of 'inner', 'outer', defaulting to 'outer'; whether to intersection (inner) or union\\n(outer) together indexes along the other axes\\njoin_axes Specific indexes to use for the other n-1 axes instead of performing union/intersection logic\\nkeys Values to associate with objects being concatenated, forming a hierarchical index along the\\nconcatenation axis. Can either be a list or array of arbitrary values, an array of tuples, or a list of\\narrays (if multiple level arrays passed in levels)\\nlevels Specific indexes to use as hierarchical index level or levels if keys passed\"),\n",
       " Document(metadata={}, page_content='arrays (if multiple level arrays passed in levels)\\nlevels Specific indexes to use as hierarchical index level or levels if keys passed\\nnames Names for created hierarchical levels if keys and / or levels passed\\nverify_integrity Check new axis in concatenated object for duplicates and raise exception if so. By default\\n(False) allows duplicates\\nignore_index Do not preserve indexes along concatenation axis, instead producing a new\\nrange(total_length) index\\nCombining Data with Overlap\\nAnother data combination situation can’t be expressed as either a merge or concate-\\nnation operation. You may have two datasets whose indexes overlap in full or part. As\\na motivating example, consider NumPy’s where function, which expressed a vectorized\\nif-else:\\n188 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"In [84]: a = Series([np.nan, 2.5, np.nan, 3.5, 4.5, np.nan],\\n   ....:            index=['f', 'e', 'd', 'c', 'b', 'a'])\\nIn [85]: b = Series(np.arange(len(a), dtype=np.float64),\\n   ....:            index=['f', 'e', 'd', 'c', 'b', 'a'])\\nIn [86]: b[-1] = np.nan\\nIn [87]: a        In [88]: b        In [89]: np.where(pd.isnull(a), b, a)\\nOut[87]:          Out[88]:          Out[89]:\\nf    NaN          f     0           f    0.0\\ne    2.5          e     1           e    2.5\\nd    NaN          d     2           d    2.0\\nc    3.5          c     3           c    3.5\\nb    4.5          b     4           b    4.5\\na    NaN          a   NaN           a    NaN\\nSeries has a combine_first method, which performs the equivalent of this operation\\nplus data alignment:\\nIn [90]: b[:-2].combine_first(a[2:])\\nOut[90]:\\na    NaN\\nb    4.5\\nc    3.0\\nd    2.0\\ne    1.0\\nf    0.0\\nWith DataFrames, combine_first naturally does the same thing column by column, so\"),\n",
       " Document(metadata={}, page_content=\"In [90]: b[:-2].combine_first(a[2:])\\nOut[90]:\\na    NaN\\nb    4.5\\nc    3.0\\nd    2.0\\ne    1.0\\nf    0.0\\nWith DataFrames, combine_first naturally does the same thing column by column, so\\nyou can think of it as “patching” missing data in the calling object with data from the\\nobject you pass:\\nIn [91]: df1 = DataFrame({'a': [1., np.nan, 5., np.nan],\\n   ....:                  'b': [np.nan, 2., np.nan, 6.],\\n   ....:                  'c': range(2, 18, 4)})\\nIn [92]: df2 = DataFrame({'a': [5., 4., np.nan, 3., 7.],\\n   ....:                  'b': [np.nan, 3., 4., 6., 8.]})\\nIn [93]: df1.combine_first(df2)\\nOut[93]:\\n   a   b   c\\n0  1 NaN   2\\n1  4   2   6\\n2  5   4  10\\n3  3   6  14\\n4  7   8 NaN\\nReshaping and Pivoting\\nThere are a number of fundamental operations for rearranging tabular data. These are\\nalternatingly referred to as reshape or pivot operations.\\nReshaping and Pivoting | 189\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"Reshaping with Hierarchical Indexing\\nHierarchical indexing provides a consistent way to rearrange data in a DataFrame.\\nThere are two primary actions:\\n• stack: this “rotates” or pivots from the columns in the data to the rows\\n• unstack: this pivots from the rows into the columns\\nI’ll illustrate these operations through a series of examples. Consider a small DataFrame\\nwith string arrays as row and column indexes:\\nIn [94]: data = DataFrame(np.arange(6).reshape((2, 3)),\\n   ....:                  index=pd.Index(['Ohio', 'Colorado'], name='state'),\\n   ....:                  columns=pd.Index(['one', 'two', 'three'], name='number'))\\nIn [95]: data\\nOut[95]:\\nnumber    one  two  three\\nstate\\nOhio        0    1      2\\nColorado    3    4      5\\nUsing the stack method on this data pivots the columns into the rows, producing a\\nSeries:\\nIn [96]: result = data.stack()\\nIn [97]: result\\nOut[97]:\\nstate     number\\nOhio      one       0\\n          two       1\\n          three     2\\nColorado  one       3\"),\n",
       " Document(metadata={}, page_content=\"Series:\\nIn [96]: result = data.stack()\\nIn [97]: result\\nOut[97]:\\nstate     number\\nOhio      one       0\\n          two       1\\n          three     2\\nColorado  one       3\\n          two       4\\n          three     5\\nFrom a hierarchically-indexed Series, you can rearrange the data back into a DataFrame\\nwith unstack:\\nIn [98]: result.unstack()\\nOut[98]:\\nnumber    one  two  three\\nstate\\nOhio        0    1      2\\nColorado    3    4      5\\nBy default the innermost level is unstacked (same with stack). You can unstack a dif-\\nferent level by passing a level number or name:\\nIn [99]: result.unstack(0)        In [100]: result.unstack('state')\\nOut[99]:                          Out[100]:\\nstate   Ohio  Colorado            state   Ohio  Colorado\\nnumber                            number\\none        0         3            one        0         3\\n190 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"two        1         4            two        1         4\\nthree      2         5            three      2         5\\nUnstacking might introduce missing data if all of the values in the level aren’t found in\\neach of the subgroups:\\nIn [101]: s1 = Series([0, 1, 2, 3], index=['a', 'b', 'c', 'd'])\\nIn [102]: s2 = Series([4, 5, 6], index=['c', 'd', 'e'])\\nIn [103]: data2 = pd.concat([s1, s2], keys=['one', 'two'])\\nIn [104]: data2.unstack()\\nOut[104]:\\n      a   b  c  d   e\\none   0   1  2  3 NaN\\ntwo NaN NaN  4  5   6\\nStacking filters out missing data by default, so the operation is easily invertible:\\nIn [105]: data2.unstack().stack()      In [106]: data2.unstack().stack(dropna=False)\\nOut[105]:                              Out[106]:\\none  a    0                            one  a     0\\n     b    1                                 b     1\\n     c    2                                 c     2\\n     d    3                                 d     3\\ntwo  c    4                                 e   NaN\"),\n",
       " Document(metadata={}, page_content=\"c    2                                 c     2\\n     d    3                                 d     3\\ntwo  c    4                                 e   NaN\\n     d    5                            two  a   NaN\\n     e    6                                 b   NaN\\n                                            c     4\\n                                            d     5\\n                                            e     6\\nWhen unstacking in a DataFrame, the level unstacked becomes the lowest level in the\\nresult:\\nIn [107]: df = DataFrame({'left': result, 'right': result + 5},\\n   .....:                columns=pd.Index(['left', 'right'], name='side'))\\nIn [108]: df\\nOut[108]:\\nside             left  right\\nstate    number\\nOhio     one        0      5\\n         two        1      6\\n         three      2      7\\nColorado one        3      8\\n         two        4      9\\n         three      5     10\\nIn [109]: df.unstack('state')                In [110]: df.unstack('state').stack('side')\"),\n",
       " Document(metadata={}, page_content=\"Colorado one        3      8\\n         two        4      9\\n         three      5     10\\nIn [109]: df.unstack('state')                In [110]: df.unstack('state').stack('side')\\nOut[109]:                                    Out[110]:\\nside    left            right                state         Ohio  Colorado\\nstate   Ohio  Colorado   Ohio  Colorado      number side\\nnumber                                       one    left      0         3\\none        0         3      5         8             right     5         8\\ntwo        1         4      6         9      two    left      1         4\\nReshaping and Pivoting | 191\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content='three      2         5      7        10             right     6         9\\n                                             three  left      2         5\\n                                                    right     7        10\\nPivoting “long” to “wide” Format\\nA common way to store multiple time series in databases and CSV is in so-called long\\nor stacked format:\\nIn [116]: ldata[:10]\\nOut[116]:\\n                 date     item     value\\n0 1959-03-31 00:00:00  realgdp  2710.349\\n1 1959-03-31 00:00:00     infl     0.000\\n2 1959-03-31 00:00:00    unemp     5.800\\n3 1959-06-30 00:00:00  realgdp  2778.801\\n4 1959-06-30 00:00:00     infl     2.340\\n5 1959-06-30 00:00:00    unemp     5.100\\n6 1959-09-30 00:00:00  realgdp  2775.488\\n7 1959-09-30 00:00:00     infl     2.740\\n8 1959-09-30 00:00:00    unemp     5.300\\n9 1959-12-31 00:00:00  realgdp  2785.204\\nData is frequently stored this way in relational databases like MySQL as a fixed schema'),\n",
       " Document(metadata={}, page_content=\"8 1959-09-30 00:00:00    unemp     5.300\\n9 1959-12-31 00:00:00  realgdp  2785.204\\nData is frequently stored this way in relational databases like MySQL as a fixed schema\\n(column names and data types) allows the number of distinct values in the item column\\nto increase or decrease as data is added or deleted in the table. In the above example\\ndate and item would usually be the primary keys (in relational database parlance),\\noffering both relational integrity and easier joins and programmatic queries in many\\ncases. The downside, of course, is that the data may not be easy to work with in long\\nformat; you might prefer to have a DataFrame containing one column per distinct\\nitem value indexed by timestamps in the date column. DataFrame’s pivot method per-\\nforms exactly this transformation:\\nIn [117]: pivoted = ldata.pivot('date', 'item', 'value')\\nIn [118]: pivoted.head()\\nOut[118]:\\nitem        infl   realgdp  unemp\\ndate\\n1959-03-31  0.00  2710.349    5.8\\n1959-06-30  2.34  2778.801    5.1\"),\n",
       " Document(metadata={}, page_content=\"In [117]: pivoted = ldata.pivot('date', 'item', 'value')\\nIn [118]: pivoted.head()\\nOut[118]:\\nitem        infl   realgdp  unemp\\ndate\\n1959-03-31  0.00  2710.349    5.8\\n1959-06-30  2.34  2778.801    5.1\\n1959-09-30  2.74  2775.488    5.3\\n1959-12-31  0.27  2785.204    5.6\\n1960-03-31  2.31  2847.699    5.2\\nThe first two values passed are the columns to be used as the row and column index,\\nand finally an optional value column to fill the DataFrame. Suppose you had two value\\ncolumns that you wanted to reshape simultaneously:\\nIn [119]: ldata['value2'] = np.random.randn(len(ldata))\\nIn [120]: ldata[:10]\\nOut[120]:\\n192 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"date     item     value    value2\\n0 1959-03-31 00:00:00  realgdp  2710.349  1.669025\\n1 1959-03-31 00:00:00     infl     0.000 -0.438570\\n2 1959-03-31 00:00:00    unemp     5.800 -0.539741\\n3 1959-06-30 00:00:00  realgdp  2778.801  0.476985\\n4 1959-06-30 00:00:00     infl     2.340  3.248944\\n5 1959-06-30 00:00:00    unemp     5.100 -1.021228\\n6 1959-09-30 00:00:00  realgdp  2775.488 -0.577087\\n7 1959-09-30 00:00:00     infl     2.740  0.124121\\n8 1959-09-30 00:00:00    unemp     5.300  0.302614\\n9 1959-12-31 00:00:00  realgdp  2785.204  0.523772\\nBy omitting the last argument, you obtain a DataFrame with hierarchical columns:\\nIn [121]: pivoted = ldata.pivot('date', 'item')\\nIn [122]: pivoted[:5]\\nOut[122]:\\n            value                     value2\\nitem         infl   realgdp  unemp      infl   realgdp     unemp\\ndate\\n1959-03-31   0.00  2710.349    5.8 -0.438570  1.669025 -0.539741\\n1959-06-30   2.34  2778.801    5.1  3.248944  0.476985 -1.021228\"),\n",
       " Document(metadata={}, page_content=\"item         infl   realgdp  unemp      infl   realgdp     unemp\\ndate\\n1959-03-31   0.00  2710.349    5.8 -0.438570  1.669025 -0.539741\\n1959-06-30   2.34  2778.801    5.1  3.248944  0.476985 -1.021228\\n1959-09-30   2.74  2775.488    5.3  0.124121 -0.577087  0.302614\\n1959-12-31   0.27  2785.204    5.6  0.000940  0.523772  1.343810\\n1960-03-31   2.31  2847.699    5.2 -0.831154 -0.713544 -2.370232\\nIn [123]: pivoted['value'][:5]\\nOut[123]:\\nitem        infl   realgdp  unemp\\ndate\\n1959-03-31  0.00  2710.349    5.8\\n1959-06-30  2.34  2778.801    5.1\\n1959-09-30  2.74  2775.488    5.3\\n1959-12-31  0.27  2785.204    5.6\\n1960-03-31  2.31  2847.699    5.2\\nNote that pivot is just a shortcut for creating a hierarchical index using set_index and\\nreshaping with unstack:\\nIn [124]: unstacked = ldata.set_index(['date', 'item']).unstack('item')\\nIn [125]: unstacked[:7]\\nOut[125]:\\n            value                     value2\\nitem         infl   realgdp  unemp      infl   realgdp     unemp\\ndate\"),\n",
       " Document(metadata={}, page_content='In [125]: unstacked[:7]\\nOut[125]:\\n            value                     value2\\nitem         infl   realgdp  unemp      infl   realgdp     unemp\\ndate\\n1959-03-31   0.00  2710.349    5.8 -0.438570  1.669025 -0.539741\\n1959-06-30   2.34  2778.801    5.1  3.248944  0.476985 -1.021228\\n1959-09-30   2.74  2775.488    5.3  0.124121 -0.577087  0.302614\\n1959-12-31   0.27  2785.204    5.6  0.000940  0.523772  1.343810\\n1960-03-31   2.31  2847.699    5.2 -0.831154 -0.713544 -2.370232\\n1960-06-30   0.14  2834.390    5.2 -0.860757 -1.860761  0.560145\\n1960-09-30   2.70  2839.022    5.6  0.119827 -1.265934 -1.063512\\nReshaping and Pivoting | 193\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"Data Transformation\\nSo far in this chapter we’ve been concerned with rearranging data. Filtering, cleaning,\\nand other tranformations are another class of important operations.\\nRemoving Duplicates\\nDuplicate rows may be found in a DataFrame for any number of reasons. Here is an\\nexample:\\nIn [126]: data = DataFrame({'k1': ['one'] * 3 + ['two'] * 4,\\n   .....:                   'k2': [1, 1, 2, 3, 3, 4, 4]})\\nIn [127]: data\\nOut[127]:\\n    k1  k2\\n0  one   1\\n1  one   1\\n2  one   2\\n3  two   3\\n4  two   3\\n5  two   4\\n6  two   4\\nThe DataFrame method duplicated returns a boolean Series indicating whether each\\nrow is a duplicate or not:\\nIn [128]: data.duplicated()\\nOut[128]:\\n0    False\\n1     True\\n2    False\\n3    False\\n4     True\\n5    False\\n6     True\\nRelatedly, drop_duplicates returns a DataFrame where the duplicated array is True:\\nIn [129]: data.drop_duplicates()\\nOut[129]:\\n    k1  k2\\n0  one   1\\n2  one   2\\n3  two   3\\n5  two   4\"),\n",
       " Document(metadata={}, page_content=\"6     True\\nRelatedly, drop_duplicates returns a DataFrame where the duplicated array is True:\\nIn [129]: data.drop_duplicates()\\nOut[129]:\\n    k1  k2\\n0  one   1\\n2  one   2\\n3  two   3\\n5  two   4\\nBoth of these methods by default consider all of the columns; alternatively you can\\nspecify any subset of them to detect duplicates. Suppose we had an additional column\\nof values and wanted to filter duplicates only based on the 'k1' column:\\nIn [130]: data['v1'] = range(7)\\nIn [131]: data.drop_duplicates(['k1'])\\n194 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"Out[131]:\\n    k1  k2  v1\\n0  one   1   0\\n3  two   3   3\\nduplicated and drop_duplicates by default keep the first observed value combination.\\nPassing take_last=True will return the last one:\\nIn [132]: data.drop_duplicates(['k1', 'k2'], take_last=True)\\nOut[132]:\\n    k1  k2  v1\\n1  one   1   1\\n2  one   2   2\\n4  two   3   4\\n6  two   4   6\\nTransforming Data Using a Function or Mapping\\nFor many data sets, you may wish to perform some transformation based on the values\\nin an array, Series, or column in a DataFrame. Consider the following hypothetical data\\ncollected about some kinds of meat:\\nIn [133]: data = DataFrame({'food': ['bacon', 'pulled pork', 'bacon', 'Pastrami',\\n   .....:                            'corned beef', 'Bacon', 'pastrami', 'honey ham',\\n   .....:                            'nova lox'],\\n   .....:                   'ounces': [4, 3, 12, 6, 7.5, 8, 3, 5, 6]})\\nIn [134]: data\\nOut[134]:\\n          food  ounces\\n0        bacon     4.0\\n1  pulled pork     3.0\\n2        bacon    12.0\"),\n",
       " Document(metadata={}, page_content=\".....:                   'ounces': [4, 3, 12, 6, 7.5, 8, 3, 5, 6]})\\nIn [134]: data\\nOut[134]:\\n          food  ounces\\n0        bacon     4.0\\n1  pulled pork     3.0\\n2        bacon    12.0\\n3     Pastrami     6.0\\n4  corned beef     7.5\\n5        Bacon     8.0\\n6     pastrami     3.0\\n7    honey ham     5.0\\n8     nova lox     6.0\\nSuppose you wanted to add a column indicating the type of animal that each food came\\nfrom. Let’s write down a mapping of each distinct meat type to the kind of animal:\\nmeat_to_animal = {\\n  'bacon': 'pig',\\n  'pulled pork': 'pig',\\n  'pastrami': 'cow',\\n  'corned beef': 'cow',\\n  'honey ham': 'pig',\\n  'nova lox': 'salmon'\\n}\\nData Transformation | 195\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"The map method on a Series accepts a function or dict-like object containing a mapping,\\nbut here we have a small problem in that some of the meats above are capitalized and\\nothers are not. Thus, we also need to convert each value to lower case:\\nIn [136]: data['animal'] = data['food'].map(str.lower).map(meat_to_animal)\\nIn [137]: data\\nOut[137]:\\n          food  ounces  animal\\n0        bacon     4.0     pig\\n1  pulled pork     3.0     pig\\n2        bacon    12.0     pig\\n3     Pastrami     6.0     cow\\n4  corned beef     7.5     cow\\n5        Bacon     8.0     pig\\n6     pastrami     3.0     cow\\n7    honey ham     5.0     pig\\n8     nova lox     6.0  salmon\\nWe could also have passed a function that does all the work:\\nIn [138]: data['food'].map(lambda x: meat_to_animal[x.lower()])\\nOut[138]:\\n0       pig\\n1       pig\\n2       pig\\n3       cow\\n4       cow\\n5       pig\\n6       cow\\n7       pig\\n8    salmon\\nName: food\\nUsing map is a convenient way to perform element-wise transformations and other data\"),\n",
       " Document(metadata={}, page_content='1       pig\\n2       pig\\n3       cow\\n4       cow\\n5       pig\\n6       cow\\n7       pig\\n8    salmon\\nName: food\\nUsing map is a convenient way to perform element-wise transformations and other data\\ncleaning-related operations.\\nReplacing Values\\nFilling in missing data with the fillna method can be thought of as a special case of\\nmore general value replacement. While map, as you’ve seen above, can be used to modify\\na subset of values in an object, replace provides a simpler and more flexible way to do\\nso. Let’s consider this Series:\\nIn [139]: data = Series([1., -999., 2., -999., -1000., 3.])\\nIn [140]: data\\nOut[140]:\\n0       1\\n1    -999\\n2       2\\n3    -999\\n4   -1000\\n5       3\\n196 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='The -999 values might be sentinel values for missing data. To replace these with NA\\nvalues that pandas understands, we can use replace, producing a new Series:\\nIn [141]: data.replace(-999, np.nan)\\nOut[141]:\\n0       1\\n1     NaN\\n2       2\\n3     NaN\\n4   -1000\\n5       3\\nIf you want to replace multiple values at once, you instead pass a list then the substitute\\nvalue:\\nIn [142]: data.replace([-999, -1000], np.nan)\\nOut[142]:\\n0     1\\n1   NaN\\n2     2\\n3   NaN\\n4   NaN\\n5     3\\nTo use a different replacement for each value, pass a list of substitutes:\\nIn [143]: data.replace([-999, -1000], [np.nan, 0])\\nOut[143]:\\n0     1\\n1   NaN\\n2     2\\n3   NaN\\n4     0\\n5     3\\nThe argument passed can also be a dict:\\nIn [144]: data.replace({-999: np.nan, -1000: 0})\\nOut[144]:\\n0     1\\n1   NaN\\n2     2\\n3   NaN\\n4     0\\n5     3\\nRenaming Axis Indexes\\nLike values in a Series, axis labels can be similarly transformed by a function or mapping'),\n",
       " Document(metadata={}, page_content=\"Out[144]:\\n0     1\\n1   NaN\\n2     2\\n3   NaN\\n4     0\\n5     3\\nRenaming Axis Indexes\\nLike values in a Series, axis labels can be similarly transformed by a function or mapping\\nof some form to produce new, differently labeled objects. The axes can also be modified\\nin place without creating a new data structure. Here’s a simple example:\\nIn [145]: data = DataFrame(np.arange(12).reshape((3, 4)),\\n   .....:                  index=['Ohio', 'Colorado', 'New York'],\\n   .....:                  columns=['one', 'two', 'three', 'four'])\\nData Transformation | 197\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"Like a Series, the axis indexes have a map method:\\nIn [146]: data.index.map(str.upper)\\nOut[146]: array([OHIO, COLORADO, NEW YORK], dtype=object)\\nYou can assign to index, modifying the DataFrame in place:\\nIn [147]: data.index = data.index.map(str.upper)\\nIn [148]: data\\nOut[148]:\\n          one  two  three  four\\nOHIO        0    1      2     3\\nCOLORADO    4    5      6     7\\nNEW YORK    8    9     10    11\\nIf you want to create a transformed version of a data set without modifying the original,\\na useful method is rename:\\nIn [149]: data.rename(index=str.title, columns=str.upper)\\nOut[149]:\\n          ONE  TWO  THREE  FOUR\\nOhio        0    1      2     3\\nColorado    4    5      6     7\\nNew York    8    9     10    11\\nNotably, rename can be used in conjunction with a dict-like object providing new values\\nfor a subset of the axis labels:\\nIn [150]: data.rename(index={'OHIO': 'INDIANA'},\\n   .....:             columns={'three': 'peekaboo'})\\nOut[150]:\\n          one  two  peekaboo  four\"),\n",
       " Document(metadata={}, page_content=\"for a subset of the axis labels:\\nIn [150]: data.rename(index={'OHIO': 'INDIANA'},\\n   .....:             columns={'three': 'peekaboo'})\\nOut[150]:\\n          one  two  peekaboo  four\\nINDIANA     0    1         2     3\\nCOLORADO    4    5         6     7\\nNEW YORK    8    9        10    11\\nrename saves having to copy the DataFrame manually and assign to its index and col\\numns attributes. Should you wish to modify a data set in place, pass inplace=True:\\n# Always returns a reference to a DataFrame\\nIn [151]: _ = data.rename(index={'OHIO': 'INDIANA'}, inplace=True)\\nIn [152]: data\\nOut[152]:\\n          one  two  three  four\\nINDIANA     0    1      2     3\\nCOLORADO    4    5      6     7\\nNEW YORK    8    9     10    11\\n198 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content='Discretization and Binning\\nContinuous data is often discretized or otherwised separated into “bins” for analysis.\\nSuppose you have data about a group of people in a study, and you want to group them\\ninto discrete age buckets:\\nIn [153]: ages = [20, 22, 25, 27, 21, 23, 37, 31, 61, 45, 41, 32]\\nLet’s divide these into bins of 18 to 25, 26 to 35, 35 to 60, and finally 60 and older. To\\ndo so, you have to use cut, a function in pandas:\\nIn [154]: bins = [18, 25, 35, 60, 100]\\nIn [155]: cats = pd.cut(ages, bins)\\nIn [156]: cats\\nOut[156]:\\nCategorical:\\narray([(18, 25], (18, 25], (18, 25], (25, 35], (18, 25], (18, 25],\\n       (35, 60], (25, 35], (60, 100], (35, 60], (35, 60], (25, 35]], dtype=object)\\nLevels (4): Index([(18, 25], (25, 35], (35, 60], (60, 100]], dtype=object)\\nThe object pandas returns is a special Categorical object. You can treat it like an array\\nof strings indicating the bin name; internally it contains a levels array indicating the'),\n",
       " Document(metadata={}, page_content='The object pandas returns is a special Categorical object. You can treat it like an array\\nof strings indicating the bin name; internally it contains a levels array indicating the\\ndistinct category names along with a labeling for the ages data in the labels attribute:\\nIn [157]: cats.labels\\nOut[157]: array([0, 0, 0, 1, 0, 0, 2, 1, 3, 2, 2, 1])\\nIn [158]: cats.levels\\nOut[158]: Index([(18, 25], (25, 35], (35, 60], (60, 100]], dtype=object)\\nIn [159]: pd.value_counts(cats)\\nOut[159]:\\n(18, 25]     5\\n(35, 60]     3\\n(25, 35]     3\\n(60, 100]    1\\nConsistent with mathematical notation for intervals, a parenthesis means that the side\\nis open while the square bracket means it is closed (inclusive). Which side is closed can\\nbe changed by passing right=False:\\nIn [160]: pd.cut(ages, [18, 26, 36, 61, 100], right=False)\\nOut[160]:\\nCategorical:\\narray([[18, 26), [18, 26), [18, 26), [26, 36), [18, 26), [18, 26),\\n       [36, 61), [26, 36), [61, 100), [36, 61), [36, 61), [26, 36)], dtype=object)'),\n",
       " Document(metadata={}, page_content=\"Out[160]:\\nCategorical:\\narray([[18, 26), [18, 26), [18, 26), [26, 36), [18, 26), [18, 26),\\n       [36, 61), [26, 36), [61, 100), [36, 61), [36, 61), [26, 36)], dtype=object)\\nLevels (4): Index([[18, 26), [26, 36), [36, 61), [61, 100)], dtype=object)\\nYou can also pass your own bin names by passing a list or array to the labels option:\\nIn [161]: group_names = ['Youth', 'YoungAdult', 'MiddleAged', 'Senior']\\nIn [162]: pd.cut(ages, bins, labels=group_names)\\nOut[162]:\\nData Transformation | 199\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content='Categorical:\\narray([Youth, Youth, Youth, YoungAdult, Youth, Youth, MiddleAged,\\n       YoungAdult, Senior, MiddleAged, MiddleAged, YoungAdult], dtype=object)\\nLevels (4): Index([Youth, YoungAdult, MiddleAged, Senior], dtype=object)\\nIf you pass cut a integer number of bins instead of explicit bin edges, it will compute\\nequal-length bins based on the minimum and maximum values in the data. Consider\\nthe case of some uniformly distributed data chopped into fourths:\\nIn [163]: data = np.random.rand(20)\\nIn [164]: pd.cut(data, 4, precision=2)\\nOut[164]:\\nCategorical:\\narray([(0.45, 0.67], (0.23, 0.45], (0.0037, 0.23], (0.45, 0.67],\\n       (0.67, 0.9], (0.45, 0.67], (0.67, 0.9], (0.23, 0.45], (0.23, 0.45],\\n       (0.67, 0.9], (0.67, 0.9], (0.67, 0.9], (0.23, 0.45], (0.23, 0.45],\\n       (0.23, 0.45], (0.67, 0.9], (0.0037, 0.23], (0.0037, 0.23],\\n       (0.23, 0.45], (0.23, 0.45]], dtype=object)\\nLevels (4): Index([(0.0037, 0.23], (0.23, 0.45], (0.45, 0.67],'),\n",
       " Document(metadata={}, page_content='(0.23, 0.45], (0.67, 0.9], (0.0037, 0.23], (0.0037, 0.23],\\n       (0.23, 0.45], (0.23, 0.45]], dtype=object)\\nLevels (4): Index([(0.0037, 0.23], (0.23, 0.45], (0.45, 0.67],\\n                   (0.67, 0.9]], dtype=object)\\nA closely related function, qcut, bins the data based on sample quantiles. Depending\\non the distribution of the data, using cut will not usually result in each bin having the\\nsame number of data points. Since qcut uses sample quantiles instead, by definition\\nyou will obtain roughly equal-size bins:\\nIn [165]: data = np.random.randn(1000) # Normally distributed\\nIn [166]: cats = pd.qcut(data, 4) # Cut into quartiles\\nIn [167]: cats\\nOut[167]:\\nCategorical:\\narray([(-0.022, 0.641], [-3.745, -0.635], (0.641, 3.26], ...,\\n       (-0.635, -0.022], (0.641, 3.26], (-0.635, -0.022]], dtype=object)\\nLevels (4): Index([[-3.745, -0.635], (-0.635, -0.022], (-0.022, 0.641],\\n                   (0.641, 3.26]], dtype=object)\\nIn [168]: pd.value_counts(cats)\\nOut[168]:'),\n",
       " Document(metadata={}, page_content='Levels (4): Index([[-3.745, -0.635], (-0.635, -0.022], (-0.022, 0.641],\\n                   (0.641, 3.26]], dtype=object)\\nIn [168]: pd.value_counts(cats)\\nOut[168]:\\n[-3.745, -0.635]    250\\n(0.641, 3.26]       250\\n(-0.635, -0.022]    250\\n(-0.022, 0.641]     250\\nSimilar to cut you can pass your own quantiles (numbers between 0 and 1, inclusive):\\nIn [169]: pd.qcut(data, [0, 0.1, 0.5, 0.9, 1.])\\nOut[169]:\\nCategorical:\\narray([(-0.022, 1.302], (-1.266, -0.022], (-0.022, 1.302], ...,\\n       (-1.266, -0.022], (-0.022, 1.302], (-1.266, -0.022]], dtype=object)\\nLevels (4): Index([[-3.745, -1.266], (-1.266, -0.022], (-0.022, 1.302],\\n                   (1.302, 3.26]], dtype=object)\\n200 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='We’ll return to cut and qcut later in the chapter on aggregation and group operations,\\nas these discretization functions are especially useful for quantile and group analysis.\\nDetecting and Filtering Outliers\\nFiltering or transforming outliers is largely a matter of applying array operations. Con-\\nsider a DataFrame with some normally distributed data:\\nIn [170]: np.random.seed(12345)\\nIn [171]: data = DataFrame(np.random.randn(1000, 4))\\nIn [172]: data.describe()\\nOut[172]:\\n                 0            1            2            3\\ncount  1000.000000  1000.000000  1000.000000  1000.000000\\nmean     -0.067684     0.067924     0.025598    -0.002298\\nstd       0.998035     0.992106     1.006835     0.996794\\nmin      -3.428254    -3.548824    -3.184377    -3.745356\\n25%      -0.774890    -0.591841    -0.641675    -0.644144\\n50%      -0.116401     0.101143     0.002073    -0.013611\\n75%       0.616366     0.780282     0.680391     0.654328\\nmax       3.366626     2.653656     3.260383     3.927528'),\n",
       " Document(metadata={}, page_content='50%      -0.116401     0.101143     0.002073    -0.013611\\n75%       0.616366     0.780282     0.680391     0.654328\\nmax       3.366626     2.653656     3.260383     3.927528\\nSuppose you wanted to find values in one of the columns exceeding three in magnitude:\\nIn [173]: col = data[3]\\nIn [174]: col[np.abs(col) > 3]\\nOut[174]:\\n97     3.927528\\n305   -3.399312\\n400   -3.745356\\nName: 3\\nTo select all rows having a value exceeding 3 or -3, you can use the any method on a\\nboolean DataFrame:\\nIn [175]: data[(np.abs(data) > 3).any(1)]\\nOut[175]:\\n            0         1         2         3\\n5   -0.539741  0.476985  3.248944 -1.021228\\n97  -0.774363  0.552936  0.106061  3.927528\\n102 -0.655054 -0.565230  3.176873  0.959533\\n305 -2.315555  0.457246 -0.025907 -3.399312\\n324  0.050188  1.951312  3.260383  0.963301\\n400  0.146326  0.508391 -0.196713 -3.745356\\n499 -0.293333 -0.242459 -3.056990  1.918403\\n523 -3.428254 -0.296336 -0.439938 -0.867165\\n586  0.275144  1.179227 -3.184377  1.369891'),\n",
       " Document(metadata={}, page_content='400  0.146326  0.508391 -0.196713 -3.745356\\n499 -0.293333 -0.242459 -3.056990  1.918403\\n523 -3.428254 -0.296336 -0.439938 -0.867165\\n586  0.275144  1.179227 -3.184377  1.369891\\n808 -0.362528 -3.548824  1.553205 -2.186301\\n900  3.366626 -2.372214  0.851010  1.332846\\nValues can just as easily be set based on these criteria. Here is code to cap values outside\\nthe interval -3 to 3:\\nData Transformation | 201\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='In [176]: data[np.abs(data) > 3] = np.sign(data) * 3\\nIn [177]: data.describe()\\nOut[177]:\\n                 0            1            2            3\\ncount  1000.000000  1000.000000  1000.000000  1000.000000\\nmean     -0.067623     0.068473     0.025153    -0.002081\\nstd       0.995485     0.990253     1.003977     0.989736\\nmin      -3.000000    -3.000000    -3.000000    -3.000000\\n25%      -0.774890    -0.591841    -0.641675    -0.644144\\n50%      -0.116401     0.101143     0.002073    -0.013611\\n75%       0.616366     0.780282     0.680391     0.654328\\nmax       3.000000     2.653656     3.000000     3.000000\\nThe ufunc np.sign returns an array of 1 and -1 depending on the sign of the values.\\nPermutation and Random Sampling\\nPermuting (randomly reordering) a Series or the rows in a DataFrame is easy to do using\\nthe numpy.random.permutation function. Calling permutation with the length of the axis\\nyou want to permute produces an array of integers indicating the new ordering:'),\n",
       " Document(metadata={}, page_content='the numpy.random.permutation function. Calling permutation with the length of the axis\\nyou want to permute produces an array of integers indicating the new ordering:\\nIn [178]: df = DataFrame(np.arange(5 * 4).reshape(5, 4))\\nIn [179]: sampler = np.random.permutation(5)\\nIn [180]: sampler\\nOut[180]: array([1, 0, 2, 3, 4])\\nThat array can then be used in ix-based indexing or the take function:\\nIn [181]: df             In [182]: df.take(sampler)\\nOut[181]:                Out[182]:\\n    0   1   2   3            0   1   2   3\\n0   0   1   2   3        1   4   5   6   7\\n1   4   5   6   7        0   0   1   2   3\\n2   8   9  10  11        2   8   9  10  11\\n3  12  13  14  15        3  12  13  14  15\\n4  16  17  18  19        4  16  17  18  19\\nTo select a random subset without replacement, one way is to slice off the first k ele-\\nments of the array returned by permutation, where k is the desired subset size. There\\nare much more efficient sampling-without-replacement algorithms, but this is an easy'),\n",
       " Document(metadata={}, page_content='ments of the array returned by permutation, where k is the desired subset size. There\\nare much more efficient sampling-without-replacement algorithms, but this is an easy\\nstrategy that uses readily available tools:\\nIn [183]: df.take(np.random.permutation(len(df))[:3])\\nOut[183]:\\n    0   1   2   3\\n1   4   5   6   7\\n3  12  13  14  15\\n4  16  17  18  19\\nTo generate a sample with replacement, the fastest way is to use np.random.randint to\\ndraw random integers:\\n202 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"In [184]: bag = np.array([5, 7, -1, 6, 4])\\nIn [185]: sampler = np.random.randint(0, len(bag), size=10)\\nIn [186]: sampler\\nOut[186]: array([4, 4, 2, 2, 2, 0, 3, 0, 4, 1])\\nIn [187]: draws = bag.take(sampler)\\nIn [188]: draws\\nOut[188]: array([ 4,  4, -1, -1, -1,  5,  6,  5,  4,  7])\\nComputing Indicator/Dummy Variables\\nAnother type of transformation for statistical modeling or machine learning applica-\\ntions is converting a categorical variable into a “dummy” or “indicator” matrix. If a\\ncolumn in a DataFrame has k distinct values, you would derive a matrix or DataFrame\\ncontaining k columns containing all 1’s and 0’s. pandas has a get_dummies function for\\ndoing this, though devising one yourself is not difficult. Let’s return to an earlier ex-\\nample DataFrame:\\nIn [189]: df = DataFrame({'key': ['b', 'b', 'a', 'c', 'a', 'b'],\\n   .....:                 'data1': range(6)})\\nIn [190]: pd.get_dummies(df['key'])\\nOut[190]:\\n   a  b  c\\n0  0  1  0\\n1  0  1  0\\n2  1  0  0\\n3  0  0  1\\n4  1  0  0\\n5  0  1  0\"),\n",
       " Document(metadata={}, page_content=\".....:                 'data1': range(6)})\\nIn [190]: pd.get_dummies(df['key'])\\nOut[190]:\\n   a  b  c\\n0  0  1  0\\n1  0  1  0\\n2  1  0  0\\n3  0  0  1\\n4  1  0  0\\n5  0  1  0\\nIn some cases, you may want to add a prefix to the columns in the indicator DataFrame,\\nwhich can then be merged with the other data. get_dummies has a prefix argument for\\ndoing just this:\\nIn [191]: dummies = pd.get_dummies(df['key'], prefix='key')\\nIn [192]: df_with_dummy = df[['data1']].join(dummies)\\nIn [193]: df_with_dummy\\nOut[193]:\\n   data1  key_a  key_b  key_c\\n0      0      0      1      0\\n1      1      0      1      0\\n2      2      1      0      0\\n3      3      0      0      1\\n4      4      1      0      0\\n5      5      0      1      0\\nData Transformation | 203\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"If a row in a DataFrame belongs to multiple categories, things are a bit more compli-\\ncated. Let’s return to the MovieLens 1M dataset from earlier in the book:\\nIn [194]: mnames = ['movie_id', 'title', 'genres']\\nIn [195]: movies = pd.read_table('ch07/movies.dat', sep='::', header=None,\\n   .....:                         names=mnames)\\nIn [196]: movies[:10]\\nOut[196]:\\n   movie_id                               title                        genres\\n0         1                    Toy Story (1995)   Animation|Children's|Comedy\\n1         2                      Jumanji (1995)  Adventure|Children's|Fantasy\\n2         3             Grumpier Old Men (1995)                Comedy|Romance\\n3         4            Waiting to Exhale (1995)                  Comedy|Drama\\n4         5  Father of the Bride Part II (1995)                        Comedy\\n5         6                         Heat (1995)         Action|Crime|Thriller\\n6         7                      Sabrina (1995)                Comedy|Romance\"),\n",
       " Document(metadata={}, page_content=\"5         6                         Heat (1995)         Action|Crime|Thriller\\n6         7                      Sabrina (1995)                Comedy|Romance\\n7         8                 Tom and Huck (1995)          Adventure|Children's\\n8         9                 Sudden Death (1995)                        Action\\n9        10                    GoldenEye (1995)     Action|Adventure|Thriller\\nAdding indicator variables for each genre requires a little bit of wrangling. First, we\\nextract the list of unique genres in the dataset (using a nice set.union trick):\\nIn [197]: genre_iter = (set(x.split('|')) for x in movies.genres)\\nIn [198]: genres = sorted(set.union(*genre_iter))\\nNow, one way to construct the indicator DataFrame is to start with a DataFrame of all\\nzeros:\\nIn [199]: dummies = DataFrame(np.zeros((len(movies), len(genres))), columns=genres)\\nNow, iterate through each movie and set entries in each row of dummies to 1:\\nIn [200]: for i, gen in enumerate(movies.genres):\"),\n",
       " Document(metadata={}, page_content=\"Now, iterate through each movie and set entries in each row of dummies to 1:\\nIn [200]: for i, gen in enumerate(movies.genres):\\n   .....:     dummies.ix[i, gen.split('|')] = 1\\nThen, as above, you can combine this with movies:\\nIn [201]: movies_windic = movies.join(dummies.add_prefix('Genre_'))\\nIn [202]: movies_windic.ix[0]\\nOut[202]:\\nmovie_id                                       1\\ntitle                           Toy Story (1995)\\ngenres               Animation|Children's|Comedy\\nGenre_Action                                   0\\nGenre_Adventure                                0\\nGenre_Animation                                1\\nGenre_Children's                               1\\nGenre_Comedy                                   1\\nGenre_Crime                                    0\\nGenre_Documentary                              0\\nGenre_Drama                                    0\\nGenre_Fantasy                                  0\\n204 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\"),\n",
       " Document(metadata={}, page_content='Genre_Drama                                    0\\nGenre_Fantasy                                  0\\n204 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Genre_Film-Noir                                0\\nGenre_Horror                                   0\\nGenre_Musical                                  0\\nGenre_Mystery                                  0\\nGenre_Romance                                  0\\nGenre_Sci-Fi                                   0\\nGenre_Thriller                                 0\\nGenre_War                                      0\\nGenre_Western                                  0\\nName: 0\\nFor much larger data, this method of constructing indicator variables\\nwith multiple membership is not especially speedy. A lower-level func-\\ntion leveraging the internals of the DataFrame could certainly be writ-\\nten.\\nA useful recipe for statistical applications is to combine get_dummies with a discretiza-\\ntion function like cut:\\nIn [204]: values = np.random.rand(10)\\nIn [205]: values\\nOut[205]:\\narray([ 0.9296,  0.3164,  0.1839,  0.2046,  0.5677,  0.5955,  0.9645,\\n        0.6532,  0.7489,  0.6536])\\nIn [206]: bins = [0, 0.2, 0.4, 0.6, 0.8, 1]'),\n",
       " Document(metadata={}, page_content='In [205]: values\\nOut[205]:\\narray([ 0.9296,  0.3164,  0.1839,  0.2046,  0.5677,  0.5955,  0.9645,\\n        0.6532,  0.7489,  0.6536])\\nIn [206]: bins = [0, 0.2, 0.4, 0.6, 0.8, 1]\\nIn [207]: pd.get_dummies(pd.cut(values, bins))\\nOut[207]:\\n   (0, 0.2]  (0.2, 0.4]  (0.4, 0.6]  (0.6, 0.8]  (0.8, 1]\\n0         0           0           0           0         1\\n1         0           1           0           0         0\\n2         1           0           0           0         0\\n3         0           1           0           0         0\\n4         0           0           1           0         0\\n5         0           0           1           0         0\\n6         0           0           0           0         1\\n7         0           0           0           1         0\\n8         0           0           0           1         0\\n9         0           0           0           1         0\\nString Manipulation\\nPython has long been a popular data munging language in part due to its ease-of-use'),\n",
       " Document(metadata={}, page_content='9         0           0           0           1         0\\nString Manipulation\\nPython has long been a popular data munging language in part due to its ease-of-use\\nfor string and text processing. Most text operations are made simple with the string\\nobject’s built-in methods. For more complex pattern matching and text manipulations,\\nregular expressions may be needed. pandas adds to the mix by enabling you to apply\\nstring and regular expressions concisely on whole arrays of data, additionally handling\\nthe annoyance of missing data.\\nString Manipulation | 205\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"String Object Methods\\nIn many string munging and scripting applications, built-in string methods are suffi-\\ncient. As an example, a comma-separated string can be broken into pieces with split:\\nIn [208]: val = 'a,b,  guido'\\nIn [209]: val.split(',')\\nOut[209]: ['a', 'b', '  guido']\\nsplit is often combined with strip to trim whitespace (including newlines):\\nIn [210]: pieces = [x.strip() for x in val.split(',')]\\nIn [211]: pieces\\nOut[211]: ['a', 'b', 'guido']\\nThese substrings could be concatenated together with a two-colon delimiter using ad-\\ndition:\\nIn [212]: first, second, third = pieces\\nIn [213]: first + '::' + second + '::' + third\\nOut[213]: 'a::b::guido'\\nBut, this isn’t a practical generic method. A faster and more Pythonic way is to pass a\\nlist or tuple to the join method on the string '::':\\nIn [214]: '::'.join(pieces)\\nOut[214]: 'a::b::guido'\\nOther methods are concerned with locating substrings. Using Python’s in keyword is\"),\n",
       " Document(metadata={}, page_content=\"list or tuple to the join method on the string '::':\\nIn [214]: '::'.join(pieces)\\nOut[214]: 'a::b::guido'\\nOther methods are concerned with locating substrings. Using Python’s in keyword is\\nthe best way to detect a substring, though index and find can also be used:\\nIn [215]: 'guido' in val\\nOut[215]: True\\nIn [216]: val.index(',')        In [217]: val.find(':')\\nOut[216]: 1                     Out[217]: -1\\nNote the difference between find and index is that index raises an exception if the string\\nisn’t found (versus returning -1):\\nIn [218]: val.index(':')\\n---------------------------------------------------------------------------\\nValueError                                Traceback (most recent call last)\\n<ipython-input-218-280f8b2856ce> in <module>()\\n----> 1 val.index(':')\\nValueError: substring not found\\nRelatedly, count returns the number of occurrences of a particular substring:\\nIn [219]: val.count(',')\\nOut[219]: 2\"),\n",
       " Document(metadata={}, page_content=\"----> 1 val.index(':')\\nValueError: substring not found\\nRelatedly, count returns the number of occurrences of a particular substring:\\nIn [219]: val.count(',')\\nOut[219]: 2\\nreplace will substitute occurrences of one pattern for another. This is commonly used\\nto delete patterns, too, by passing an empty string:\\n206 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"In [220]: val.replace(',', '::')        In [221]: val.replace(',', '')\\nOut[220]: 'a::b::  guido'               Out[221]: 'ab  guido'\\nRegular expressions can also be used with many of these operations as you’ll see below.\\nTable 7-3. Python built-in string methods\\nArgument Description\\ncount Return the number of non-overlapping occurrences of substring in the string.\\nendswith, startswith Returns True if string ends with suffix (starts with prefix).\\njoin Use string as delimiter for concatenating a sequence of other strings.\\nindex Return position of first character in substring if found in the string. Raises ValueEr\\nror if not found.\\nfind Return position of first character of first occurrence of substring in the string. Like\\nindex, but returns -1 if not found.\\nrfind Return position of first character of last occurrence of substring in the string. Returns -1\\nif not found.\\nreplace Replace occurrences of string with another string.\"),\n",
       " Document(metadata={}, page_content='rfind Return position of first character of last occurrence of substring in the string. Returns -1\\nif not found.\\nreplace Replace occurrences of string with another string.\\nstrip, rstrip, lstrip Trim whitespace, including newlines; equivalent to x.strip() (and rstrip,\\nlstrip, respectively) for each element.\\nsplit Break string into list of substrings using passed delimiter.\\nlower, upper Convert alphabet characters to lowercase or uppercase, respectively.\\nljust, rjust Left justify or right justify, respectively. Pad opposite side of string with spaces (or some\\nother fill character) to return a string with a minimum width.\\nRegular expressions\\nRegular expressions provide a flexible way to search or match string patterns in text. A\\nsingle expression, commonly called a regex, is a string formed according to the regular\\nexpression language. Python’s built-in re module is responsible for applying regular\\nexpressions to strings; I’ll give a number of examples of its use here.'),\n",
       " Document(metadata={}, page_content='expression language. Python’s built-in re module is responsible for applying regular\\nexpressions to strings; I’ll give a number of examples of its use here.\\nThe art of writing regular expressions could be a chapter of its own and\\nthus is outside the book’s scope. There are many excellent tutorials and\\nreferences on the internet, such as Zed Shaw’s Learn Regex The Hard\\nWay (http://regex.learncodethehardway.org/book/).\\nThe re module functions fall into three categories: pattern matching, substitution, and\\nsplitting. Naturally these are all related; a regex describes a pattern to locate in the text,\\nwhich can then be used for many purposes. Let’s look at a simple example: suppose I\\nwanted to split a string with a variable number of whitespace characters (tabs, spaces,\\nand newlines). The regex describing one or more whitespace characters is \\\\s+:\\nString Manipulation | 207\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='In [222]: import re\\nIn [223]: text = \"foo    bar\\\\t baz  \\\\tqux\"\\nIn [224]: re.split(\\'\\\\s+\\', text)\\nOut[224]: [\\'foo\\', \\'bar\\', \\'baz\\', \\'qux\\']\\nWhen you call re.split(\\'\\\\s+\\', text), the regular expression is first compiled, then its\\nsplit method is called on the passed text. You can compile the regex yourself with \\nre.compile, forming a reusable regex object:\\nIn [225]: regex = re.compile(\\'\\\\s+\\')\\nIn [226]: regex.split(text)\\nOut[226]: [\\'foo\\', \\'bar\\', \\'baz\\', \\'qux\\']\\nIf, instead, you wanted to get a list of all patterns matching the regex, you can use the \\nfindall method:\\nIn [227]: regex.findall(text)\\nOut[227]: [\\'    \\', \\'\\\\t \\', \\'  \\\\t\\']\\nTo avoid unwanted escaping with \\\\ in a regular expression, use raw\\nstring literals like r\\'C:\\\\x\\' instead of the equivalent \\'C:\\\\\\\\x\\'.\\nCreating a regex object with re.compile is highly recommended if you intend to apply\\nthe same expression to many strings; doing so will save CPU cycles.\\nmatch and search are closely related to findall. While findall returns all matches in a'),\n",
       " Document(metadata={}, page_content='the same expression to many strings; doing so will save CPU cycles.\\nmatch and search are closely related to findall. While findall returns all matches in a\\nstring, search returns only the first match. More rigidly, match only matches at the\\nbeginning of the string. As a less trivial example, let’s consider a block of text and a\\nregular expression capable of identifying most email addresses:\\ntext = \"\"\"Dave dave@google.com\\nSteve steve@gmail.com\\nRob rob@gmail.com\\nRyan ryan@yahoo.com\\n\"\"\"\\npattern = r\\'[A-Z0-9._%+-]+@[A-Z0-9.-]+\\\\.[A-Z]{2,4}\\'\\n# re.IGNORECASE makes the regex case-insensitive\\nregex = re.compile(pattern, flags=re.IGNORECASE)\\nUsing findall on the text produces a list of the e-mail addresses:\\nIn [229]: regex.findall(text)\\nOut[229]: [\\'dave@google.com\\', \\'steve@gmail.com\\', \\'rob@gmail.com\\', \\'ryan@yahoo.com\\']\\nsearch returns a special match object for the first email address in the text. For the\\nabove regex, the match object can only tell us the start and end position of the pattern'),\n",
       " Document(metadata={}, page_content='search returns a special match object for the first email address in the text. For the\\nabove regex, the match object can only tell us the start and end position of the pattern\\nin the string:\\n208 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"In [230]: m = regex.search(text)\\nIn [231]: m\\nOut[231]: <_sre.SRE_Match at 0x10a05de00>\\nIn [232]: text[m.start():m.end()]\\nOut[232]: 'dave@google.com'\\nregex.match returns None, as it only will match if the pattern occurs at the start of the\\nstring:\\nIn [233]: print regex.match(text)\\nNone\\nRelatedly, sub will return a new string with occurrences of the pattern replaced by the\\na new string:\\nIn [234]: print regex.sub('REDACTED', text)\\nDave REDACTED\\nSteve REDACTED\\nRob REDACTED\\nRyan REDACTED\\nSuppose you wanted to find email addresses and simultaneously segment each address\\ninto its 3 components: username, domain name, and domain suffix. To do this, put\\nparentheses around the parts of the pattern to segment:\\nIn [235]: pattern = r'([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\\\\.([A-Z]{2,4})'\\nIn [236]: regex = re.compile(pattern, flags=re.IGNORECASE)\\nA match object produced by this modified regex returns a tuple of the pattern compo-\\nnents with its groups method:\\nIn [237]: m = regex.match('wesm@bright.net')\"),\n",
       " Document(metadata={}, page_content=\"A match object produced by this modified regex returns a tuple of the pattern compo-\\nnents with its groups method:\\nIn [237]: m = regex.match('wesm@bright.net')\\nIn [238]: m.groups()\\nOut[238]: ('wesm', 'bright', 'net')\\nfindall returns a list of tuples when the pattern has groups:\\nIn [239]: regex.findall(text)\\nOut[239]:\\n[('dave', 'google', 'com'),\\n ('steve', 'gmail', 'com'),\\n ('rob', 'gmail', 'com'),\\n ('ryan', 'yahoo', 'com')]\\nsub also has access to groups in each match using special symbols like \\\\1, \\\\2, etc.:\\nIn [240]: print regex.sub(r'Username: \\\\1, Domain: \\\\2, Suffix: \\\\3', text)\\nDave Username: dave, Domain: google, Suffix: com\\nSteve Username: steve, Domain: gmail, Suffix: com\\nRob Username: rob, Domain: gmail, Suffix: com\\nRyan Username: ryan, Domain: yahoo, Suffix: com\\nString Manipulation | 209\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content='There is much more to regular expressions in Python, most of which is outside the\\nbook’s scope. To give you a flavor, one variation on the above email regex gives names\\nto the match groups:\\nregex = re.compile(r\"\"\"\\n    (?P<username>[A-Z0-9._%+-]+)\\n    @\\n    (?P<domain>[A-Z0-9.-]+)\\n    \\\\.\\n    (?P<suffix>[A-Z]{2,4})\"\"\", flags=re.IGNORECASE|re.VERBOSE)\\nThe match object produced by such a regex can produce a handy dict with the specified\\ngroup names:\\nIn [242]: m = regex.match(\\'wesm@bright.net\\')\\nIn [243]: m.groupdict()\\nOut[243]: {\\'domain\\': \\'bright\\', \\'suffix\\': \\'net\\', \\'username\\': \\'wesm\\'}\\nTable 7-4. Regular expression methods\\nArgument Description\\nfindall, finditer Return all non-overlapping matching patterns in a string. findall returns a list of all\\npatterns while finditer returns them one by one from an iterator.\\nmatch Match pattern at start of string and optionally segment pattern components into groups.\\nIf the pattern matches, returns a match object, otherwise None.'),\n",
       " Document(metadata={}, page_content=\"match Match pattern at start of string and optionally segment pattern components into groups.\\nIf the pattern matches, returns a match object, otherwise None.\\nsearch Scan string for match to pattern; returning a match object if so. Unlike match, the match\\ncan be anywhere in the string as opposed to only at the beginning.\\nsplit Break string into pieces at each occurrence of pattern.\\nsub, subn Replace all (sub) or first n occurrences (subn) of pattern in string with replacement\\nexpression. Use symbols \\\\1, \\\\2, ... to refer to match group elements in the re-\\nplacement string.\\nVectorized string functions in pandas\\nCleaning up a messy data set for analysis often requires a lot of string munging and\\nregularization. To complicate matters, a column containing strings will sometimes have\\nmissing data:\\nIn [244]: data = {'Dave': 'dave@google.com', 'Steve': 'steve@gmail.com',\\n   .....:         'Rob': 'rob@gmail.com', 'Wes': np.nan}\\nIn [245]: data = Series(data)\"),\n",
       " Document(metadata={}, page_content=\"missing data:\\nIn [244]: data = {'Dave': 'dave@google.com', 'Steve': 'steve@gmail.com',\\n   .....:         'Rob': 'rob@gmail.com', 'Wes': np.nan}\\nIn [245]: data = Series(data)\\nIn [246]: data                  In [247]: data.isnull()\\nOut[246]:                       Out[247]:\\nDave     dave@google.com        Dave     False\\nRob        rob@gmail.com        Rob      False\\nSteve    steve@gmail.com        Steve    False\\nWes                  NaN        Wes       True\\n210 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"String and regular expression methods can be applied (passing a lambda or other func-\\ntion) to each value using data.map, but it will fail on the NA. To cope with this, Series\\nhas concise methods for string operations that skip NA values. These are accessed\\nthrough Series’s str attribute; for example, we could check whether each email address\\nhas 'gmail' in it with str.contains:\\nIn [248]: data.str.contains('gmail')\\nOut[248]:\\nDave     False\\nRob       True\\nSteve     True\\nWes        NaN\\nRegular expressions can be used, too, along with any re options like IGNORECASE:\\nIn [249]: pattern\\nOut[249]: '([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\\\\\\\\.([A-Z]{2,4})'\\nIn [250]: data.str.findall(pattern, flags=re.IGNORECASE)\\nOut[250]:\\nDave     [('dave', 'google', 'com')]\\nRob        [('rob', 'gmail', 'com')]\\nSteve    [('steve', 'gmail', 'com')]\\nWes                              NaN\\nThere are a couple of ways to do vectorized element retrieval. Either use str.get or\\nindex into the str attribute:\"),\n",
       " Document(metadata={}, page_content=\"Steve    [('steve', 'gmail', 'com')]\\nWes                              NaN\\nThere are a couple of ways to do vectorized element retrieval. Either use str.get or\\nindex into the str attribute:\\nIn [251]: matches = data.str.match(pattern, flags=re.IGNORECASE)\\nIn [252]: matches\\nOut[252]:\\nDave     ('dave', 'google', 'com')\\nRob        ('rob', 'gmail', 'com')\\nSteve    ('steve', 'gmail', 'com')\\nWes                            NaN\\nIn [253]: matches.str.get(1)      In [254]: matches.str[0]\\nOut[253]:                         Out[254]:\\nDave     google                   Dave      dave\\nRob       gmail                   Rob        rob\\nSteve     gmail                   Steve    steve\\nWes         NaN                   Wes        NaN\\nYou can similarly slice strings using this syntax:\\nIn [255]: data.str[:5]\\nOut[255]:\\nDave     dave@\\nRob      rob@g\\nSteve    steve\\nWes        NaN\\nString Manipulation | 211\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"Table 7-5. Vectorized string methods\\nMethod Description\\ncat Concatenate strings element-wise with optional delimiter\\ncontains Return boolean array if each string contains pattern/regex\\ncount Count occurrences of pattern\\nendswith, startswith Equivalent to x.endswith(pattern) or x.startswith(pattern) for each el-\\nement.\\nfindall Compute list of all occurrences of pattern/regex for each string\\nget Index into each element (retrieve i-th element)\\njoin Join strings in each element of the Series with passed separator\\nlen Compute length of each string\\nlower, upper Convert cases; equivalent to x.lower() or x.upper() for each element.\\nmatch Use re.match with the passed regular expression on each element, returning matched\\ngroups as list.\\npad Add whitespace to left, right, or both sides of strings\\ncenter Equivalent to pad(side='both')\\nrepeat Duplicate values; for example s.str.repeat(3) equivalent to x * 3 for each string.\\nreplace Replace occurrences of pattern/regex with some other string\"),\n",
       " Document(metadata={}, page_content='center Equivalent to pad(side=\\'both\\')\\nrepeat Duplicate values; for example s.str.repeat(3) equivalent to x * 3 for each string.\\nreplace Replace occurrences of pattern/regex with some other string\\nslice Slice each string in the Series.\\nsplit Split strings on delimiter or regular expression\\nstrip, rstrip, lstrip Trim whitespace, including newlines; equivalent to x.strip() (and rstrip,\\nlstrip, respectively) for each element.\\nExample: USDA Food Database\\nThe US Department of Agriculture makes available a database of food nutrient infor-\\nmation. Ashley Williams, an English hacker, has made available a version of this da-\\ntabase in JSON format ( http://ashleyw.co.uk/project/food-nutrient-database). The re-\\ncords look like this:\\n{\\n  \"id\": 21441,\\n  \"description\": \"KENTUCKY FRIED CHICKEN, Fried Chicken, EXTRA CRISPY,\\nWing, meat and skin with breading\",\\n  \"tags\": [\"KFC\"],\\n  \"manufacturer\": \"Kentucky Fried Chicken\",\\n  \"group\": \"Fast Foods\",\\n  \"portions\": [\\n    {\\n      \"amount\": 1,'),\n",
       " Document(metadata={}, page_content='Wing, meat and skin with breading\",\\n  \"tags\": [\"KFC\"],\\n  \"manufacturer\": \"Kentucky Fried Chicken\",\\n  \"group\": \"Fast Foods\",\\n  \"portions\": [\\n    {\\n      \"amount\": 1,\\n      \"unit\": \"wing, with skin\",\\n      \"grams\": 68.0\\n    },\\n212 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='...\\n  ],\\n  \"nutrients\": [\\n    {\\n      \"value\": 20.8,\\n      \"units\": \"g\",\\n      \"description\": \"Protein\",\\n      \"group\": \"Composition\"\\n    },\\n    ...\\n  ]\\n}\\nEach food has a number of identifying attributes along with two lists of nutrients and\\nportion sizes. Having the data in this form is not particularly amenable for analysis, so\\nwe need to do some work to wrangle the data into a better form.\\nAfter downloading and extracting the data from the link above, you can load it into\\nPython with any JSON library of your choosing. I’ll use the built-in Python json mod-\\nule:\\nIn [256]: import json\\nIn [257]: db = json.load(open(\\'ch07/foods-2011-10-03.json\\'))\\nIn [258]: len(db)\\nOut[258]: 6636\\nEach entry in db is a dict containing all the data for a single food. The \\'nutrients\\' field\\nis a list of dicts, one for each nutrient:\\nIn [259]: db[0].keys()        In [260]: db[0][\\'nutrients\\'][0]\\nOut[259]:                     Out[260]:\\n[u\\'portions\\',                 {u\\'description\\': u\\'Protein\\','),\n",
       " Document(metadata={}, page_content=\"In [259]: db[0].keys()        In [260]: db[0]['nutrients'][0]\\nOut[259]:                     Out[260]:\\n[u'portions',                 {u'description': u'Protein',\\n u'description',               u'group': u'Composition',\\n u'tags',                      u'units': u'g',\\n u'nutrients',                 u'value': 25.18}\\n u'group',\\n u'id',\\n u'manufacturer']\\nIn [261]: nutrients = DataFrame(db[0]['nutrients'])\\nIn [262]: nutrients[:7]\\nOut[262]:\\n                   description        group units    value\\n0                      Protein  Composition     g    25.18\\n1            Total lipid (fat)  Composition     g    29.20\\n2  Carbohydrate, by difference  Composition     g     3.06\\n3                          Ash        Other     g     3.28\\n4                       Energy       Energy  kcal   376.00\\n5                        Water  Composition     g    39.28\\n6                       Energy       Energy    kJ  1573.00\\nExample: USDA Food Database | 213\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"When converting a list of dicts to a DataFrame, we can specify a list of fields to extract.\\nWe’ll take the food names, group, id, and manufacturer:\\nIn [263]: info_keys = ['description', 'group', 'id', 'manufacturer']\\nIn [264]: info = DataFrame(db, columns=info_keys)\\nIn [265]: info[:5]\\nOut[265]:\\n                          description                   group    id manufacturer\\n0                     Cheese, caraway  Dairy and Egg Products  1008\\n1                     Cheese, cheddar  Dairy and Egg Products  1009\\n2                        Cheese, edam  Dairy and Egg Products  1018\\n3                        Cheese, feta  Dairy and Egg Products  1019\\n4  Cheese, mozzarella, part skim milk  Dairy and Egg Products  1028\\nIn [266]: info\\nOut[266]:\\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 6636 entries, 0 to 6635\\nData columns:\\ndescription     6636  non-null values\\ngroup           6636  non-null values\\nid              6636  non-null values\\nmanufacturer    5195  non-null values\"),\n",
       " Document(metadata={}, page_content='Data columns:\\ndescription     6636  non-null values\\ngroup           6636  non-null values\\nid              6636  non-null values\\nmanufacturer    5195  non-null values\\ndtypes: int64(1), object(3)\\nYou can see the distribution of food groups with value_counts:\\nIn [267]: pd.value_counts(info.group)[:10]\\nOut[267]:\\nVegetables and Vegetable Products    812\\nBeef Products                        618\\nBaked Products                       496\\nBreakfast Cereals                    403\\nLegumes and Legume Products          365\\nFast Foods                           365\\nLamb, Veal, and Game Products        345\\nSweets                               341\\nPork Products                        328\\nFruits and Fruit Juices              328\\nNow, to do some analysis on all of the nutrient data, it’s easiest to assemble the nutrients\\nfor each food into a single large table. To do so, we need to take several steps. First, I’ll\\nconvert each list of food nutrients to a DataFrame, add a column for the food id, and'),\n",
       " Document(metadata={}, page_content=\"for each food into a single large table. To do so, we need to take several steps. First, I’ll\\nconvert each list of food nutrients to a DataFrame, add a column for the food id, and\\nappend the DataFrame to a list. Then, these can be concatenated together with concat:\\nnutrients = []\\nfor rec in db:\\n    fnuts = DataFrame(rec['nutrients'])\\n    fnuts['id'] = rec['id']\\n    nutrients.append(fnuts)\\nnutrients = pd.concat(nutrients, ignore_index=True)\\n214 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"If all goes well, nutrients should look like this:\\nIn [269]: nutrients\\nOut[269]:\\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 389355 entries, 0 to 389354\\nData columns:\\ndescription    389355  non-null values\\ngroup          389355  non-null values\\nunits          389355  non-null values\\nvalue          389355  non-null values\\nid             389355  non-null values\\ndtypes: float64(1), int64(1), object(3)\\nI noticed that, for whatever reason, there are duplicates in this DataFrame, so it makes\\nthings easier to drop them:\\nIn [270]: nutrients.duplicated().sum()\\nOut[270]: 14179\\nIn [271]: nutrients = nutrients.drop_duplicates()\\nSince 'group' and 'description' is in both DataFrame objects, we can rename them to\\nmake it clear what is what:\\nIn [272]: col_mapping = {'description' : 'food',\\n   .....:                'group'       : 'fgroup'}\\nIn [273]: info = info.rename(columns=col_mapping, copy=False)\\nIn [274]: info\\nOut[274]:\\n<class 'pandas.core.frame.DataFrame'>\"),\n",
       " Document(metadata={}, page_content=\".....:                'group'       : 'fgroup'}\\nIn [273]: info = info.rename(columns=col_mapping, copy=False)\\nIn [274]: info\\nOut[274]:\\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 6636 entries, 0 to 6635\\nData columns:\\nfood            6636  non-null values\\nfgroup          6636  non-null values\\nid              6636  non-null values\\nmanufacturer    5195  non-null values\\ndtypes: int64(1), object(3)\\nIn [275]: col_mapping = {'description' : 'nutrient',\\n   .....:                'group' : 'nutgroup'}\\nIn [276]: nutrients = nutrients.rename(columns=col_mapping, copy=False)\\nIn [277]: nutrients\\nOut[277]:\\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 375176 entries, 0 to 389354\\nData columns:\\nnutrient    375176  non-null values\\nnutgroup    375176  non-null values\\nunits       375176  non-null values\\nvalue       375176  non-null values\\nExample: USDA Food Database | 215\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"id          375176  non-null values\\ndtypes: float64(1), int64(1), object(3)\\nWith all of this done, we’re ready to merge info with nutrients:\\nIn [278]: ndata = pd.merge(nutrients, info, on='id', how='outer')\\nIn [279]: ndata\\nOut[279]:\\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 375176 entries, 0 to 375175\\nData columns:\\nnutrient        375176  non-null values\\nnutgroup        375176  non-null values\\nunits           375176  non-null values\\nvalue           375176  non-null values\\nid              375176  non-null values\\nfood            375176  non-null values\\nfgroup          375176  non-null values\\nmanufacturer    293054  non-null values\\ndtypes: float64(1), int64(1), object(6)\\nIn [280]: ndata.ix[30000]\\nOut[280]:\\nnutrient                       Folic acid\\nnutgroup                         Vitamins\\nunits                                 mcg\\nvalue                                   0\\nid                                   5658\\nfood            Ostrich, top loin, cooked\"),\n",
       " Document(metadata={}, page_content=\"units                                 mcg\\nvalue                                   0\\nid                                   5658\\nfood            Ostrich, top loin, cooked\\nfgroup                   Poultry Products\\nmanufacturer\\nName: 30000\\nThe tools that you need to slice and dice, aggregate, and visualize this dataset will be\\nexplored in detail in the next two chapters, so after you get a handle on those methods\\nyou might return to this dataset. For example, we could a plot of median values by food\\ngroup and nutrient type (see Figure 7-1):\\nIn [281]: result = ndata.groupby(['nutrient', 'fgroup'])['value'].quantile(0.5)\\nIn [282]: result['Zinc, Zn'].order().plot(kind='barh')\\nWith a little cleverness, you can find which food is most dense in each nutrient:\\nby_nutrient = ndata.groupby(['nutgroup', 'nutrient'])\\nget_maximum = lambda x: x.xs(x.value.idxmax())\\nget_minimum = lambda x: x.xs(x.value.idxmin())\\nmax_foods = by_nutrient.apply(get_maximum)[['value', 'food']]\"),\n",
       " Document(metadata={}, page_content=\"get_maximum = lambda x: x.xs(x.value.idxmax())\\nget_minimum = lambda x: x.xs(x.value.idxmin())\\nmax_foods = by_nutrient.apply(get_maximum)[['value', 'food']]\\n# make the food a little smaller\\nmax_foods.food = max_foods.food.str[:50]\\n216 | Chapter 7: \\u2002Data Wrangling: Clean, Transform, Merge, Reshape\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"The resulting DataFrame is a bit too large to display in the book; here is just the 'Amino\\nAcids' nutrient group:\\nIn [284]: max_foods.ix['Amino Acids']['food']\\nOut[284]:\\nnutrient\\nAlanine                           Gelatins, dry powder, unsweetened\\nArginine                               Seeds, sesame flour, low-fat\\nAspartic acid                                   Soy protein isolate\\nCystine                Seeds, cottonseed flour, low fat (glandless)\\nGlutamic acid                                   Soy protein isolate\\nGlycine                           Gelatins, dry powder, unsweetened\\nHistidine                Whale, beluga, meat, dried (Alaska Native)\\nHydroxyproline    KENTUCKY FRIED CHICKEN, Fried Chicken, ORIGINAL R\\nIsoleucine        Soy protein isolate, PROTEIN TECHNOLOGIES INTERNA\\nLeucine           Soy protein isolate, PROTEIN TECHNOLOGIES INTERNA\\nLysine            Seal, bearded (Oogruk), meat, dried (Alaska Nativ\\nMethionine                    Fish, cod, Atlantic, dried and salted\"),\n",
       " Document(metadata={}, page_content='Lysine            Seal, bearded (Oogruk), meat, dried (Alaska Nativ\\nMethionine                    Fish, cod, Atlantic, dried and salted\\nPhenylalanine     Soy protein isolate, PROTEIN TECHNOLOGIES INTERNA\\nProline                           Gelatins, dry powder, unsweetened\\nSerine            Soy protein isolate, PROTEIN TECHNOLOGIES INTERNA\\nThreonine         Soy protein isolate, PROTEIN TECHNOLOGIES INTERNA\\nTryptophan         Sea lion, Steller, meat with fat (Alaska Native)\\nTyrosine          Soy protein isolate, PROTEIN TECHNOLOGIES INTERNA\\nValine            Soy protein isolate, PROTEIN TECHNOLOGIES INTERNA\\nName: food\\nFigure 7-1. Median Zinc values by nutrient group\\nExample: USDA Food Database | 217\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='www.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='CHAPTER 8\\nPlotting and Visualization\\nMaking plots and static or interactive visualizations is one of the most important tasks\\nin data analysis. It may be a part of the exploratory process; for example, helping iden-\\ntify outliers, needed data transformations, or coming up with ideas for models. For\\nothers, building an interactive visualization for the web using a toolkit like d3.js (http:\\n//d3js.org/) may be the end goal. Python has many visualization tools (see the end of\\nthis chapter), but I’ll be mainly focused on matplotlib ( http://matplotlib.sourceforge\\n.net).\\nmatplotlib is a (primarily 2D) desktop plotting package designed for creating publica-\\ntion-quality plots. The project was started by John Hunter in 2002 to enable a MAT-\\nLAB-like plotting interface in Python. He, Fernando Pérez (of IPython), and others have\\ncollaborated for many years since then to make IPython combined with matplotlib a\\nvery functional and productive environment for scientific computing. When used in'),\n",
       " Document(metadata={}, page_content='collaborated for many years since then to make IPython combined with matplotlib a\\nvery functional and productive environment for scientific computing. When used in\\ntandem with a GUI toolkit (for example, within IPython), matplotlib has interactive\\nfeatures like zooming and panning. It supports many different GUI backends on all\\noperating systems and additionally can export graphics to all of the common vector\\nand raster graphics formats: PDF, SVG, JPG, PNG, BMP, GIF, etc. I have used it to\\nproduce almost all of the graphics outside of diagrams in this book.\\nmatplotlib has a number of add-on toolkits, such as mplot3d for 3D plots and basemap\\nfor mapping and projections. I will give an example using basemap to plot data on a map\\nand to read shapefiles at the end of the chapter.\\nTo follow along with the code examples in the chapter, make sure you have started\\nIPython in Pylab mode (ipython --pylab) or enabled GUI event loop integration with\\nthe %gui magic.\\nA Brief matplotlib API Primer'),\n",
       " Document(metadata={}, page_content='IPython in Pylab mode (ipython --pylab) or enabled GUI event loop integration with\\nthe %gui magic.\\nA Brief matplotlib API Primer\\nThere are several ways to interact with matplotlib. The most common is through pylab\\nmode in IPython by running ipython --pylab. This launches IPython configured to be\\nable to support the matplotlib GUI backend of your choice (Tk, wxPython, PyQt, Mac\\n219\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='OS X native, GTK). For most users, the default backend will be sufficient. Pylab mode\\nalso imports a large set of modules and functions into IPython to provide a more MAT-\\nLAB-like interface. You can test that everything is working by making a simple plot:\\nplot(np.arange(10))\\nIf everything is set up right, a new window should pop up with a line plot. You can\\nclose it by using the mouse or entering close(). Matplotlib API functions like plot and \\nclose are all in the matplotlib.pyplot module, which is typically imported by conven-\\ntion as:\\nimport matplotlib.pyplot as plt\\nWhile the pandas plotting functions described later deal with many of the mundane\\ndetails of making plots, should you wish to customize them beyond the function op-\\ntions provided you will need to learn a bit about the matplotlib API.\\nThere is not enough room in the book to give a comprehensive treatment\\nto the breadth and depth of functionality in matplotlib. It should be'),\n",
       " Document(metadata={}, page_content='There is not enough room in the book to give a comprehensive treatment\\nto the breadth and depth of functionality in matplotlib. It should be\\nenough to teach you the ropes to get up and running. The matplotlib\\ngallery and documentation are the best resource for becoming a plotting\\nguru and using advanced features.\\nFigures and Subplots\\nPlots in matplotlib reside within a Figure object. You can create a new figure with\\nplt.figure:\\nIn [13]: fig = plt.figure()\\nFigure 8-1. A more complex matplotlib financial plot\\n220 | Chapter 8: \\u2002Plotting and Visualization\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='If you are in pylab mode in IPython, a new empty window should pop up. plt.fig\\nure has a number of options, notably figsize will guarantee the figure has a certain size\\nand aspect ratio if saved to disk. Figures in matplotlib also support a numbering scheme\\n(for example, plt.figure(2)) that mimics MATLAB. You can get a reference to the\\nactive figure using plt.gcf().\\nYou can’t make a plot with a blank figure. You have to create one or more subplots\\nusing add_subplot:\\nIn [14]: ax1 = fig.add_subplot(2, 2, 1)\\nThis means that the figure should be 2 × 2, and we’re selecting the first of 4 subplots\\n(numbered from 1). If you create the next two subplots, you’ll end up with a figure that\\nlooks like Figure 8-2.\\nIn [15]: ax2 = fig.add_subplot(2, 2, 2)\\nIn [16]: ax3 = fig.add_subplot(2, 2, 3)\\nFigure 8-2. An empty matplotlib Figure with 3 subplots\\nWhen you issue a plotting command like plt.plot([1.5, 3.5, -2, 1.6]), matplotlib'),\n",
       " Document(metadata={}, page_content=\"In [16]: ax3 = fig.add_subplot(2, 2, 3)\\nFigure 8-2. An empty matplotlib Figure with 3 subplots\\nWhen you issue a plotting command like plt.plot([1.5, 3.5, -2, 1.6]), matplotlib\\ndraws on the last figure and subplot used (creating one if necessary), thus hiding the\\nfigure and subplot creation. Thus, if we run the following command, you’ll get some-\\nthing like Figure 8-3:\\nIn [17]: from numpy.random import randn\\nIn [18]: plt.plot(randn(50).cumsum(), 'k--')\\nThe 'k--' is a style option instructing matplotlib to plot a black dashed line. The objects\\nreturned by fig.add_subplot above are AxesSubplot objects, on which you can directly\\nplot on the other empty subplots by calling each one’s instance methods, see Figure 8-4:\\nA Brief matplotlib API Primer | 221\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"In [19]: _ = ax1.hist(randn(100), bins=20, color='k', alpha=0.3)\\nIn [20]: ax2.scatter(np.arange(30), np.arange(30) + 3 * randn(30))\\nYou can find a comprehensive catalogue of plot types in the matplotlib documentation.\\nSince creating a figure with multiple subplots according to a particular layout is such\\na common task, there is a convenience method, plt.subplots, that creates a new figure\\nand returns a NumPy array containing the created subplot objects:\\nFigure 8-3. Figure after single plot\\nFigure 8-4. Figure after additional plots\\n222 | Chapter 8: \\u2002Plotting and Visualization\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content='In [22]: fig, axes = plt.subplots(2, 3)\\nIn [23]: axes\\nOut[23]: \\narray([[Axes(0.125,0.536364;0.227941x0.363636),\\n        Axes(0.398529,0.536364;0.227941x0.363636),\\n        Axes(0.672059,0.536364;0.227941x0.363636)],\\n       [Axes(0.125,0.1;0.227941x0.363636),\\n        Axes(0.398529,0.1;0.227941x0.363636),\\n        Axes(0.672059,0.1;0.227941x0.363636)]], dtype=object)\\nThis is very useful as the axes array can be easily indexed like a two-dimensional array;\\nfor example, axes[0, 1]. You can also indicate that subplots should have the same X\\nor Y axis using sharex and sharey, respectively. This is especially useful when comparing\\ndata on the same scale; otherwise, matplotlib auto-scales plot limits independently. See\\nTable 8-1 for more on this method.\\nTable 8-1. pyplot.subplots options\\nArgument Description\\nnrows Number of rows of subplots\\nncols Number of columns of subplots\\nsharex All subplots should use the same X-axis ticks (adjusting the xlim will affect all subplots)'),\n",
       " Document(metadata={}, page_content='Argument Description\\nnrows Number of rows of subplots\\nncols Number of columns of subplots\\nsharex All subplots should use the same X-axis ticks (adjusting the xlim will affect all subplots)\\nsharey All subplots should use the same Y-axis ticks (adjusting the ylim will affect all subplots)\\nsubplot_kw Dict of keywords for creating the\\n**fig_kw Additional keywords to subplots are used when creating the figure, such as plt.subplots(2, 2,\\nfigsize=(8, 6))\\nAdjusting the spacing around subplots\\nBy default matplotlib leaves a certain amount of padding around the outside of the\\nsubplots and spacing between subplots. This spacing is all specified relative to the\\nheight and width of the plot, so that if you resize the plot either programmatically or\\nmanually using the GUI window, the plot will dynamically adjust itself. The spacing\\ncan be most easily changed using the subplots_adjust Figure method, also available as\\na top-level function:\\nsubplots_adjust(left=None, bottom=None, right=None, top=None,'),\n",
       " Document(metadata={}, page_content=\"can be most easily changed using the subplots_adjust Figure method, also available as\\na top-level function:\\nsubplots_adjust(left=None, bottom=None, right=None, top=None,\\n                wspace=None, hspace=None)\\nwspace and hspace controls the percent of the figure width and figure height, respec-\\ntively, to use as spacing between subplots. Here is a small example where I shrink the\\nspacing all the way to zero (see Figure 8-5):\\nfig, axes = plt.subplots(2, 2, sharex=True, sharey=True)\\nfor i in range(2):\\n    for j in range(2):\\n        axes[i, j].hist(randn(500), bins=50, color='k', alpha=0.5)\\nplt.subplots_adjust(wspace=0, hspace=0)\\nA Brief matplotlib API Primer | 223\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"Figure 8-5. Figure with no inter-subplot spacing\\nYou may notice that the axis labels overlap. matplotlib doesn’t check whether the labels\\noverlap, so in a case like this you would need to fix the labels yourself by specifying\\nexplicit tick locations and tick labels. More on this in the coming sections.\\nColors, Markers, and Line Styles\\nMatplotlib’s main plot function accepts arrays of X and Y coordinates and optionally\\na string abbreviation indicating color and line style. For example, to plot x versus y with\\ngreen dashes, you would execute:\\nax.plot(x, y, 'g--')\\nThis way of specifying both color and linestyle in a string is provided as a convenience;\\nin practice if you were creating plots programmatically you might prefer not to have to\\nmunge strings together to create plots with the desired style. The same plot could also\\nhave been expressed more explicitly as:\\nax.plot(x, y, linestyle='--', color='g')\\nThere are a number of color abbreviations provided for commonly-used colors, but any\"),\n",
       " Document(metadata={}, page_content=\"have been expressed more explicitly as:\\nax.plot(x, y, linestyle='--', color='g')\\nThere are a number of color abbreviations provided for commonly-used colors, but any\\ncolor on the spectrum can be used by specifying its RGB value (for example, '#CECE\\nCE'). You can see the full set of linestyles by looking at the docstring for plot.\\nLine plots can additionally have markers to highlight the actual data points. Since mat-\\nplotlib creates a continuous line plot, interpolating between points, it can occasionally\\nbe unclear where the points lie. The marker can be part of the style string, which must\\nhave color followed by marker type and line style (see Figure 8-6):\\nIn [28]: plt.plot(randn(30).cumsum(), 'ko--')\\n224 | Chapter 8: \\u2002Plotting and Visualization\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"This could also have been written more explicitly as:\\nplot(randn(30).cumsum(), color='k', linestyle='dashed', marker='o')\\nFor line plots, you will notice that subsequent points are linearly interpolated by de-\\nfault. This can be altered with the drawstyle option:\\nIn [30]: data = randn(30).cumsum()\\nIn [31]: plt.plot(data, 'k--', label='Default')\\nOut[31]: [<matplotlib.lines.Line2D at 0x461cdd0>]\\nIn [32]: plt.plot(data, 'k-', drawstyle='steps-post', label='steps-post')\\nOut[32]: [<matplotlib.lines.Line2D at 0x461f350>]\\nIn [33]: plt.legend(loc='best')\\nTicks, Labels, and Legends\\nFor most kinds of plot decorations, there are two main ways to do things: using the\\nprocedural pyplot interface (which will be very familiar to MATLAB users) and the\\nmore object-oriented native matplotlib API.\\nThe pyplot interface, designed for interactive use, consists of methods like xlim,\\nxticks, and xticklabels. These control the plot range, tick locations, and tick labels,\"),\n",
       " Document(metadata={}, page_content='The pyplot interface, designed for interactive use, consists of methods like xlim,\\nxticks, and xticklabels. These control the plot range, tick locations, and tick labels,\\nrespectively. They can be used in two ways:\\n• Called with no arguments returns the current parameter value. For example \\nplt.xlim() returns the current X axis plotting range\\nFigure 8-6. Line plot with markers example\\nA Brief matplotlib API Primer | 225\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='• Called with parameters sets the parameter value. So plt.xlim([0, 10]), sets the X\\naxis range to 0 to 10\\nAll such methods act on the active or most recently-created AxesSubplot. Each of them\\ncorresponds to two methods on the subplot object itself; in the case of xlim these are \\nax.get_xlim and ax.set_xlim. I prefer to use the subplot instance methods myself in\\nthe interest of being explicit (and especially when working with multiple subplots), but\\nyou can certainly use whichever you find more convenient.\\nSetting the title, axis labels, ticks, and ticklabels\\nTo illustrate customizing the axes, I’ll create a simple figure and plot of a random walk\\n(see Figure 8-8):\\nIn [34]: fig = plt.figure(); ax = fig.add_subplot(1, 1, 1)\\nIn [35]: ax.plot(randn(1000).cumsum())\\nTo change the X axis ticks, it’s easiest to use set_xticks and set_xticklabels. The\\nformer instructs matplotlib where to place the ticks along the data range; by default'),\n",
       " Document(metadata={}, page_content=\"To change the X axis ticks, it’s easiest to use set_xticks and set_xticklabels. The\\nformer instructs matplotlib where to place the ticks along the data range; by default\\nthese locations will also be the labels. But we can set any other values as the labels using\\nset_xticklabels:\\nIn [36]: ticks = ax.set_xticks([0, 250, 500, 750, 1000])\\nIn [37]: labels = ax.set_xticklabels(['one', 'two', 'three', 'four', 'five'],\\n   ....:                             rotation=30, fontsize='small')\\nLastly, set_xlabel gives a name to the X axis and set_title the subplot title:\\nFigure 8-7. Line plot with different drawstyle options\\n226 | Chapter 8: \\u2002Plotting and Visualization\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"In [38]: ax.set_title('My first matplotlib plot')\\nOut[38]: <matplotlib.text.Text at 0x7f9190912850>\\nIn [39]: ax.set_xlabel('Stages')\\nSee Figure 8-9 for the resulting figure. Modifying the Y axis consists of the same process,\\nsubstituting y for x in the above.\\nFigure 8-9. Simple plot for illustrating xticks\\nFigure 8-8. Simple plot for illustrating xticks\\nA Brief matplotlib API Primer | 227\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"Adding legends\\nLegends are another critical element for identifying plot elements. There are a couple\\nof ways to add one. The easiest is to pass the label argument when adding each piece\\nof the plot:\\nIn [40]: fig = plt.figure(); ax = fig.add_subplot(1, 1, 1)\\nIn [41]: ax.plot(randn(1000).cumsum(), 'k', label='one')\\nOut[41]: [<matplotlib.lines.Line2D at 0x4720a90>]\\nIn [42]: ax.plot(randn(1000).cumsum(), 'k--', label='two')\\nOut[42]: [<matplotlib.lines.Line2D at 0x4720f90>]\\nIn [43]: ax.plot(randn(1000).cumsum(), 'k.', label='three')\\nOut[43]: [<matplotlib.lines.Line2D at 0x4723550>]\\nOnce you’ve done this, you can either call ax.legend() or plt.legend() to automatically\\ncreate a legend:\\nIn [44]: ax.legend(loc='best')\\nSee Figure 8-10. The loc tells matplotlib where to place the plot. If you aren’t picky\\n'best' is a good option, as it will choose a location that is most out of the way. To\\nexclude one or more elements from the legend, pass no label or label='_nolegend_'.\"),\n",
       " Document(metadata={}, page_content=\"'best' is a good option, as it will choose a location that is most out of the way. To\\nexclude one or more elements from the legend, pass no label or label='_nolegend_'.\\nAnnotations and Drawing on a Subplot\\nIn addition to the standard plot types, you may wish to draw your own plot annotations,\\nwhich could consist of text, arrows, or other shapes.\\nFigure 8-10. Simple plot with 3 lines and legend\\n228 | Chapter 8: \\u2002Plotting and Visualization\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"Annotations and text can be added using the text, arrow, and annotate functions.\\ntext draws text at given coordinates (x, y) on the plot with optional custom styling:\\nax.text(x, y, 'Hello world!',\\n        family='monospace', fontsize=10)\\nAnnotations can draw both text and arrows arranged appropriately. As an example,\\nlet’s plot the closing S&P 500 index price since 2007 (obtained from Yahoo! Finance)\\nand annotate it with some of the important dates from the 2008-2009 financial crisis.\\nSee Figure 8-11 for the result:\\nfrom datetime import datetime\\nfig = plt.figure()\\nax = fig.add_subplot(1, 1, 1)\\ndata = pd.read_csv('ch08/spx.csv', index_col=0, parse_dates=True)\\nspx = data['SPX']\\nspx.plot(ax=ax, style='k-')\\ncrisis_data = [\\n    (datetime(2007, 10, 11), 'Peak of bull market'),\\n    (datetime(2008, 3, 12), 'Bear Stearns Fails'),\\n    (datetime(2008, 9, 15), 'Lehman Bankruptcy')\\n]\\nfor date, label in crisis_data:\\n    ax.annotate(label, xy=(date, spx.asof(date) + 50),\"),\n",
       " Document(metadata={}, page_content=\"(datetime(2008, 3, 12), 'Bear Stearns Fails'),\\n    (datetime(2008, 9, 15), 'Lehman Bankruptcy')\\n]\\nfor date, label in crisis_data:\\n    ax.annotate(label, xy=(date, spx.asof(date) + 50),\\n                xytext=(date, spx.asof(date) + 200),\\n                arrowprops=dict(facecolor='black'),\\n                horizontalalignment='left', verticalalignment='top')\\n# Zoom in on 2007-2010\\nax.set_xlim(['1/1/2007', '1/1/2011'])\\nax.set_ylim([600, 1800])\\nax.set_title('Important dates in 2008-2009 financial crisis')\\nSee the online matplotlib gallery for many more annotation examples to learn from.\\nDrawing shapes requires some more care. matplotlib has objects that represent many\\ncommon shapes, referred to as patches. Some of these, like Rectangle and Circle are\\nfound in matplotlib.pyplot, but the full set is located in matplotlib.patches.\\nTo add a shape to a plot, you create the patch object shp and add it to a subplot by\\ncalling ax.add_patch(shp) (see Figure 8-12):\\nfig = plt.figure()\"),\n",
       " Document(metadata={}, page_content=\"To add a shape to a plot, you create the patch object shp and add it to a subplot by\\ncalling ax.add_patch(shp) (see Figure 8-12):\\nfig = plt.figure()\\nax = fig.add_subplot(1, 1, 1)\\nrect = plt.Rectangle((0.2, 0.75), 0.4, 0.15, color='k', alpha=0.3)\\ncirc = plt.Circle((0.7, 0.2), 0.15, color='b', alpha=0.3)\\npgon = plt.Polygon([[0.15, 0.15], [0.35, 0.4], [0.2, 0.6]],\\n                   color='g', alpha=0.5)\\nA Brief matplotlib API Primer | 229\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content='ax.add_patch(rect)\\nax.add_patch(circ)\\nax.add_patch(pgon)\\nFigure 8-11. Important dates in 2008-2009 financial crisis\\nFigure 8-12. Figure composed from 3 different patches\\nIf you look at the implementation of many familiar plot types, you will see that they\\nare assembled from patches.\\n230 | Chapter 8: \\u2002Plotting and Visualization\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"Saving Plots to File\\nThe active figure can be saved to file using plt.savefig. This method is equivalent to\\nthe figure object’s savefig instance method. For example, to save an SVG version of a\\nfigure, you need only type:\\nplt.savefig('figpath.svg')\\nThe file type is inferred from the file extension. So if you used .pdf instead you would\\nget a PDF. There are a couple of important options that I use frequently for publishing\\ngraphics: dpi, which controls the dots-per-inch resolution, and bbox_inches, which can\\ntrim the whitespace around the actual figure. To get the same plot as a PNG above with\\nminimal whitespace around the plot and at 400 DPI, you would do:\\nplt.savefig('figpath.png', dpi=400, bbox_inches='tight')\\nsavefig doesn’t have to write to disk; it can also write to any file-like object, such as a\\nStringIO:\\nfrom io import StringIO\\nbuffer = StringIO()\\nplt.savefig(buffer)\\nplot_data = buffer.getvalue()\\nFor example, this is useful for serving dynamically-generated images over the web.\"),\n",
       " Document(metadata={}, page_content=\"StringIO:\\nfrom io import StringIO\\nbuffer = StringIO()\\nplt.savefig(buffer)\\nplot_data = buffer.getvalue()\\nFor example, this is useful for serving dynamically-generated images over the web.\\nTable 8-2. Figure.savefig options\\nArgument Description\\nfname String containing a filepath or a Python file-like object. The figure format is inferred from the file\\nextension, e.g. .pdf for PDF or .png for PNG.\\ndpi The figure resolution in dots per inch; defaults to 100 out of the box but can be configured\\nfacecolor, edge\\ncolor\\nThe color of the figure background outside of the subplots. 'w' (white), by default\\nformat The explicit file format to use ('png', 'pdf', 'svg', 'ps', 'eps', ...)\\nbbox_inches The portion of the figure to save. If 'tight' is passed, will attempt to trim the empty space around\\nthe figure\\nmatplotlib Configuration\\nmatplotlib comes configured with color schemes and defaults that are geared primarily\"),\n",
       " Document(metadata={}, page_content=\"the figure\\nmatplotlib Configuration\\nmatplotlib comes configured with color schemes and defaults that are geared primarily\\ntoward preparing figures for publication. Fortunately, nearly all of the default behavior\\ncan be customized via an extensive set of global parameters governing figure size, sub-\\nplot spacing, colors, font sizes, grid styles, and so on. There are two main ways to\\ninteract with the matplotlib configuration system. The first is programmatically from\\nPython using the rc method. For example, to set the global default figure size to be 10\\nx 10, you could enter:\\nplt.rc('figure', figsize=(10, 10))\\nA Brief matplotlib API Primer | 231\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"The first argument to rc is the component you wish to customize, such as 'figure',\\n'axes', 'xtick', 'ytick', 'grid', 'legend' or many others. After that can follow a\\nsequence of keyword arguments indicating the new parameters. An easy way to write\\ndown the options in your program is as a dict:\\nfont_options = {'family' : 'monospace',\\n                'weight' : 'bold',\\n                'size'   : 'small'}\\nplt.rc('font', **font_options)\\nFor more extensive customization and to see a list of all the options, matplotlib comes\\nwith a configuration file matplotlibrc in the matplotlib/mpl-data directory. If you cus-\\ntomize this file and place it in your home directory titled .matplotlibrc, it will be loaded\\neach time you use matplotlib.\\nPlotting Functions in pandas\\nAs you’ve seen, matplotlib is actually a fairly low-level tool. You assemble a plot from\\nits base components: the data display (the type of plot: line, bar, box, scatter, contour,\"),\n",
       " Document(metadata={}, page_content='As you’ve seen, matplotlib is actually a fairly low-level tool. You assemble a plot from\\nits base components: the data display (the type of plot: line, bar, box, scatter, contour,\\netc.), legend, title, tick labels, and other annotations. Part of the reason for this is that\\nin many cases the data needed to make a complete plot is spread across many objects.\\nIn pandas we have row labels, column labels, and possibly grouping information. This\\nmeans that many kinds of fully-formed plots that would ordinarily require a lot of\\nmatplotlib code can be expressed in one or two concise statements. Therefore, pandas\\nhas an increasing number of high-level plotting methods for creating standard visual-\\nizations that take advantage of how data is organized in DataFrame objects.\\nAs of this writing, the plotting functionality in pandas is undergoing\\nquite a bit of work. As part of the 2012 Google Summer of Code pro-\\ngram, a student is working full time to add features and to make the'),\n",
       " Document(metadata={}, page_content='quite a bit of work. As part of the 2012 Google Summer of Code pro-\\ngram, a student is working full time to add features and to make the\\ninterface more consistent and usable. Thus, it’s possible that this code\\nmay fall out-of-date faster than the other things in this book. The online\\npandas documentation will be the best resource in that event.\\nLine Plots\\nSeries and DataFrame each have a plot method for making many different plot types.\\nBy default, they make line plots (see Figure 8-13):\\nIn [55]: s = Series(np.random.randn(10).cumsum(), index=np.arange(0, 100, 10))\\nIn [56]: s.plot()\\nThe Series object’s index is passed to matplotlib for plotting on the X axis, though this\\ncan be disabled by passing use_index=False. The X axis ticks and limits can be adjusted\\nusing the xticks and xlim options, and Y axis respectively using yticks and ylim. See\\n232 | Chapter 8: \\u2002Plotting and Visualization\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"Table 8-3 for a full listing of plot options. I’ll comment on a few more of them through-\\nout this section and leave the rest to you to explore.\\nMost of pandas’s plotting methods accept an optional ax parameter, which can be a\\nmatplotlib subplot object. This gives you more flexible placement of subplots in a grid\\nlayout. There will be more on this in the later section on the matplotlib API.\\nDataFrame’s plot method plots each of its columns as a different line on the same\\nsubplot, creating a legend automatically (see Figure 8-14):\\nIn [57]: df = DataFrame(np.random.randn(10, 4).cumsum(0),\\n   ....:                columns=['A', 'B', 'C', 'D'],\\n   ....:                index=np.arange(0, 100, 10))\\nIn [58]: df.plot()\\nAdditional keyword arguments to plot are passed through to the re-\\nspective matplotlib plotting function, so you can further customize\\nthese plots by learning more about the matplotlib API.\\nTable 8-3. Series.plot method arguments\\nArgument Description\\nlabel Label for plot legend\"),\n",
       " Document(metadata={}, page_content=\"these plots by learning more about the matplotlib API.\\nTable 8-3. Series.plot method arguments\\nArgument Description\\nlabel Label for plot legend\\nax matplotlib subplot object to plot on. If nothing passed, uses active matplotlib subplot\\nstyle Style string, like 'ko--', to be passed to matplotlib.\\nalpha The plot fill opacity (from 0 to 1)\\nFigure 8-13. Simple Series plot example\\nPlotting Functions in pandas | 233\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"Argument Description\\nkind Can be 'line', 'bar', 'barh', 'kde'\\nlogy Use logarithmic scaling on the Y axis\\nuse_index Use the object index for tick labels\\nrot Rotation of tick labels (0 through 360)\\nxticks Values to use for X axis ticks\\nyticks Values to use for Y axis ticks\\nxlim X axis limits (e.g. [0, 10])\\nylim Y axis limits\\ngrid Display axis grid (on by default)\\nDataFrame has a number of options allowing some flexibility with how the columns\\nare handled; for example, whether to plot them all on the same subplot or to create\\nseparate subplots. See Table 8-4 for more on these.\\nTable 8-4. DataFrame-specific plot arguments\\nArgument Description\\nsubplots Plot each DataFrame column in a separate subplot\\nsharex If subplots=True, share the same X axis, linking ticks and limits\\nsharey If subplots=True, share the same Y axis\\nfigsize Size of figure to create as tuple\\nFigure 8-14. Simple DataFrame plot example\\n234 | Chapter 8: \\u2002Plotting and Visualization\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"Argument Description\\ntitle Plot title as string\\nlegend Add a subplot legend (True by default)\\nsort_columns Plot columns in alphabetical order; by default uses existing column order\\nFor time series plotting, see Chapter 10.\\nBar Plots\\nMaking bar plots instead of line plots is a simple as passing kind='bar' (for vertical\\nbars) or kind='barh' (for horizontal bars). In this case, the Series or DataFrame index\\nwill be used as the X (bar) or Y (barh) ticks (see Figure 8-15):\\nIn [59]: fig, axes = plt.subplots(2, 1)\\nIn [60]: data = Series(np.random.rand(16), index=list('abcdefghijklmnop'))\\nIn [61]: data.plot(kind='bar', ax=axes[0], color='k', alpha=0.7)\\nOut[61]: <matplotlib.axes.AxesSubplot at 0x4ee7750>\\nIn [62]: data.plot(kind='barh', ax=axes[1], color='k', alpha=0.7)\\nFor more on the plt.subplots function and matplotlib axes and figures,\\nsee the later section in this chapter.\\nWith a DataFrame, bar plots group the values in each row together in a group in bars,\"),\n",
       " Document(metadata={}, page_content=\"For more on the plt.subplots function and matplotlib axes and figures,\\nsee the later section in this chapter.\\nWith a DataFrame, bar plots group the values in each row together in a group in bars,\\nside by side, for each value. See Figure 8-16:\\nIn [63]: df = DataFrame(np.random.rand(6, 4),\\n   ....:                index=['one', 'two', 'three', 'four', 'five', 'six'],\\n   ....:                columns=pd.Index(['A', 'B', 'C', 'D'], name='Genus'))\\nIn [64]: df\\nOut[64]: \\nGenus         A         B         C         D\\none    0.301686  0.156333  0.371943  0.270731\\ntwo    0.750589  0.525587  0.689429  0.358974\\nthree  0.381504  0.667707  0.473772  0.632528\\nfour   0.942408  0.180186  0.708284  0.641783\\nfive   0.840278  0.909589  0.010041  0.653207\\nsix    0.062854  0.589813  0.811318  0.060217\\nIn [65]: df.plot(kind='bar')\\nPlotting Functions in pandas | 235\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"Figure 8-15. Horizonal and vertical bar plot example\\nNote that the name “Genus” on the DataFrame’s columns is used to title the legend.\\nStacked bar plots are created from a DataFrame by passing stacked=True, resulting in\\nthe value in each row being stacked together (see Figure 8-17):\\nIn [67]: df.plot(kind='barh', stacked=True, alpha=0.5)\\nA useful recipe for bar plots (as seen in an earlier chapter) is to visualize\\na Series’s value frequency using value_counts: s.value_counts\\n().plot(kind='bar')\\nReturning to the tipping data set used earlier in the book, suppose we wanted to make\\na stacked bar plot showing the percentage of data points for each party size on each\\nday. I load the data using read_csv and make a cross-tabulation by day and party size:\\nIn [68]: tips = pd.read_csv('ch08/tips.csv')\\nIn [69]: party_counts = pd.crosstab(tips.day, tips.size)\\nIn [70]: party_counts\\nOut[70]: \\nsize  1   2   3   4  5  6\\nday                      \\nFri   1  16   1   1  0  0\\nSat   2  53  18  13  1  0\"),\n",
       " Document(metadata={}, page_content='In [69]: party_counts = pd.crosstab(tips.day, tips.size)\\nIn [70]: party_counts\\nOut[70]: \\nsize  1   2   3   4  5  6\\nday                      \\nFri   1  16   1   1  0  0\\nSat   2  53  18  13  1  0\\nSun   0  39  15  18  3  1\\nThur  1  48   4   5  1  3\\n236 | Chapter 8: \\u2002Plotting and Visualization\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='# Not many 1- and 6-person parties\\nIn [71]: party_counts = party_counts.ix[:, 2:5]\\nFigure 8-16. DataFrame bar plot example\\nFigure 8-17. DataFrame stacked bar plot example\\nThen, normalize so that each row sums to 1 (I have to cast to float to avoid integer\\ndivision issues on Python 2.7) and make the plot (see Figure 8-18):\\n# Normalize to sum to 1\\nIn [72]: party_pcts = party_counts.div(party_counts.sum(1).astype(float), axis=0)\\nPlotting Functions in pandas | 237\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"In [73]: party_pcts\\nOut[73]: \\nsize         2         3         4         5\\nday                                         \\nFri   0.888889  0.055556  0.055556  0.000000\\nSat   0.623529  0.211765  0.152941  0.011765\\nSun   0.520000  0.200000  0.240000  0.040000\\nThur  0.827586  0.068966  0.086207  0.017241\\nIn [74]: party_pcts.plot(kind='bar', stacked=True)\\nFigure 8-18. Fraction of parties by size on each day\\nSo you can see that party sizes appear to increase on the weekend in this data set.\\nHistograms and Density Plots\\nA histogram, with which you may be well-acquainted, is a kind of bar plot that gives a\\ndiscretized display of value frequency. The data points are split into discrete, evenly\\nspaced bins, and the number of data points in each bin is plotted. Using the tipping\\ndata from before, we can make a histogram of tip percentages of the total bill using the \\nhist method on the Series (see Figure 8-19):\\nIn [76]: tips['tip_pct'] = tips['tip'] / tips['total_bill']\"),\n",
       " Document(metadata={}, page_content=\"data from before, we can make a histogram of tip percentages of the total bill using the \\nhist method on the Series (see Figure 8-19):\\nIn [76]: tips['tip_pct'] = tips['tip'] / tips['total_bill']\\nIn [77]: tips['tip_pct'].hist(bins=50)\\n238 | Chapter 8: \\u2002Plotting and Visualization\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"Figure 8-19. Histogram of tip percentages\\nA related plot type is a density plot, which is formed by computing an estimate of a\\ncontinuous probability distribution that might have generated the observed data. A\\nusual procedure is to approximate this distribution as a mixture of kernels, that is,\\nsimpler distributions like the normal (Gaussian) distribution. Thus, density plots are\\nalso known as KDE (kernel density estimate) plots. Using plot with kind='kde' makes\\na density plot using the standard mixture-of-normals KDE (see Figure 8-20):\\nIn [79]: tips['tip_pct'].plot(kind='kde')\\nThese two plot types are often plotted together; the histogram in normalized form (to\\ngive a binned density) with a kernel density estimate plotted on top. As an example,\\nconsider a bimodal distribution consisting of draws from two different standard normal\\ndistributions (see Figure 8-21):\\nIn [81]: comp1 = np.random.normal(0, 1, size=200)  # N(0, 1)\\nIn [82]: comp2 = np.random.normal(10, 2, size=200)  # N(10, 4)\"),\n",
       " Document(metadata={}, page_content=\"distributions (see Figure 8-21):\\nIn [81]: comp1 = np.random.normal(0, 1, size=200)  # N(0, 1)\\nIn [82]: comp2 = np.random.normal(10, 2, size=200)  # N(10, 4)\\nIn [83]: values = Series(np.concatenate([comp1, comp2]))\\nIn [84]: values.hist(bins=100, alpha=0.3, color='k', normed=True)\\nOut[84]: <matplotlib.axes.AxesSubplot at 0x5cd2350>\\nIn [85]: values.plot(kind='kde', style='k--')\\nScatter Plots\\nScatter plots are a useful way of examining the relationship between two one-dimen-\\nsional data series. matplotlib has a scatter plotting method that is the workhorse of\\nPlotting Functions in pandas | 239\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"making these kinds of plots. To give an example, I load the macrodata dataset from the\\nstatsmodels project, select a few variables, then compute log differences:\\nIn [86]: macro = pd.read_csv('ch08/macrodata.csv')\\nIn [87]: data = macro[['cpi', 'm1', 'tbilrate', 'unemp']]\\nIn [88]: trans_data = np.log(data).diff().dropna()\\nFigure 8-20. Density plot of tip percentages\\nFigure 8-21. Normalized histogram of normal mixture with density estimate\\n240 | Chapter 8: \\u2002Plotting and Visualization\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"In [89]: trans_data[-5:]\\nOut[89]: \\n          cpi        m1  tbilrate     unemp\\n198 -0.007904  0.045361 -0.396881  0.105361\\n199 -0.021979  0.066753 -2.277267  0.139762\\n200  0.002340  0.010286  0.606136  0.160343\\n201  0.008419  0.037461 -0.200671  0.127339\\n202  0.008894  0.012202 -0.405465  0.042560\\nIt’s easy to plot a simple scatter plot using plt.scatter (see Figure 8-22):\\nIn [91]: plt.scatter(trans_data['m1'], trans_data['unemp'])\\nOut[91]: <matplotlib.collections.PathCollection at 0x43c31d0>\\nIn [92]: plt.title('Changes in log %s vs. log %s' % ('m1', 'unemp'))\\nFigure 8-22. A simple scatter plot\\nIn exploratory data analysis it’s helpful to be able to look at all the scatter plots among\\na group of variables; this is known as a pairs plot or scatter plot matrix. Making such a\\nplot from scratch is a bit of work, so pandas has a scatter_matrix function for creating\\none from a DataFrame. It also supports placing histograms or density plots of each\"),\n",
       " Document(metadata={}, page_content=\"plot from scratch is a bit of work, so pandas has a scatter_matrix function for creating\\none from a DataFrame. It also supports placing histograms or density plots of each\\nvariable along the diagonal. See Figure 8-23 for the resulting plot:\\nIn [93]: scatter_matrix(trans_data, diagonal='kde', color='k', alpha=0.3)\\nPlotting Maps: Visualizing Haiti Earthquake Crisis Data\\nUshahidi is a non-profit software company that enables crowdsourcing of information\\nrelated to natural disasters and geopolitical events via text message. Many of these data\\nsets are then published on their website for analysis and visualization. I downloaded\\nPlotting Maps: Visualizing Haiti Earthquake Crisis Data | 241\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"the data collected during the 2010 Haiti earthquake crisis and aftermath, and I’ll show\\nyou how I prepared the data for analysis and visualization using pandas and other tools\\nwe have looked at thus far. After downloading the CSV file from the above link, we can\\nload it into a DataFrame using read_csv:\\nIn [94]: data = pd.read_csv('ch08/Haiti.csv')\\nIn [95]: data\\nOut[95]: \\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 3593 entries, 0 to 3592\\nData columns:\\nSerial            3593  non-null values\\nINCIDENT TITLE    3593  non-null values\\nINCIDENT DATE     3593  non-null values\\nLOCATION          3593  non-null values\\nDESCRIPTION       3593  non-null values\\nCATEGORY          3587  non-null values\\nLATITUDE          3593  non-null values\\nLONGITUDE         3593  non-null values\\nAPPROVED          3593  non-null values\\nVERIFIED          3593  non-null values\\ndtypes: float64(2), int64(1), object(7)\\nIt’s easy now to tinker with this data set to see what kinds of things we might want to\"),\n",
       " Document(metadata={}, page_content=\"VERIFIED          3593  non-null values\\ndtypes: float64(2), int64(1), object(7)\\nIt’s easy now to tinker with this data set to see what kinds of things we might want to\\ndo with it. Each row represents a report sent from someone’s mobile phone indicating\\nan emergency or some other problem. Each has an associated timestamp and a location\\nas latitude and longitude:\\nIn [96]: data[['INCIDENT DATE', 'LATITUDE', 'LONGITUDE']][:10]\\nOut[96]: \\n      INCIDENT DATE   LATITUDE   LONGITUDE\\nFigure 8-23. Scatter plot matrix of statsmodels macro data\\n242 | Chapter 8: \\u2002Plotting and Visualization\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"0  05/07/2010 17:26  18.233333  -72.533333\\n1  28/06/2010 23:06  50.226029    5.729886\\n2  24/06/2010 16:21  22.278381  114.174287\\n3  20/06/2010 21:59  44.407062    8.933989\\n4  18/05/2010 16:26  18.571084  -72.334671\\n5  26/04/2010 13:14  18.593707  -72.310079\\n6  26/04/2010 14:19  18.482800  -73.638800\\n7  26/04/2010 14:27  18.415000  -73.195000\\n8  15/03/2010 10:58  18.517443  -72.236841\\n9  15/03/2010 11:00  18.547790  -72.410010\\nThe CATEGORY field contains a comma-separated list of codes indicating the type of\\nmessage:\\nIn [97]: data['CATEGORY'][:6]\\nOut[97]: \\n0          1. Urgences | Emergency, 3. Public Health, \\n1    1. Urgences | Emergency, 2. Urgences logistiques \\n2    2. Urgences logistiques | Vital Lines, 8. Autre |\\n3                            1. Urgences | Emergency, \\n4                            1. Urgences | Emergency, \\n5                       5e. Communication lines down, \\nName: CATEGORY\\nIf you notice above in the data summary, some of the categories are missing, so we\"),\n",
       " Document(metadata={}, page_content='5                       5e. Communication lines down, \\nName: CATEGORY\\nIf you notice above in the data summary, some of the categories are missing, so we\\nmight want to drop these data points. Additionally, calling describe shows that there\\nare some aberrant locations:\\nIn [98]: data.describe()\\nOut[98]: \\n            Serial     LATITUDE    LONGITUDE\\ncount  3593.000000  3593.000000  3593.000000\\nmean   2080.277484    18.611495   -72.322680\\nstd    1171.100360     0.738572     3.650776\\nmin       4.000000    18.041313   -74.452757\\n25%    1074.000000    18.524070   -72.417500\\n50%    2163.000000    18.539269   -72.335000\\n75%    3088.000000    18.561820   -72.293570\\nmax    4052.000000    50.226029   114.174287\\nCleaning the bad locations and removing the missing categories is now fairly simple:\\nIn [99]: data = data[(data.LATITUDE > 18) & (data.LATITUDE < 20) &\\n   ....:             (data.LONGITUDE > -75) & (data.LONGITUDE < -70)\\n   ....:             & data.CATEGORY.notnull()]'),\n",
       " Document(metadata={}, page_content=\"In [99]: data = data[(data.LATITUDE > 18) & (data.LATITUDE < 20) &\\n   ....:             (data.LONGITUDE > -75) & (data.LONGITUDE < -70)\\n   ....:             & data.CATEGORY.notnull()]\\nNow we might want to do some analysis or visualization of this data by category, but\\neach category field may have multiple categories. Additionally, each category is given\\nas a code plus an English and possibly also a French code name. Thus, a little bit of\\nwrangling is required to get the data into a more agreeable form. First, I wrote these\\ntwo functions to get a list of all the categories and to split each category into a code and\\nan English name:\\ndef to_cat_list(catstr):\\n    stripped = (x.strip() for x in catstr.split(','))\\nPlotting Maps: Visualizing Haiti Earthquake Crisis Data | 243\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"return [x for x in stripped if x]\\ndef get_all_categories(cat_series):\\n    cat_sets = (set(to_cat_list(x)) for x in cat_series)\\n    return sorted(set.union(*cat_sets))\\ndef get_english(cat):\\n    code, names = cat.split('.')\\n    if '|' in names:\\n        names = names.split(' | ')[1]\\n    return code, names.strip()\\nYou can test out that the get_english function does what you expect:\\nIn [101]: get_english('2. Urgences logistiques | Vital Lines')\\nOut[101]: ('2', 'Vital Lines')\\nNow, I make a dict mapping code to name because we’ll use the codes for analysis.\\nWe’ll use this later when adorning plots (note the use of a generator expression in lieu\\nof a list comprehension):\\nIn [102]: all_cats = get_all_categories(data.CATEGORY)\\n# Generator expression\\nIn [103]: english_mapping = dict(get_english(x) for x in all_cats)\\nIn [104]: english_mapping['2a']\\nOut[104]: 'Food Shortage'\\nIn [105]: english_mapping['6c']\\nOut[105]: 'Earthquake and aftershocks'\"),\n",
       " Document(metadata={}, page_content=\"In [103]: english_mapping = dict(get_english(x) for x in all_cats)\\nIn [104]: english_mapping['2a']\\nOut[104]: 'Food Shortage'\\nIn [105]: english_mapping['6c']\\nOut[105]: 'Earthquake and aftershocks'\\nThere are many ways to go about augmenting the data set to be able to easily select\\nrecords by category. One way is to add indicator (or dummy) columns, one for each\\ncategory. To do that, first extract the unique category codes and construct a DataFrame\\nof zeros having those as its columns and the same index as data:\\ndef get_code(seq):\\n    return [x.split('.')[0] for x in seq if x]\\nall_codes = get_code(all_cats)\\ncode_index = pd.Index(np.unique(all_codes))\\ndummy_frame = DataFrame(np.zeros((len(data), len(code_index))),\\n                        index=data.index, columns=code_index)\\nIf all goes well, dummy_frame should look something like this:\\nIn [107]: dummy_frame.ix[:, :6]\\nOut[107]: \\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 3569 entries, 0 to 3592\\nData columns:\"),\n",
       " Document(metadata={}, page_content=\"If all goes well, dummy_frame should look something like this:\\nIn [107]: dummy_frame.ix[:, :6]\\nOut[107]: \\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 3569 entries, 0 to 3592\\nData columns:\\n1     3569  non-null values\\n1a    3569  non-null values\\n1b    3569  non-null values\\n1c    3569  non-null values\\n244 | Chapter 8: \\u2002Plotting and Visualization\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"1d    3569  non-null values\\n2     3569  non-null values\\ndtypes: float64(6)\\nAs you recall, the trick is then to set the appropriate entries of each row to 1, lastly\\njoining this with data:\\nfor row, cat in zip(data.index, data.CATEGORY):\\n    codes = get_code(to_cat_list(cat))\\n    dummy_frame.ix[row, codes] = 1\\ndata = data.join(dummy_frame.add_prefix('category_'))\\ndata finally now has new columns like:\\nIn [109]: data.ix[:, 10:15]\\nOut[109]: \\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 3569 entries, 0 to 3592\\nData columns:\\ncategory_1     3569  non-null values\\ncategory_1a    3569  non-null values\\ncategory_1b    3569  non-null values\\ncategory_1c    3569  non-null values\\ncategory_1d    3569  non-null values\\ndtypes: float64(5)\\nLet’s make some plots! As this is spatial data, we’d like to plot the data by category on\\na map of Haiti. The basemap toolkit (http://matplotlib.github.com/basemap), an add-on\\nto matplotlib, enables plotting 2D data on maps in Python. basemap provides many\"),\n",
       " Document(metadata={}, page_content=\"a map of Haiti. The basemap toolkit (http://matplotlib.github.com/basemap), an add-on\\nto matplotlib, enables plotting 2D data on maps in Python. basemap provides many\\ndifferent globe projections and a means for transforming projecting latitude and lon-\\ngitude coordinates on the globe onto a two-dimensional matplotlib plot. After some\\ntrial and error and using the above data as a guideline, I wrote this function which draws\\na simple black and white map of Haiti:\\nfrom mpl_toolkits.basemap import Basemap\\nimport matplotlib.pyplot as plt\\ndef basic_haiti_map(ax=None, lllat=17.25, urlat=20.25,\\n                    lllon=-75, urlon=-71):\\n    # create polar stereographic Basemap instance.\\n    m = Basemap(ax=ax, projection='stere',\\n                lon_0=(urlon + lllon) / 2,\\n                lat_0=(urlat + lllat) / 2,\\n                llcrnrlat=lllat, urcrnrlat=urlat,\\n                llcrnrlon=lllon, urcrnrlon=urlon,\\n                resolution='f')\"),\n",
       " Document(metadata={}, page_content=\"lat_0=(urlat + lllat) / 2,\\n                llcrnrlat=lllat, urcrnrlat=urlat,\\n                llcrnrlon=lllon, urcrnrlon=urlon,\\n                resolution='f')\\n    # draw coastlines, state and country boundaries, edge of map.\\n    m.drawcoastlines()\\n    m.drawstates()\\n    m.drawcountries()\\n    return m\\nThe idea, now, is that the returned Basemap object, knows how to transform coordinates\\nonto the canvas. I wrote the following code to plot the data observations for a number\\nPlotting Maps: Visualizing Haiti Earthquake Crisis Data | 245\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"of report categories. For each category, I filter down the data set to the coordinates\\nlabeled by that category, plot a Basemap on the appropriate subplot, transform the co-\\nordinates, then plot the points using the Basemap’s plot method:\\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 10))\\nfig.subplots_adjust(hspace=0.05, wspace=0.05)\\nto_plot = ['2a', '1', '3c', '7a']\\nlllat=17.25; urlat=20.25; lllon=-75; urlon=-71\\nfor code, ax in zip(to_plot, axes.flat):\\n    m = basic_haiti_map(ax, lllat=lllat, urlat=urlat,\\n                        lllon=lllon, urlon=urlon)\\n    cat_data = data[data['category_%s' % code] == 1]\\n    # compute map proj coordinates.\\n    x, y = m(cat_data.LONGITUDE, cat_data.LATITUDE)\\n    m.plot(x, y, 'k.', alpha=0.5)\\n    ax.set_title('%s: %s' % (code, english_mapping[code]))\\nThe resulting figure can be seen in Figure 8-24.\\nIt seems from the plot that most of the data is concentrated around the most populous\"),\n",
       " Document(metadata={}, page_content=\"ax.set_title('%s: %s' % (code, english_mapping[code]))\\nThe resulting figure can be seen in Figure 8-24.\\nIt seems from the plot that most of the data is concentrated around the most populous\\ncity, Port-au-Prince. basemap allows you to overlap additional map data which comes\\nfrom what are called shapefiles. I first downloaded a shapefile with roads in Port-au-\\nPrince (see http://cegrp.cga.harvard.edu/haiti/?q=resources_data). The Basemap object\\nconveniently has a readshapefile method so that, after extracting the road data archive,\\nI added just the following lines to my code:\\nFigure 8-24. Haiti crisis data for 4 categories\\n246 | Chapter 8: \\u2002Plotting and Visualization\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"shapefile_path = 'ch08/PortAuPrince_Roads/PortAuPrince_Roads'\\nm.readshapefile(shapefile_path, 'roads')\\nAfter a little more trial and error with the latitude and longitude boundaries, I was able\\nto make Figure 8-25 for the “Food shortage” category.\\nPython Visualization Tool Ecosystem\\nAs is common with open source, there are a plethora of options for creating graphics\\nin Python (too many to list). In addition to open source, there are numerous commercial\\nlibraries with Python bindings.\\nIn this chapter and throughout the book, I have been primarily concerned with mat-\\nplotlib as it is the most widely used plotting tool in Python. While it’s an important\\npart of the scientific Python ecosystem, matplotlib has plenty of shortcomings when it\\ncomes to the creation and display of statistical graphics. MATLAB users will likely find\\nmatplotlib familiar, while R users (especially users of the excellent ggplot2 and trel\"),\n",
       " Document(metadata={}, page_content='comes to the creation and display of statistical graphics. MATLAB users will likely find\\nmatplotlib familiar, while R users (especially users of the excellent ggplot2 and trel\\nlis packages) may be somewhat disappointed (at least as of this writing). It is possible\\nto make beautiful plots for display on the web in matplotlib, but doing so often requires\\nsignificant effort as the library is designed for the printed page. Aesthetics aside, it is\\nsufficient for most needs. In pandas, I, along with the other developers, have sought to\\nbuild a convenient user interface that makes it easier to make most kinds of plots com-\\nmonplace in data analysis.\\nThere are a number of other visualization tools in wide use. I list a few of them here\\nand encourage you to explore the ecosystem.\\nFigure 8-25. Food shortage reports in Port-au-Prince during the Haiti earthquake crisis\\nPython Visualization Tool Ecosystem | 247\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Chaco\\nChaco (http://code.enthought.com/chaco/), developed by Enthought, is a plotting tool-\\nkit suitable both for static plotting and interactive visualizations. It is especially well-\\nsuited for expressing complex visualizations with data interrelationships. Compared\\nwith matplotlib, Chaco has much better support for interacting with plot elements and\\nrendering is very fast, making it a good choice for building interactive GUI applications.\\nFigure 8-26. A Chaco example plot\\nmayavi\\nThe mayavi project, developed by Prabhu Ramachandran, Gaël Varoquaux, and others,\\nis a 3D graphics toolkit built on the open source C++ graphics library VTK. mayavi,\\nlike matplotlib, integrates with IPython so that it is easy to use interactively. The plots\\ncan be panned, rotated, and zoomed using the mouse and keyboard. I used mayavi to\\nmake one of the illustrations of broadcasting in Chapter 12. While I don’t show any\\nmayavi-using code here, there is plenty of documentation and examples available on-'),\n",
       " Document(metadata={}, page_content='make one of the illustrations of broadcasting in Chapter 12. While I don’t show any\\nmayavi-using code here, there is plenty of documentation and examples available on-\\nline. In many cases, I believe it is a good alternative to a technology like WebGL, though\\nthe graphics are harder to share in interactive form.\\nOther Packages\\nOf course, there are numerous other visualization libraries and applications available\\nin Python: PyQwt, Veusz, gnuplot-py, biggles, and others. I have seen PyQwt put to\\ngood use in GUI applications built using the Qt application framework using PyQt.\\nWhile many of these libraries continue to be under active development (some of them\\n248 | Chapter 8: \\u2002Plotting and Visualization\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='are part of much larger applications), I have noted in the last few years a general trend\\ntoward web-based technologies and away from desktop graphics. I’ll say a few more\\nwords about this in the next section.\\nThe Future of Visualization Tools?\\nVisualizations built on web technologies (that is, JavaScript-based) appear to be the\\ninevitable future. Doubtlessly you have used many different kinds of static or interactive\\nvisualizations built in Flash or JavaScript over the years. New toolkits (such as d3.js\\nand its numerous off-shoot projects) for building such displays are appearing all the\\ntime. In contrast, development in non web-based visualization has slowed significantly\\nin recent years. This holds true of Python as well as other data analysis and statistical\\ncomputing environments like R.\\nThe development challenge, then, will be in building tighter integration between data\\nanalysis and preparation tools, such as pandas, and the web browser. I am hopeful that'),\n",
       " Document(metadata={}, page_content='The development challenge, then, will be in building tighter integration between data\\nanalysis and preparation tools, such as pandas, and the web browser. I am hopeful that\\nthis will become a fruitful point of collaboration between Python and non-Python users\\nas well.\\nPython Visualization Tool Ecosystem | 249\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='www.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='CHAPTER 9\\nData Aggregation and Group\\nOperations\\nCategorizing a data set and applying a function to each group, whether an aggregation\\nor transformation, is often a critical component of a data analysis workflow. After\\nloading, merging, and preparing a data set, a familiar task is to compute group statistics\\nor possibly pivot tables for reporting or visualization purposes. pandas provides a flex-\\nible and high-performance groupby facility, enabling you to slice and dice, and sum-\\nmarize data sets in a natural way.\\nOne reason for the popularity of relational databases and SQL (which stands for\\n“structured query language”) is the ease with which data can be joined, filtered, trans-\\nformed, and aggregated. However, query languages like SQL are rather limited in the\\nkinds of group operations that can be performed. As you will see, with the expressive-\\nness and power of Python and pandas, we can perform much more complex grouped'),\n",
       " Document(metadata={}, page_content='kinds of group operations that can be performed. As you will see, with the expressive-\\nness and power of Python and pandas, we can perform much more complex grouped\\noperations by utilizing any function that accepts a pandas object or NumPy array. In\\nthis chapter, you will learn how to:\\n• Split a pandas object into pieces using one or more keys (in the form of functions,\\narrays, or DataFrame column names)\\n• Computing group summary statistics, like count, mean, or standard deviation, or\\na user-defined function\\n• Apply a varying set of functions to each column of a DataFrame\\n• Apply within-group transformations or other manipulations, like normalization,\\nlinear regression, rank, or subset selection\\n• Compute pivot tables and cross-tabulations\\n• Perform quantile analysis and other data-derived group analyses\\n251\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Aggregation of time series data, a special use case of groupby, is referred\\nto as resampling in this book and will receive separate treatment in\\nChapter 10.\\nGroupBy Mechanics\\nHadley Wickham, an author of many popular packages for the R programming lan-\\nguage, coined the term split-apply-combine for talking about group operations, and I\\nthink that’s a good description of the process. In the first stage of the process, data\\ncontained in a pandas object, whether a Series, DataFrame, or otherwise, is split into\\ngroups based on one or more keys that you provide. The splitting is performed on a\\nparticular axis of an object. For example, a DataFrame can be grouped on its rows\\n(axis=0) or its columns (axis=1). Once this is done, a function is applied to each group,\\nproducing a new value. Finally, the results of all those function applications are com-\\nbined into a result object. The form of the resulting object will usually depend on what’s'),\n",
       " Document(metadata={}, page_content='producing a new value. Finally, the results of all those function applications are com-\\nbined into a result object. The form of the resulting object will usually depend on what’s\\nbeing done to the data. See Figure 9-1 for a mockup of a simple group aggregation.\\nFigure 9-1. Illustration of a group aggregation\\nEach grouping key can take many forms, and the keys do not have to be all of the same\\ntype:\\n• A list or array of values that is the same length as the axis being grouped\\n• A value indicating a column name in a DataFrame\\n252 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"• A dict or Series giving a correspondence between the values on the axis being\\ngrouped and the group names\\n• A function to be invoked on the axis index or the individual labels in the index\\nNote that the latter three methods are all just shortcuts for producing an array of values\\nto be used to split up the object. Don’t worry if this all seems very abstract. Throughout\\nthis chapter, I will give many examples of all of these methods. To get started, here is\\na very simple small tabular dataset as a DataFrame:\\nIn [13]: df = DataFrame({'key1' : ['a', 'a', 'b', 'b', 'a'],\\n   ....:                 'key2' : ['one', 'two', 'one', 'two', 'one'],\\n   ....:                 'data1' : np.random.randn(5),\\n   ....:                 'data2' : np.random.randn(5)})\\nIn [14]: df\\nOut[14]: \\n      data1     data2 key1 key2\\n0 -0.204708  1.393406    a  one\\n1  0.478943  0.092908    a  two\\n2 -0.519439  0.281746    b  one\\n3 -0.555730  0.769023    b  two\\n4  1.965781  1.246435    a  one\"),\n",
       " Document(metadata={}, page_content=\"data1     data2 key1 key2\\n0 -0.204708  1.393406    a  one\\n1  0.478943  0.092908    a  two\\n2 -0.519439  0.281746    b  one\\n3 -0.555730  0.769023    b  two\\n4  1.965781  1.246435    a  one\\nSuppose you wanted to compute the mean of the data1 column using the groups labels\\nfrom key1. There are a number of ways to do this. One is to access data1 and call\\ngroupby with the column (a Series) at key1:\\nIn [15]: grouped = df['data1'].groupby(df['key1'])\\nIn [16]: grouped\\nOut[16]: <pandas.core.groupby.SeriesGroupBy at 0x2d78b10>\\nThis grouped variable is now a GroupBy object. It has not actually computed anything\\nyet except for some intermediate data about the group key df['key1']. The idea is that\\nthis object has all of the information needed to then apply some operation to each of\\nthe groups. For example, to compute group means we can call the GroupBy’s mean\\nmethod:\\nIn [17]: grouped.mean()\\nOut[17]: \\nkey1\\na       0.746672\\nb      -0.537585\"),\n",
       " Document(metadata={}, page_content=\"the groups. For example, to compute group means we can call the GroupBy’s mean\\nmethod:\\nIn [17]: grouped.mean()\\nOut[17]: \\nkey1\\na       0.746672\\nb      -0.537585\\nLater, I'll explain more about what’s going on when you call .mean(). The important\\nthing here is that the data (a Series) has been aggregated according to the group key,\\nproducing a new Series that is now indexed by the unique values in the key1 column.\\nThe result index has the name 'key1' because the DataFrame column df['key1'] did.\\nIf instead we had passed multiple arrays as a list, we get something different:\\nIn [18]: means = df['data1'].groupby([df['key1'], df['key2']]).mean()\\nGroupBy Mechanics | 253\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"In [19]: means\\nOut[19]: \\nkey1  key2\\na     one     0.880536\\n      two     0.478943\\nb     one    -0.519439\\n      two    -0.555730\\nIn this case, we grouped the data using two keys, and the resulting Series now has a\\nhierarchical index consisting of the unique pairs of keys observed:\\nIn [20]: means.unstack()\\nOut[20]: \\nkey2       one       two\\nkey1                    \\na     0.880536  0.478943\\nb    -0.519439 -0.555730\\nIn these examples, the group keys are all Series, though they could be any arrays of the\\nright length:\\nIn [21]: states = np.array(['Ohio', 'California', 'California', 'Ohio', 'Ohio'])\\nIn [22]: years = np.array([2005, 2005, 2006, 2005, 2006])\\nIn [23]: df['data1'].groupby([states, years]).mean()\\nOut[23]: \\nCalifornia  2005    0.478943\\n            2006   -0.519439\\nOhio        2005   -0.380219\\n            2006    1.965781\\nFrequently the grouping information to be found in the same DataFrame as the data\\nyou want to work on. In that case, you can pass column names (whether those are\"),\n",
       " Document(metadata={}, page_content=\"2006    1.965781\\nFrequently the grouping information to be found in the same DataFrame as the data\\nyou want to work on. In that case, you can pass column names (whether those are\\nstrings, numbers, or other Python objects) as the group keys:\\nIn [24]: df.groupby('key1').mean()\\nOut[24]: \\n         data1     data2\\nkey1                    \\na     0.746672  0.910916\\nb    -0.537585  0.525384\\nIn [25]: df.groupby(['key1', 'key2']).mean()\\nOut[25]: \\n              data1     data2\\nkey1 key2                    \\na    one   0.880536  1.319920\\n     two   0.478943  0.092908\\nb    one  -0.519439  0.281746\\n     two  -0.555730  0.769023\\nYou may have noticed in the first case df.groupby('key1').mean() that there is no\\nkey2 column in the result. Because df['key2'] is not numeric data, it is said to be a\\nnuisance column, which is therefore excluded from the result. By default, all of the\\n254 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"numeric columns are aggregated, though it is possible to filter down to a subset as you’ll\\nsee soon.\\nRegardless of the objective in using groupby, a generally useful GroupBy method is \\nsize which return a Series containing group sizes:\\nIn [26]: df.groupby(['key1', 'key2']).size()\\nOut[26]: \\nkey1  key2\\na     one     2\\n      two     1\\nb     one     1\\n      two     1\\nAs of this writing, any missing values in a group key will be excluded\\nfrom the result. It’s possible (and, in fact, quite likely), that by the time\\nyou are reading this there will be an option to include the NA group in\\nthe result.\\nIterating Over Groups\\nThe GroupBy object supports iteration, generating a sequence of 2-tuples containing\\nthe group name along with the chunk of data. Consider the following small example\\ndata set:\\nIn [27]: for name, group in df.groupby('key1'):\\n   ....:     print name\\n   ....:     print group\\n   ....:\\na\\n      data1     data2 key1 key2\\n0 -0.204708  1.393406    a  one\\n1  0.478943  0.092908    a  two\"),\n",
       " Document(metadata={}, page_content=\"....:     print name\\n   ....:     print group\\n   ....:\\na\\n      data1     data2 key1 key2\\n0 -0.204708  1.393406    a  one\\n1  0.478943  0.092908    a  two\\n4  1.965781  1.246435    a  one\\nb\\n      data1     data2 key1 key2\\n2 -0.519439  0.281746    b  one\\n3 -0.555730  0.769023    b  two\\nIn the case of multiple keys, the first element in the tuple will be a tuple of key values:\\nIn [28]: for (k1, k2), group in df.groupby(['key1', 'key2']):\\n   ....:     print k1, k2\\n   ....:     print group\\n   ....:\\na one\\n      data1     data2 key1 key2\\n0 -0.204708  1.393406    a  one\\n4  1.965781  1.246435    a  one\\na two\\n      data1     data2 key1 key2\\n1  0.478943  0.092908    a  two\\nb one\\n      data1     data2 key1 key2\\nGroupBy Mechanics | 255\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"2 -0.519439  0.281746    b  one\\nb two\\n     data1     data2 key1 key2\\n3 -0.55573  0.769023    b  two\\nOf course, you can choose to do whatever you want with the pieces of data. A recipe\\nyou may find useful is computing a dict of the data pieces as a one-liner:\\nIn [29]: pieces = dict(list(df.groupby('key1')))\\nIn [30]: pieces['b']\\nOut[30]: \\n      data1     data2 key1 key2\\n2 -0.519439  0.281746    b  one\\n3 -0.555730  0.769023    b  two\\nBy default groupby groups on axis=0, but you can group on any of the other axes. For\\nexample, we could group the columns of our example df here by dtype like so:\\nIn [31]: df.dtypes\\nOut[31]: \\ndata1    float64\\ndata2    float64\\nkey1      object\\nkey2      object\\nIn [32]: grouped = df.groupby(df.dtypes, axis=1)\\nIn [33]: dict(list(grouped))\\nOut[33]: \\n{dtype('float64'):       data1     data2\\n0 -0.204708  1.393406\\n1  0.478943  0.092908\\n2 -0.519439  0.281746\\n3 -0.555730  0.769023\\n4  1.965781  1.246435,\\n dtype('object'):   key1 key2\\n0    a  one\\n1    a  two\\n2    b  one\"),\n",
       " Document(metadata={}, page_content=\"0 -0.204708  1.393406\\n1  0.478943  0.092908\\n2 -0.519439  0.281746\\n3 -0.555730  0.769023\\n4  1.965781  1.246435,\\n dtype('object'):   key1 key2\\n0    a  one\\n1    a  two\\n2    b  one\\n3    b  two\\n4    a  one}\\nSelecting a Column or Subset of Columns\\nIndexing a GroupBy object created from a DataFrame with a column name or array of\\ncolumn names has the effect of selecting those columns for aggregation. This means that:\\ndf.groupby('key1')['data1']\\ndf.groupby('key1')[['data2']]\\nare syntactic sugar for:\\ndf['data1'].groupby(df['key1'])\\ndf[['data2']].groupby(df['key1'])\\n256 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"Especially for large data sets, it may be desirable to aggregate only a few columns. For\\nexample, in the above data set, to compute means for just the data2 column and get\\nthe result as a DataFrame, we could write:\\nIn [34]: df.groupby(['key1', 'key2'])[['data2']].mean()\\nOut[34]: \\n              data2\\nkey1 key2          \\na    one   1.319920\\n     two   0.092908\\nb    one   0.281746\\n     two   0.769023\\nThe object returned by this indexing operation is a grouped DataFrame if a list or array\\nis passed and a grouped Series is just a single column name that is passed as a scalar:\\nIn [35]: s_grouped = df.groupby(['key1', 'key2'])['data2']\\nIn [36]: s_grouped\\nOut[36]: <pandas.core.groupby.SeriesGroupBy at 0x2e215d0>\\nIn [37]: s_grouped.mean()\\nOut[37]: \\nkey1  key2\\na     one     1.319920\\n      two     0.092908\\nb     one     0.281746\\n      two     0.769023\\nName: data2\\nGrouping with Dicts and Series\\nGrouping information may exist in a form other than an array. Let’s consider another\\nexample DataFrame:\"),\n",
       " Document(metadata={}, page_content=\"b     one     0.281746\\n      two     0.769023\\nName: data2\\nGrouping with Dicts and Series\\nGrouping information may exist in a form other than an array. Let’s consider another\\nexample DataFrame:\\nIn [38]: people = DataFrame(np.random.randn(5, 5),\\n   ....:                    columns=['a', 'b', 'c', 'd', 'e'],\\n   ....:                    index=['Joe', 'Steve', 'Wes', 'Jim', 'Travis'])\\nIn [39]: people.ix[2:3, ['b', 'c']] = np.nan # Add a few NA values\\nIn [40]: people\\nOut[40]: \\n               a         b         c         d         e\\nJoe     1.007189 -1.296221  0.274992  0.228913  1.352917\\nSteve   0.886429 -2.001637 -0.371843  1.669025 -0.438570\\nWes    -0.539741       NaN       NaN -1.021228 -0.577087\\nJim     0.124121  0.302614  0.523772  0.000940  1.343810\\nTravis -0.713544 -0.831154 -2.370232 -1.860761 -0.860757\\nNow, suppose I have a group correspondence for the columns and want to sum together\\nthe columns by group:\\nIn [41]: mapping = {'a': 'red', 'b': 'red', 'c': 'blue',\"),\n",
       " Document(metadata={}, page_content=\"Now, suppose I have a group correspondence for the columns and want to sum together\\nthe columns by group:\\nIn [41]: mapping = {'a': 'red', 'b': 'red', 'c': 'blue',\\n   ....:            'd': 'blue', 'e': 'red', 'f' : 'orange'}\\nGroupBy Mechanics | 257\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content='Now, you could easily construct an array from this dict to pass to groupby, but instead\\nwe can just pass the dict:\\nIn [42]: by_column = people.groupby(mapping, axis=1)\\nIn [43]: by_column.sum()\\nOut[43]: \\n            blue       red\\nJoe     0.503905  1.063885\\nSteve   1.297183 -1.553778\\nWes    -1.021228 -1.116829\\nJim     0.524712  1.770545\\nTravis -4.230992 -2.405455\\nThe same functionality holds for Series, which can be viewed as a fixed size mapping.\\nWhen I used Series as group keys in the above examples, pandas does, in fact, inspect\\neach Series to ensure that its index is aligned with the axis it’s grouping:\\nIn [44]: map_series = Series(mapping)\\nIn [45]: map_series\\nOut[45]: \\na       red\\nb       red\\nc      blue\\nd      blue\\ne       red\\nf    orange\\nIn [46]: people.groupby(map_series, axis=1).count()\\nOut[46]: \\n        blue  red\\nJoe        2    3\\nSteve      2    3\\nWes        1    2\\nJim        2    3\\nTravis     2    3\\nGrouping with Functions'),\n",
       " Document(metadata={}, page_content='In [46]: people.groupby(map_series, axis=1).count()\\nOut[46]: \\n        blue  red\\nJoe        2    3\\nSteve      2    3\\nWes        1    2\\nJim        2    3\\nTravis     2    3\\nGrouping with Functions\\nUsing Python functions in what can be fairly creative ways is a more abstract way of\\ndefining a group mapping compared with a dict or Series. Any function passed as a\\ngroup key will be called once per index value, with the return values being used as the\\ngroup names. More concretely, consider the example DataFrame from the previous\\nsection, which has people’s first names as index values. Suppose you wanted to group\\nby the length of the names; you could compute an array of string lengths, but instead\\nyou can just pass the len function:\\nIn [47]: people.groupby(len).sum()\\nOut[47]: \\n          a         b         c         d         e\\n3  0.591569 -0.993608  0.798764 -0.791374  2.119639\\n258 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"5  0.886429 -2.001637 -0.371843  1.669025 -0.438570\\n6 -0.713544 -0.831154 -2.370232 -1.860761 -0.860757\\nMixing functions with arrays, dicts, or Series is not a problem as everything gets con-\\nverted to arrays internally:\\nIn [48]: key_list = ['one', 'one', 'one', 'two', 'two']\\nIn [49]: people.groupby([len, key_list]).min()\\nOut[49]: \\n              a         b         c         d         e\\n3 one -0.539741 -1.296221  0.274992 -1.021228 -0.577087\\n  two  0.124121  0.302614  0.523772  0.000940  1.343810\\n5 one  0.886429 -2.001637 -0.371843  1.669025 -0.438570\\n6 two -0.713544 -0.831154 -2.370232 -1.860761 -0.860757\\nGrouping by Index Levels\\nA final convenience for hierarchically-indexed data sets is the ability to aggregate using\\none of the levels of an axis index. To do this, pass the level number or name using the \\nlevel keyword:\\nIn [50]: columns = pd.MultiIndex.from_arrays([['US', 'US', 'US', 'JP', 'JP'],\\n   ....:                                     [1, 3, 5, 1, 3]], names=['cty', 'tenor'])\"),\n",
       " Document(metadata={}, page_content=\"level keyword:\\nIn [50]: columns = pd.MultiIndex.from_arrays([['US', 'US', 'US', 'JP', 'JP'],\\n   ....:                                     [1, 3, 5, 1, 3]], names=['cty', 'tenor'])\\nIn [51]: hier_df = DataFrame(np.random.randn(4, 5), columns=columns)\\nIn [52]: hier_df\\nOut[52]: \\ncty          US                            JP          \\ntenor         1         3         5         1         3\\n0      0.560145 -1.265934  0.119827 -1.063512  0.332883\\n1     -2.359419 -0.199543 -1.541996 -0.970736 -1.307030\\n2      0.286350  0.377984 -0.753887  0.331286  1.349742\\n3      0.069877  0.246674 -0.011862  1.004812  1.327195\\nIn [53]: hier_df.groupby(level='cty', axis=1).count()\\nOut[53]: \\ncty  JP  US\\n0     2   3\\n1     2   3\\n2     2   3\\n3     2   3\\nData Aggregation\\nBy aggregation, I am generally referring to any data transformation that produces scalar\\nvalues from arrays. In the examples above I have used several of them, such as mean,\"),\n",
       " Document(metadata={}, page_content='Data Aggregation\\nBy aggregation, I am generally referring to any data transformation that produces scalar\\nvalues from arrays. In the examples above I have used several of them, such as mean,\\ncount, min and sum. You may wonder what is going on when you invoke mean() on a\\nGroupBy object. Many common aggregations, such as those found in Table 9-1, have\\noptimized implementations that compute the statistics on the dataset in place. How-\\never, you are not limited to only this set of methods. You can use aggregations of your\\nData Aggregation | 259\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"own devising and additionally call any method that is also defined on the grouped\\nobject. For example, as you recall quantile computes sample quantiles of a Series or a\\nDataFrame’s columns 1:\\nIn [54]: df\\nOut[54]: \\n      data1     data2 key1 key2\\n0 -0.204708  1.393406    a  one\\n1  0.478943  0.092908    a  two\\n2 -0.519439  0.281746    b  one\\n3 -0.555730  0.769023    b  two\\n4  1.965781  1.246435    a  one\\nIn [55]: grouped = df.groupby('key1')\\nIn [56]: grouped['data1'].quantile(0.9)\\nOut[56]: \\nkey1\\na       1.668413\\nb      -0.523068\\nWhile quantile is not explicitly implemented for GroupBy, it is a Series method and\\nthus available for use. Internally, GroupBy efficiently slices up the Series, calls\\npiece.quantile(0.9) for each piece, then assembles those results together into the result\\nobject.\\nTo use your own aggregation functions, pass any function that aggregates an array to\\nthe aggregate or agg method:\\nIn [57]: def peak_to_peak(arr):\\n   ....:     return arr.max() - arr.min()\"),\n",
       " Document(metadata={}, page_content='object.\\nTo use your own aggregation functions, pass any function that aggregates an array to\\nthe aggregate or agg method:\\nIn [57]: def peak_to_peak(arr):\\n   ....:     return arr.max() - arr.min()\\nIn [58]: grouped.agg(peak_to_peak)\\nOut[58]: \\n         data1     data2\\nkey1                    \\na     2.170488  1.300498\\nb     0.036292  0.487276\\nYou’ll notice that some methods like describe also work, even though they are not\\naggregations, strictly speaking:\\nIn [59]: grouped.describe()\\nOut[59]: \\n               data1     data2\\nkey1                          \\na    count  3.000000  3.000000\\n     mean   0.746672  0.910916\\n     std    1.109736  0.712217\\n     min   -0.204708  0.092908\\n     25%    0.137118  0.669671\\n     50%    0.478943  1.246435\\n1. Note that quantile performs linear interpolation if there is no value at exactly the passed percentile.\\n260 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='75%    1.222362  1.319920\\n     max    1.965781  1.393406\\nb    count  2.000000  2.000000\\n     mean  -0.537585  0.525384\\n     std    0.025662  0.344556\\n     min   -0.555730  0.281746\\n     25%   -0.546657  0.403565\\n     50%   -0.537585  0.525384\\n     75%   -0.528512  0.647203\\n     max   -0.519439  0.769023\\nI will explain in more detail what has happened here in the next major section on group-\\nwise operations and transformations.\\nYou may notice that custom aggregation functions are much slower than\\nthe optimized functions found in Table 9-1. This is because there is\\nsignificant overhead (function calls, data rearrangement) in construct-\\ning the intermediate group data chunks.\\nTable 9-1. Optimized groupby methods\\nFunction name Description\\ncount Number of non-NA values in the group\\nsum Sum of non-NA values\\nmean Mean of non-NA values\\nmedian Arithmetic median of non-NA values\\nstd, var Unbiased (n - 1 denominator) standard deviation and variance\\nmin, max Minimum and maximum of non-NA values'),\n",
       " Document(metadata={}, page_content=\"mean Mean of non-NA values\\nmedian Arithmetic median of non-NA values\\nstd, var Unbiased (n - 1 denominator) standard deviation and variance\\nmin, max Minimum and maximum of non-NA values\\nprod Product of non-NA values\\nfirst, last First and last non-NA values\\nTo illustrate some more advanced aggregation features, I’ll use a less trivial dataset, a\\ndataset on restaurant tipping. I obtained it from the R reshape2 package; it was origi-\\nnally found in Bryant & Smith’s 1995 text on business statistics (and found in the book’s\\nGitHub repository). After loading it with read_csv, I add a tipping percentage column\\ntip_pct.\\nIn [60]: tips = pd.read_csv('ch08/tips.csv')\\n# Add tip percentage of total bill\\nIn [61]: tips['tip_pct'] = tips['tip'] / tips['total_bill']\\nIn [62]: tips[:6]\\nOut[62]: \\n   total_bill   tip     sex smoker  day    time  size   tip_pct\\n0       16.99  1.01  Female     No  Sun  Dinner     2  0.059447\\n1       10.34  1.66    Male     No  Sun  Dinner     3  0.160542\"),\n",
       " Document(metadata={}, page_content='total_bill   tip     sex smoker  day    time  size   tip_pct\\n0       16.99  1.01  Female     No  Sun  Dinner     2  0.059447\\n1       10.34  1.66    Male     No  Sun  Dinner     3  0.160542\\nData Aggregation | 261\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"2       21.01  3.50    Male     No  Sun  Dinner     3  0.166587\\n3       23.68  3.31    Male     No  Sun  Dinner     2  0.139780\\n4       24.59  3.61  Female     No  Sun  Dinner     4  0.146808\\n5       25.29  4.71    Male     No  Sun  Dinner     4  0.186240\\nColumn-wise and Multiple Function Application\\nAs you’ve seen above, aggregating a Series or all of the columns of a DataFrame is a\\nmatter of using aggregate with the desired function or calling a method like mean or\\nstd. However, you may want to aggregate using a different function depending on the\\ncolumn or multiple functions at once. Fortunately, this is straightforward to do, which\\nI’ll illustrate through a number of examples. First, I’ll group the tips by sex and smoker:\\nIn [63]: grouped = tips.groupby(['sex', 'smoker'])\\nNote that for descriptive statistics like those in Table 9-1, you can pass the name of the\\nfunction as a string:\\nIn [64]: grouped_pct = grouped['tip_pct']\\nIn [65]: grouped_pct.agg('mean')\\nOut[65]: \\nsex     smoker\"),\n",
       " Document(metadata={}, page_content=\"function as a string:\\nIn [64]: grouped_pct = grouped['tip_pct']\\nIn [65]: grouped_pct.agg('mean')\\nOut[65]: \\nsex     smoker\\nFemale  No        0.156921\\n        Yes       0.182150\\nMale    No        0.160669\\n        Yes       0.152771\\nName: tip_pct\\nIf you pass a list of functions or function names instead, you get back a DataFrame with\\ncolumn names taken from the functions:\\nIn [66]: grouped_pct.agg(['mean', 'std', peak_to_peak])\\nOut[66]: \\n                   mean       std  peak_to_peak\\nsex    smoker                                  \\nFemale No      0.156921  0.036421      0.195876\\n       Yes     0.182150  0.071595      0.360233\\nMale   No      0.160669  0.041849      0.220186\\n       Yes     0.152771  0.090588      0.674707\\nYou don’t need to accept the names that GroupBy gives to the columns; notably \\nlambda functions have the name '<lambda>' which make them hard to identify (you can\\nsee for yourself by looking at a function’s __name__ attribute). As such, if you pass a list\"),\n",
       " Document(metadata={}, page_content=\"lambda functions have the name '<lambda>' which make them hard to identify (you can\\nsee for yourself by looking at a function’s __name__ attribute). As such, if you pass a list\\nof (name, function) tuples, the first element of each tuple will be used as the DataFrame\\ncolumn names (you can think of a list of 2-tuples as an ordered mapping):\\nIn [67]: grouped_pct.agg([('foo', 'mean'), ('bar', np.std)])\\nOut[67]: \\n                    foo       bar\\nsex    smoker                    \\nFemale No      0.156921  0.036421\\n       Yes     0.182150  0.071595\\n262 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"Male   No      0.160669  0.041849\\n       Yes     0.152771  0.090588\\nWith a DataFrame, you have more options as you can specify a list of functions to apply\\nto all of the columns or different functions per column. To start, suppose we wanted\\nto compute the same three statistics for the tip_pct and total_bill columns:\\nIn [68]: functions = ['count', 'mean', 'max']\\nIn [69]: result = grouped['tip_pct', 'total_bill'].agg(functions)\\nIn [70]: result\\nOut[70]: \\n               tip_pct                      total_bill                  \\n                 count      mean       max       count       mean    max\\nsex    smoker                                                           \\nFemale No           54  0.156921  0.252672          54  18.105185  35.83\\n       Yes          33  0.182150  0.416667          33  17.977879  44.30\\nMale   No           97  0.160669  0.291990          97  19.791237  48.33\\n       Yes          60  0.152771  0.710345          60  22.284500  50.81\"),\n",
       " Document(metadata={}, page_content=\"Male   No           97  0.160669  0.291990          97  19.791237  48.33\\n       Yes          60  0.152771  0.710345          60  22.284500  50.81\\nAs you can see, the resulting DataFrame has hierarchical columns, the same as you\\nwould get aggregating each column separately and using concat to glue the results\\ntogether using the column names as the keys argument:\\nIn [71]: result['tip_pct']\\nOut[71]: \\n               count      mean       max\\nsex    smoker                           \\nFemale No         54  0.156921  0.252672\\n       Yes        33  0.182150  0.416667\\nMale   No         97  0.160669  0.291990\\n       Yes        60  0.152771  0.710345\\nAs above, a list of tuples with custom names can be passed:\\nIn [72]: ftuples = [('Durchschnitt', 'mean'), ('Abweichung', np.var)]\\nIn [73]: grouped['tip_pct', 'total_bill'].agg(ftuples)\\nOut[73]: \\n                    tip_pct                total_bill            \\n               Durchschnitt  Abweichung  Durchschnitt  Abweichung\"),\n",
       " Document(metadata={}, page_content=\"In [73]: grouped['tip_pct', 'total_bill'].agg(ftuples)\\nOut[73]: \\n                    tip_pct                total_bill            \\n               Durchschnitt  Abweichung  Durchschnitt  Abweichung\\nsex    smoker                                                    \\nFemale No          0.156921    0.001327     18.105185   53.092422\\n       Yes         0.182150    0.005126     17.977879   84.451517\\nMale   No          0.160669    0.001751     19.791237   76.152961\\n       Yes         0.152771    0.008206     22.284500   98.244673\\nNow, suppose you wanted to apply potentially different functions to one or more of\\nthe columns. The trick is to pass a dict to agg that contains a mapping of column names\\nto any of the function specifications listed so far:\\nIn [74]: grouped.agg({'tip' : np.max, 'size' : 'sum'})\\nOut[74]: \\n               size   tip\\nsex    smoker            \\nData Aggregation | 263\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"Female No       140   5.2\\n       Yes       74   6.5\\nMale   No       263   9.0\\n       Yes      150  10.0\\nIn [75]: grouped.agg({'tip_pct' : ['min', 'max', 'mean', 'std'],\\n   ....:              'size' : 'sum'})\\nOut[75]: \\n                tip_pct                                size\\n                    min       max      mean       std   sum\\nsex    smoker                                              \\nFemale No      0.056797  0.252672  0.156921  0.036421   140\\n       Yes     0.056433  0.416667  0.182150  0.071595    74\\nMale   No      0.071804  0.291990  0.160669  0.041849   263\\n       Yes     0.035638  0.710345  0.152771  0.090588   150\\nA DataFrame will have hierarchical columns only if multiple functions are applied to\\nat least one column.\\nReturning Aggregated Data in “unindexed” Form\\nIn all of the examples up until now, the aggregated data comes back with an index,\\npotentially hierarchical, composed from the unique group key combinations observed.\"),\n",
       " Document(metadata={}, page_content=\"In all of the examples up until now, the aggregated data comes back with an index,\\npotentially hierarchical, composed from the unique group key combinations observed.\\nSince this isn’t always desirable, you can disable this behavior in most cases by passing\\nas_index=False to groupby:\\nIn [76]: tips.groupby(['sex', 'smoker'], as_index=False).mean()\\nOut[76]: \\n      sex smoker  total_bill       tip      size   tip_pct\\n0  Female     No   18.105185  2.773519  2.592593  0.156921\\n1  Female    Yes   17.977879  2.931515  2.242424  0.182150\\n2    Male     No   19.791237  3.113402  2.711340  0.160669\\n3    Male    Yes   22.284500  3.051167  2.500000  0.152771\\nOf course, it’s always possible to obtain the result in this format by calling\\nreset_index on the result.\\nUsing groupby in this way is generally less flexible; results with hier-\\narchical columns, for example, are not currently implemented as the\\nform of the result would have to be somewhat arbitrary.\\nGroup-wise Operations and Transformations\"),\n",
       " Document(metadata={}, page_content='archical columns, for example, are not currently implemented as the\\nform of the result would have to be somewhat arbitrary.\\nGroup-wise Operations and Transformations\\nAggregation is only one kind of group operation. It is a special case in the more general\\nclass of data transformations; that is, it accepts functions that reduce a one-dimensional\\narray to a scalar value. In this section, I will introduce you to the transform and apply\\nmethods, which will enable you to do many other kinds of group operations.\\nSuppose, instead, we wanted to add a column to a DataFrame containing group means\\nfor each index. One way to do this is to aggregate, then merge:\\n264 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"In [77]: df\\nOut[77]: \\n      data1     data2 key1 key2\\n0 -0.204708  1.393406    a  one\\n1  0.478943  0.092908    a  two\\n2 -0.519439  0.281746    b  one\\n3 -0.555730  0.769023    b  two\\n4  1.965781  1.246435    a  one\\nIn [78]: k1_means = df.groupby('key1').mean().add_prefix('mean_')\\nIn [79]: k1_means\\nOut[79]: \\n      mean_data1  mean_data2\\nkey1                        \\na       0.746672    0.910916\\nb      -0.537585    0.525384\\nIn [80]: pd.merge(df, k1_means, left_on='key1', right_index=True)\\nOut[80]: \\n      data1     data2 key1 key2  mean_data1  mean_data2\\n0 -0.204708  1.393406    a  one    0.746672    0.910916\\n1  0.478943  0.092908    a  two    0.746672    0.910916\\n4  1.965781  1.246435    a  one    0.746672    0.910916\\n2 -0.519439  0.281746    b  one   -0.537585    0.525384\\n3 -0.555730  0.769023    b  two   -0.537585    0.525384\\nThis works, but is somewhat inflexible. You can think of the operation as transforming\"),\n",
       " Document(metadata={}, page_content=\"2 -0.519439  0.281746    b  one   -0.537585    0.525384\\n3 -0.555730  0.769023    b  two   -0.537585    0.525384\\nThis works, but is somewhat inflexible. You can think of the operation as transforming\\nthe two data columns using the np.mean function. Let’s look back at the people Data-\\nFrame from earlier in the chapter and use the transform method on GroupBy:\\nIn [81]: key = ['one', 'two', 'one', 'two', 'one']\\nIn [82]: people.groupby(key).mean()\\nOut[82]: \\n            a         b         c         d         e\\none -0.082032 -1.063687 -1.047620 -0.884358 -0.028309\\ntwo  0.505275 -0.849512  0.075965  0.834983  0.452620\\nIn [83]: people.groupby(key).transform(np.mean)\\nOut[83]: \\n               a         b         c         d         e\\nJoe    -0.082032 -1.063687 -1.047620 -0.884358 -0.028309\\nSteve   0.505275 -0.849512  0.075965  0.834983  0.452620\\nWes    -0.082032 -1.063687 -1.047620 -0.884358 -0.028309\\nJim     0.505275 -0.849512  0.075965  0.834983  0.452620\"),\n",
       " Document(metadata={}, page_content='Steve   0.505275 -0.849512  0.075965  0.834983  0.452620\\nWes    -0.082032 -1.063687 -1.047620 -0.884358 -0.028309\\nJim     0.505275 -0.849512  0.075965  0.834983  0.452620\\nTravis -0.082032 -1.063687 -1.047620 -0.884358 -0.028309\\nAs you may guess, transform applies a function to each group, then places the results\\nin the appropriate locations. If each group produces a scalar value, it will be propagated\\n(broadcasted). Suppose instead you wanted to subtract the mean value from each\\ngroup. To do this, create a demeaning function and pass it to transform:\\nIn [84]: def demean(arr):\\n   ....:     return arr - arr.mean()\\nGroup-wise Operations and Transformations | 265\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='In [85]: demeaned = people.groupby(key).transform(demean)\\nIn [86]: demeaned\\nOut[86]: \\n               a         b         c         d         e\\nJoe     1.089221 -0.232534  1.322612  1.113271  1.381226\\nSteve   0.381154 -1.152125 -0.447807  0.834043 -0.891190\\nWes    -0.457709       NaN       NaN -0.136869 -0.548778\\nJim    -0.381154  1.152125  0.447807 -0.834043  0.891190\\nTravis -0.631512  0.232534 -1.322612 -0.976402 -0.832448\\nYou can check that demeaned now has zero group means:\\nIn [87]: demeaned.groupby(key).mean()\\nOut[87]: \\n     a  b  c  d  e\\none  0 -0  0  0  0\\ntwo -0  0  0  0  0\\nAs you’ll see in the next section, group demeaning can be achieved using apply also.\\nApply: General split-apply-combine\\nLike aggregate, transform is a more specialized function having rigid requirements: the\\npassed function must either produce a scalar value to be broadcasted (like np.mean) or\\na transformed array of the same size. The most general purpose GroupBy method is'),\n",
       " Document(metadata={}, page_content=\"passed function must either produce a scalar value to be broadcasted (like np.mean) or\\na transformed array of the same size. The most general purpose GroupBy method is\\napply, which is the subject of the rest of this section. As in Figure 9-1, apply splits the\\nobject being manipulated into pieces, invokes the passed function on each piece, then\\nattempts to concatenate the pieces together.\\nReturning to the tipping data set above, suppose you wanted to select the top five\\ntip_pct values by group. First, it’s straightforward to write a function that selects the\\nrows with the largest values in a particular column:\\nIn [88]: def top(df, n=5, column='tip_pct'):\\n   ....:     return df.sort_index(by=column)[-n:]\\nIn [89]: top(tips, n=6)\\nOut[89]: \\n     total_bill   tip     sex smoker  day    time  size   tip_pct\\n109       14.31  4.00  Female    Yes  Sat  Dinner     2  0.279525\\n183       23.17  6.50    Male    Yes  Sun  Dinner     4  0.280535\"),\n",
       " Document(metadata={}, page_content=\"total_bill   tip     sex smoker  day    time  size   tip_pct\\n109       14.31  4.00  Female    Yes  Sat  Dinner     2  0.279525\\n183       23.17  6.50    Male    Yes  Sun  Dinner     4  0.280535\\n232       11.61  3.39    Male     No  Sat  Dinner     2  0.291990\\n67         3.07  1.00  Female    Yes  Sat  Dinner     1  0.325733\\n178        9.60  4.00  Female    Yes  Sun  Dinner     2  0.416667\\n172        7.25  5.15    Male    Yes  Sun  Dinner     2  0.710345\\nNow, if we group by smoker, say, and call apply with this function, we get the following:\\nIn [90]: tips.groupby('smoker').apply(top)\\nOut[90]: \\n            total_bill   tip     sex smoker   day    time  size   tip_pct\\nsmoker                                                                   \\n266 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content='No     88        24.71  5.85    Male     No  Thur   Lunch     2  0.236746\\n       185       20.69  5.00    Male     No   Sun  Dinner     5  0.241663\\n       51        10.29  2.60  Female     No   Sun  Dinner     2  0.252672\\n       149        7.51  2.00    Male     No  Thur   Lunch     2  0.266312\\n       232       11.61  3.39    Male     No   Sat  Dinner     2  0.291990\\nYes    109       14.31  4.00  Female    Yes   Sat  Dinner     2  0.279525\\n       183       23.17  6.50    Male    Yes   Sun  Dinner     4  0.280535\\n       67         3.07  1.00  Female    Yes   Sat  Dinner     1  0.325733\\n       178        9.60  4.00  Female    Yes   Sun  Dinner     2  0.416667\\n       172        7.25  5.15    Male    Yes   Sun  Dinner     2  0.710345\\nWhat has happened here? The top function is called on each piece of the DataFrame,\\nthen the results are glued together using pandas.concat, labeling the pieces with the\\ngroup names. The result therefore has a hierarchical index whose inner level contains'),\n",
       " Document(metadata={}, page_content=\"then the results are glued together using pandas.concat, labeling the pieces with the\\ngroup names. The result therefore has a hierarchical index whose inner level contains\\nindex values from the original DataFrame.\\nIf you pass a function to apply that takes other arguments or keywords, you can pass\\nthese after the function:\\nIn [91]: tips.groupby(['smoker', 'day']).apply(top, n=1, column='total_bill')\\nOut[91]: \\n                 total_bill    tip     sex smoker   day    time  size   tip_pct\\nsmoker day                                                                     \\nNo     Fri  94        22.75   3.25  Female     No   Fri  Dinner     2  0.142857\\n       Sat  212       48.33   9.00    Male     No   Sat  Dinner     4  0.186220\\n       Sun  156       48.17   5.00    Male     No   Sun  Dinner     6  0.103799\\n       Thur 142       41.19   5.00    Male     No  Thur   Lunch     5  0.121389\\nYes    Fri  95        40.17   4.73    Male    Yes   Fri  Dinner     4  0.117750\"),\n",
       " Document(metadata={}, page_content=\"Thur 142       41.19   5.00    Male     No  Thur   Lunch     5  0.121389\\nYes    Fri  95        40.17   4.73    Male    Yes   Fri  Dinner     4  0.117750\\n       Sat  170       50.81  10.00    Male    Yes   Sat  Dinner     3  0.196812\\n       Sun  182       45.35   3.50    Male    Yes   Sun  Dinner     3  0.077178\\n       Thur 197       43.11   5.00  Female    Yes  Thur   Lunch     4  0.115982\\nBeyond these basic usage mechanics, getting the most out of apply is\\nlargely a matter of creativity. What occurs inside the function passed is\\nup to you; it only needs to return a pandas object or a scalar value. The\\nrest of this chapter will mainly consist of examples showing you how to\\nsolve various problems using groupby.\\nYou may recall above I called describe on a GroupBy object:\\nIn [92]: result = tips.groupby('smoker')['tip_pct'].describe()\\nIn [93]: result\\nOut[93]: \\nsmoker       \\nNo      count    151.000000\\n        mean       0.159328\\n        std        0.039910\"),\n",
       " Document(metadata={}, page_content=\"In [92]: result = tips.groupby('smoker')['tip_pct'].describe()\\nIn [93]: result\\nOut[93]: \\nsmoker       \\nNo      count    151.000000\\n        mean       0.159328\\n        std        0.039910\\n        min        0.056797\\n        25%        0.136906\\n        50%        0.155625\\n        75%        0.185014\\n        max        0.291990\\nGroup-wise Operations and Transformations | 267\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"Yes     count     93.000000\\n        mean       0.163196\\n        std        0.085119\\n        min        0.035638\\n        25%        0.106771\\n        50%        0.153846\\n        75%        0.195059\\n        max        0.710345\\nIn [94]: result.unstack('smoker')\\nOut[94]: \\nsmoker          No        Yes\\ncount   151.000000  93.000000\\nmean      0.159328   0.163196\\nstd       0.039910   0.085119\\nmin       0.056797   0.035638\\n25%       0.136906   0.106771\\n50%       0.155625   0.153846\\n75%       0.185014   0.195059\\nmax       0.291990   0.710345\\nInside GroupBy, when you invoke a method like describe, it is actually just a shortcut\\nfor:\\nf = lambda x: x.describe()\\ngrouped.apply(f)\\nSuppressing the group keys\\nIn the examples above, you see that the resulting object has a hierarchical index formed\\nfrom the group keys along with the indexes of each piece of the original object. This\\ncan be disabled by passing group_keys=False to groupby:\\nIn [95]: tips.groupby('smoker', group_keys=False).apply(top)\"),\n",
       " Document(metadata={}, page_content=\"can be disabled by passing group_keys=False to groupby:\\nIn [95]: tips.groupby('smoker', group_keys=False).apply(top)\\nOut[95]: \\n     total_bill   tip     sex smoker   day    time  size   tip_pct\\n88        24.71  5.85    Male     No  Thur   Lunch     2  0.236746\\n185       20.69  5.00    Male     No   Sun  Dinner     5  0.241663\\n51        10.29  2.60  Female     No   Sun  Dinner     2  0.252672\\n149        7.51  2.00    Male     No  Thur   Lunch     2  0.266312\\n232       11.61  3.39    Male     No   Sat  Dinner     2  0.291990\\n109       14.31  4.00  Female    Yes   Sat  Dinner     2  0.279525\\n183       23.17  6.50    Male    Yes   Sun  Dinner     4  0.280535\\n67         3.07  1.00  Female    Yes   Sat  Dinner     1  0.325733\\n178        9.60  4.00  Female    Yes   Sun  Dinner     2  0.416667\\n172        7.25  5.15    Male    Yes   Sun  Dinner     2  0.710345\\nQuantile and Bucket Analysis\\nAs you may recall from Chapter 7, pandas has some tools, in particular cut and qcut,\"),\n",
       " Document(metadata={}, page_content='172        7.25  5.15    Male    Yes   Sun  Dinner     2  0.710345\\nQuantile and Bucket Analysis\\nAs you may recall from Chapter 7, pandas has some tools, in particular cut and qcut,\\nfor slicing data up into buckets with bins of your choosing or by sample quantiles.\\nCombining these functions with groupby, it becomes very simple to perform bucket or\\n268 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"quantile analysis on a data set. Consider a simple random data set and an equal-length\\nbucket categorization using cut:\\nIn [96]: frame = DataFrame({'data1': np.random.randn(1000),\\n   ....:                    'data2': np.random.randn(1000)})\\nIn [97]: factor = pd.cut(frame.data1, 4)\\nIn [98]: factor[:10]\\nOut[98]: \\nCategorical: \\narray([(-1.23, 0.489], (-2.956, -1.23], (-1.23, 0.489], (0.489, 2.208],\\n       (-1.23, 0.489], (0.489, 2.208], (-1.23, 0.489], (-1.23, 0.489],\\n       (0.489, 2.208], (0.489, 2.208]], dtype=object)\\nLevels (4): Index([(-2.956, -1.23], (-1.23, 0.489], (0.489, 2.208],\\n                   (2.208, 3.928]], dtype=object)\\nThe Factor object returned by cut can be passed directly to groupby. So we could com-\\npute a set of statistics for the data2 column like so:\\nIn [99]: def get_stats(group):\\n   ....:     return {'min': group.min(), 'max': group.max(),\\n   ....:             'count': group.count(), 'mean': group.mean()}\\nIn [100]: grouped = frame.data2.groupby(factor)\"),\n",
       " Document(metadata={}, page_content=\"....:     return {'min': group.min(), 'max': group.max(),\\n   ....:             'count': group.count(), 'mean': group.mean()}\\nIn [100]: grouped = frame.data2.groupby(factor)\\nIn [101]: grouped.apply(get_stats).unstack()\\nOut[101]: \\n                 count       max      mean       min\\ndata1                                               \\n(-1.23, 0.489]     598  3.260383 -0.002051 -2.989741\\n(-2.956, -1.23]     95  1.670835 -0.039521 -3.399312\\n(0.489, 2.208]     297  2.954439  0.081822 -3.745356\\n(2.208, 3.928]      10  1.765640  0.024750 -1.929776\\nThese were equal-length buckets; to compute equal-size buckets based on sample\\nquantiles, use qcut. I’ll pass labels=False to just get quantile numbers.\\n# Return quantile numbers\\nIn [102]: grouping = pd.qcut(frame.data1, 10, labels=False)\\nIn [103]: grouped = frame.data2.groupby(grouping)\\nIn [104]: grouped.apply(get_stats).unstack()\\nOut[104]: \\n   count       max      mean       min\\n0    100  1.670835 -0.049902 -3.399312\"),\n",
       " Document(metadata={}, page_content='In [103]: grouped = frame.data2.groupby(grouping)\\nIn [104]: grouped.apply(get_stats).unstack()\\nOut[104]: \\n   count       max      mean       min\\n0    100  1.670835 -0.049902 -3.399312\\n1    100  2.628441  0.030989 -1.950098\\n2    100  2.527939 -0.067179 -2.925113\\n3    100  3.260383  0.065713 -2.315555\\n4    100  2.074345 -0.111653 -2.047939\\n5    100  2.184810  0.052130 -2.989741\\n6    100  2.458842 -0.021489 -2.223506\\n7    100  2.954439 -0.026459 -3.056990\\n8    100  2.735527  0.103406 -3.745356\\n9    100  2.377020  0.220122 -2.064111\\nGroup-wise Operations and Transformations | 269\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"Example: Filling Missing Values with Group-specific Values\\nWhen cleaning up missing data, in some cases you will filter out data observations\\nusing dropna, but in others you may want to impute (fill in) the NA values using a fixed\\nvalue or some value derived from the data. fillna is the right tool to use; for example\\nhere I fill in NA values with the mean:\\nIn [105]: s = Series(np.random.randn(6))\\nIn [106]: s[::2] = np.nan\\nIn [107]: s\\nOut[107]: \\n0         NaN\\n1   -0.125921\\n2         NaN\\n3   -0.884475\\n4         NaN\\n5    0.227290\\nIn [108]: s.fillna(s.mean())\\nOut[108]: \\n0   -0.261035\\n1   -0.125921\\n2   -0.261035\\n3   -0.884475\\n4   -0.261035\\n5    0.227290\\nSuppose you need the fill value to vary by group. As you may guess, you need only\\ngroup the data and use apply with a function that calls fillna on each data chunk. Here\\nis some sample data on some US states divided into eastern and western states:\\nIn [109]: states = ['Ohio', 'New York', 'Vermont', 'Florida',\"),\n",
       " Document(metadata={}, page_content=\"is some sample data on some US states divided into eastern and western states:\\nIn [109]: states = ['Ohio', 'New York', 'Vermont', 'Florida',\\n   .....:           'Oregon', 'Nevada', 'California', 'Idaho']\\nIn [110]: group_key = ['East'] * 4 + ['West'] * 4\\nIn [111]: data = Series(np.random.randn(8), index=states)\\nIn [112]: data[['Vermont', 'Nevada', 'Idaho']] = np.nan\\nIn [113]: data\\nOut[113]: \\nOhio          0.922264\\nNew York     -2.153545\\nVermont            NaN\\nFlorida      -0.375842\\nOregon        0.329939\\nNevada             NaN\\nCalifornia    1.105913\\nIdaho              NaN\\nIn [114]: data.groupby(group_key).mean()\\nOut[114]: \\n270 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"East   -0.535707\\nWest    0.717926\\nWe can fill the NA values using the group means like so:\\nIn [115]: fill_mean = lambda g: g.fillna(g.mean())\\nIn [116]: data.groupby(group_key).apply(fill_mean)\\nOut[116]: \\nOhio          0.922264\\nNew York     -2.153545\\nVermont      -0.535707\\nFlorida      -0.375842\\nOregon        0.329939\\nNevada        0.717926\\nCalifornia    1.105913\\nIdaho         0.717926\\nIn another case, you might have pre-defined fill values in your code that vary by group.\\nSince the groups have a name attribute set internally, we can use that:\\nIn [117]: fill_values = {'East': 0.5, 'West': -1}\\nIn [118]: fill_func = lambda g: g.fillna(fill_values[g.name])\\nIn [119]: data.groupby(group_key).apply(fill_func)\\nOut[119]: \\nOhio          0.922264\\nNew York     -2.153545\\nVermont       0.500000\\nFlorida      -0.375842\\nOregon        0.329939\\nNevada       -1.000000\\nCalifornia    1.105913\\nIdaho        -1.000000\\nExample: Random Sampling and Permutation\"),\n",
       " Document(metadata={}, page_content=\"Vermont       0.500000\\nFlorida      -0.375842\\nOregon        0.329939\\nNevada       -1.000000\\nCalifornia    1.105913\\nIdaho        -1.000000\\nExample: Random Sampling and Permutation\\nSuppose you wanted to draw a random sample (with or without replacement) from a\\nlarge dataset for Monte Carlo simulation purposes or some other application. There\\nare a number of ways to perform the “draws”; some are much more efficient than others.\\nOne way is to select the first K elements of np.random.permutation(N), where N is the\\nsize of your complete dataset and K the desired sample size. As a more fun example,\\nhere’s a way to construct a deck of English-style playing cards:\\n# Hearts, Spades, Clubs, Diamonds\\nsuits = ['H', 'S', 'C', 'D']\\ncard_val = (range(1, 11) + [10] * 3) * 4\\nbase_names = ['A'] + range(2, 11) + ['J', 'K', 'Q']\\ncards = []\\nfor suit in ['H', 'S', 'C', 'D']:\\n    cards.extend(str(num) + suit for num in base_names)\\ndeck = Series(card_val, index=cards)\"),\n",
       " Document(metadata={}, page_content=\"base_names = ['A'] + range(2, 11) + ['J', 'K', 'Q']\\ncards = []\\nfor suit in ['H', 'S', 'C', 'D']:\\n    cards.extend(str(num) + suit for num in base_names)\\ndeck = Series(card_val, index=cards)\\nGroup-wise Operations and Transformations | 271\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content='So now we have a Series of length 52 whose index contains card names and values are\\nthe ones used in blackjack and other games (to keep things simple, I just let the ace be\\n1):\\nIn [121]: deck[:13]\\nOut[121]: \\nAH      1\\n2H      2\\n3H      3\\n4H      4\\n5H      5\\n6H      6\\n7H      7\\n8H      8\\n9H      9\\n10H    10\\nJH     10\\nKH     10\\nQH     10\\nNow, based on what I said above, drawing a hand of 5 cards from the desk could be\\nwritten as:\\nIn [122]: def draw(deck, n=5):\\n   .....:     return deck.take(np.random.permutation(len(deck))[:n])\\nIn [123]: draw(deck)\\nOut[123]: \\nAD     1\\n8C     8\\n5H     5\\nKC    10\\n2C     2\\nSuppose you wanted two random cards from each suit. Because the suit is the last\\ncharacter of each card name, we can group based on this and use apply:\\nIn [124]: get_suit = lambda card: card[-1] # last letter is suit\\nIn [125]: deck.groupby(get_suit).apply(draw, n=2)\\nOut[125]: \\nC  2C     2\\n   3C     3\\nD  KD    10\\n   8D     8\\nH  KH    10\\n   3H     3\\nS  2S     2\\n   4S     4\\n# alternatively'),\n",
       " Document(metadata={}, page_content='In [125]: deck.groupby(get_suit).apply(draw, n=2)\\nOut[125]: \\nC  2C     2\\n   3C     3\\nD  KD    10\\n   8D     8\\nH  KH    10\\n   3H     3\\nS  2S     2\\n   4S     4\\n# alternatively\\nIn [126]: deck.groupby(get_suit, group_keys=False).apply(draw, n=2)\\nOut[126]: \\nKC    10\\nJC    10\\nAD     1\\n272 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"5D     5\\n5H     5\\n6H     6\\n7S     7\\nKS    10\\nExample: Group Weighted Average and Correlation\\nUnder the split-apply-combine paradigm of groupby, operations between columns in a\\nDataFrame or two Series, such a group weighted average, become a routine affair. As\\nan example, take this dataset containing group keys, values, and some weights:\\nIn [127]: df = DataFrame({'category': ['a', 'a', 'a', 'a', 'b', 'b', 'b', 'b'],\\n   .....:                 'data': np.random.randn(8),\\n   .....:                 'weights': np.random.rand(8)})\\nIn [128]: df\\nOut[128]: \\n  category      data   weights\\n0        a  1.561587  0.957515\\n1        a  1.219984  0.347267\\n2        a -0.482239  0.581362\\n3        a  0.315667  0.217091\\n4        b -0.047852  0.894406\\n5        b -0.454145  0.918564\\n6        b -0.556774  0.277825\\n7        b  0.253321  0.955905\\nThe group weighted average by category would then be:\\nIn [129]: grouped = df.groupby('category')\"),\n",
       " Document(metadata={}, page_content=\"5        b -0.454145  0.918564\\n6        b -0.556774  0.277825\\n7        b  0.253321  0.955905\\nThe group weighted average by category would then be:\\nIn [129]: grouped = df.groupby('category')\\nIn [130]: get_wavg = lambda g: np.average(g['data'], weights=g['weights'])\\nIn [131]: grouped.apply(get_wavg)\\nOut[131]: \\ncategory\\na           0.811643\\nb          -0.122262\\nAs a less trivial example, consider a data set from Yahoo! Finance containing end of\\nday prices for a few stocks and the S&P 500 index (the SPX ticker):\\nIn [132]: close_px = pd.read_csv('ch09/stock_px.csv', parse_dates=True, index_col=0)\\nIn [133]: close_px\\nOut[133]: \\n<class 'pandas.core.frame.DataFrame'>\\nDatetimeIndex: 2214 entries, 2003-01-02 00:00:00 to 2011-10-14 00:00:00\\nData columns:\\nAAPL    2214  non-null values\\nMSFT    2214  non-null values\\nXOM     2214  non-null values\\nSPX     2214  non-null values\\ndtypes: float64(4)\\nGroup-wise Operations and Transformations | 273\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"In [134]: close_px[-4:]\\nOut[134]: \\n              AAPL   MSFT    XOM      SPX\\n2011-10-11  400.29  27.00  76.27  1195.54\\n2011-10-12  402.19  26.96  77.16  1207.25\\n2011-10-13  408.43  27.18  76.37  1203.66\\n2011-10-14  422.00  27.27  78.11  1224.58\\nOne task of interest might be to compute a DataFrame consisting of the yearly corre-\\nlations of daily returns (computed from percent changes) with SPX. Here is one way to\\ndo it:\\nIn [135]: rets = close_px.pct_change().dropna()\\nIn [136]: spx_corr = lambda x: x.corrwith(x['SPX'])\\nIn [137]: by_year = rets.groupby(lambda x: x.year)\\nIn [138]: by_year.apply(spx_corr)\\nOut[138]: \\n          AAPL      MSFT       XOM  SPX\\n2003  0.541124  0.745174  0.661265    1\\n2004  0.374283  0.588531  0.557742    1\\n2005  0.467540  0.562374  0.631010    1\\n2006  0.428267  0.406126  0.518514    1\\n2007  0.508118  0.658770  0.786264    1\\n2008  0.681434  0.804626  0.828303    1\\n2009  0.707103  0.654902  0.797921    1\\n2010  0.710105  0.730118  0.839057    1\"),\n",
       " Document(metadata={}, page_content=\"2006  0.428267  0.406126  0.518514    1\\n2007  0.508118  0.658770  0.786264    1\\n2008  0.681434  0.804626  0.828303    1\\n2009  0.707103  0.654902  0.797921    1\\n2010  0.710105  0.730118  0.839057    1\\n2011  0.691931  0.800996  0.859975    1\\nThere is, of course, nothing to stop you from computing inter-column correlations:\\n# Annual correlation of Apple with Microsoft\\nIn [139]: by_year.apply(lambda g: g['AAPL'].corr(g['MSFT']))\\nOut[139]: \\n2003    0.480868\\n2004    0.259024\\n2005    0.300093\\n2006    0.161735\\n2007    0.417738\\n2008    0.611901\\n2009    0.432738\\n2010    0.571946\\n2011    0.581987\\nExample: Group-wise Linear Regression\\nIn the same vein as the previous example, you can use groupby to perform more complex\\ngroup-wise statistical analysis, as long as the function returns a pandas object or scalar\\nvalue. For example, I can define the following regress function (using the statsmo\\ndels econometrics library) which executes an ordinary least squares (OLS) regression\\non each chunk of data:\"),\n",
       " Document(metadata={}, page_content='value. For example, I can define the following regress function (using the statsmo\\ndels econometrics library) which executes an ordinary least squares (OLS) regression\\non each chunk of data:\\n274 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"import statsmodels.api as sm\\ndef regress(data, yvar, xvars):\\n    Y = data[yvar]\\n    X = data[xvars]\\n    X['intercept'] = 1.\\n    result = sm.OLS(Y, X).fit()\\n    return result.params\\nNow, to run a yearly linear regression of AAPL on SPX returns, I execute:\\nIn [141]: by_year.apply(regress, 'AAPL', ['SPX'])\\nOut[141]: \\n           SPX  intercept\\n2003  1.195406   0.000710\\n2004  1.363463   0.004201\\n2005  1.766415   0.003246\\n2006  1.645496   0.000080\\n2007  1.198761   0.003438\\n2008  0.968016  -0.001110\\n2009  0.879103   0.002954\\n2010  1.052608   0.001261\\n2011  0.806605   0.001514\\nPivot Tables and Cross-Tabulation\\nA pivot table is a data summarization tool frequently found in spreadsheet programs\\nand other data analysis software. It aggregates a table of data by one or more keys,\\narranging the data in a rectangle with some of the group keys along the rows and some\\nalong the columns. Pivot tables in Python with pandas are made possible using the\"),\n",
       " Document(metadata={}, page_content=\"arranging the data in a rectangle with some of the group keys along the rows and some\\nalong the columns. Pivot tables in Python with pandas are made possible using the\\ngroupby facility described in this chapter combined with reshape operations utilizing\\nhierarchical indexing. DataFrame has a pivot_table method, and additionally there is\\na top-level pandas.pivot_table function. In addition to providing a convenience inter-\\nface to groupby, pivot_table also can add partial totals, also known as margins.\\nReturning to the tipping data set, suppose I wanted to compute a table of group means\\n(the default pivot_table aggregation type) arranged by sex and smoker on the rows:\\nIn [142]: tips.pivot_table(rows=['sex', 'smoker'])\\nOut[142]: \\n                   size       tip   tip_pct  total_bill\\nsex    smoker                                          \\nFemale No      2.592593  2.773519  0.156921   18.105185\\n       Yes     2.242424  2.931515  0.182150   17.977879\"),\n",
       " Document(metadata={}, page_content=\"sex    smoker                                          \\nFemale No      2.592593  2.773519  0.156921   18.105185\\n       Yes     2.242424  2.931515  0.182150   17.977879\\nMale   No      2.711340  3.113402  0.160669   19.791237\\n       Yes     2.500000  3.051167  0.152771   22.284500\\nThis could have been easily produced using groupby. Now, suppose we want to aggre-\\ngate only tip_pct and size, and additionally group by day. I’ll put smoker in the table\\ncolumns and day in the rows:\\nIn [143]: tips.pivot_table(['tip_pct', 'size'], rows=['sex', 'day'],\\n   .....:                  cols='smoker')\\nOut[143]: \\nPivot Tables and Cross-Tabulation | 275\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content='tip_pct                size          \\nsmoker             No       Yes        No       Yes\\nsex    day                                         \\nFemale Fri   0.165296  0.209129  2.500000  2.000000\\n       Sat   0.147993  0.163817  2.307692  2.200000\\n       Sun   0.165710  0.237075  3.071429  2.500000\\n       Thur  0.155971  0.163073  2.480000  2.428571\\nMale   Fri   0.138005  0.144730  2.000000  2.125000\\n       Sat   0.162132  0.139067  2.656250  2.629630\\n       Sun   0.158291  0.173964  2.883721  2.600000\\n       Thur  0.165706  0.164417  2.500000  2.300000\\nThis table could be augmented to include partial totals by passing margins=True. This\\nhas the effect of adding All row and column labels, with corresponding values being\\nthe group statistics for all the data within a single tier. In this below example, the All\\nvalues are means without taking into account smoker vs. non-smoker (the All columns)\\nor any of the two levels of grouping on the rows (the All row):'),\n",
       " Document(metadata={}, page_content=\"values are means without taking into account smoker vs. non-smoker (the All columns)\\nor any of the two levels of grouping on the rows (the All row):\\nIn [144]: tips.pivot_table(['tip_pct', 'size'], rows=['sex', 'day'],\\n   .....:                  cols='smoker', margins=True)\\nOut[144]: \\n                 size                       tip_pct                    \\nsmoker             No       Yes       All        No       Yes       All\\nsex    day                                                             \\nFemale Fri   2.500000  2.000000  2.111111  0.165296  0.209129  0.199388\\n       Sat   2.307692  2.200000  2.250000  0.147993  0.163817  0.156470\\n       Sun   3.071429  2.500000  2.944444  0.165710  0.237075  0.181569\\n       Thur  2.480000  2.428571  2.468750  0.155971  0.163073  0.157525\\nMale   Fri   2.000000  2.125000  2.100000  0.138005  0.144730  0.143385\\n       Sat   2.656250  2.629630  2.644068  0.162132  0.139067  0.151577\"),\n",
       " Document(metadata={}, page_content=\"Male   Fri   2.000000  2.125000  2.100000  0.138005  0.144730  0.143385\\n       Sat   2.656250  2.629630  2.644068  0.162132  0.139067  0.151577\\n       Sun   2.883721  2.600000  2.810345  0.158291  0.173964  0.162344\\n       Thur  2.500000  2.300000  2.433333  0.165706  0.164417  0.165276\\nAll          2.668874  2.408602  2.569672  0.159328  0.163196  0.160803\\nTo use a different aggregation function, pass it to aggfunc. For example, 'count' or\\nlen will give you a cross-tabulation (count or frequency) of group sizes:\\nIn [145]: tips.pivot_table('tip_pct', rows=['sex', 'smoker'], cols='day',\\n   .....:                  aggfunc=len, margins=True)\\nOut[145]: \\nday            Fri  Sat  Sun  Thur  All\\nsex    smoker                          \\nFemale No        2   13   14    25   54\\n       Yes       7   15    4     7   33\\nMale   No        2   32   43    20   97\\n       Yes       8   27   15    10   60\\nAll             19   87   76    62  244\"),\n",
       " Document(metadata={}, page_content=\"Female No        2   13   14    25   54\\n       Yes       7   15    4     7   33\\nMale   No        2   32   43    20   97\\n       Yes       8   27   15    10   60\\nAll             19   87   76    62  244\\nIf some combinations are empty (or otherwise NA), you may wish to pass a fill_value:\\nIn [146]: tips.pivot_table('size', rows=['time', 'sex', 'smoker'],\\n   .....:                  cols='day', aggfunc='sum', fill_value=0)\\nOut[146]: \\nday                   Fri  Sat  Sun  Thur\\ntime   sex    smoker                     \\nDinner Female No        2   30   43     2\\n276 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"Yes       8   33   10     0\\n       Male   No        4   85  124     0\\n              Yes      12   71   39     0\\nLunch  Female No        3    0    0    60\\n              Yes       6    0    0    17\\n       Male   No        0    0    0    50\\n              Yes       5    0    0    23\\nSee Table 9-2 for a summary of pivot_table methods.\\nTable 9-2. pivot_table options\\nFunction name Description\\nvalues Column name or names to aggregate. By default aggregates all numeric columns\\nrows Column names or other group keys to group on the rows of the resulting pivot table\\ncols Column names or other group keys to group on the columns of the resulting pivot table\\naggfunc Aggregation function or list of functions; 'mean' by default. Can be any function valid in a groupby context\\nfill_value Replace missing values in result table\\nmargins Add row/column subtotals and grand total, False by default\\nCross-Tabulations: Crosstab\"),\n",
       " Document(metadata={}, page_content='fill_value Replace missing values in result table\\nmargins Add row/column subtotals and grand total, False by default\\nCross-Tabulations: Crosstab\\nA cross-tabulation (or crosstab for short) is a special case of a pivot table that computes\\ngroup frequencies. Here is a canonical example taken from the Wikipedia page on cross-\\ntabulation:\\nIn [150]: data\\nOut[150]: \\n   Sample  Gender    Handedness\\n0       1  Female  Right-handed\\n1       2    Male   Left-handed\\n2       3  Female  Right-handed\\n3       4    Male  Right-handed\\n4       5    Male   Left-handed\\n5       6    Male  Right-handed\\n6       7  Female  Right-handed\\n7       8  Female   Left-handed\\n8       9    Male  Right-handed\\n9      10  Female  Right-handed\\nAs part of some survey analysis, we might want to summarize this data by gender and\\nhandedness. You could use pivot_table to do this, but the pandas.crosstab function\\nis very convenient:\\nIn [151]: pd.crosstab(data.Gender, data.Handedness, margins=True)\\nOut[151]:'),\n",
       " Document(metadata={}, page_content='handedness. You could use pivot_table to do this, but the pandas.crosstab function\\nis very convenient:\\nIn [151]: pd.crosstab(data.Gender, data.Handedness, margins=True)\\nOut[151]: \\nHandedness  Left-handed  Right-handed  All\\nGender                                    \\nFemale                1             4    5\\nMale                  2             3    5\\nAll                   3             7   10\\nPivot Tables and Cross-Tabulation | 277\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"The first two arguments to crosstab can each either be an array or Series or a list of\\narrays. As in the tips data:\\nIn [152]: pd.crosstab([tips.time, tips.day], tips.smoker, margins=True)\\nOut[152]: \\nsmoker        No  Yes  All\\ntime   day                \\nDinner Fri     3    9   12\\n       Sat    45   42   87\\n       Sun    57   19   76\\n       Thur    1    0    1\\nLunch  Fri     1    6    7\\n       Thur   44   17   61\\nAll          151   93  244\\nExample: 2012 Federal Election Commission Database\\nThe US Federal Election Commission publishes data on contributions to political cam-\\npaigns. This includes contributor names, occupation and employer, address, and con-\\ntribution amount. An interesting dataset is from the 2012 US presidential election\\n(http://www.fec.gov/disclosurep/PDownload.do). As of this writing (June 2012), the full\\ndataset for all states is a 150 megabyte CSV file P00000001-ALL.csv, which can be loaded\\nwith pandas.read_csv:\\nIn [13]: fec = pd.read_csv('ch09/P00000001-ALL.csv')\"),\n",
       " Document(metadata={}, page_content=\"dataset for all states is a 150 megabyte CSV file P00000001-ALL.csv, which can be loaded\\nwith pandas.read_csv:\\nIn [13]: fec = pd.read_csv('ch09/P00000001-ALL.csv')\\nIn [14]: fec\\nOut[14]:\\n<class 'pandas.core.frame.DataFrame'>\\nInt64Index: 1001731 entries, 0 to 1001730\\nData columns:\\ncmte_id              1001731  non-null values\\ncand_id              1001731  non-null values\\ncand_nm              1001731  non-null values\\ncontbr_nm            1001731  non-null values\\ncontbr_city          1001716  non-null values\\ncontbr_st            1001727  non-null values\\ncontbr_zip           1001620  non-null values\\ncontbr_employer      994314   non-null values\\ncontbr_occupation    994433   non-null values\\ncontb_receipt_amt    1001731  non-null values\\ncontb_receipt_dt     1001731  non-null values\\nreceipt_desc         14166    non-null values\\nmemo_cd              92482    non-null values\\nmemo_text            97770    non-null values\\nform_tp              1001731  non-null values\"),\n",
       " Document(metadata={}, page_content='receipt_desc         14166    non-null values\\nmemo_cd              92482    non-null values\\nmemo_text            97770    non-null values\\nform_tp              1001731  non-null values\\nfile_num             1001731  non-null values\\ndtypes: float64(1), int64(1), object(14)\\nA sample record in the DataFrame looks like this:\\nIn [15]: fec.ix[123456]\\nOut[15]:\\ncmte_id                             C00431445\\n278 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='cand_id                             P80003338\\ncand_nm                         Obama, Barack\\ncontbr_nm                         ELLMAN, IRA\\ncontbr_city                             TEMPE\\ncontbr_st                                  AZ\\ncontbr_zip                          852816719\\ncontbr_employer      ARIZONA STATE UNIVERSITY\\ncontbr_occupation                   PROFESSOR\\ncontb_receipt_amt                          50\\ncontb_receipt_dt                    01-DEC-11\\nreceipt_desc                              NaN\\nmemo_cd                                   NaN\\nmemo_text                                 NaN\\nform_tp                                 SA17A\\nfile_num                               772372\\nName: 123456\\nYou can probably think of many ways to start slicing and dicing this data to extract\\ninformative statistics about donors and patterns in the campaign contributions. I’ll\\nspend the next several pages showing you a number of different analyses that apply\\ntechniques you have learned about so far.'),\n",
       " Document(metadata={}, page_content=\"spend the next several pages showing you a number of different analyses that apply\\ntechniques you have learned about so far.\\nYou can see that there are no political party affiliations in the data, so this would be\\nuseful to add. You can get a list of all the unique political candidates using unique (note\\nthat NumPy suppresses the quotes around the strings in the output):\\nIn [16]: unique_cands = fec.cand_nm.unique()\\nIn [17]: unique_cands\\nOut[17]:\\narray([Bachmann, Michelle, Romney, Mitt, Obama, Barack,\\n       Roemer, Charles E. 'Buddy' III, Pawlenty, Timothy,\\n       Johnson, Gary Earl, Paul, Ron, Santorum, Rick, Cain, Herman,\\n       Gingrich, Newt, McCotter, Thaddeus G, Huntsman, Jon, Perry, Rick], dtype=object)\\nIn [18]: unique_cands[2]\\nOut[18]: 'Obama, Barack'\\nAn easy way to indicate party affiliation is using a dict:2\\nparties = {'Bachmann, Michelle': 'Republican',\\n           'Cain, Herman': 'Republican',\\n           'Gingrich, Newt': 'Republican',\"),\n",
       " Document(metadata={}, page_content='An easy way to indicate party affiliation is using a dict:2\\nparties = {\\'Bachmann, Michelle\\': \\'Republican\\',\\n           \\'Cain, Herman\\': \\'Republican\\',\\n           \\'Gingrich, Newt\\': \\'Republican\\',\\n           \\'Huntsman, Jon\\': \\'Republican\\',\\n           \\'Johnson, Gary Earl\\': \\'Republican\\',\\n           \\'McCotter, Thaddeus G\\': \\'Republican\\',\\n           \\'Obama, Barack\\': \\'Democrat\\',\\n           \\'Paul, Ron\\': \\'Republican\\',\\n           \\'Pawlenty, Timothy\\': \\'Republican\\',\\n           \\'Perry, Rick\\': \\'Republican\\',\\n           \"Roemer, Charles E. \\'Buddy\\' III\": \\'Republican\\',\\n2. This makes the simplifying assumption that Gary Johnson is a Republican even though he later became\\nthe Libertarian party candidate.\\nExample: 2012 Federal Election Commission Database | 279\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"'Romney, Mitt': 'Republican',\\n           'Santorum, Rick': 'Republican'}\\nNow, using this mapping and the map method on Series objects, you can compute an\\narray of political parties from the candidate names:\\nIn [20]: fec.cand_nm[123456:123461]\\nOut[20]:\\n123456    Obama, Barack\\n123457    Obama, Barack\\n123458    Obama, Barack\\n123459    Obama, Barack\\n123460    Obama, Barack\\nName: cand_nm\\nIn [21]: fec.cand_nm[123456:123461].map(parties)\\nOut[21]:\\n123456    Democrat\\n123457    Democrat\\n123458    Democrat\\n123459    Democrat\\n123460    Democrat\\nName: cand_nm\\n# Add it as a column\\nIn [22]: fec['party'] = fec.cand_nm.map(parties)\\nIn [23]: fec['party'].value_counts()\\nOut[23]:\\nDemocrat      593746\\nRepublican    407985\\nA couple of data preparation points. First, this data includes both contributions and\\nrefunds (negative contribution amount):\\nIn [24]: (fec.contb_receipt_amt > 0).value_counts()\\nOut[24]:\\nTrue     991475\\nFalse     10256\"),\n",
       " Document(metadata={}, page_content=\"refunds (negative contribution amount):\\nIn [24]: (fec.contb_receipt_amt > 0).value_counts()\\nOut[24]:\\nTrue     991475\\nFalse     10256\\nTo simplify the analysis, I’ll restrict the data set to positive contributions:\\nIn [25]: fec = fec[fec.contb_receipt_amt > 0]\\nSince Barack Obama and Mitt Romney are the main two candidates, I’ll also prepare\\na subset that just has contributions to their campaigns:\\nIn [26]: fec_mrbo = fec[fec.cand_nm.isin(['Obama, Barack', 'Romney, Mitt'])]\\nDonation Statistics by Occupation and Employer\\nDonations by occupation is another oft-studied statistic. For example, lawyers (attor-\\nneys) tend to donate more money to Democrats, while business executives tend to\\ndonate more to Republicans. You have no reason to believe me; you can see for yourself\\nin the data. First, the total number of donations by occupation is easy:\\n280 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"In [27]: fec.contbr_occupation.value_counts()[:10]\\nOut[27]:\\nRETIRED                                   233990\\nINFORMATION REQUESTED                      35107\\nATTORNEY                                   34286\\nHOMEMAKER                                  29931\\nPHYSICIAN                                  23432\\nINFORMATION REQUESTED PER BEST EFFORTS     21138\\nENGINEER                                   14334\\nTEACHER                                    13990\\nCONSULTANT                                 13273\\nPROFESSOR                                  12555\\nYou will notice by looking at the occupations that many refer to the same basic job\\ntype, or there are several variants of the same thing. Here is a code snippet illustrates a\\ntechnique for cleaning up a few of them by mapping from one occupation to another;\\nnote the “trick” of using dict.get to allow occupations with no mapping to “pass\\nthrough”:\\nocc_mapping = {\\n   'INFORMATION REQUESTED PER BEST EFFORTS' : 'NOT PROVIDED',\"),\n",
       " Document(metadata={}, page_content=\"note the “trick” of using dict.get to allow occupations with no mapping to “pass\\nthrough”:\\nocc_mapping = {\\n   'INFORMATION REQUESTED PER BEST EFFORTS' : 'NOT PROVIDED',\\n   'INFORMATION REQUESTED' : 'NOT PROVIDED',\\n   'INFORMATION REQUESTED (BEST EFFORTS)' : 'NOT PROVIDED',\\n   'C.E.O.': 'CEO'\\n}\\n# If no mapping provided, return x\\nf = lambda x: occ_mapping.get(x, x)\\nfec.contbr_occupation = fec.contbr_occupation.map(f)\\nI’ll also do the same thing for employers:\\nemp_mapping = {\\n   'INFORMATION REQUESTED PER BEST EFFORTS' : 'NOT PROVIDED',\\n   'INFORMATION REQUESTED' : 'NOT PROVIDED',\\n   'SELF' : 'SELF-EMPLOYED',\\n   'SELF EMPLOYED' : 'SELF-EMPLOYED',\\n}\\n# If no mapping provided, return x\\nf = lambda x: emp_mapping.get(x, x)\\nfec.contbr_employer = fec.contbr_employer.map(f)\\nNow, you can use pivot_table to aggregate the data by party and occupation, then\\nfilter down to the subset that donated at least $2 million overall:\\nIn [34]: by_occupation = fec.pivot_table('contb_receipt_amt',\"),\n",
       " Document(metadata={}, page_content=\"filter down to the subset that donated at least $2 million overall:\\nIn [34]: by_occupation = fec.pivot_table('contb_receipt_amt',\\n   ....:                                 rows='contbr_occupation',\\n   ....:                                 cols='party', aggfunc='sum')\\nIn [35]: over_2mm = by_occupation[by_occupation.sum(1) > 2000000]\\nIn [36]: over_2mm\\nOut[36]:\\nparty                 Democrat       Republican\\ncontbr_occupation\\nExample: 2012 Federal Election Commission Database | 281\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"ATTORNEY           11141982.97   7477194.430000\\nCEO                 2074974.79   4211040.520000\\nCONSULTANT          2459912.71   2544725.450000\\nENGINEER             951525.55   1818373.700000\\nEXECUTIVE           1355161.05   4138850.090000\\nHOMEMAKER           4248875.80  13634275.780000\\nINVESTOR             884133.00   2431768.920000\\nLAWYER              3160478.87    391224.320000\\nMANAGER              762883.22   1444532.370000\\nNOT PROVIDED        4866973.96  20565473.010000\\nOWNER               1001567.36   2408286.920000\\nPHYSICIAN           3735124.94   3594320.240000\\nPRESIDENT           1878509.95   4720923.760000\\nPROFESSOR           2165071.08    296702.730000\\nREAL ESTATE          528902.09   1625902.250000\\nRETIRED            25305116.38  23561244.489999\\nSELF-EMPLOYED        672393.40   1640252.540000\\nIt can be easier to look at this data graphically as a bar plot ( 'barh' means horizontal\\nbar plot, see Figure 9-2):\\nIn [38]: over_2mm.plot(kind='barh')\"),\n",
       " Document(metadata={}, page_content=\"It can be easier to look at this data graphically as a bar plot ( 'barh' means horizontal\\nbar plot, see Figure 9-2):\\nIn [38]: over_2mm.plot(kind='barh')\\nFigure 9-2. Total donations by party for top occupations\\nYou might be interested in the top donor occupations or top companies donating to\\nObama and Romney. To do this, you can group by candidate name and use a variant\\nof the top method from earlier in the chapter:\\ndef get_top_amounts(group, key, n=5):\\n    totals = group.groupby(key)['contb_receipt_amt'].sum()\\n    # Order totals by key in descending order\\n    return totals.order(ascending=False)[-n:]\\n282 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"Then aggregated by occupation and employer:\\nIn [40]: grouped = fec_mrbo.groupby('cand_nm')\\nIn [41]: grouped.apply(get_top_amounts, 'contbr_occupation', n=7)\\nOut[41]:\\ncand_nm        contbr_occupation\\nObama, Barack  RETIRED              25305116.38\\n               ATTORNEY             11141982.97\\n               NOT PROVIDED          4866973.96\\n               HOMEMAKER             4248875.80\\n               PHYSICIAN             3735124.94\\n               LAWYER                3160478.87\\n               CONSULTANT            2459912.71\\nRomney, Mitt   RETIRED              11508473.59\\n               NOT PROVIDED         11396894.84\\n               HOMEMAKER             8147446.22\\n               ATTORNEY              5364718.82\\n               PRESIDENT             2491244.89\\n               EXECUTIVE             2300947.03\\n               C.E.O.                1968386.11\\nName: contb_receipt_amt\\nIn [42]: grouped.apply(get_top_amounts, 'contbr_employer', n=10)\\nOut[42]:\\ncand_nm        contbr_employer\"),\n",
       " Document(metadata={}, page_content=\"C.E.O.                1968386.11\\nName: contb_receipt_amt\\nIn [42]: grouped.apply(get_top_amounts, 'contbr_employer', n=10)\\nOut[42]:\\ncand_nm        contbr_employer\\nObama, Barack  RETIRED               22694358.85\\n               SELF-EMPLOYED         18626807.16\\n               NOT EMPLOYED           8586308.70\\n               NOT PROVIDED           5053480.37\\n               HOMEMAKER              2605408.54\\n               STUDENT                 318831.45\\n               VOLUNTEER               257104.00\\n               MICROSOFT               215585.36\\n               SIDLEY AUSTIN LLP       168254.00\\n               REFUSED                 149516.07\\nRomney, Mitt   NOT PROVIDED          12059527.24\\n               RETIRED               11506225.71\\n               HOMEMAKER              8147196.22\\n               SELF-EMPLOYED          7414115.22\\n               STUDENT                 496490.94\\n               CREDIT SUISSE           281150.00\"),\n",
       " Document(metadata={}, page_content='HOMEMAKER              8147196.22\\n               SELF-EMPLOYED          7414115.22\\n               STUDENT                 496490.94\\n               CREDIT SUISSE           281150.00\\n               MORGAN STANLEY          267266.00\\n               GOLDMAN SACH & CO.      238250.00\\n               BARCLAYS CAPITAL        162750.00\\n               H.I.G. CAPITAL          139500.00\\nName: contb_receipt_amt\\nBucketing Donation Amounts\\nA useful way to analyze this data is to use the cut function to discretize the contributor\\namounts into buckets by contribution size:\\nIn [43]: bins = np.array([0, 1, 10, 100, 1000, 10000, 100000, 1000000, 10000000])\\nExample: 2012 Federal Election Commission Database | 283\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"In [44]: labels = pd.cut(fec_mrbo.contb_receipt_amt, bins)\\nIn [45]: labels\\nOut[45]:\\nCategorical:contb_receipt_amt\\narray([(10, 100], (100, 1000], (100, 1000], ..., (1, 10], (10, 100],\\n       (100, 1000]], dtype=object)\\nLevels (8): array([(0, 1], (1, 10], (10, 100], (100, 1000], (1000, 10000],\\n       (10000, 100000], (100000, 1000000], (1000000, 10000000]], dtype=object)\\nWe can then group the data for Obama and Romney by name and bin label to get a\\nhistogram by donation size:\\nIn [46]: grouped = fec_mrbo.groupby(['cand_nm', labels])\\nIn [47]: grouped.size().unstack(0)\\nOut[47]:\\ncand_nm              Obama, Barack  Romney, Mitt\\ncontb_receipt_amt\\n(0, 1]                         493            77\\n(1, 10]                      40070          3681\\n(10, 100]                   372280         31853\\n(100, 1000]                 153991         43357\\n(1000, 10000]                22284         26186\\n(10000, 100000]                  2             1\\n(100000, 1000000]                3           NaN\"),\n",
       " Document(metadata={}, page_content='(100, 1000]                 153991         43357\\n(1000, 10000]                22284         26186\\n(10000, 100000]                  2             1\\n(100000, 1000000]                3           NaN\\n(1000000, 10000000]              4           NaN\\nThis data shows that Obama has received a significantly larger number of small don-\\nations than Romney. You can also sum the contribution amounts and normalize within\\nbuckets to visualize percentage of total donations of each size by candidate:\\nIn [48]: bucket_sums = grouped.contb_receipt_amt.sum().unstack(0)\\nIn [49]: bucket_sums\\nOut[49]:\\ncand_nm              Obama, Barack  Romney, Mitt\\ncontb_receipt_amt\\n(0, 1]                      318.24         77.00\\n(1, 10]                  337267.62      29819.66\\n(10, 100]              20288981.41    1987783.76\\n(100, 1000]            54798531.46   22363381.69\\n(1000, 10000]          51753705.67   63942145.42\\n(10000, 100000]           59100.00      12700.00\\n(100000, 1000000]       1490683.08           NaN'),\n",
       " Document(metadata={}, page_content='(100, 1000]            54798531.46   22363381.69\\n(1000, 10000]          51753705.67   63942145.42\\n(10000, 100000]           59100.00      12700.00\\n(100000, 1000000]       1490683.08           NaN\\n(1000000, 10000000]     7148839.76           NaN\\nIn [50]: normed_sums = bucket_sums.div(bucket_sums.sum(axis=1), axis=0)\\nIn [51]: normed_sums\\nOut[51]:\\ncand_nm              Obama, Barack  Romney, Mitt\\ncontb_receipt_amt\\n(0, 1]                    0.805182      0.194818\\n(1, 10]                   0.918767      0.081233\\n(10, 100]                 0.910769      0.089231\\n284 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"(100, 1000]               0.710176      0.289824\\n(1000, 10000]             0.447326      0.552674\\n(10000, 100000]           0.823120      0.176880\\n(100000, 1000000]         1.000000           NaN\\n(1000000, 10000000]       1.000000           NaN\\nIn [52]: normed_sums[:-2].plot(kind='barh', stacked=True)\\nI excluded the two largest bins as these are not donations by individuals. See Fig-\\nure 9-3 for the resulting figure.\\nFigure 9-3. Percentage of total donations received by candidates for each donation size\\nThere are of course many refinements and improvements of this analysis. For example,\\nyou could aggregate donations by donor name and zip code to adjust for donors who\\ngave many small amounts versus one or more large donations. I encourage you to\\ndownload it and explore it yourself.\\nDonation Statistics by State\\nAggregating the data by candidate and state is a routine affair:\\nIn [53]: grouped = fec_mrbo.groupby(['cand_nm', 'contbr_st'])\"),\n",
       " Document(metadata={}, page_content=\"download it and explore it yourself.\\nDonation Statistics by State\\nAggregating the data by candidate and state is a routine affair:\\nIn [53]: grouped = fec_mrbo.groupby(['cand_nm', 'contbr_st'])\\nIn [54]: totals = grouped.contb_receipt_amt.sum().unstack(0).fillna(0)\\nIn [55]: totals = totals[totals.sum(1) > 100000]\\nIn [56]: totals[:10]\\nOut[56]:\\ncand_nm    Obama, Barack  Romney, Mitt\\ncontbr_st\\nExample: 2012 Federal Election Commission Database | 285\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content='AK             281840.15      86204.24\\nAL             543123.48     527303.51\\nAR             359247.28     105556.00\\nAZ            1506476.98    1888436.23\\nCA           23824984.24   11237636.60\\nCO            2132429.49    1506714.12\\nCT            2068291.26    3499475.45\\nDC            4373538.80    1025137.50\\nDE             336669.14      82712.00\\nFL            7318178.58    8338458.81\\nIf you divide each row by the total contribution amount, you get the relative percentage\\nof total donations by state for each candidate:\\nIn [57]: percent = totals.div(totals.sum(1), axis=0)\\nIn [58]: percent[:10]\\nOut[58]:\\ncand_nm    Obama, Barack  Romney, Mitt\\ncontbr_st\\nAK              0.765778      0.234222\\nAL              0.507390      0.492610\\nAR              0.772902      0.227098\\nAZ              0.443745      0.556255\\nCA              0.679498      0.320502\\nCO              0.585970      0.414030\\nCT              0.371476      0.628524\\nDC              0.810113      0.189887'),\n",
       " Document(metadata={}, page_content=\"AZ              0.443745      0.556255\\nCA              0.679498      0.320502\\nCO              0.585970      0.414030\\nCT              0.371476      0.628524\\nDC              0.810113      0.189887\\nDE              0.802776      0.197224\\nFL              0.467417      0.532583\\nI thought it would be interesting to look at this data plotted on a map, using ideas from\\nChapter 8. After locating a shape file for the state boundaries (http://nationalatlas.gov/\\natlasftp.html?openChapters=chpbound) and learning a bit more about matplotlib and\\nits basemap toolkit (I was aided by a blog posting from Thomas Lecocq) 3, I ended up\\nwith the following code for plotting these relative percentages:\\nfrom mpl_toolkits.basemap import Basemap, cm\\nimport numpy as np\\nfrom matplotlib import rcParams\\nfrom matplotlib.collections import LineCollection\\nimport matplotlib.pyplot as plt\\nfrom shapelib import ShapeFile\\nimport dbflib\\nobama = percent['Obama, Barack']\\nfig = plt.figure(figsize=(12, 12))\"),\n",
       " Document(metadata={}, page_content=\"from matplotlib.collections import LineCollection\\nimport matplotlib.pyplot as plt\\nfrom shapelib import ShapeFile\\nimport dbflib\\nobama = percent['Obama, Barack']\\nfig = plt.figure(figsize=(12, 12))\\nax = fig.add_axes([0.1,0.1,0.8,0.8])\\nlllat = 21; urlat = 53; lllon = -118; urlon = -62\\n3. http://www.geophysique.be/2011/01/27/matplotlib-basemap-tutorial-07-shapefiles-unleached/\\n286 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"m = Basemap(ax=ax, projection='stere',\\n            lon_0=(urlon + lllon) / 2, lat_0=(urlat + lllat) / 2,\\n            llcrnrlat=lllat, urcrnrlat=urlat, llcrnrlon=lllon,\\n            urcrnrlon=urlon, resolution='l')\\nm.drawcoastlines()\\nm.drawcountries()\\nshp = ShapeFile('../states/statesp020')\\ndbf = dbflib.open('../states/statesp020')\\nfor npoly in range(shp.info()[0]):\\n    # Draw colored polygons on the map\\n    shpsegs = []\\n    shp_object = shp.read_object(npoly)\\n    verts = shp_object.vertices()\\n    rings = len(verts)\\n    for ring in range(rings):\\n        lons, lats = zip(*verts[ring])\\n        x, y = m(lons, lats)\\n        shpsegs.append(zip(x,y))\\n        if ring == 0:\\n            shapedict = dbf.read_record(npoly)\\n        name = shapedict['STATE']\\n    lines = LineCollection(shpsegs,antialiaseds=(1,))\\n    # state_to_code dict, e.g. 'ALASKA' -> 'AK', omitted\\n    try:\\n        per = obama[state_to_code[name.upper()]]\\n    except KeyError:\\n        continue\\n    lines.set_facecolors('k')\"),\n",
       " Document(metadata={}, page_content=\"# state_to_code dict, e.g. 'ALASKA' -> 'AK', omitted\\n    try:\\n        per = obama[state_to_code[name.upper()]]\\n    except KeyError:\\n        continue\\n    lines.set_facecolors('k')\\n    lines.set_alpha(0.75 * per) # Shrink the percentage a bit\\n    lines.set_edgecolors('k')\\n    lines.set_linewidth(0.3)\\n    ax.add_collection(lines)\\nplt.show()\\nSee Figure 9-4 for the result.\\nExample: 2012 Federal Election Commission Database | 287\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content='Figure 9-4. US map aggregated donation statistics overlay (darker means more Democratic)\\n288 | Chapter 9: \\u2002Data Aggregation and Group Operations\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='CHAPTER 10\\nTime Series\\nTime series data is an important form of structured data in many different fields, such\\nas finance, economics, ecology, neuroscience, or physics. Anything that is observed or\\nmeasured at many points in time forms a time series. Many time series are fixed fre-\\nquency, which is to say that data points occur at regular intervals according to some\\nrule, such as every 15 seconds, every 5 minutes, or once per month. Time series can\\nalso be irregular without a fixed unit or time or offset between units. How you mark\\nand refer to time series data depends on the application and you may have one of the\\nfollowing:\\n• Timestamps, specific instants in time\\n• Fixed periods, such as the month January 2007 or the full year 2010\\n• Intervals of time, indicated by a start and end timestamp. Periods can be thought\\nof as special cases of intervals\\n• Experiment or elapsed time; each timestamp is a measure of time relative to a'),\n",
       " Document(metadata={}, page_content='• Intervals of time, indicated by a start and end timestamp. Periods can be thought\\nof as special cases of intervals\\n• Experiment or elapsed time; each timestamp is a measure of time relative to a\\nparticular start time. For example, the diameter of a cookie baking each second\\nsince being placed in the oven\\nIn this chapter, I am mainly concerned with time series in the first 3 categories, though\\nmany of the techniques can be applied to experimental time series where the index may\\nbe an integer or floating point number indicating elapsed time from the start of the\\nexperiment. The simplest and most widely used kind of time series are those indexed\\nby timestamp.\\npandas provides a standard set of time series tools and data algorithms. With this, you\\ncan efficiently work with very large time series and easily slice and dice, aggregate, and\\nresample irregular and fixed frequency time series. As you might guess, many of these'),\n",
       " Document(metadata={}, page_content='can efficiently work with very large time series and easily slice and dice, aggregate, and\\nresample irregular and fixed frequency time series. As you might guess, many of these\\ntools are especially useful for financial and economics applications, but you could cer-\\ntainly use them to analyze server log data, too.\\n289\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Some of the features and code, in particular period logic, presented in\\nthis chapter were derived from the now defunct scikits.timeseries li-\\nbrary.\\nDate and Time Data Types and Tools\\nThe Python standard library includes data types for date and time data, as well as\\ncalendar-related functionality. The datetime, time, and calendar modules are the main\\nplaces to start. The datetime.datetime type, or simply datetime, is widely used:\\nIn [317]: from datetime import datetime\\nIn [318]: now = datetime.now()\\nIn [319]: now\\nOut[319]: datetime.datetime(2012, 8, 4, 17, 9, 21, 832092)\\nIn [320]: now.year, now.month, now.day\\nOut[320]: (2012, 8, 4)\\ndatetime stores both the date and time down to the microsecond. datetime.time\\ndelta represents the temporal difference between two datetime objects:\\nIn [321]: delta = datetime(2011, 1, 7) - datetime(2008, 6, 24, 8, 15)\\nIn [322]: delta\\nOut[322]: datetime.timedelta(926, 56700)\\nIn [323]: delta.days        In [324]: delta.seconds'),\n",
       " Document(metadata={}, page_content='In [321]: delta = datetime(2011, 1, 7) - datetime(2008, 6, 24, 8, 15)\\nIn [322]: delta\\nOut[322]: datetime.timedelta(926, 56700)\\nIn [323]: delta.days        In [324]: delta.seconds\\nOut[323]: 926               Out[324]: 56700\\nYou can add (or subtract) a timedelta or multiple thereof to a datetime object to yield\\na new shifted object:\\nIn [325]: from datetime import timedelta\\nIn [326]: start = datetime(2011, 1, 7)\\nIn [327]: start + timedelta(12)\\nOut[327]: datetime.datetime(2011, 1, 19, 0, 0)\\nIn [328]: start - 2 * timedelta(12)\\nOut[328]: datetime.datetime(2010, 12, 14, 0, 0)\\nThe data types in the datetime module are summarized in Table 10-1. While this chap-\\nter is mainly concerned with the data types in pandas and higher level time series ma-\\nnipulation, you will undoubtedly encounter the datetime-based types in many other\\nplaces in Python the wild.\\n290 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"Table 10-1. Types in datetime module\\nType Description\\ndate Store calendar date (year, month, day) using the Gregorian calendar.\\ntime Store time of day as hours, minutes, seconds, and microseconds\\ndatetime Stores both date and time\\ntimedelta Represents the difference between two datetime values (as days, seconds, and micro-\\nseconds)\\nConverting between string and datetime\\ndatetime objects and pandas Timestamp objects, which I’ll introduce later, can be for-\\nmatted as strings using str or the strftime method, passing a format specification:\\nIn [329]: stamp = datetime(2011, 1, 3)\\nIn [330]: str(stamp)                   In [331]: stamp.strftime('%Y-%m-%d')\\nOut[330]: '2011-01-03 00:00:00'        Out[331]: '2011-01-03'\\nSee Table 10-2 for a complete list of the format codes. These same format codes can be\\nused to convert strings to dates using datetime.strptime:\\nIn [332]: value = '2011-01-03'\\nIn [333]: datetime.strptime(value, '%Y-%m-%d')\\nOut[333]: datetime.datetime(2011, 1, 3, 0, 0)\"),\n",
       " Document(metadata={}, page_content=\"used to convert strings to dates using datetime.strptime:\\nIn [332]: value = '2011-01-03'\\nIn [333]: datetime.strptime(value, '%Y-%m-%d')\\nOut[333]: datetime.datetime(2011, 1, 3, 0, 0)\\nIn [334]: datestrs = ['7/6/2011', '8/6/2011']\\nIn [335]: [datetime.strptime(x, '%m/%d/%Y') for x in datestrs]\\nOut[335]: [datetime.datetime(2011, 7, 6, 0, 0), datetime.datetime(2011, 8, 6, 0, 0)]\\ndatetime.strptime is the best way to parse a date with a known format. However, it\\ncan be a bit annoying to have to write a format spec each time, especially for common\\ndate formats. In this case, you can use the parser.parse method in the third party \\ndateutil package:\\nIn [336]: from dateutil.parser import parse\\nIn [337]: parse('2011-01-03')\\nOut[337]: datetime.datetime(2011, 1, 3, 0, 0)\\ndateutil is capable of parsing almost any human-intelligible date representation:\\nIn [338]: parse('Jan 31, 1997 10:45 PM')\\nOut[338]: datetime.datetime(1997, 1, 31, 22, 45)\"),\n",
       " Document(metadata={}, page_content=\"dateutil is capable of parsing almost any human-intelligible date representation:\\nIn [338]: parse('Jan 31, 1997 10:45 PM')\\nOut[338]: datetime.datetime(1997, 1, 31, 22, 45)\\nIn international locales, day appearing before month is very common, so you can pass\\ndayfirst=True to indicate this:\\nIn [339]: parse('6/12/2011', dayfirst=True)\\nOut[339]: datetime.datetime(2011, 12, 6, 0, 0)\\nDate and Time Data Types and Tools | 291\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"pandas is generally oriented toward working with arrays of dates, whether used as an\\naxis index or a column in a DataFrame. The to_datetime method parses many different\\nkinds of date representations. Standard date formats like ISO8601 can be parsed very\\nquickly.\\nIn [340]: datestrs\\nOut[340]: ['7/6/2011', '8/6/2011']\\nIn [341]: pd.to_datetime(datestrs)\\nOut[341]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n[2011-07-06 00:00:00, 2011-08-06 00:00:00]\\nLength: 2, Freq: None, Timezone: None\\nIt also handles values that should be considered missing (None, empty string, etc.):\\nIn [342]: idx = pd.to_datetime(datestrs + [None])\\nIn [343]: idx\\nOut[343]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n[2011-07-06 00:00:00, ..., NaT]\\nLength: 3, Freq: None, Timezone: None\\nIn [344]: idx[2]\\nOut[344]: NaT\\nIn [345]: pd.isnull(idx)\\nOut[345]: array([False, False, True], dtype=bool)\\nNaT (Not a Time) is pandas’s NA value for timestamp data.\"),\n",
       " Document(metadata={}, page_content=\"In [344]: idx[2]\\nOut[344]: NaT\\nIn [345]: pd.isnull(idx)\\nOut[345]: array([False, False, True], dtype=bool)\\nNaT (Not a Time) is pandas’s NA value for timestamp data.\\ndateutil.parser is a useful, but not perfect tool. Notably, it will recog-\\nnize some strings as dates that you might prefer that it didn’t, like\\n'42' will be parsed as the year 2042 with today’s calendar date.\\nTable 10-2. Datetime format specification (ISO C89 compatible)\\nType Description\\n%Y 4-digit year\\n%y 2-digit year\\n%m 2-digit month [01, 12]\\n%d 2-digit day [01, 31]\\n%H Hour (24-hour clock) [00, 23]\\n%I Hour (12-hour clock) [01, 12]\\n%M 2-digit minute [00, 59]\\n%S Second [00, 61] (seconds 60, 61 account for leap seconds)\\n%w Weekday as integer [0 (Sunday), 6]\\n292 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content='Type Description\\n%U Week number of the year [00, 53]. Sunday is considered the first day of the week, and days before the first\\nSunday of the year are “week 0”.\\n%W Week number of the year [00, 53]. Monday is considered the first day of the week, and days before the first\\nMonday of the year are “week 0”.\\n%z UTC time zone offset as +HHMM or -HHMM, empty if time zone naive\\n%F Shortcut for %Y-%m-%d, for example 2012-4-18\\n%D Shortcut for %m/%d/%y, for example 04/18/12\\ndatetime objects also have a number of locale-specific formatting options for systems\\nin other countries or languages. For example, the abbreviated month names will be\\ndifferent on German or French systems compared with English systems. \\nTable 10-3. Locale-specific date formatting\\nType Description\\n%a Abbreviated weekday name\\n%A Full weekday name\\n%b Abbreviated month name\\n%B Full month name\\n%c Full date and time, for example ‘Tue 01 May 2012 04:20:57 PM’\\n%p Locale equivalent of AM or PM'),\n",
       " Document(metadata={}, page_content='%a Abbreviated weekday name\\n%A Full weekday name\\n%b Abbreviated month name\\n%B Full month name\\n%c Full date and time, for example ‘Tue 01 May 2012 04:20:57 PM’\\n%p Locale equivalent of AM or PM\\n%x Locale-appropriate formatted date; e.g. in US May 1, 2012 yields ’05/01/2012’\\n%X Locale-appropriate time, e.g. ’04:24:12 PM’\\nTime Series Basics\\nThe most basic kind of time series object in pandas is a Series indexed by timestamps,\\nwhich is often represented external to pandas as Python strings or datetime objects:\\nIn [346]: from datetime import datetime\\nIn [347]: dates = [datetime(2011, 1, 2), datetime(2011, 1, 5), datetime(2011, 1, 7),\\n   .....:          datetime(2011, 1, 8), datetime(2011, 1, 10), datetime(2011, 1, 12)]\\nIn [348]: ts = Series(np.random.randn(6), index=dates)\\nIn [349]: ts\\nOut[349]: \\n2011-01-02    0.690002\\n2011-01-05    1.001543\\n2011-01-07   -0.503087\\n2011-01-08   -0.622274\\nTime Series Basics | 293\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"2011-01-10   -0.921169\\n2011-01-12   -0.726213\\nUnder the hood, these datetime objects have been put in a DatetimeIndex, and the\\nvariable ts is now of type TimeSeries:\\nIn [350]: type(ts)\\nOut[350]: pandas.core.series.TimeSeries\\nIn [351]: ts.index\\nOut[351]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n[2011-01-02 00:00:00, ..., 2011-01-12 00:00:00]\\nLength: 6, Freq: None, Timezone: None\\nIt’s not necessary to use the TimeSeries constructor explicitly; when\\ncreating a Series with a DatetimeIndex, pandas knows that the object is\\na time series.\\nLike other Series, arithmetic operations between differently-indexed time series auto-\\nmatically align on the dates:\\nIn [352]: ts + ts[::2]\\nOut[352]: \\n2011-01-02    1.380004\\n2011-01-05         NaN\\n2011-01-07   -1.006175\\n2011-01-08         NaN\\n2011-01-10   -1.842337\\n2011-01-12         NaN\\npandas stores timestamps using NumPy’s datetime64 data type at the nanosecond res-\\nolution:\\nIn [353]: ts.index.dtype\\nOut[353]: dtype('datetime64[ns]')\"),\n",
       " Document(metadata={}, page_content=\"2011-01-10   -1.842337\\n2011-01-12         NaN\\npandas stores timestamps using NumPy’s datetime64 data type at the nanosecond res-\\nolution:\\nIn [353]: ts.index.dtype\\nOut[353]: dtype('datetime64[ns]')\\nScalar values from a DatetimeIndex are pandas Timestamp objects\\nIn [354]: stamp = ts.index[0]\\nIn [355]: stamp\\nOut[355]: <Timestamp: 2011-01-02 00:00:00>\\nA Timestamp can be substituted anywhere you would use a datetime object. Addition-\\nally, it can store frequency information (if any) and understands how to do time zone\\nconversions and other kinds of manipulations. More on both of these things later.\\nIndexing, Selection, Subsetting\\nTimeSeries is a subclass of Series and thus behaves in the same way with regard to\\nindexing and selecting data based on label:\\n294 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"In [356]: stamp = ts.index[2]\\nIn [357]: ts[stamp]\\nOut[357]: -0.50308739136034464\\nAs a convenience, you can also pass a string that is interpretable as a date:\\nIn [358]: ts['1/10/2011']             In [359]: ts['20110110']      \\nOut[358]: -0.92116860801301081        Out[359]: -0.92116860801301081\\nFor longer time series, a year or only a year and month can be passed to easily select\\nslices of data:\\nIn [360]: longer_ts = Series(np.random.randn(1000),\\n   .....:                    index=pd.date_range('1/1/2000', periods=1000))\\nIn [361]: longer_ts\\nOut[361]: \\n2000-01-01    0.222896\\n2000-01-02    0.051316\\n2000-01-03   -1.157719\\n2000-01-04    0.816707\\n...\\n2002-09-23   -0.395813\\n2002-09-24   -0.180737\\n2002-09-25    1.337508\\n2002-09-26   -0.416584\\nFreq: D, Length: 1000\\nIn [362]: longer_ts['2001']        In [363]: longer_ts['2001-05']\\nOut[362]:                          Out[363]:                     \\n2001-01-01   -1.499503             2001-05-01    1.662014\"),\n",
       " Document(metadata={}, page_content=\"In [362]: longer_ts['2001']        In [363]: longer_ts['2001-05']\\nOut[362]:                          Out[363]:                     \\n2001-01-01   -1.499503             2001-05-01    1.662014        \\n2001-01-02    0.545154             2001-05-02   -1.189203        \\n2001-01-03    0.400823             2001-05-03    0.093597        \\n2001-01-04   -1.946230             2001-05-04   -0.539164        \\n...                                ...                           \\n2001-12-28   -1.568139             2001-05-28   -0.683066        \\n2001-12-29   -0.900887             2001-05-29   -0.950313        \\n2001-12-30    0.652346             2001-05-30    0.400710        \\n2001-12-31    0.871600             2001-05-31   -0.126072        \\nFreq: D, Length: 365               Freq: D, Length: 31\\nSlicing with dates works just like with a regular Series:\\nIn [364]: ts[datetime(2011, 1, 7):]\\nOut[364]: \\n2011-01-07   -0.503087\\n2011-01-08   -0.622274\\n2011-01-10   -0.921169\\n2011-01-12   -0.726213\"),\n",
       " Document(metadata={}, page_content=\"Slicing with dates works just like with a regular Series:\\nIn [364]: ts[datetime(2011, 1, 7):]\\nOut[364]: \\n2011-01-07   -0.503087\\n2011-01-08   -0.622274\\n2011-01-10   -0.921169\\n2011-01-12   -0.726213\\nBecause most time series data is ordered chronologically, you can slice with timestamps\\nnot contained in a time series to perform a range query:\\nIn [365]: ts                  In [366]: ts['1/6/2011':'1/11/2011']\\nOut[365]:                     Out[366]:                           \\n2011-01-02    0.690002        2011-01-07   -0.503087              \\nTime Series Basics | 295\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"2011-01-05    1.001543        2011-01-08   -0.622274              \\n2011-01-07   -0.503087        2011-01-10   -0.921169              \\n2011-01-08   -0.622274                                            \\n2011-01-10   -0.921169                                            \\n2011-01-12   -0.726213\\nAs before you can pass either a string date, datetime, or Timestamp. Remember that\\nslicing in this manner produces views on the source time series just like slicing NumPy\\narrays. There is an equivalent instance method truncate which slices a TimeSeries be-\\ntween two dates:\\nIn [367]: ts.truncate(after='1/9/2011')\\nOut[367]: \\n2011-01-02    0.690002\\n2011-01-05    1.001543\\n2011-01-07   -0.503087\\n2011-01-08   -0.622274\\nAll of the above holds true for DataFrame as well, indexing on its rows:\\nIn [368]: dates = pd.date_range('1/1/2000', periods=100, freq='W-WED')\\nIn [369]: long_df = DataFrame(np.random.randn(100, 4),\\n   .....:                     index=dates,\"),\n",
       " Document(metadata={}, page_content=\"In [368]: dates = pd.date_range('1/1/2000', periods=100, freq='W-WED')\\nIn [369]: long_df = DataFrame(np.random.randn(100, 4),\\n   .....:                     index=dates,\\n   .....:                     columns=['Colorado', 'Texas', 'New York', 'Ohio'])\\nIn [370]: long_df.ix['5-2001']\\nOut[370]: \\n            Colorado     Texas  New York      Ohio\\n2001-05-02  0.943479 -0.349366  0.530412 -0.508724\\n2001-05-09  0.230643 -0.065569 -0.248717 -0.587136\\n2001-05-16 -1.022324  1.060661  0.954768 -0.511824\\n2001-05-23 -1.387680  0.767902 -1.164490  1.527070\\n2001-05-30  0.287542  0.715359 -0.345805  0.470886\\nTime Series with Duplicate Indices\\nIn some applications, there may be multiple data observations falling on a particular\\ntimestamp. Here is an example:\\nIn [371]: dates = pd.DatetimeIndex(['1/1/2000', '1/2/2000', '1/2/2000', '1/2/2000',\\n   .....:                           '1/3/2000'])\\nIn [372]: dup_ts = Series(np.arange(5), index=dates)\\nIn [373]: dup_ts\\nOut[373]: \\n2000-01-01    0\\n2000-01-02    1\"),\n",
       " Document(metadata={}, page_content=\".....:                           '1/3/2000'])\\nIn [372]: dup_ts = Series(np.arange(5), index=dates)\\nIn [373]: dup_ts\\nOut[373]: \\n2000-01-01    0\\n2000-01-02    1\\n2000-01-02    2\\n2000-01-02    3\\n2000-01-03    4\\nWe can tell that the index is not unique by checking its is_unique property:\\n296 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"In [374]: dup_ts.index.is_unique\\nOut[374]: False\\nIndexing into this time series will now either produce scalar values or slices depending\\non whether a timestamp is duplicated:\\nIn [375]: dup_ts['1/3/2000']  # not duplicated\\nOut[375]: 4\\nIn [376]: dup_ts['1/2/2000']  # duplicated\\nOut[376]: \\n2000-01-02    1\\n2000-01-02    2\\n2000-01-02    3\\nSuppose you wanted to aggregate the data having non-unique timestamps. One way\\nto do this is to use groupby and pass level=0 (the only level of indexing!):\\nIn [377]: grouped = dup_ts.groupby(level=0)\\nIn [378]: grouped.mean()      In [379]: grouped.count()\\nOut[378]:                     Out[379]:                \\n2000-01-01    0               2000-01-01    1          \\n2000-01-02    2               2000-01-02    3          \\n2000-01-03    4               2000-01-03    1\\nDate Ranges, Frequencies, and Shifting\\nGeneric time series in pandas are assumed to be irregular; that is, they have no fixed\"),\n",
       " Document(metadata={}, page_content=\"2000-01-03    4               2000-01-03    1\\nDate Ranges, Frequencies, and Shifting\\nGeneric time series in pandas are assumed to be irregular; that is, they have no fixed\\nfrequency. For many applications this is sufficient. However, it’s often desirable to work\\nrelative to a fixed frequency, such as daily, monthly, or every 15 minutes, even if that\\nmeans introducing missing values into a time series. Fortunately pandas has a full suite\\nof standard time series frequencies and tools for resampling, inferring frequencies, and\\ngenerating fixed frequency date ranges. For example, in the example time series, con-\\nverting it to be fixed daily frequency can be accomplished by calling resample:\\nIn [380]: ts                  In [381]: ts.resample('D')\\nOut[380]:                     Out[381]:                 \\n2011-01-02    0.690002        2011-01-02    0.690002    \\n2011-01-05    1.001543        2011-01-03         NaN    \\n2011-01-07   -0.503087        2011-01-04         NaN\"),\n",
       " Document(metadata={}, page_content='2011-01-02    0.690002        2011-01-02    0.690002    \\n2011-01-05    1.001543        2011-01-03         NaN    \\n2011-01-07   -0.503087        2011-01-04         NaN    \\n2011-01-08   -0.622274        2011-01-05    1.001543    \\n2011-01-10   -0.921169        2011-01-06         NaN    \\n2011-01-12   -0.726213        2011-01-07   -0.503087    \\n                              2011-01-08   -0.622274    \\n                              2011-01-09         NaN    \\n                              2011-01-10   -0.921169    \\n                              2011-01-11         NaN    \\n                              2011-01-12   -0.726213    \\n                              Freq: D\\nConversion between frequencies or resampling is a big enough topic to have its own\\nsection later. Here I’ll show you how to use the base frequencies and multiples thereof.\\nDate Ranges, Frequencies, and Shifting | 297\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"Generating Date Ranges\\nWhile I used it previously without explanation, you may have guessed that pan\\ndas.date_range is responsible for generating a DatetimeIndex with an indicated length\\naccording to a particular frequency:\\nIn [382]: index = pd.date_range('4/1/2012', '6/1/2012')\\nIn [383]: index\\nOut[383]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n[2012-04-01 00:00:00, ..., 2012-06-01 00:00:00]\\nLength: 62, Freq: D, Timezone: None\\nBy default, date_range generates daily timestamps. If you pass only a start or end date,\\nyou must pass a number of periods to generate:\\nIn [384]: pd.date_range(start='4/1/2012', periods=20)\\nOut[384]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n[2012-04-01 00:00:00, ..., 2012-04-20 00:00:00]\\nLength: 20, Freq: D, Timezone: None\\nIn [385]: pd.date_range(end='6/1/2012', periods=20)\\nOut[385]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n[2012-05-13 00:00:00, ..., 2012-06-01 00:00:00]\\nLength: 20, Freq: D, Timezone: None\"),\n",
       " Document(metadata={}, page_content=\"In [385]: pd.date_range(end='6/1/2012', periods=20)\\nOut[385]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n[2012-05-13 00:00:00, ..., 2012-06-01 00:00:00]\\nLength: 20, Freq: D, Timezone: None\\nThe start and end dates define strict boundaries for the generated date index. For ex-\\nample, if you wanted a date index containing the last business day of each month, you\\nwould pass the 'BM' frequency (business end of month) and only dates falling on or\\ninside the date interval will be included:\\nIn [386]: pd.date_range('1/1/2000', '12/1/2000', freq='BM')\\nOut[386]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n[2000-01-31 00:00:00, ..., 2000-11-30 00:00:00]\\nLength: 11, Freq: BM, Timezone: None\\ndate_range by default preserves the time (if any) of the start or end timestamp:\\nIn [387]: pd.date_range('5/2/2012 12:56:31', periods=5)\\nOut[387]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n[2012-05-02 12:56:31, ..., 2012-05-06 12:56:31]\\nLength: 5, Freq: D, Timezone: None\"),\n",
       " Document(metadata={}, page_content=\"In [387]: pd.date_range('5/2/2012 12:56:31', periods=5)\\nOut[387]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n[2012-05-02 12:56:31, ..., 2012-05-06 12:56:31]\\nLength: 5, Freq: D, Timezone: None\\nSometimes you will have start or end dates with time information but want to generate\\na set of timestamps normalized to midnight as a convention. To do this, there is a\\nnormalize option:\\nIn [388]: pd.date_range('5/2/2012 12:56:31', periods=5, normalize=True)\\nOut[388]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n298 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"[2012-05-02 00:00:00, ..., 2012-05-06 00:00:00]\\nLength: 5, Freq: D, Timezone: None\\nFrequencies and Date Offsets\\nFrequencies in pandas are composed of a base frequency and a multiplier. Base fre-\\nquencies are typically referred to by a string alias, like 'M' for monthly or 'H' for hourly.\\nFor each base frequency, there is an object defined generally referred to as a date off-\\nset. For example, hourly frequency can be represented with the Hour class:\\nIn [389]: from pandas.tseries.offsets import Hour, Minute\\nIn [390]: hour = Hour()\\nIn [391]: hour\\nOut[391]: <1 Hour>\\nYou can define a multiple of an offset by passing an integer:\\nIn [392]: four_hours = Hour(4)\\nIn [393]: four_hours\\nOut[393]: <4 Hours>\\nIn most applications, you would never need to explicitly create one of these objects,\\ninstead using a string alias like 'H' or '4H'. Putting an integer before the base frequency\\ncreates a multiple:\\nIn [394]: pd.date_range('1/1/2000', '1/3/2000 23:59', freq='4h')\\nOut[394]:\"),\n",
       " Document(metadata={}, page_content=\"instead using a string alias like 'H' or '4H'. Putting an integer before the base frequency\\ncreates a multiple:\\nIn [394]: pd.date_range('1/1/2000', '1/3/2000 23:59', freq='4h')\\nOut[394]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n[2000-01-01 00:00:00, ..., 2000-01-03 20:00:00]\\nLength: 18, Freq: 4H, Timezone: None\\nMany offsets can be combined together by addition:\\nIn [395]: Hour(2) + Minute(30)\\nOut[395]: <150 Minutes>\\nSimilarly, you can pass frequency strings like '2h30min' which will effectively be parsed\\nto the same expression:\\nIn [396]: pd.date_range('1/1/2000', periods=10, freq='1h30min')\\nOut[396]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n[2000-01-01 00:00:00, ..., 2000-01-01 13:30:00]\\nLength: 10, Freq: 90T, Timezone: None\\nSome frequencies describe points in time that are not evenly spaced. For example,\\n'M' (calendar month end) and 'BM' (last business/weekday of month) depend on the\\nnumber of days in a month and, in the latter case, whether the month ends on a weekend\"),\n",
       " Document(metadata={}, page_content=\"'M' (calendar month end) and 'BM' (last business/weekday of month) depend on the\\nnumber of days in a month and, in the latter case, whether the month ends on a weekend\\nor not. For lack of a better term, I call these anchored offsets.\\nSee Table 10-4 for a listing of frequency codes and date offset classes available in pandas.\\nDate Ranges, Frequencies, and Shifting | 299\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content='Users can define their own custom frequency classes to provide date\\nlogic not available in pandas, though the full details of that are outside\\nthe scope of this book.\\nTable 10-4. Base Time Series Frequencies\\nAlias Offset Type Description\\nD Day Calendar daily\\nB BusinessDay Business daily\\nH Hour Hourly\\nT or min Minute Minutely\\nS Second Secondly\\nL or ms Milli Millisecond (1/1000th of 1 second)\\nU Micro Microsecond (1/1000000th of 1 second)\\nM MonthEnd Last calendar day of month\\nBM BusinessMonthEnd Last business day (weekday) of month\\nMS MonthBegin First calendar day of month\\nBMS BusinessMonthBegin First weekday of month\\nW-MON, W-TUE, ... Week Weekly on given day of week: MON, TUE, WED, THU, FRI, SAT,\\nor SUN.\\nWOM-1MON, WOM-2MON, ... WeekOfMonth Generate weekly dates in the first, second, third, or fourth week\\nof the month. For example, WOM-3FRI for the 3rd Friday of\\neach month.\\nQ-JAN, Q-FEB, ... QuarterEnd Quarterly dates anchored on last calendar day of each month,'),\n",
       " Document(metadata={}, page_content='of the month. For example, WOM-3FRI for the 3rd Friday of\\neach month.\\nQ-JAN, Q-FEB, ... QuarterEnd Quarterly dates anchored on last calendar day of each month,\\nfor year ending in indicated month: JAN, FEB, MAR, APR, MAY,\\nJUN, JUL, AUG, SEP, OCT, NOV, or DEC.\\nBQ-JAN, BQ-FEB, ... BusinessQuarterEnd Quarterly dates anchored on last weekday day of each month,\\nfor year ending in indicated month\\nQS-JAN, QS-FEB, ... QuarterBegin Quarterly dates anchored on first calendar day of each month,\\nfor year ending in indicated month\\nBQS-JAN, BQS-FEB, ... BusinessQuarterBegin Quarterly dates anchored on first weekday day of each month,\\nfor year ending in indicated month\\nA-JAN, A-FEB, ... YearEnd Annual dates anchored on last calendar day of given month:\\nJAN, FEB, MAR, APR, MAY, JUN, JUL, AUG, SEP, OCT, NOV, or DEC.\\nBA-JAN, BA-FEB, ... BusinessYearEnd Annual dates anchored on last weekday of given month\\nAS-JAN, AS-FEB, ... YearBegin Annual dates anchored on first day of given month'),\n",
       " Document(metadata={}, page_content='BA-JAN, BA-FEB, ... BusinessYearEnd Annual dates anchored on last weekday of given month\\nAS-JAN, AS-FEB, ... YearBegin Annual dates anchored on first day of given month\\nBAS-JAN, BAS-FEB, ... BusinessYearBegin Annual dates anchored on first weekday of given month\\n300 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"Week of month dates\\nOne useful frequency class is “week of month”, starting with WOM. This enables you to\\nget dates like the third Friday of each month:\\nIn [397]: rng = pd.date_range('1/1/2012', '9/1/2012', freq='WOM-3FRI')\\nIn [398]: list(rng)\\nOut[398]: \\n[<Timestamp: 2012-01-20 00:00:00>,\\n <Timestamp: 2012-02-17 00:00:00>,\\n <Timestamp: 2012-03-16 00:00:00>,\\n <Timestamp: 2012-04-20 00:00:00>,\\n <Timestamp: 2012-05-18 00:00:00>,\\n <Timestamp: 2012-06-15 00:00:00>,\\n <Timestamp: 2012-07-20 00:00:00>,\\n <Timestamp: 2012-08-17 00:00:00>]\\nTraders of US equity options will recognize these dates as the standard dates of monthly\\nexpiry.\\nShifting (Leading and Lagging) Data\\n“Shifting” refers to moving data backward and forward through time. Both Series and\\nDataFrame have a shift method for doing naive shifts forward or backward, leaving\\nthe index unmodified:\\nIn [399]: ts = Series(np.random.randn(4),\\n   .....:             index=pd.date_range('1/1/2000', periods=4, freq='M'))\"),\n",
       " Document(metadata={}, page_content=\"the index unmodified:\\nIn [399]: ts = Series(np.random.randn(4),\\n   .....:             index=pd.date_range('1/1/2000', periods=4, freq='M'))\\nIn [400]: ts                In [401]: ts.shift(2)       In [402]: ts.shift(-2)\\nOut[400]:                   Out[401]:                   Out[402]:             \\n2000-01-31    0.575283      2000-01-31         NaN      2000-01-31    1.814582\\n2000-02-29    0.304205      2000-02-29         NaN      2000-02-29    1.634858\\n2000-03-31    1.814582      2000-03-31    0.575283      2000-03-31         NaN\\n2000-04-30    1.634858      2000-04-30    0.304205      2000-04-30         NaN\\nFreq: M                     Freq: M                     Freq: M\\nA common use of shift is computing percent changes in a time series or multiple time\\nseries as DataFrame columns. This is expressed as\\nts / ts.shift(1) - 1\\nBecause naive shifts leave the index unmodified, some data is discarded. Thus if the\"),\n",
       " Document(metadata={}, page_content=\"series as DataFrame columns. This is expressed as\\nts / ts.shift(1) - 1\\nBecause naive shifts leave the index unmodified, some data is discarded. Thus if the\\nfrequency is known, it can be passed to shift to advance the timestamps instead of\\nsimply the data:\\nIn [403]: ts.shift(2, freq='M')\\nOut[403]: \\n2000-03-31    0.575283\\n2000-04-30    0.304205\\n2000-05-31    1.814582\\n2000-06-30    1.634858\\nFreq: M\\nDate Ranges, Frequencies, and Shifting | 301\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"Other frequencies can be passed, too, giving you a lot of flexibility in how to lead and\\nlag the data:\\nIn [404]: ts.shift(3, freq='D')        In [405]: ts.shift(1, freq='3D')\\nOut[404]:                              Out[405]:                       \\n2000-02-03    0.575283                 2000-02-03    0.575283          \\n2000-03-03    0.304205                 2000-03-03    0.304205          \\n2000-04-03    1.814582                 2000-04-03    1.814582          \\n2000-05-03    1.634858                 2000-05-03    1.634858          \\n                                                                       \\nIn [406]: ts.shift(1, freq='90T')\\nOut[406]: \\n2000-01-31 01:30:00    0.575283\\n2000-02-29 01:30:00    0.304205\\n2000-03-31 01:30:00    1.814582\\n2000-04-30 01:30:00    1.634858\\nShifting dates with offsets\\nThe pandas date offsets can also be used with datetime or Timestamp objects:\\nIn [407]: from pandas.tseries.offsets import Day, MonthEnd\\nIn [408]: now = datetime(2011, 11, 17)\"),\n",
       " Document(metadata={}, page_content='The pandas date offsets can also be used with datetime or Timestamp objects:\\nIn [407]: from pandas.tseries.offsets import Day, MonthEnd\\nIn [408]: now = datetime(2011, 11, 17)\\nIn [409]: now + 3 * Day()\\nOut[409]: datetime.datetime(2011, 11, 20, 0, 0)\\nIf you add an anchored offset like MonthEnd, the first increment will roll forward a date\\nto the next date according to the frequency rule:\\nIn [410]: now + MonthEnd()\\nOut[410]: datetime.datetime(2011, 11, 30, 0, 0)\\nIn [411]: now + MonthEnd(2)\\nOut[411]: datetime.datetime(2011, 12, 31, 0, 0)\\nAnchored offsets can explicitly “roll” dates forward or backward using their rollfor\\nward and rollback methods, respectively:\\nIn [412]: offset = MonthEnd()\\nIn [413]: offset.rollforward(now)\\nOut[413]: datetime.datetime(2011, 11, 30, 0, 0)\\nIn [414]: offset.rollback(now)\\nOut[414]: datetime.datetime(2011, 10, 31, 0, 0)\\nA clever use of date offsets is to use these methods with groupby:\\nIn [415]: ts = Series(np.random.randn(20),'),\n",
       " Document(metadata={}, page_content=\"In [414]: offset.rollback(now)\\nOut[414]: datetime.datetime(2011, 10, 31, 0, 0)\\nA clever use of date offsets is to use these methods with groupby:\\nIn [415]: ts = Series(np.random.randn(20),\\n   .....:             index=pd.date_range('1/15/2000', periods=20, freq='4d'))\\nIn [416]: ts.groupby(offset.rollforward).mean()\\nOut[416]: \\n2000-01-31   -0.448874\\n302 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"2000-02-29   -0.683663\\n2000-03-31    0.251920\\nOf course, an easier and faster way to do this is using resample (much more on this later):\\nIn [417]: ts.resample('M', how='mean')\\nOut[417]: \\n2000-01-31   -0.448874\\n2000-02-29   -0.683663\\n2000-03-31    0.251920\\nFreq: M\\nTime Zone Handling\\nWorking with time zones is generally considered one of the most unpleasant parts of\\ntime series manipulation. In particular, daylight savings time (DST) transitions are a\\ncommon source of complication. As such, many time series users choose to work with\\ntime series in coordinated universal time or UTC, which is the successor to Greenwich\\nMean Time and is the current international standard. Time zones are expressed as\\noffsets from UTC; for example, New York is four hours behind UTC during daylight\\nsavings time and 5 hours the rest of the year.\\nIn Python, time zone information comes from the 3rd party pytz library, which exposes\"),\n",
       " Document(metadata={}, page_content=\"savings time and 5 hours the rest of the year.\\nIn Python, time zone information comes from the 3rd party pytz library, which exposes\\nthe Olson database, a compilation of world time zone information. This is especially\\nimportant for historical data because the DST transition dates (and even UTC offsets)\\nhave been changed numerous times depending on the whims of local governments. In\\nthe United States,the DST transition times have been changed many times since 1900!\\nFor detailed information about pytz library, you’ll need to look at that library’s docu-\\nmentation. As far as this book is concerned, pandas wraps pytz’s functionality so you\\ncan ignore its API outside of the time zone names. Time zone names can be found\\ninteractively and in the docs:\\nIn [418]: import pytz\\nIn [419]: pytz.common_timezones[-5:]\\nOut[419]: ['US/Eastern', 'US/Hawaii', 'US/Mountain', 'US/Pacific', 'UTC']\\nTo get a time zone object from pytz, use pytz.timezone:\\nIn [420]: tz = pytz.timezone('US/Eastern')\\nIn [421]: tz\"),\n",
       " Document(metadata={}, page_content=\"Out[419]: ['US/Eastern', 'US/Hawaii', 'US/Mountain', 'US/Pacific', 'UTC']\\nTo get a time zone object from pytz, use pytz.timezone:\\nIn [420]: tz = pytz.timezone('US/Eastern')\\nIn [421]: tz\\nOut[421]: <DstTzInfo 'US/Eastern' EST-1 day, 19:00:00 STD>\\nMethods in pandas will accept either time zone names or these objects. I recommend\\njust using the names.\\nTime Zone Handling | 303\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"Localization and Conversion\\nBy default, time series in pandas are time zone naive. Consider the following time series:\\nrng = pd.date_range('3/9/2012 9:30', periods=6, freq='D')\\nts = Series(np.random.randn(len(rng)), index=rng)\\nThe index’s tz field is None:\\nIn [423]: print(ts.index.tz)\\nNone\\nDate ranges can be generated with a time zone set:\\nIn [424]: pd.date_range('3/9/2012 9:30', periods=10, freq='D', tz='UTC')\\nOut[424]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n[2012-03-09 09:30:00, ..., 2012-03-18 09:30:00]\\nLength: 10, Freq: D, Timezone: UTC\\nConversion from naive to localized is handled by the tz_localize method:\\nIn [425]: ts_utc = ts.tz_localize('UTC')\\nIn [426]: ts_utc\\nOut[426]: \\n2012-03-09 09:30:00+00:00    0.414615\\n2012-03-10 09:30:00+00:00    0.427185\\n2012-03-11 09:30:00+00:00    1.172557\\n2012-03-12 09:30:00+00:00   -0.351572\\n2012-03-13 09:30:00+00:00    1.454593\\n2012-03-14 09:30:00+00:00    2.043319\\nFreq: D\\nIn [427]: ts_utc.index\\nOut[427]:\"),\n",
       " Document(metadata={}, page_content=\"2012-03-11 09:30:00+00:00    1.172557\\n2012-03-12 09:30:00+00:00   -0.351572\\n2012-03-13 09:30:00+00:00    1.454593\\n2012-03-14 09:30:00+00:00    2.043319\\nFreq: D\\nIn [427]: ts_utc.index\\nOut[427]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n[2012-03-09 09:30:00, ..., 2012-03-14 09:30:00]\\nLength: 6, Freq: D, Timezone: UTC\\nOnce a time series has been localized to a particular time zone, it can be converted to\\nanother time zone using tz_convert:\\nIn [428]: ts_utc.tz_convert('US/Eastern')\\nOut[428]: \\n2012-03-09 04:30:00-05:00    0.414615\\n2012-03-10 04:30:00-05:00    0.427185\\n2012-03-11 05:30:00-04:00    1.172557\\n2012-03-12 05:30:00-04:00   -0.351572\\n2012-03-13 05:30:00-04:00    1.454593\\n2012-03-14 05:30:00-04:00    2.043319\\nFreq: D\\nIn the case of the above time series, which straddles a DST transition in the US/Eastern\\ntime zone, we could localize to EST and convert to, say, UTC or Berlin time:\\nIn [429]: ts_eastern = ts.tz_localize('US/Eastern')\\n304 | Chapter 10: \\u2002Time Series\"),\n",
       " Document(metadata={}, page_content=\"time zone, we could localize to EST and convert to, say, UTC or Berlin time:\\nIn [429]: ts_eastern = ts.tz_localize('US/Eastern')\\n304 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"In [430]: ts_eastern.tz_convert('UTC')\\nOut[430]: \\n2012-03-09 14:30:00+00:00    0.414615\\n2012-03-10 14:30:00+00:00    0.427185\\n2012-03-11 13:30:00+00:00    1.172557\\n2012-03-12 13:30:00+00:00   -0.351572\\n2012-03-13 13:30:00+00:00    1.454593\\n2012-03-14 13:30:00+00:00    2.043319\\nFreq: D\\nIn [431]: ts_eastern.tz_convert('Europe/Berlin')\\nOut[431]: \\n2012-03-09 15:30:00+01:00    0.414615\\n2012-03-10 15:30:00+01:00    0.427185\\n2012-03-11 14:30:00+01:00    1.172557\\n2012-03-12 14:30:00+01:00   -0.351572\\n2012-03-13 14:30:00+01:00    1.454593\\n2012-03-14 14:30:00+01:00    2.043319\\nFreq: D\\ntz_localize and tz_convert are also instance methods on DatetimeIndex:\\nIn [432]: ts.index.tz_localize('Asia/Shanghai')\\nOut[432]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n[2012-03-09 09:30:00, ..., 2012-03-14 09:30:00]\\nLength: 6, Freq: D, Timezone: Asia/Shanghai\\nLocalizing naive timestamps also checks for ambiguous or non-existent\\ntimes around daylight savings time transitions.\"),\n",
       " Document(metadata={}, page_content=\"Length: 6, Freq: D, Timezone: Asia/Shanghai\\nLocalizing naive timestamps also checks for ambiguous or non-existent\\ntimes around daylight savings time transitions.\\nOperations with Time Zone−aware Timestamp Objects\\nSimilar to time series and date ranges, individual Timestamp objects similarly can be\\nlocalized from naive to time zone-aware and converted from one time zone to another:\\nIn [433]: stamp = pd.Timestamp('2011-03-12 04:00')\\nIn [434]: stamp_utc = stamp.tz_localize('utc')\\nIn [435]: stamp_utc.tz_convert('US/Eastern')\\nOut[435]: <Timestamp: 2011-03-11 23:00:00-0500 EST, tz=US/Eastern>\\nYou can also pass a time zone when creating the Timestamp:\\nIn [436]: stamp_moscow = pd.Timestamp('2011-03-12 04:00', tz='Europe/Moscow')\\nIn [437]: stamp_moscow\\nOut[437]: <Timestamp: 2011-03-12 04:00:00+0300 MSK, tz=Europe/Moscow>\\nTime zone-aware Timestamp objects internally store a UTC timestamp value as nano-\\nseconds since the UNIX epoch (January 1, 1970); this UTC value is invariant between\"),\n",
       " Document(metadata={}, page_content='Time zone-aware Timestamp objects internally store a UTC timestamp value as nano-\\nseconds since the UNIX epoch (January 1, 1970); this UTC value is invariant between\\ntime zone conversions:\\nTime Zone Handling | 305\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"In [438]: stamp_utc.value\\nOut[438]: 1299902400000000000\\nIn [439]: stamp_utc.tz_convert('US/Eastern').value\\nOut[439]: 1299902400000000000\\nWhen performing time arithmetic using pandas’s DateOffset objects, daylight savings\\ntime transitions are respected where possible:\\n# 30 minutes before DST transition\\nIn [440]: from pandas.tseries.offsets import Hour\\nIn [441]: stamp = pd.Timestamp('2012-03-12 01:30', tz='US/Eastern')\\nIn [442]: stamp\\nOut[442]: <Timestamp: 2012-03-12 01:30:00-0400 EDT, tz=US/Eastern>\\nIn [443]: stamp + Hour()\\nOut[443]: <Timestamp: 2012-03-12 02:30:00-0400 EDT, tz=US/Eastern>\\n# 90 minutes before DST transition\\nIn [444]: stamp = pd.Timestamp('2012-11-04 00:30', tz='US/Eastern')\\nIn [445]: stamp\\nOut[445]: <Timestamp: 2012-11-04 00:30:00-0400 EDT, tz=US/Eastern>\\nIn [446]: stamp + 2 * Hour()\\nOut[446]: <Timestamp: 2012-11-04 01:30:00-0500 EST, tz=US/Eastern>\\nOperations between Different Time Zones\"),\n",
       " Document(metadata={}, page_content=\"In [446]: stamp + 2 * Hour()\\nOut[446]: <Timestamp: 2012-11-04 01:30:00-0500 EST, tz=US/Eastern>\\nOperations between Different Time Zones\\nIf two time series with different time zones are combined, the result will be UTC. Since\\nthe timestamps are stored under the hood in UTC, this is a straightforward operation\\nand requires no conversion to happen:\\nIn [447]: rng = pd.date_range('3/7/2012 9:30', periods=10, freq='B')\\nIn [448]: ts = Series(np.random.randn(len(rng)), index=rng)\\nIn [449]: ts\\nOut[449]: \\n2012-03-07 09:30:00   -1.749309\\n2012-03-08 09:30:00   -0.387235\\n2012-03-09 09:30:00   -0.208074\\n2012-03-12 09:30:00   -1.221957\\n2012-03-13 09:30:00   -0.067460\\n2012-03-14 09:30:00    0.229005\\n2012-03-15 09:30:00   -0.576234\\n2012-03-16 09:30:00    0.816895\\n2012-03-19 09:30:00   -0.772192\\n2012-03-20 09:30:00   -1.333576\\nFreq: B\\nIn [450]: ts1 = ts[:7].tz_localize('Europe/London')\\n306 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"In [451]: ts2 = ts1[2:].tz_convert('Europe/Moscow')\\nIn [452]: result = ts1 + ts2\\nIn [453]: result.index\\nOut[453]: \\n<class 'pandas.tseries.index.DatetimeIndex'>\\n[2012-03-07 09:30:00, ..., 2012-03-15 09:30:00]\\nLength: 7, Freq: B, Timezone: UTC\\nPeriods and Period Arithmetic\\nPeriods represent time spans, like days, months, quarters, or years. The Period class\\nrepresents this data type, requiring a string or integer and a frequency from the above\\ntable:\\nIn [454]: p = pd.Period(2007, freq='A-DEC')\\nIn [455]: p\\nOut[455]: Period('2007', 'A-DEC')\\nIn this case, the Period object represents the full timespan from January 1, 2007 to\\nDecember 31, 2007, inclusive. Conveniently, adding and subtracting integers from pe-\\nriods has the effect of shifting by their frequency:\\nIn [456]: p + 5                          In [457]: p - 2                  \\nOut[456]: Period('2012', 'A-DEC')        Out[457]: Period('2005', 'A-DEC')\"),\n",
       " Document(metadata={}, page_content=\"In [456]: p + 5                          In [457]: p - 2                  \\nOut[456]: Period('2012', 'A-DEC')        Out[457]: Period('2005', 'A-DEC')\\nIf two periods have the same frequency, their difference is the number of units between\\nthem:\\nIn [458]: pd.Period('2014', freq='A-DEC') - p\\nOut[458]: 7\\nRegular ranges of periods can be constructed using the period_range function:\\nIn [459]: rng = pd.period_range('1/1/2000', '6/30/2000', freq='M')\\nIn [460]: rng\\nOut[460]: \\n<class 'pandas.tseries.period.PeriodIndex'>\\nfreq: M\\n[2000-01, ..., 2000-06]\\nlength: 6\\nThe PeriodIndex class stores a sequence of periods and can serve as an axis index in\\nany pandas data structure:\\nIn [461]: Series(np.random.randn(6), index=rng)\\nOut[461]: \\n2000-01   -0.309119\\n2000-02    0.028558\\n2000-03    1.129605\\n2000-04   -0.374173\\n2000-05   -0.011401\\nPeriods and Period Arithmetic | 307\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"2000-06    0.272924\\nFreq: M\\nIf you have an array of strings, you can also appeal to the PeriodIndex class itself:\\nIn [462]: values = ['2001Q3', '2002Q2', '2003Q1']\\nIn [463]: index = pd.PeriodIndex(values, freq='Q-DEC')\\nIn [464]: index\\nOut[464]: \\n<class 'pandas.tseries.period.PeriodIndex'>\\nfreq: Q-DEC\\n[2001Q3, ..., 2003Q1]\\nlength: 3\\nPeriod Frequency Conversion\\nPeriods and PeriodIndex objects can be converted to another frequency using their \\nasfreq method. As an example, suppose we had an annual period and wanted to convert\\nit into a monthly period either at the start or end of the year. This is fairly straightfor-\\nward:\\nIn [465]: p = pd.Period('2007', freq='A-DEC')\\nIn [466]: p.asfreq('M', how='start')      In [467]: p.asfreq('M', how='end')\\nOut[466]: Period('2007-01', 'M')         Out[467]: Period('2007-12', 'M')\\nYou can think of Period('2007', 'A-DEC') as being a cursor pointing to a span of time,\\nsubdivided by monthly periods. See Figure 10-1 for an illustration of this. For a fiscal\"),\n",
       " Document(metadata={}, page_content=\"You can think of Period('2007', 'A-DEC') as being a cursor pointing to a span of time,\\nsubdivided by monthly periods. See Figure 10-1 for an illustration of this. For a fiscal\\nyear ending on a month other than December, the monthly subperiods belonging are\\ndifferent:\\nIn [468]: p = pd.Period('2007', freq='A-JUN')\\nIn [469]: p.asfreq('M', 'start')       In [470]: p.asfreq('M', 'end')   \\nOut[469]: Period('2006-07', 'M')      Out[470]: Period('2007-07', 'M')\\nWhen converting from high to low frequency, the superperiod will be determined de-\\npending on where the subperiod “belongs”. For example, in A-JUN frequency, the month\\nAug-2007 is actually part of the 2008 period:\\nIn [471]: p = pd.Period('2007-08', 'M')\\nIn [472]: p.asfreq('A-JUN')\\nOut[472]: Period('2008', 'A-JUN')\\nWhole PeriodIndex objects or TimeSeries can be similarly converted with the same\\nsemantics:\\nIn [473]: rng = pd.period_range('2006', '2009', freq='A-DEC')\\nIn [474]: ts = Series(np.random.randn(len(rng)), index=rng)\"),\n",
       " Document(metadata={}, page_content=\"semantics:\\nIn [473]: rng = pd.period_range('2006', '2009', freq='A-DEC')\\nIn [474]: ts = Series(np.random.randn(len(rng)), index=rng)\\nIn [475]: ts\\n308 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"Out[475]: \\n2006   -0.601544\\n2007    0.574265\\n2008   -0.194115\\n2009    0.202225\\nFreq: A-DEC\\nIn [476]: ts.asfreq('M', how='start')      In [477]: ts.asfreq('B', how='end')\\nOut[476]:                                  Out[477]:                          \\n2006-01   -0.601544                        2006-12-29   -0.601544            \\n2007-01    0.574265                        2007-12-31    0.574265            \\n2008-01   -0.194115                        2008-12-31   -0.194115            \\n2009-01    0.202225                        2009-12-31    0.202225            \\nFreq: M                                    Freq: B\\nFigure 10-1. Period frequency conversion illustration\\nQuarterly Period Frequencies\\nQuarterly data is standard in accounting, finance, and other fields. Much quarterly data\\nis reported relative to a fiscal year end, typically the last calendar or business day of one\\nof the 12 months of the year. As such, the period 2012Q4 has a different meaning de-\"),\n",
       " Document(metadata={}, page_content=\"is reported relative to a fiscal year end, typically the last calendar or business day of one\\nof the 12 months of the year. As such, the period 2012Q4 has a different meaning de-\\npending on fiscal year end. pandas supports all 12 possible quarterly frequencies as Q-\\nJAN through Q-DEC:\\nIn [478]: p = pd.Period('2012Q4', freq='Q-JAN')\\nIn [479]: p\\nOut[479]: Period('2012Q4', 'Q-JAN')\\nIn the case of fiscal year ending in January, 2012Q4 runs from November through Jan-\\nuary, which you can check by converting to daily frequency. See Figure 10-2 for an\\nillustration:\\nIn [480]: p.asfreq('D', 'start')          In [481]: p.asfreq('D', 'end')      \\nOut[480]: Period('2011-11-01', 'D')      Out[481]: Period('2012-01-31', 'D')\\nPeriods and Period Arithmetic | 309\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"Thus, it’s possible to do period arithmetic very easily; for example, to get the timestamp\\nat 4PM on the 2nd to last business day of the quarter, you could do:\\nIn [482]: p4pm = (p.asfreq('B', 'e') - 1).asfreq('T', 's') + 16 * 60\\nIn [483]: p4pm\\nOut[483]: Period('2012-01-30 16:00', 'T')\\nIn [484]: p4pm.to_timestamp()\\nOut[484]: <Timestamp: 2012-01-30 16:00:00>\\nFigure 10-2. Different quarterly frequency conventions\\nGenerating quarterly ranges works as you would expect using period_range. Arithmetic\\nis identical, too:\\nIn [485]: rng = pd.period_range('2011Q3', '2012Q4', freq='Q-JAN')\\nIn [486]: ts = Series(np.arange(len(rng)), index=rng)\\nIn [487]: ts\\nOut[487]: \\n2011Q3    0\\n2011Q4    1\\n2012Q1    2\\n2012Q2    3\\n2012Q3    4\\n2012Q4    5\\nFreq: Q-JAN\\nIn [488]: new_rng = (rng.asfreq('B', 'e') - 1).asfreq('T', 's') + 16 * 60\\nIn [489]: ts.index = new_rng.to_timestamp()\\nIn [490]: ts\\nOut[490]: \\n2010-10-28 16:00:00    0\\n2011-01-28 16:00:00    1\\n2011-04-28 16:00:00    2\\n2011-07-28 16:00:00    3\"),\n",
       " Document(metadata={}, page_content='In [489]: ts.index = new_rng.to_timestamp()\\nIn [490]: ts\\nOut[490]: \\n2010-10-28 16:00:00    0\\n2011-01-28 16:00:00    1\\n2011-04-28 16:00:00    2\\n2011-07-28 16:00:00    3\\n2011-10-28 16:00:00    4\\n2012-01-30 16:00:00    5\\n310 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"Converting Timestamps to Periods (and Back)\\nSeries and DataFrame objects indexed by timestamps can be converted to periods using\\nthe to_period method:\\nIn [491]: rng = pd.date_range('1/1/2000', periods=3, freq='M')\\nIn [492]: ts = Series(randn(3), index=rng)\\nIn [493]: pts = ts.to_period()\\nIn [494]: ts                  In [495]: pts       \\nOut[494]:                     Out[495]:           \\n2000-01-31   -0.505124        2000-01   -0.505124\\n2000-02-29    2.954439        2000-02    2.954439\\n2000-03-31   -2.630247        2000-03   -2.630247\\nFreq: M                       Freq: M\\nSince periods always refer to non-overlapping timespans, a timestamp can only belong\\nto a single period for a given frequency. While the frequency of the new PeriodIndex is\\ninferred from the timestamps by default, you can specify any frequency you want. There\\nis also no problem with having duplicate periods in the result:\\nIn [496]: rng = pd.date_range('1/29/2000', periods=6, freq='D')\"),\n",
       " Document(metadata={}, page_content=\"is also no problem with having duplicate periods in the result:\\nIn [496]: rng = pd.date_range('1/29/2000', periods=6, freq='D')\\nIn [497]: ts2 = Series(randn(6), index=rng)\\nIn [498]: ts2.to_period('M')\\nOut[498]: \\n2000-01   -0.352453\\n2000-01   -0.477808\\n2000-01    0.161594\\n2000-02    1.686833\\n2000-02    0.821965\\n2000-02   -0.667406\\nFreq: M\\nTo convert back to timestamps, use to_timestamp:\\nIn [499]: pts = ts.to_period()\\nIn [500]: pts\\nOut[500]: \\n2000-01   -0.505124\\n2000-02    2.954439\\n2000-03   -2.630247\\nFreq: M\\nIn [501]: pts.to_timestamp(how='end')\\nOut[501]: \\n2000-01-31   -0.505124\\n2000-02-29    2.954439\\n2000-03-31   -2.630247\\nFreq: M\\nPeriods and Period Arithmetic | 311\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"Creating a PeriodIndex from Arrays\\nFixed frequency data sets are sometimes stored with timespan information spread\\nacross multiple columns. For example, in this macroeconomic data set, the year and\\nquarter are in different columns:\\nIn [502]: data = pd.read_csv('ch08/macrodata.csv')\\nIn [503]: data.year            In [504]: data.quarter    \\nOut[503]:                      Out[504]:                 \\n0    1959                      0    1                    \\n1    1959                      1    2                    \\n2    1959                      2    3                    \\n3    1959                      3    4                    \\n...                            ...                       \\n199    2008                    199    4                  \\n200    2009                    200    1                  \\n201    2009                    201    2                  \\n202    2009                    202    3                  \\nName: year, Length: 203        Name: quarter, Length: 203\"),\n",
       " Document(metadata={}, page_content=\"201    2009                    201    2                  \\n202    2009                    202    3                  \\nName: year, Length: 203        Name: quarter, Length: 203\\nBy passing these arrays to PeriodIndex with a frequency, they can be combined to form\\nan index for the DataFrame:\\nIn [505]: index = pd.PeriodIndex(year=data.year, quarter=data.quarter, freq='Q-DEC')\\nIn [506]: index\\nOut[506]: \\n<class 'pandas.tseries.period.PeriodIndex'>\\nfreq: Q-DEC\\n[1959Q1, ..., 2009Q3]\\nlength: 203\\nIn [507]: data.index = index\\nIn [508]: data.infl\\nOut[508]: \\n1959Q1    0.00\\n1959Q2    2.34\\n1959Q3    2.74\\n1959Q4    0.27\\n...\\n2008Q4   -8.79\\n2009Q1    0.94\\n2009Q2    3.37\\n2009Q3    3.56\\nFreq: Q-DEC, Name: infl, Length: 203\\nResampling and Frequency Conversion\\nResampling refers to the process of converting a time series from one frequency to\\nanother. Aggregating higher frequency data to lower frequency is called downsam-\\npling, while converting lower frequency to higher frequency is called upsampling. Not\"),\n",
       " Document(metadata={}, page_content='another. Aggregating higher frequency data to lower frequency is called downsam-\\npling, while converting lower frequency to higher frequency is called upsampling. Not\\n312 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"all resampling falls into either of these categories; for example, converting W-WED (weekly\\non Wednesday) to W-FRI is neither upsampling nor downstampling.\\npandas objects are equipped with a resample method, which is the workhorse function\\nfor all frequency conversion:\\nIn [509]: rng = pd.date_range('1/1/2000', periods=100, freq='D')\\nIn [510]: ts = Series(randn(len(rng)), index=rng)\\nIn [511]: ts.resample('M', how='mean')\\nOut[511]: \\n2000-01-31    0.170876\\n2000-02-29    0.165020\\n2000-03-31    0.095451\\n2000-04-30    0.363566\\nFreq: M\\nIn [512]: ts.resample('M', how='mean', kind='period')\\nOut[512]: \\n2000-01    0.170876\\n2000-02    0.165020\\n2000-03    0.095451\\n2000-04    0.363566\\nFreq: M\\nresample is a flexible and high-performance method that can be used to process very\\nlarge time series. I’ll illustrate its semantics and use through a series of examples.\\nTable 10-5. Resample method arguments\\nArgument Description\"),\n",
       " Document(metadata={}, page_content=\"large time series. I’ll illustrate its semantics and use through a series of examples.\\nTable 10-5. Resample method arguments\\nArgument Description\\nfreq String or DateOffset indicating desired resampled frequency, e.g. ‘M', ’5min', or Sec\\nond(15)\\nhow='mean' Function name or array function producing aggregated value, for example 'mean',\\n'ohlc', np.max. Defaults to 'mean'. Other common values: 'first', 'last',\\n'median', 'ohlc', 'max', 'min'.\\naxis=0 Axis to resample on, default axis=0\\nfill_method=None How to interpolate when upsampling, as in 'ffill' or 'bfill'. By default does no\\ninterpolation.\\nclosed='right' In downsampling, which end of each interval is closed (inclusive), 'right' or\\n'left'. Defaults to 'right'\\nlabel='right' In downsampling, how to label the aggregated result, with the 'right' or 'left'\\nbin edge. For example, the 9:30 to 9:35 5-minute interval could be labeled 9:30 or\\n9:35. Defaults to 'right' (or 9:35, in this example).\"),\n",
       " Document(metadata={}, page_content=\"bin edge. For example, the 9:30 to 9:35 5-minute interval could be labeled 9:30 or\\n9:35. Defaults to 'right' (or 9:35, in this example).\\nloffset=None Time adjustment to the bin labels, such as '-1s' / Second(-1) to shift the aggregate\\nlabels one second earlier\\nlimit=None When forward or backward filling, the maximum number of periods to fill\\nResampling and Frequency Conversion | 313\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"Argument Description\\nkind=None Aggregate to periods ('period') or timestamps ('timestamp'); defaults to kind of\\nindex the time series has\\nconvention=None When resampling periods, the convention ('start' or 'end') for converting the low\\nfrequency period to high frequency. Defaults to 'end'\\nDownsampling\\nAggregating data to a regular, lower frequency is a pretty normal time series task. The\\ndata you’re aggregating doesn’t need to be fixed frequently; the desired frequency de-\\nfines bin edges that are used to slice the time series into pieces to aggregate. For example,\\nto convert to monthly, 'M' or 'BM', the data need to be chopped up into one month\\nintervals. Each interval is said to be half-open; a data point can only belong to one\\ninterval, and the union of the intervals must make up the whole time frame. There are\\na couple things to think about when using resample to downsample data:\\n• Which side of each interval is closed\"),\n",
       " Document(metadata={}, page_content=\"interval, and the union of the intervals must make up the whole time frame. There are\\na couple things to think about when using resample to downsample data:\\n• Which side of each interval is closed\\n• How to label each aggregated bin, either with the start of the interval or the end\\nTo illustrate, let’s look at some one-minute data:\\nIn [513]: rng = pd.date_range('1/1/2000', periods=12, freq='T')\\nIn [514]: ts = Series(np.arange(12), index=rng)\\nIn [515]: ts\\nOut[515]: \\n2000-01-01 00:00:00     0\\n2000-01-01 00:01:00     1\\n2000-01-01 00:02:00     2\\n2000-01-01 00:03:00     3\\n2000-01-01 00:04:00     4\\n2000-01-01 00:05:00     5\\n2000-01-01 00:06:00     6\\n2000-01-01 00:07:00     7\\n2000-01-01 00:08:00     8\\n2000-01-01 00:09:00     9\\n2000-01-01 00:10:00    10\\n2000-01-01 00:11:00    11\\nFreq: T\\nSuppose you wanted to aggregate this data into five-minute chunks or bars by taking\\nthe sum of each group:\\nIn [516]: ts.resample('5min', how='sum')\\nOut[516]: \\n2000-01-01 00:00:00     0\\n2000-01-01 00:05:00    15\"),\n",
       " Document(metadata={}, page_content=\"the sum of each group:\\nIn [516]: ts.resample('5min', how='sum')\\nOut[516]: \\n2000-01-01 00:00:00     0\\n2000-01-01 00:05:00    15\\n2000-01-01 00:10:00    40\\n2000-01-01 00:15:00    11\\nFreq: 5T\\n314 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"The frequency you pass defines bin edges in five-minute increments. By default, the\\nright bin edge is inclusive, so the 00:05 value is included in the 00:00 to 00:05 inter-\\nval.1 Passing closed='left' changes the interval to be closed on the left:\\nIn [517]: ts.resample('5min', how='sum', closed='left')\\nOut[517]: \\n2000-01-01 00:05:00    10\\n2000-01-01 00:10:00    35\\n2000-01-01 00:15:00    21\\nFreq: 5T\\nAs you can see, the resulting time series is labeled by the timestamps from the right side\\nof each bin. By passing label='left' you can label them with the left bin edge:\\nIn [518]: ts.resample('5min', how='sum', closed='left', label='left')\\nOut[518]: \\n2000-01-01 00:00:00    10\\n2000-01-01 00:05:00    35\\n2000-01-01 00:10:00    21\\nFreq: 5T\\nSee Figure 10-3 for an illustration of minutely data being resampled to five-minute.\\nFigure 10-3. 5-minute resampling illustration of closed, label conventions\\nLastly, you might want to shift the result index by some amount, say subtracting one\"),\n",
       " Document(metadata={}, page_content=\"Figure 10-3. 5-minute resampling illustration of closed, label conventions\\nLastly, you might want to shift the result index by some amount, say subtracting one\\nsecond from the right edge to make it more clear which interval the timestamp refers\\nto. To do this, pass a string or date offset to loffset:\\nIn [519]: ts.resample('5min', how='sum', loffset='-1s')\\nOut[519]: \\n1999-12-31 23:59:59     0\\n2000-01-01 00:04:59    15\\n2000-01-01 00:09:59    40\\n2000-01-01 00:14:59    11\\nFreq: 5T\\n1. The choice of closed='right', label='right' as the default might seem a bit odd to some users. In\\npractice the choice is somewhat arbitrary; for some target frequencies, closed='left' is preferable, while\\nfor others closed='right' makes more sense. The important thing is that you keep in mind exactly how\\nyou are segmenting the data.\\nResampling and Frequency Conversion | 315\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"This also could have been accomplished by calling the shift method on the result\\nwithout the loffset.\\nOpen-High-Low-Close (OHLC) resampling\\nIn finance, an ubiquitous way to aggregate a time series is to compute four values for\\neach bucket: the first (open), last (close), maximum (high), and minimal (low) values.\\nBy passing how='ohlc' you will obtain a DataFrame having columns containing these\\nfour aggregates, which are efficiently computed in a single sweep of the data:\\nIn [520]: ts.resample('5min', how='ohlc')\\nOut[520]: \\n                     open  high  low  close\\n2000-01-01 00:00:00     0     0    0      0\\n2000-01-01 00:05:00     1     5    1      5\\n2000-01-01 00:10:00     6    10    6     10\\n2000-01-01 00:15:00    11    11   11     11\\nResampling with GroupBy\\nAn alternate way to downsample is to use pandas’s groupby functionality. For example,\\nyou can group by month or weekday by passing a function that accesses those fields\\non the time series’s index:\"),\n",
       " Document(metadata={}, page_content=\"An alternate way to downsample is to use pandas’s groupby functionality. For example,\\nyou can group by month or weekday by passing a function that accesses those fields\\non the time series’s index:\\nIn [521]: rng = pd.date_range('1/1/2000', periods=100, freq='D')\\nIn [522]: ts = Series(np.arange(100), index=rng)\\nIn [523]: ts.groupby(lambda x: x.month).mean()\\nOut[523]: \\n1    15\\n2    45\\n3    75\\n4    95\\nIn [524]: ts.groupby(lambda x: x.weekday).mean()\\nOut[524]: \\n0    47.5\\n1    48.5\\n2    49.5\\n3    50.5\\n4    51.5\\n5    49.0\\n6    50.0\\nUpsampling and Interpolation\\nWhen converting from a low frequency to a higher frequency, no aggregation is needed.\\nLet’s consider a DataFrame with some weekly data:\\nIn [525]: frame = DataFrame(np.random.randn(2, 4),\\n   .....:                   index=pd.date_range('1/1/2000', periods=2, freq='W-WED'),\\n   .....:                   columns=['Colorado', 'Texas', 'New York', 'Ohio'])\\n316 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"In [526]: frame[:5]\\nOut[526]: \\n            Colorado     Texas  New York     Ohio\\n2000-01-05 -0.609657 -0.268837  0.195592  0.85979\\n2000-01-12 -0.263206  1.141350 -0.101937 -0.07666\\nWhen resampling this to daily frequency, by default missing values are introduced:\\nIn [527]: df_daily = frame.resample('D')\\nIn [528]: df_daily\\nOut[528]: \\n            Colorado     Texas  New York     Ohio\\n2000-01-05 -0.609657 -0.268837  0.195592  0.85979\\n2000-01-06       NaN       NaN       NaN      NaN\\n2000-01-07       NaN       NaN       NaN      NaN\\n2000-01-08       NaN       NaN       NaN      NaN\\n2000-01-09       NaN       NaN       NaN      NaN\\n2000-01-10       NaN       NaN       NaN      NaN\\n2000-01-11       NaN       NaN       NaN      NaN\\n2000-01-12 -0.263206  1.141350 -0.101937 -0.07666\\nSuppose you wanted to fill forward each weekly value on the non-Wednesdays. The\\nsame filling or interpolation methods available in the fillna and reindex methods are\\navailable for resampling:\"),\n",
       " Document(metadata={}, page_content=\"Suppose you wanted to fill forward each weekly value on the non-Wednesdays. The\\nsame filling or interpolation methods available in the fillna and reindex methods are\\navailable for resampling:\\nIn [529]: frame.resample('D', fill_method='ffill')\\nOut[529]: \\n            Colorado     Texas  New York     Ohio\\n2000-01-05 -0.609657 -0.268837  0.195592  0.85979\\n2000-01-06 -0.609657 -0.268837  0.195592  0.85979\\n2000-01-07 -0.609657 -0.268837  0.195592  0.85979\\n2000-01-08 -0.609657 -0.268837  0.195592  0.85979\\n2000-01-09 -0.609657 -0.268837  0.195592  0.85979\\n2000-01-10 -0.609657 -0.268837  0.195592  0.85979\\n2000-01-11 -0.609657 -0.268837  0.195592  0.85979\\n2000-01-12 -0.263206  1.141350 -0.101937 -0.07666\\nYou can similarly choose to only fill a certain number of periods forward to limit how\\nfar to continue using an observed value:\\nIn [530]: frame.resample('D', fill_method='ffill', limit=2)\\nOut[530]: \\n            Colorado     Texas  New York     Ohio\"),\n",
       " Document(metadata={}, page_content=\"far to continue using an observed value:\\nIn [530]: frame.resample('D', fill_method='ffill', limit=2)\\nOut[530]: \\n            Colorado     Texas  New York     Ohio\\n2000-01-05 -0.609657 -0.268837  0.195592  0.85979\\n2000-01-06 -0.609657 -0.268837  0.195592  0.85979\\n2000-01-07 -0.609657 -0.268837  0.195592  0.85979\\n2000-01-08       NaN       NaN       NaN      NaN\\n2000-01-09       NaN       NaN       NaN      NaN\\n2000-01-10       NaN       NaN       NaN      NaN\\n2000-01-11       NaN       NaN       NaN      NaN\\n2000-01-12 -0.263206  1.141350 -0.101937 -0.07666\\nNotably, the new date index need not overlap with the old one at all:\\nResampling and Frequency Conversion | 317\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"In [531]: frame.resample('W-THU', fill_method='ffill')\\nOut[531]: \\n            Colorado     Texas  New York     Ohio\\n2000-01-06 -0.609657 -0.268837  0.195592  0.85979\\n2000-01-13 -0.263206  1.141350 -0.101937 -0.07666\\nResampling with Periods\\nResampling data indexed by periods is reasonably straightforward and works as you\\nwould hope:\\nIn [532]: frame = DataFrame(np.random.randn(24, 4),\\n   .....:                   index=pd.period_range('1-2000', '12-2001', freq='M'),\\n   .....:                   columns=['Colorado', 'Texas', 'New York', 'Ohio'])\\nIn [533]: frame[:5]\\nOut[533]: \\n         Colorado     Texas  New York      Ohio\\n2000-01  0.120837  1.076607  0.434200  0.056432\\n2000-02 -0.378890  0.047831  0.341626  1.567920\\n2000-03 -0.047619 -0.821825 -0.179330 -0.166675\\n2000-04  0.333219 -0.544615 -0.653635 -2.311026\\n2000-05  1.612270 -0.806614  0.557884  0.580201\\nIn [534]: annual_frame = frame.resample('A-DEC', how='mean')\\nIn [535]: annual_frame\\nOut[535]:\"),\n",
       " Document(metadata={}, page_content=\"2000-04  0.333219 -0.544615 -0.653635 -2.311026\\n2000-05  1.612270 -0.806614  0.557884  0.580201\\nIn [534]: annual_frame = frame.resample('A-DEC', how='mean')\\nIn [535]: annual_frame\\nOut[535]: \\n      Colorado     Texas  New York      Ohio\\n2000  0.352070 -0.553642  0.196642 -0.094099\\n2001  0.158207  0.042967 -0.360755  0.184687\\nUpsampling is more nuanced as you must make a decision about which end of the\\ntimespan in the new frequency to place the values before resampling, just like the \\nasfreq method. The convention argument defaults to 'end' but can also be 'start':\\n# Q-DEC: Quarterly, year ending in December\\nIn [536]: annual_frame.resample('Q-DEC', fill_method='ffill')\\nOut[536]: \\n        Colorado     Texas  New York      Ohio\\n2000Q4  0.352070 -0.553642  0.196642 -0.094099\\n2001Q1  0.352070 -0.553642  0.196642 -0.094099\\n2001Q2  0.352070 -0.553642  0.196642 -0.094099\\n2001Q3  0.352070 -0.553642  0.196642 -0.094099\\n2001Q4  0.158207  0.042967 -0.360755  0.184687\"),\n",
       " Document(metadata={}, page_content=\"2001Q1  0.352070 -0.553642  0.196642 -0.094099\\n2001Q2  0.352070 -0.553642  0.196642 -0.094099\\n2001Q3  0.352070 -0.553642  0.196642 -0.094099\\n2001Q4  0.158207  0.042967 -0.360755  0.184687\\nIn [537]: annual_frame.resample('Q-DEC', fill_method='ffill', convention='start')\\nOut[537]: \\n        Colorado     Texas  New York      Ohio\\n2000Q1  0.352070 -0.553642  0.196642 -0.094099\\n2000Q2  0.352070 -0.553642  0.196642 -0.094099\\n2000Q3  0.352070 -0.553642  0.196642 -0.094099\\n2000Q4  0.352070 -0.553642  0.196642 -0.094099\\n2001Q1  0.158207  0.042967 -0.360755  0.184687\\n318 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"Since periods refer to timespans, the rules about upsampling and downsampling are\\nmore rigid:\\n• In downsampling, the target frequency must be a subperiod of the source frequency.\\n• In upsampling, the target frequency must be a superperiod of the source frequency.\\nIf these rules are not satisfied, an exception will be raised. This mainly affects the quar-\\nterly, annual, and weekly frequencies; for example, the timespans defined by Q-MAR only\\nline up with A-MAR, A-JUN, A-SEP, and A-DEC:\\nIn [538]: annual_frame.resample('Q-MAR', fill_method='ffill')\\nOut[538]: \\n        Colorado     Texas  New York      Ohio\\n2001Q3  0.352070 -0.553642  0.196642 -0.094099\\n2001Q4  0.352070 -0.553642  0.196642 -0.094099\\n2002Q1  0.352070 -0.553642  0.196642 -0.094099\\n2002Q2  0.352070 -0.553642  0.196642 -0.094099\\n2002Q3  0.158207  0.042967 -0.360755  0.184687\\nTime Series Plotting\\nPlots with pandas time series have improved date formatting compared with matplotlib\"),\n",
       " Document(metadata={}, page_content=\"2002Q2  0.352070 -0.553642  0.196642 -0.094099\\n2002Q3  0.158207  0.042967 -0.360755  0.184687\\nTime Series Plotting\\nPlots with pandas time series have improved date formatting compared with matplotlib\\nout of the box. As an example, I downloaded some stock price data on a few common\\nUS stock from Yahoo! Finance:\\nIn [539]: close_px_all = pd.read_csv('ch09/stock_px.csv', parse_dates=True, index_col=0)\\nIn [540]: close_px = close_px_all[['AAPL', 'MSFT', 'XOM']]\\nIn [541]: close_px = close_px.resample('B', fill_method='ffill')\\nIn [542]: close_px\\nOut[542]: \\n<class 'pandas.core.frame.DataFrame'>\\nDatetimeIndex: 2292 entries, 2003-01-02 00:00:00 to 2011-10-14 00:00:00\\nFreq: B\\nData columns:\\nAAPL    2292  non-null values\\nMSFT    2292  non-null values\\nXOM     2292  non-null values\\ndtypes: float64(3)\\nCalling plot on one of the columns grenerates a simple plot, seen in Figure 10-4.\\nIn [544]: close_px['AAPL'].plot()\\nWhen called on a DataFrame, as you would expect, all of the time series are drawn on\"),\n",
       " Document(metadata={}, page_content=\"In [544]: close_px['AAPL'].plot()\\nWhen called on a DataFrame, as you would expect, all of the time series are drawn on\\na single subplot with a legend indicating which is which. I’ll plot only the year 2009\\ndata so you can see how both months and years are formatted on the X axis; see\\nFigure 10-5.\\nIn [546]: close_px.ix['2009'].plot()\\nTime Series Plotting | 319\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"In [548]: close_px['AAPL'].ix['01-2011':'03-2011'].plot()\\nQuarterly frequency data is also more nicely formatted with quarterly markers, some-\\nthing that would be quite a bit more work to do by hand. See Figure 10-7.\\nIn [550]: appl_q = close_px['AAPL'].resample('Q-DEC', fill_method='ffill')\\nIn [551]: appl_q.ix['2009':].plot()\\nA last feature of time series plotting in pandas is that by right-clicking and dragging to\\nzoom in and out, the dates will be dynamically expanded or contracted and reformat-\\nting depending on the timespan contained in the plot view. This is of course only true\\nwhen using matplotlib in interactive mode.\\nMoving Window Functions\\nA common class of array transformations intended for time series operations are sta-\\ntistics and other functions evaluated over a sliding window or with exponentially de-\\nFigure 10-4. AAPL Daily Price\\nFigure 10-5. Stock Prices in 2009\\n320 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content='caying weights. I call these moving window functions, even though it includes functions\\nwithout a fixed-length window like exponentially-weighted moving average. Like other\\nstatistical functions, these also automatically exclude missing data.\\nrolling_mean is one of the simplest such functions. It takes a TimeSeries or DataFrame\\nalong with a window (expressed as a number of periods):\\nIn [555]: close_px.AAPL.plot()\\nOut[555]: <matplotlib.axes.AxesSubplot at 0x1099b3990>\\nIn [556]: pd.rolling_mean(close_px.AAPL, 250).plot()\\nSee Figure 10-8 for the plot. By default functions like rolling_mean require the indicated\\nnumber of non-NA observations. This behavior can be changed to account for missing\\ndata and, in particular, the fact that you will have fewer than window periods of data at\\nthe beginning of the time series (see Figure 10-9):\\nFigure 10-6. Apple Daily Price in 1/2011-3/2011\\nFigure 10-7. Apple Quarterly Price 2009-2011\\nMoving Window Functions | 321\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='In [558]: appl_std250 = pd.rolling_std(close_px.AAPL, 250, min_periods=10)\\nIn [559]: appl_std250[5:12]\\nOut[559]: \\n2003-01-09         NaN\\n2003-01-10         NaN\\n2003-01-13         NaN\\n2003-01-14         NaN\\n2003-01-15    0.077496\\n2003-01-16    0.074760\\n2003-01-17    0.112368\\nFreq: B\\nIn [560]: appl_std250.plot()\\nFigure 10-8. Apple Price with 250-day MA\\nFigure 10-9. Apple 250-day daily return standard deviation\\nTo compute an expanding window mean, you can see that an expanding window is just\\na special case where the window is the length of the time series, but only one or more\\nperiods is required to compute a value:\\n322 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='# Define expanding mean in terms of rolling_mean\\nIn [561]: expanding_mean = lambda x: rolling_mean(x, len(x), min_periods=1)\\nCalling rolling_mean and friends on a DataFrame applies the transformation to each\\ncolumn (see Figure 10-10):\\nIn [563]: pd.rolling_mean(close_px, 60).plot(logy=True)\\nFigure 10-10. Stocks Prices 60-day MA (log Y-axis)\\nSee Table 10-6 for a listing of related functions in pandas.\\nTable 10-6. Moving window and exponentially-weighted functions\\nFunction Description\\nrolling_count Returns number of non-NA observations in each trailing window.\\nrolling_sum Moving window sum.\\nrolling_mean Moving window mean.\\nrolling_median Moving window median.\\nrolling_var, rolling_std Moving window variance and standard deviation, respectively. Uses n - 1 denom-\\ninator.\\nrolling_skew, rolling_kurt Moving window skewness (3rd moment) and kurtosis (4th moment), respectively.\\nrolling_min, rolling_max Moving window minimum and maximum.'),\n",
       " Document(metadata={}, page_content='inator.\\nrolling_skew, rolling_kurt Moving window skewness (3rd moment) and kurtosis (4th moment), respectively.\\nrolling_min, rolling_max Moving window minimum and maximum.\\nrolling_quantile Moving window score at percentile/sample quantile.\\nrolling_corr, rolling_cov Moving window correlation and covariance.\\nrolling_apply Apply generic array function over a moving window.\\newma Exponentially-weighted moving average.\\newmvar, ewmstd Exponentially-weighted moving variance and standard deviation.\\newmcorr, ewmcov Exponentially-weighted moving correlation and covariance.\\nMoving Window Functions | 323\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='bottleneck, a Python library by Keith Goodman, provides an alternate\\nimplementation of NaN-friendly moving window functions and may be\\nworth looking at depending on your application.\\nExponentially-weighted functions\\nAn alternative to using a static window size with equally-weighted observations is to\\nspecify a constant decay factor to give more weight to more recent observations. In\\nmathematical terms, if mat is the moving average result at time t and x is the time series\\nin question, each value in the result is computed as mat = a * mat - 1 + (a - 1) * x_t, where\\na is the decay factor. There are a couple of ways to specify the decay factor, a popular\\none is using a span, which makes the result comparable to a simple moving window\\nfunction with window size equal to the span.\\nSince an exponentially-weighted statistic places more weight on more recent observa-\\ntions, it “adapts” faster to changes compared with the equal-weighted version. Here’s'),\n",
       " Document(metadata={}, page_content=\"Since an exponentially-weighted statistic places more weight on more recent observa-\\ntions, it “adapts” faster to changes compared with the equal-weighted version. Here’s\\nan example comparing a 60-day moving average of Apple’s stock price with an EW\\nmoving average with span=60 (see Figure 10-11):\\nfig, axes = plt.subplots(nrows=2, ncols=1, sharex=True, sharey=True,\\n                         figsize=(12, 7))\\naapl_px = close_px.AAPL['2005':'2009']\\nma60 = pd.rolling_mean(aapl_px, 60, min_periods=50)\\newma60 = pd.ewma(aapl_px, span=60)\\naapl_px.plot(style='k-', ax=axes[0])\\nma60.plot(style='k--', ax=axes[0])\\naapl_px.plot(style='k-', ax=axes[1])\\newma60.plot(style='k--', ax=axes[1])\\naxes[0].set_title('Simple MA')\\naxes[1].set_title('Exponentially-weighted MA')\\nBinary Moving Window Functions\\nSome statistical operators, like correlation and covariance, need to operate on two time\\nseries. As an example, financial analysts are often interested in a stock’s correlation to\"),\n",
       " Document(metadata={}, page_content='Some statistical operators, like correlation and covariance, need to operate on two time\\nseries. As an example, financial analysts are often interested in a stock’s correlation to\\na benchmark index like the S&P 500. We can compute that by computing the percent\\nchanges and using rolling_corr (see Figure 10-12):\\nIn [570]: spx_rets = spx_px / spx_px.shift(1) - 1\\nIn [571]: returns = close_px.pct_change()\\nIn [572]: corr = pd.rolling_corr(returns.AAPL, spx_rets, 125, min_periods=100)\\nIn [573]: corr.plot()\\n324 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Suppose you wanted to compute the correlation of the S&P 500 index with many stocks\\nat once. Writing a loop and creating a new DataFrame would be easy but maybe get\\nrepetitive, so if you pass a TimeSeries and a DataFrame, a function like rolling_corr\\nwill compute the correlation of the TimeSeries (spx_rets in this case) with each column\\nin the DataFrame. See Figure 10-13 for the plot of the result:\\nIn [575]: corr = pd.rolling_corr(returns, spx_rets, 125, min_periods=100)\\nIn [576]: corr.plot()\\nFigure 10-11. Simple moving average versus exponentially-weighted\\nFigure 10-12. Six-month AAPL return correlation to S&P 500\\nMoving Window Functions | 325\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='User-Defined Moving Window Functions\\nThe rolling_apply function provides a means to apply an array function of your own\\ndevising over a moving window. The only requirement is that the function produce a\\nsingle value (a reduction) from each piece of the array. For example, while we can\\ncompute sample quantiles using rolling_quantile, we might be interested in the per-\\ncentile rank of a particular value over the sample. The scipy.stats.percentileof\\nscore function does just this:\\nIn [578]: from scipy.stats import percentileofscore\\nIn [579]: score_at_2percent = lambda x: percentileofscore(x, 0.02)\\nIn [580]: result = pd.rolling_apply(returns.AAPL, 250, score_at_2percent)\\nIn [581]: result.plot()\\nFigure 10-13. Six-month return correlations to S&P 500\\nFigure 10-14. Percentile rank of 2% AAPL return over 1 year window\\n326 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Performance and Memory Usage Notes\\nTimestamps and periods are represented as 64-bit integers using NumPy’s date\\ntime64 dtype. This means that for each data point, there is an associated 8 bytes of\\nmemory per timestamp. Thus, a time series with 1 million float64 data points has a\\nmemory footprint of approximately 16 megabytes. Since pandas makes every effort to\\nshare indexes among time series, creating views on existing time series do not cause\\nany more memory to be used. Additionally, indexes for lower frequencies (daily and\\nup) are stored in a central cache, so that any fixed-frequency index is a view on the date\\ncache. Thus, if you have a large collection of low-frequency time series, the memory\\nfootprint of the indexes will not be as significant.\\nPerformance-wise, pandas has been highly optimized for data alignment operations\\n(the behind-the-scenes work of differently indexed ts1 + ts2) and resampling. Here is\\nan example of aggregating 10MM data points to OHLC:'),\n",
       " Document(metadata={}, page_content=\"(the behind-the-scenes work of differently indexed ts1 + ts2) and resampling. Here is\\nan example of aggregating 10MM data points to OHLC:\\nIn [582]: rng = pd.date_range('1/1/2000', periods=10000000, freq='10ms')\\nIn [583]: ts = Series(np.random.randn(len(rng)), index=rng)\\nIn [584]: ts\\nOut[584]: \\n2000-01-01 00:00:00          -1.402235\\n2000-01-01 00:00:00.010000    2.424667\\n2000-01-01 00:00:00.020000   -1.956042\\n2000-01-01 00:00:00.030000   -0.897339\\n...\\n2000-01-02 03:46:39.960000    0.495530\\n2000-01-02 03:46:39.970000    0.574766\\n2000-01-02 03:46:39.980000    1.348374\\n2000-01-02 03:46:39.990000    0.665034\\nFreq: 10L, Length: 10000000\\nIn [585]: ts.resample('15min', how='ohlc')\\nOut[585]: \\n<class 'pandas.core.frame.DataFrame'>\\nDatetimeIndex: 113 entries, 2000-01-01 00:00:00 to 2000-01-02 04:00:00\\nFreq: 15T\\nData columns:\\nopen     113  non-null values\\nhigh     113  non-null values\\nlow      113  non-null values\\nclose    113  non-null values\\ndtypes: float64(4)\"),\n",
       " Document(metadata={}, page_content=\"Freq: 15T\\nData columns:\\nopen     113  non-null values\\nhigh     113  non-null values\\nlow      113  non-null values\\nclose    113  non-null values\\ndtypes: float64(4)\\nIn [586]: %timeit ts.resample('15min', how='ohlc')\\n10 loops, best of 3: 61.1 ms per loop\\nThe runtime may depend slightly on the relative size of the aggregated result; higher\\nfrequency aggregates unsurprisingly take longer to compute:\\nIn [587]: rng = pd.date_range('1/1/2000', periods=10000000, freq='1s')\\nPerformance and Memory Usage Notes | 327\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"In [588]: ts = Series(np.random.randn(len(rng)), index=rng)\\nIn [589]: %timeit ts.resample('15s', how='ohlc')\\n1 loops, best of 3: 88.2 ms per loop\\nIt’s possible that by the time you read this, the performance of these algorithms may\\nbe even further improved. As an example, there are currently no optimizations for\\nconversions between regular frequencies, but that would be fairly straightforward to do.\\n328 | Chapter 10: \\u2002Time Series\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content='CHAPTER 11\\nFinancial and Economic Data\\nApplications\\nThe use of Python in the financial industry has been increasing rapidly since 2005, led\\nlargely by the maturation of libraries (like NumPy and pandas) and the availability of\\nskilled Python programmers. Institutions have found that Python is well-suited both\\nas an interactive analysis environment as well as enabling robust systems to be devel-\\noped often in a fraction of the time it would have taken in Java or C++. Python is also\\nan ideal glue layer; it is easy to build Python interfaces to legacy libraries built in C or\\nC++.\\nWhile the field of financial analysis is broad enough to fill an entire book, I hope to\\nshow you how the tools in this book can be applied to a number of specific problems\\nin finance. As with other research and analysis domains, too much programming effort\\nis often spent wrangling data rather than solving the core modeling and research prob-'),\n",
       " Document(metadata={}, page_content='in finance. As with other research and analysis domains, too much programming effort\\nis often spent wrangling data rather than solving the core modeling and research prob-\\nlems. I personally got started building pandas in 2008 while grappling with inadequate\\ndata tools.\\nIn these examples, I’ll use the term cross-section to refer to data at a fixed point in time.\\nFor example, the closing prices of all the stocks in the S&P 500 index on a particular\\ndate form a cross-section. Cross-sectional data at multiple points in time over multiple\\ndata items (for example, prices together with volume) form a panel. Panel data can\\neither be represented as a hierarchically-indexed DataFrame or using the three-dimen-\\nsional Panel pandas object.\\nData Munging Topics\\nMany helpful data munging tools for financial applications are spread across the earlier\\nchapters. Here I’ll highlight a number of topics as they relate to this problem domain.\\n329\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Time Series and Cross-Section Alignment\\nOne of the most time-consuming issues in working with financial data is the so-called\\ndata alignment problem. Two related time series may have indexes that don’t line up\\nperfectly, or two DataFrame objects might have columns or row labels that don’t match.\\nUsers of MATLAB, R, and other matrix-programming languages often invest significant\\neffort in wrangling data into perfectly aligned forms. In my experience, having to align\\ndata by hand (and worse, having to verify that data is aligned) is a far too rigid and\\ntedious way to work. It is also rife with potential for bugs due to combining misaligned\\ndata.\\npandas take an alternate approach by automatically aligning data in arithmetic opera-\\ntions. In practice, this grants immense freedom and enhances your productivity. As an\\nexample, let’s consider a couple of DataFrames containing time series of stock prices\\nand volume:\\nIn [16]: prices\\nOut[16]: \\n              AAPL    JNJ      SPX    XOM'),\n",
       " Document(metadata={}, page_content='example, let’s consider a couple of DataFrames containing time series of stock prices\\nand volume:\\nIn [16]: prices\\nOut[16]: \\n              AAPL    JNJ      SPX    XOM\\n2011-09-06  379.74  64.64  1165.24  71.15\\n2011-09-07  383.93  65.43  1198.62  73.65\\n2011-09-08  384.14  64.95  1185.90  72.82\\n2011-09-09  377.48  63.64  1154.23  71.01\\n2011-09-12  379.94  63.59  1162.27  71.84\\n2011-09-13  384.62  63.61  1172.87  71.65\\n2011-09-14  389.30  63.73  1188.68  72.64\\nIn [17]: volume\\nOut[17]: \\n                AAPL       JNJ       XOM\\n2011-09-06  18173500  15848300  25416300\\n2011-09-07  12492000  10759700  23108400\\n2011-09-08  14839800  15551500  22434800\\n2011-09-09  20171900  17008200  27969100\\n2011-09-12  16697300  13448200  26205800\\nSuppose you wanted to compute a volume-weighted average price using all available\\ndata (and making the simplifying assumption that the volume data is a subset of the\\nprice data). Since pandas aligns the data automatically in arithmetic and excludes'),\n",
       " Document(metadata={}, page_content='data (and making the simplifying assumption that the volume data is a subset of the\\nprice data). Since pandas aligns the data automatically in arithmetic and excludes\\nmissing data in functions like sum, we can express this concisely as:\\nIn [18]: prices * volume\\nOut[18]: \\n                  AAPL         JNJ  SPX         XOM\\n2011-09-06  6901204890  1024434112  NaN  1808369745\\n2011-09-07  4796053560   704007171  NaN  1701933660\\n2011-09-08  5700560772  1010069925  NaN  1633702136\\n2011-09-09  7614488812  1082401848  NaN  1986085791\\n2011-09-12  6343972162   855171038  NaN  1882624672\\n2011-09-13         NaN         NaN  NaN         NaN\\n2011-09-14         NaN         NaN  NaN         NaN\\nIn [19]: vwap = (prices * volume).sum() / volume.sum()\\n330 | Chapter 11: \\u2002Financial and Economic Data Applications\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"In [20]: vwap             In [21]: vwap.dropna()\\nOut[20]:                  Out[21]:              \\nAAPL    380.655181        AAPL    380.655181    \\nJNJ      64.394769        JNJ      64.394769    \\nSPX            NaN        XOM      72.024288    \\nXOM      72.024288\\nSince SPX wasn’t found in volume, you can choose to explicitly discard that at any point.\\nShould you wish to align by hand, you can use DataFrame’s align method, which\\nreturns a tuple of reindexed versions of the two objects:\\nIn [22]: prices.align(volume, join='inner')\\nOut[22]: \\n(              AAPL    JNJ    XOM\\n2011-09-06  379.74  64.64  71.15\\n2011-09-07  383.93  65.43  73.65\\n2011-09-08  384.14  64.95  72.82\\n2011-09-09  377.48  63.64  71.01\\n2011-09-12  379.94  63.59  71.84,\\n                 AAPL       JNJ       XOM\\n2011-09-06  18173500  15848300  25416300\\n2011-09-07  12492000  10759700  23108400\\n2011-09-08  14839800  15551500  22434800\\n2011-09-09  20171900  17008200  27969100\\n2011-09-12  16697300  13448200  26205800)\"),\n",
       " Document(metadata={}, page_content=\"2011-09-07  12492000  10759700  23108400\\n2011-09-08  14839800  15551500  22434800\\n2011-09-09  20171900  17008200  27969100\\n2011-09-12  16697300  13448200  26205800)\\nAnother indispensable feature is constructing a DataFrame from a collection of poten-\\ntially differently indexed Series:\\nIn [23]: s1 = Series(range(3), index=['a', 'b', 'c'])\\nIn [24]: s2 = Series(range(4), index=['d', 'b', 'c', 'e'])\\nIn [25]: s3 = Series(range(3), index=['f', 'a', 'c'])\\nIn [26]: DataFrame({'one': s1, 'two': s2, 'three': s3})\\nOut[26]: \\n   one  three  two\\na    0      1  NaN\\nb    1    NaN    1\\nc    2      2    2\\nd  NaN    NaN    0\\ne  NaN    NaN    3\\nf  NaN      0  NaN\\nAs you have seen earlier, you can of course specify explicitly the index of the result,\\ndiscarding the rest of the data:\\nIn [27]: DataFrame({'one': s1, 'two': s2, 'three': s3}, index=list('face'))\\nOut[27]: \\n   one  three  two\\nf  NaN      0  NaN\\na    0      1  NaN\\nc    2      2    2\\ne  NaN    NaN    3\\nData Munging Topics | 331\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"Operations with Time Series of Different Frequencies\\nEconomic time series are often of annual, quarterly, monthly, daily, or some other more\\nspecialized frequency. Some are completely irregular; for example, earnings revisions\\nfor a stock may arrive at any time. The two main tools for frequency conversion and\\nrealignment are the resample and reindex methods. resample converts data to a fixed\\nfrequency while reindex conforms data to a new index. Both support optional inter-\\npolation (such as forward filling) logic.\\nLet’s consider a small weekly time series:\\nIn [28]: ts1 = Series(np.random.randn(3),\\n   ....:              index=pd.date_range('2012-6-13', periods=3, freq='W-WED'))\\nIn [29]: ts1\\nOut[29]: \\n2012-06-13   -1.124801\\n2012-06-20    0.469004\\n2012-06-27   -0.117439\\nFreq: W-WED\\nIf you resample this to business daily (Monday-Friday) frequency, you get holes on the\\ndays where there is no data:\\nIn [30]: ts1.resample('B')\\nOut[30]: \\n2012-06-13   -1.124801\\n2012-06-14         NaN\"),\n",
       " Document(metadata={}, page_content=\"If you resample this to business daily (Monday-Friday) frequency, you get holes on the\\ndays where there is no data:\\nIn [30]: ts1.resample('B')\\nOut[30]: \\n2012-06-13   -1.124801\\n2012-06-14         NaN\\n2012-06-15         NaN\\n2012-06-18         NaN\\n2012-06-19         NaN\\n2012-06-20    0.469004\\n2012-06-21         NaN\\n2012-06-22         NaN\\n2012-06-25         NaN\\n2012-06-26         NaN\\n2012-06-27   -0.117439\\nFreq: B\\nOf course, using 'ffill' as the fill_method forward fills values in those gaps. This is\\na common practice with lower frequency data as you compute a time series of values\\non each timestamp having the latest valid or “as of” value:\\nIn [31]: ts1.resample('B', fill_method='ffill')\\nOut[31]: \\n2012-06-13   -1.124801\\n2012-06-14   -1.124801\\n2012-06-15   -1.124801\\n2012-06-18   -1.124801\\n2012-06-19   -1.124801\\n2012-06-20    0.469004\\n2012-06-21    0.469004\\n2012-06-22    0.469004\\n2012-06-25    0.469004\\n2012-06-26    0.469004\\n332 | Chapter 11: \\u2002Financial and Economic Data Applications\"),\n",
       " Document(metadata={}, page_content='2012-06-19   -1.124801\\n2012-06-20    0.469004\\n2012-06-21    0.469004\\n2012-06-22    0.469004\\n2012-06-25    0.469004\\n2012-06-26    0.469004\\n332 | Chapter 11: \\u2002Financial and Economic Data Applications\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"2012-06-27   -0.117439\\nFreq: B\\nIn practice, upsampling lower frequency data to a higher, regular frequency is a fine\\nsolution, but in the more general irregular time series case it may be a poor fit. Consider\\nan irregularly sampled time series from the same general time period:\\nIn [32]: dates = pd.DatetimeIndex(['2012-6-12', '2012-6-17', '2012-6-18',\\n   ....:                           '2012-6-21', '2012-6-22', '2012-6-29'])\\nIn [33]: ts2 = Series(np.random.randn(6), index=dates)\\nIn [34]: ts2\\nOut[34]: \\n2012-06-12   -0.449429\\n2012-06-17    0.459648\\n2012-06-18   -0.172531\\n2012-06-21    0.835938\\n2012-06-22   -0.594779\\n2012-06-29    0.027197\\nIf you wanted to add the “as of” values in ts1 (forward filling) to ts2. One option would\\nbe to resample both to a regular frequency then add, but if you want to maintain the\\ndate index in ts2, using reindex is a more precise solution:\\nIn [35]: ts1.reindex(ts2.index, method='ffill')\\nOut[35]: \\n2012-06-12         NaN\\n2012-06-17   -1.124801\"),\n",
       " Document(metadata={}, page_content=\"date index in ts2, using reindex is a more precise solution:\\nIn [35]: ts1.reindex(ts2.index, method='ffill')\\nOut[35]: \\n2012-06-12         NaN\\n2012-06-17   -1.124801\\n2012-06-18   -1.124801\\n2012-06-21    0.469004\\n2012-06-22    0.469004\\n2012-06-29   -0.117439\\nIn [36]: ts2 + ts1.reindex(ts2.index, method='ffill')\\nOut[36]: \\n2012-06-12         NaN\\n2012-06-17   -0.665153\\n2012-06-18   -1.297332\\n2012-06-21    1.304942\\n2012-06-22   -0.125775\\n2012-06-29   -0.090242\\nUsing periods instead of timestamps\\nPeriods (representing time spans) provide an alternate means of working with different\\nfrequency time series, especially financial or economic series with annual or quarterly\\nfrequency having a particular reporting convention. For example, a company might\\nannounce its quarterly earnings with fiscal year ending in June, thus having Q-JUN fre-\\nquency. Consider a pair of macroeconomic time series related to GDP and inflation:\\nIn [37]: gdp = Series([1.78, 1.94, 2.08, 2.01, 2.15, 2.31, 2.46],\"),\n",
       " Document(metadata={}, page_content=\"quency. Consider a pair of macroeconomic time series related to GDP and inflation:\\nIn [37]: gdp = Series([1.78, 1.94, 2.08, 2.01, 2.15, 2.31, 2.46],\\n   ....:              index=pd.period_range('1984Q2', periods=7, freq='Q-SEP'))\\nData Munging Topics | 333\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"In [38]: infl = Series([0.025, 0.045, 0.037, 0.04],\\n   ....:               index=pd.period_range('1982', periods=4, freq='A-DEC'))\\nIn [39]: gdp          In [40]: infl\\nOut[39]:              Out[40]:     \\n1984Q2    1.78        1982    0.025\\n1984Q3    1.94        1983    0.045\\n1984Q4    2.08        1984    0.037\\n1985Q1    2.01        1985    0.040\\n1985Q2    2.15        Freq: A-DEC  \\n1985Q3    2.31                     \\n1985Q4    2.46                     \\nFreq: Q-SEP\\nUnlike time series with timestamps, operations between different-frequency time series\\nindexed by periods are not possible without explicit conversions. In this case, if we\\nknow that infl values were observed at the end of each year, we can then convert to\\nQ-SEP to get the right periods in that frequency:\\nIn [41]: infl_q = infl.asfreq('Q-SEP', how='end')\\nIn [42]: infl_q\\nOut[42]: \\n1983Q1    0.025\\n1984Q1    0.045\\n1985Q1    0.037\\n1986Q1    0.040\\nFreq: Q-SEP\\nThat time series can then be reindexed with forward-filling to match gdp:\"),\n",
       " Document(metadata={}, page_content=\"In [42]: infl_q\\nOut[42]: \\n1983Q1    0.025\\n1984Q1    0.045\\n1985Q1    0.037\\n1986Q1    0.040\\nFreq: Q-SEP\\nThat time series can then be reindexed with forward-filling to match gdp:\\nIn [43]: infl_q.reindex(gdp.index, method='ffill')\\nOut[43]: \\n1984Q2    0.045\\n1984Q3    0.045\\n1984Q4    0.045\\n1985Q1    0.037\\n1985Q2    0.037\\n1985Q3    0.037\\n1985Q4    0.037\\nFreq: Q-SEP\\nTime of Day and “as of” Data Selection\\nSuppose you have a long time series containing intraday market data and you want to\\nextract the prices at a particular time of day on each day of the data. What if the data\\nare irregular such that observations do not fall exactly on the desired time? In practice\\nthis task can make for error-prone data munging if you are not careful. Here is an\\nexample for illustration purposes:\\n# Make an intraday date range and time series\\nIn [44]: rng = pd.date_range('2012-06-01 09:30', '2012-06-01 15:59', freq='T')\\n# Make a 5-day series of 9:30-15:59 values\"),\n",
       " Document(metadata={}, page_content=\"# Make an intraday date range and time series\\nIn [44]: rng = pd.date_range('2012-06-01 09:30', '2012-06-01 15:59', freq='T')\\n# Make a 5-day series of 9:30-15:59 values\\n334 | Chapter 11: \\u2002Financial and Economic Data Applications\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content='In [45]: rng = rng.append([rng + pd.offsets.BDay(i) for i in range(1, 4)])\\nIn [46]: ts = Series(np.arange(len(rng), dtype=float), index=rng)\\nIn [47]: ts\\nOut[47]: \\n2012-06-01 09:30:00    0\\n2012-06-01 09:31:00    1\\n2012-06-01 09:32:00    2\\n2012-06-01 09:33:00    3\\n...\\n2012-06-06 15:56:00    1556\\n2012-06-06 15:57:00    1557\\n2012-06-06 15:58:00    1558\\n2012-06-06 15:59:00    1559\\nLength: 1560\\nIndexing with a Python datetime.time object will extract values at those times:\\nIn [48]: from datetime import time\\nIn [49]: ts[time(10, 0)]\\nOut[49]: \\n2012-06-01 10:00:00      30\\n2012-06-04 10:00:00     420\\n2012-06-05 10:00:00     810\\n2012-06-06 10:00:00    1200\\nUnder the hood, this uses an instance method at_time (available on individual time\\nseries and DataFrame objects alike):\\nIn [50]: ts.at_time(time(10, 0))\\nOut[50]: \\n2012-06-01 10:00:00      30\\n2012-06-04 10:00:00     420\\n2012-06-05 10:00:00     810\\n2012-06-06 10:00:00    1200'),\n",
       " Document(metadata={}, page_content='series and DataFrame objects alike):\\nIn [50]: ts.at_time(time(10, 0))\\nOut[50]: \\n2012-06-01 10:00:00      30\\n2012-06-04 10:00:00     420\\n2012-06-05 10:00:00     810\\n2012-06-06 10:00:00    1200\\nYou can select values between two times using the related between_time method:\\nIn [51]: ts.between_time(time(10, 0), time(10, 1))\\nOut[51]: \\n2012-06-01 10:00:00      30\\n2012-06-01 10:01:00      31\\n2012-06-04 10:00:00     420\\n2012-06-04 10:01:00     421\\n2012-06-05 10:00:00     810\\n2012-06-05 10:01:00     811\\n2012-06-06 10:00:00    1200\\n2012-06-06 10:01:00    1201\\nAs mentioned above, it might be the case that no data actually fall exactly at a time like\\n10 AM, but you might want to know the last known value at 10 AM:\\n# Set most of the time series randomly to NA\\nIn [53]: indexer = np.sort(np.random.permutation(len(ts))[700:])\\nData Munging Topics | 335\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"In [54]: irr_ts = ts.copy()\\nIn [55]: irr_ts[indexer] = np.nan\\nIn [56]: irr_ts['2012-06-01 09:50':'2012-06-01 10:00']\\nOut[56]: \\n2012-06-01 09:50:00    20\\n2012-06-01 09:51:00   NaN\\n2012-06-01 09:52:00    22\\n2012-06-01 09:53:00    23\\n2012-06-01 09:54:00   NaN\\n2012-06-01 09:55:00    25\\n2012-06-01 09:56:00   NaN\\n2012-06-01 09:57:00   NaN\\n2012-06-01 09:58:00   NaN\\n2012-06-01 09:59:00   NaN\\n2012-06-01 10:00:00   NaN\\nBy passing an array of timestamps to the asof method, you will obtain an array of the\\nlast valid (non-NA) values at or before each timestamp. So we construct a date range\\nat 10 AM for each day and pass that to asof:\\nIn [57]: selection = pd.date_range('2012-06-01 10:00', periods=4, freq='B')\\nIn [58]: irr_ts.asof(selection)\\nOut[58]: \\n2012-06-01 10:00:00      25\\n2012-06-04 10:00:00     420\\n2012-06-05 10:00:00     810\\n2012-06-06 10:00:00    1197\\nFreq: B\\nSplicing Together Data Sources\\nIn Chapter 7, I described a number of strategies for merging together two related data\"),\n",
       " Document(metadata={}, page_content=\"2012-06-05 10:00:00     810\\n2012-06-06 10:00:00    1197\\nFreq: B\\nSplicing Together Data Sources\\nIn Chapter 7, I described a number of strategies for merging together two related data\\nsets. In a financial or economic context, there are a few widely occurring use cases:\\n• Switching from one data source (a time series or collection of time series) to another\\nat a specific point in time\\n• “Patching” missing values in a time series at the beginning, middle, or end using\\nanother time series\\n• Completely replacing the data for a subset of symbols (countries, asset tickers, and\\nso on)\\nIn the first case, switching from one set of time series to another at a specific instant, it\\nis a matter of splicing together two TimeSeries or DataFrame objects using pandas.con\\ncat:\\nIn [59]: data1 = DataFrame(np.ones((6, 3), dtype=float),\\n   ....:                   columns=['a', 'b', 'c'],\\n   ....:                   index=pd.date_range('6/12/2012', periods=6))\"),\n",
       " Document(metadata={}, page_content=\"cat:\\nIn [59]: data1 = DataFrame(np.ones((6, 3), dtype=float),\\n   ....:                   columns=['a', 'b', 'c'],\\n   ....:                   index=pd.date_range('6/12/2012', periods=6))\\n336 | Chapter 11: \\u2002Financial and Economic Data Applications\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"In [60]: data2 = DataFrame(np.ones((6, 3), dtype=float) * 2,\\n   ....:                   columns=['a', 'b', 'c'],\\n   ....:                   index=pd.date_range('6/13/2012', periods=6))\\nIn [61]: spliced = pd.concat([data1.ix[:'2012-06-14'], data2.ix['2012-06-15':]])\\nIn [62]: spliced\\nOut[62]: \\n            a  b  c\\n2012-06-12  1  1  1\\n2012-06-13  1  1  1\\n2012-06-14  1  1  1\\n2012-06-15  2  2  2\\n2012-06-16  2  2  2\\n2012-06-17  2  2  2\\n2012-06-18  2  2  2\\nSuppose in a similar example that data1 was missing a time series present in data2:\\nIn [63]: data2 = DataFrame(np.ones((6, 4), dtype=float) * 2,\\n   ....:                   columns=['a', 'b', 'c', 'd'],\\n   ....:                   index=pd.date_range('6/13/2012', periods=6))\\nIn [64]: spliced = pd.concat([data1.ix[:'2012-06-14'], data2.ix['2012-06-15':]])\\nIn [65]: spliced\\nOut[65]: \\n            a  b  c   d\\n2012-06-12  1  1  1 NaN\\n2012-06-13  1  1  1 NaN\\n2012-06-14  1  1  1 NaN\\n2012-06-15  2  2  2   2\\n2012-06-16  2  2  2   2\"),\n",
       " Document(metadata={}, page_content=\"In [65]: spliced\\nOut[65]: \\n            a  b  c   d\\n2012-06-12  1  1  1 NaN\\n2012-06-13  1  1  1 NaN\\n2012-06-14  1  1  1 NaN\\n2012-06-15  2  2  2   2\\n2012-06-16  2  2  2   2\\n2012-06-17  2  2  2   2\\n2012-06-18  2  2  2   2\\nUsing combine_first, you can bring in data from before the splice point to extend the\\nhistory for 'd' item:\\nIn [66]: spliced_filled = spliced.combine_first(data2)\\nIn [67]: spliced_filled\\nOut[67]: \\n            a  b  c   d\\n2012-06-12  1  1  1 NaN\\n2012-06-13  1  1  1   2\\n2012-06-14  1  1  1   2\\n2012-06-15  2  2  2   2\\n2012-06-16  2  2  2   2\\n2012-06-17  2  2  2   2\\n2012-06-18  2  2  2   2\\nSince data2 does not have any values for 2012-06-12, no values are filled on that day.\\nDataFrame has a related method update for performing in-place updates. You have to\\npass overwrite=False to make it only fill the holes:\\nData Munging Topics | 337\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"In [68]: spliced.update(data2, overwrite=False)\\nIn [69]: spliced\\nOut[69]: \\n            a  b  c   d\\n2012-06-12  1  1  1 NaN\\n2012-06-13  1  1  1   2\\n2012-06-14  1  1  1   2\\n2012-06-15  2  2  2   2\\n2012-06-16  2  2  2   2\\n2012-06-17  2  2  2   2\\n2012-06-18  2  2  2   2\\nTo replace the data for a subset of symbols, you can use any of the above techniques,\\nbut sometimes it’s simpler to just set the columns directly with DataFrame indexing:\\nIn [70]: cp_spliced = spliced.copy()\\nIn [71]: cp_spliced[['a', 'c']] = data1[['a', 'c']]\\nIn [72]: cp_spliced\\nOut[72]: \\n             a  b   c   d\\n2012-06-12   1  1   1 NaN\\n2012-06-13   1  1   1   2\\n2012-06-14   1  1   1   2\\n2012-06-15   1  2   1   2\\n2012-06-16   1  2   1   2\\n2012-06-17   1  2   1   2\\n2012-06-18 NaN  2 NaN   2\\nReturn Indexes and Cumulative Returns\\nIn a financial context, returns usually refer to percent changes in the price of an asset.\\nLet’s consider price data for Apple in 2011 and 2012:\\nIn [73]: import pandas.io.data as web\"),\n",
       " Document(metadata={}, page_content=\"In a financial context, returns usually refer to percent changes in the price of an asset.\\nLet’s consider price data for Apple in 2011 and 2012:\\nIn [73]: import pandas.io.data as web\\nIn [74]: price = web.get_data_yahoo('AAPL', '2011-01-01')['Adj Close']\\nIn [75]: price[-5:]\\nOut[75]: \\nDate\\n2012-07-23    603.83\\n2012-07-24    600.92\\n2012-07-25    574.97\\n2012-07-26    574.88\\n2012-07-27    585.16\\nName: Adj Close\\nFor Apple, which has no dividends, computing the cumulative percent return between\\ntwo points in time requires computing only the percent change in the price:\\nIn [76]: price['2011-10-03'] / price['2011-3-01'] - 1\\nOut[76]: 0.072399874037388123\\n338 | Chapter 11: \\u2002Financial and Economic Data Applications\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content='For other stocks with dividend payouts, computing how much money you make from\\nholding a stock can be more complicated. The adjusted close values used here have\\nbeen adjusted for splits and dividends, however. In all cases, it’s quite common to derive\\na return index, which is a time series indicating the value of a unit investment (one\\ndollar, say). Many assumptions can underlie the return index; for example, some will\\nchoose to reinvest profit and others not. In the case of Apple, we can compute a simple\\nreturn index using cumprod:\\nIn [77]: returns = price.pct_change()\\nIn [78]: ret_index = (1 + returns).cumprod()\\nIn [79]: ret_index[0] = 1  # Set first value to 1\\nIn [80]: ret_index\\nOut[80]: \\nDate\\n2011-01-03    1.000000\\n2011-01-04    1.005219\\n2011-01-05    1.013442\\n2011-01-06    1.012623\\n...\\n2012-07-24    1.823346\\n2012-07-25    1.744607\\n2012-07-26    1.744334\\n2012-07-27    1.775526\\nLength: 396\\nWith a return index in hand, computing cumulative returns at a particular resolution'),\n",
       " Document(metadata={}, page_content=\"...\\n2012-07-24    1.823346\\n2012-07-25    1.744607\\n2012-07-26    1.744334\\n2012-07-27    1.775526\\nLength: 396\\nWith a return index in hand, computing cumulative returns at a particular resolution\\nis simple:\\nIn [81]: m_returns = ret_index.resample('BM', how='last').pct_change()\\nIn [82]: m_returns['2012']\\nOut[82]: \\nDate\\n2012-01-31    0.127111\\n2012-02-29    0.188311\\n2012-03-30    0.105284\\n2012-04-30   -0.025969\\n2012-05-31   -0.010702\\n2012-06-29    0.010853\\n2012-07-31    0.001986\\nFreq: BM\\nOf course, in this simple case (no dividends or other adjustments to take into account)\\nthese could have been computed from the daily percent changed by resampling with\\naggregation (here, to periods):\\nIn [83]: m_rets = (1 + returns).resample('M', how='prod', kind='period') - 1\\nIn [84]: m_rets['2012']\\nOut[84]: \\nDate\\nData Munging Topics | 339\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"2012-01    0.127111\\n2012-02    0.188311\\n2012-03    0.105284\\n2012-04   -0.025969\\n2012-05   -0.010702\\n2012-06    0.010853\\n2012-07    0.001986\\nFreq: M\\nIf you had dividend dates and percentages, including them in the total return per day\\nwould look like:\\nreturns[dividend_dates] += dividend_pcts\\nGroup Transforms and Analysis\\nIn Chapter 9, you learned the basics of computing group statistics and applying your\\nown transformations to groups in a dataset.\\nLet’s consider a collection of hypothetical stock portfolios. I first randomly generate a\\nbroad universe of 2000 tickers:\\nimport random; random.seed(0)\\nimport string\\nN = 1000\\ndef rands(n):\\n    choices = string.ascii_uppercase\\n    return ''.join([random.choice(choices) for _ in xrange(n)])\\ntickers = np.array([rands(5) for _ in xrange(N)])\\nI then create a DataFrame containing 3 columns representing hypothetical, but random\\nportfolios for a subset of tickers:\\nM = 500\\ndf = DataFrame({'Momentum' : np.random.randn(M) / 200 + 0.03,\"),\n",
       " Document(metadata={}, page_content=\"I then create a DataFrame containing 3 columns representing hypothetical, but random\\nportfolios for a subset of tickers:\\nM = 500\\ndf = DataFrame({'Momentum' : np.random.randn(M) / 200 + 0.03,\\n                'Value' : np.random.randn(M) / 200 + 0.08,\\n                'ShortInterest' : np.random.randn(M) / 200 - 0.02},\\n                index=tickers[:M])\\nNext, let’s create a random industry classification for the tickers. To keep things simple,\\nI’ll just keep it to 2 industries, storing the mapping in a Series:\\nind_names = np.array(['FINANCIAL', 'TECH'])\\nsampler = np.random.randint(0, len(ind_names), N)\\nindustries = Series(ind_names[sampler], index=tickers,\\n                    name='industry')\\nNow we can group by industries and carry out group aggregation and transformations:\\nIn [90]: by_industry = df.groupby(industries)\\nIn [91]: by_industry.mean()\\nOut[91]: \\n           Momentum  ShortInterest     Value\\n340 | Chapter 11: \\u2002Financial and Economic Data Applications\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content='industry                                    \\nFINANCIAL  0.029485      -0.020739  0.079929\\nTECH       0.030407      -0.019609  0.080113\\nIn [92]: by_industry.describe()\\nOut[92]: \\n                   Momentum  ShortInterest       Value\\nindustry                                              \\nFINANCIAL count  246.000000     246.000000  246.000000\\n          mean     0.029485      -0.020739    0.079929\\n          std      0.004802       0.004986    0.004548\\n          min      0.017210      -0.036997    0.067025\\n          25%      0.026263      -0.024138    0.076638\\n          50%      0.029261      -0.020833    0.079804\\n          75%      0.032806      -0.017345    0.082718\\n          max      0.045884      -0.006322    0.093334\\nTECH      count  254.000000     254.000000  254.000000\\n          mean     0.030407      -0.019609    0.080113\\n          std      0.005303       0.005074    0.004886\\n          min      0.016778      -0.032682    0.065253'),\n",
       " Document(metadata={}, page_content=\"mean     0.030407      -0.019609    0.080113\\n          std      0.005303       0.005074    0.004886\\n          min      0.016778      -0.032682    0.065253\\n          25%      0.026456      -0.022779    0.076737\\n          50%      0.030650      -0.019829    0.080296\\n          75%      0.033602      -0.016923    0.083353\\n          max      0.049638      -0.003698    0.093081\\nBy defining transformation functions, it’s easy to transform these portfolios by industry.\\nFor example, standardizing within industry is widely used in equity portfolio construc-\\ntion:\\n# Within-Industry Standardize\\ndef zscore(group):\\n    return (group - group.mean()) / group.std()\\ndf_stand = by_industry.apply(zscore)\\nYou can verify that each industry has mean 0 and standard deviation 1:\\nIn [94]: df_stand.groupby(industries).agg(['mean', 'std'])\\nOut[94]: \\n           Momentum       ShortInterest       Value     \\n               mean  std           mean  std   mean  std\"),\n",
       " Document(metadata={}, page_content=\"In [94]: df_stand.groupby(industries).agg(['mean', 'std'])\\nOut[94]: \\n           Momentum       ShortInterest       Value     \\n               mean  std           mean  std   mean  std\\nindustry                                                \\nFINANCIAL         0    1              0    1      0    1\\nTECH             -0    1             -0    1     -0    1\\nOther, built-in kinds of transformations, like rank, can be used more concisely:\\n# Within-industry rank descending\\nIn [95]: ind_rank = by_industry.rank(ascending=False)\\nIn [96]: ind_rank.groupby(industries).agg(['min', 'max'])\\nOut[96]: \\n           Momentum       ShortInterest       Value     \\n                min  max            min  max    min  max\\nindustry                                                \\nFINANCIAL         1  246              1  246      1  246\\nTECH              1  254              1  254      1  254\\nGroup Transforms and Analysis | 341\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"In quantitative equity, “rank and standardize” is a common sequence of transforms.\\nYou could do this by chaining together rank and zscore like so:\\n# Industry rank and standardize\\nIn [97]: by_industry.apply(lambda x: zscore(x.rank()))\\nOut[97]: \\n<class 'pandas.core.frame.DataFrame'>\\nIndex: 500 entries, VTKGN to PTDQE\\nData columns:\\nMomentum         500  non-null values\\nShortInterest    500  non-null values\\nValue            500  non-null values\\ndtypes: float64(3)\\nGroup Factor Exposures\\nFactor analysis is a technique in quantitative portfolio management. Portfolio holdings\\nand performance (profit and less) are decomposed using one or more factors (risk fac-\\ntors are one example) represented as a portfolio of weights. For example, a stock price’s\\nco-movement with a benchmark (like S&P 500 index) is known as its beta, a common\\nrisk factor. Let’s consider a contrived example of a portfolio constructed from 3 ran-\\ndomly-generated factors (usually called the factor loadings) and some weights:\"),\n",
       " Document(metadata={}, page_content=\"risk factor. Let’s consider a contrived example of a portfolio constructed from 3 ran-\\ndomly-generated factors (usually called the factor loadings) and some weights:\\nfrom numpy.random import rand\\nfac1, fac2, fac3 = np.random.rand(3, 1000)\\nticker_subset = tickers.take(np.random.permutation(N)[:1000])\\n# Weighted sum of factors plus noise\\nport = Series(0.7 * fac1 - 1.2 * fac2 + 0.3 * fac3 + rand(1000),\\n              index=ticker_subset)\\nfactors = DataFrame({'f1': fac1, 'f2': fac2, 'f3': fac3},\\n                    index=ticker_subset)\\nVector correlations between each factor and the portfolio may not indicate too much:\\nIn [99]: factors.corrwith(port)\\nOut[99]: \\nf1    0.402377\\nf2   -0.680980\\nf3    0.168083\\nThe standard way to compute the factor exposures is by least squares regression; using\\npandas.ols with factors as the explanatory variables we can compute exposures over\\nthe entire set of tickers:\\nIn [100]: pd.ols(y=port, x=factors).beta\\nOut[100]: \\nf1           0.761789\"),\n",
       " Document(metadata={}, page_content='pandas.ols with factors as the explanatory variables we can compute exposures over\\nthe entire set of tickers:\\nIn [100]: pd.ols(y=port, x=factors).beta\\nOut[100]: \\nf1           0.761789\\nf2          -1.208760\\nf3           0.289865\\nintercept    0.484477\\n342 | Chapter 11: \\u2002Financial and Economic Data Applications\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='As you can see, the original factor weights can nearly be recovered since there was not\\ntoo much additional random noise added to the portfolio. Using groupby you can com-\\npute exposures industry by industry. To do so, write a function like so:\\ndef beta_exposure(chunk, factors=None):\\n    return pd.ols(y=chunk, x=factors).beta\\nThen, group by industries and apply that function, passing the DataFrame of factor\\nloadings:\\nIn [102]: by_ind = port.groupby(industries)\\nIn [103]: exposures = by_ind.apply(beta_exposure, factors=factors)\\nIn [104]: exposures.unstack()\\nOut[104]: \\n                 f1        f2        f3  intercept\\nindustry                                          \\nFINANCIAL  0.790329 -1.182970  0.275624   0.455569\\nTECH       0.740857 -1.232882  0.303811   0.508188\\nDecile and Quartile Analysis\\nAnalyzing data based on sample quantiles is another important tool for financial ana-\\nlysts. For example, the performance of a stock portfolio could be broken down into'),\n",
       " Document(metadata={}, page_content=\"Decile and Quartile Analysis\\nAnalyzing data based on sample quantiles is another important tool for financial ana-\\nlysts. For example, the performance of a stock portfolio could be broken down into\\nquartiles (four equal-sized chunks) based on each stock’s price-to-earnings. Using pan\\ndas.qcut combined with groupby makes quantile analysis reasonably straightforward.\\nAs an example, let’s consider a simple trend following or momentum strategy trading\\nthe S&P 500 index via the SPY exchange-traded fund. You can download the price\\nhistory from Yahoo! Finance:\\nIn [105]: import pandas.io.data as web\\nIn [106]: data = web.get_data_yahoo('SPY', '2006-01-01')\\nIn [107]: data\\nOut[107]: \\n<class 'pandas.core.frame.DataFrame'>\\nDatetimeIndex: 1655 entries, 2006-01-03 00:00:00 to 2012-07-27 00:00:00\\nData columns:\\nOpen         1655  non-null values\\nHigh         1655  non-null values\\nLow          1655  non-null values\\nClose        1655  non-null values\\nVolume       1655  non-null values\"),\n",
       " Document(metadata={}, page_content=\"Data columns:\\nOpen         1655  non-null values\\nHigh         1655  non-null values\\nLow          1655  non-null values\\nClose        1655  non-null values\\nVolume       1655  non-null values\\nAdj Close    1655  non-null values\\ndtypes: float64(5), int64(1)\\nNow, we’ll compute daily returns and a function for transforming the returns into a\\ntrend signal formed from a lagged moving sum:\\npx = data['Adj Close']\\nreturns = px.pct_change()\\nGroup Transforms and Analysis | 343\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"def to_index(rets):\\n    index = (1 + rets).cumprod()\\n    first_loc = max(index.notnull().argmax() - 1, 0)\\n    index.values[first_loc] = 1\\n    return index\\ndef trend_signal(rets, lookback, lag):\\n    signal = pd.rolling_sum(rets, lookback, min_periods=lookback - 5)\\n    return signal.shift(lag)\\nUsing this function, we can (naively) create and test a trading strategy that trades this\\nmomentum signal every Friday:\\nIn [109]: signal = trend_signal(returns, 100, 3)\\nIn [110]: trade_friday = signal.resample('W-FRI').resample('B', fill_method='ffill')\\nIn [111]: trade_rets = trade_friday.shift(1) * returns\\nWe can then convert the strategy returns to a return index and plot them (see Fig-\\nure 11-1):\\nIn [112]: to_index(trade_rets).plot()\\nFigure 11-1. SPY momentum strategy return index\\nSuppose you wanted to decompose the strategy performance into more and less volatile\\nperiods of trading. Trailing one-year annualized standard deviation is a simple measure\"),\n",
       " Document(metadata={}, page_content='Suppose you wanted to decompose the strategy performance into more and less volatile\\nperiods of trading. Trailing one-year annualized standard deviation is a simple measure\\nof volatility, and we can compute Sharpe ratios to assess the reward-to-risk ratio in\\nvarious volatility regimes:\\nvol = pd.rolling_std(returns, 250, min_periods=200) * np.sqrt(250)\\n344 | Chapter 11: \\u2002Financial and Economic Data Applications\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"def sharpe(rets, ann=250):\\n    return rets.mean() / rets.std()  * np.sqrt(ann)\\nNow, dividing vol into quartiles with qcut and aggregating with sharpe we obtain:\\nIn [114]: trade_rets.groupby(pd.qcut(vol, 4)).agg(sharpe)\\nOut[114]: \\n[0.0955, 0.16]    0.490051\\n(0.16, 0.188]     0.482788\\n(0.188, 0.231]   -0.731199\\n(0.231, 0.457]    0.570500\\nThese results show that the strategy performed the best during the period when the\\nvolatility was the highest.\\nMore Example Applications\\nHere is a small set of additional examples.\\nSignal Frontier Analysis\\nIn this section, I’ll describe a simplified cross-sectional momentum portfolio and show\\nhow you might explore a grid of model parameterizations. First, I’ll load historical\\nprices for a portfolio of financial and technology stocks:\\nnames = ['AAPL', 'GOOG', 'MSFT', 'DELL', 'GS', 'MS', 'BAC', 'C']\\ndef get_px(stock, start, end):\\n    return web.get_data_yahoo(stock, start, end)['Adj Close']\"),\n",
       " Document(metadata={}, page_content=\"names = ['AAPL', 'GOOG', 'MSFT', 'DELL', 'GS', 'MS', 'BAC', 'C']\\ndef get_px(stock, start, end):\\n    return web.get_data_yahoo(stock, start, end)['Adj Close']\\npx = DataFrame({n: get_px(n, '1/1/2009', '6/1/2012') for n in names})\\nWe can easily plot the cumulative returns of each stock (see Figure 11-2):\\nIn [117]: px = px.asfreq('B').fillna(method='pad')\\nIn [118]: rets = px.pct_change()\\nIn [119]: ((1 + rets).cumprod() - 1).plot()\\nFor the portfolio construction, we’ll compute momentum over a certain lookback, then\\nrank in descending order and standardize:\\ndef calc_mom(price, lookback, lag):\\n    mom_ret = price.shift(lag).pct_change(lookback)\\n    ranks = mom_ret.rank(axis=1, ascending=False)\\n    demeaned = ranks - ranks.mean(axis=1)\\n    return demeaned / demeaned.std(axis=1)\\nWith this transform function in hand, we can set up a strategy backtesting function\\nthat computes a portfolio for a particular lookback and holding period (days between\\ntrading), returning the overall Sharpe ratio:\"),\n",
       " Document(metadata={}, page_content='that computes a portfolio for a particular lookback and holding period (days between\\ntrading), returning the overall Sharpe ratio:\\ncompound = lambda x : (1 + x).prod() - 1\\ndaily_sr = lambda x: x.mean() / x.std()\\nMore Example Applications | 345\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"def strat_sr(prices, lb, hold):\\n    # Compute portfolio weights\\n    freq = '%dB' % hold\\n    port = calc_mom(prices, lb, lag=1)\\n    daily_rets = prices.pct_change()\\n    # Compute portfolio returns\\n    port = port.shift(1).resample(freq, how='first')\\n    returns = daily_rets.resample(freq, how=compound)\\n    port_rets = (port * returns).sum(axis=1)\\n    return daily_sr(port_rets) * np.sqrt(252 / hold)\\nFigure 11-2. Cumulative returns for each of the stocks\\nWhen called with the prices and a parameter combination, this function returns a scalar\\nvalue:\\nIn [122]: strat_sr(px, 70, 30)\\nOut[122]: 0.27421582756800583\\nFrom there, you can evaluate the strat_sr function over a grid of parameters, storing\\nthem as you go in a defaultdict and finally putting the results in a DataFrame:\\nfrom collections import defaultdict\\nlookbacks = range(20, 90, 5)\\nholdings = range(20, 90, 5)\\ndd = defaultdict(dict)\\nfor lb in lookbacks:\\n    for hold in holdings:\\n        dd[lb][hold] = strat_sr(px, lb, hold)\"),\n",
       " Document(metadata={}, page_content='lookbacks = range(20, 90, 5)\\nholdings = range(20, 90, 5)\\ndd = defaultdict(dict)\\nfor lb in lookbacks:\\n    for hold in holdings:\\n        dd[lb][hold] = strat_sr(px, lb, hold)\\n346 | Chapter 11: \\u2002Financial and Economic Data Applications\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"ddf = DataFrame(dd)\\nddf.index.name = 'Holding Period'\\nddf.columns.name = 'Lookback Period'\\nTo visualize the results and get an idea of what’s going on, here is a function that uses\\nmatplotlib to produce a heatmap with some adornments:\\nimport matplotlib.pyplot as plt\\ndef heatmap(df, cmap=plt.cm.gray_r):\\n    fig = plt.figure()\\n    ax = fig.add_subplot(111)\\n    axim = ax.imshow(df.values, cmap=cmap, interpolation='nearest')\\n    ax.set_xlabel(df.columns.name)\\n    ax.set_xticks(np.arange(len(df.columns)))\\n    ax.set_xticklabels(list(df.columns))\\n    ax.set_ylabel(df.index.name)\\n    ax.set_yticks(np.arange(len(df.index)))\\n    ax.set_yticklabels(list(df.index))\\n    plt.colorbar(axim)\\nCalling this function on the backtest results, we get Figure 11-3:\\nIn [125]: heatmap(ddf)\\nFigure 11-3. Heatmap of momentum strategy Sharpe ratio (higher is better) over various lookbacks\\nand holding periods\\nFuture Contract Rolling\"),\n",
       " Document(metadata={}, page_content='In [125]: heatmap(ddf)\\nFigure 11-3. Heatmap of momentum strategy Sharpe ratio (higher is better) over various lookbacks\\nand holding periods\\nFuture Contract Rolling\\nA future is an ubiquitous form of derivative contract; it is an agreement to take delivery\\nof a certain asset (such as oil, gold, or shares of the FTSE 100 index) on a particular\\ndate. In practice, modeling and trading futures contracts on equities, currencies,\\nMore Example Applications | 347\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='commodities, bonds, and other asset classes is complicated by the time-limited nature\\nof each contract. For example, at any given time for a type of future (say silver or copper\\nfutures) multiple contracts with different expiration dates may be traded. In many cases,\\nthe future contract expiring next (the near contract) will be the most liquid (highest\\nvolume and lowest bid-ask spread).\\nFor the purposes of modeling and forecasting, it can be much easier to work with a \\ncontinuous return index indicating the profit and loss associated with always holding\\nthe near contract. Transitioning from an expiring contract to the next (or far) contract\\nis referred to as rolling. Computing a continuous future series from the individual con-\\ntract data is not necessarily a straightforward exercise and typically requires a deeper\\nunderstanding of the market and how the instruments are traded. For example, in\\npractice when and how quickly would you trade out of an expiring contract and into'),\n",
       " Document(metadata={}, page_content=\"understanding of the market and how the instruments are traded. For example, in\\npractice when and how quickly would you trade out of an expiring contract and into\\nthe next contract? Here I describe one such process.\\nFirst, I’ll use scaled prices for the SPY exchange-traded fund as a proxy for the S&P 500\\nindex:\\nIn [127]: import pandas.io.data as web\\n# Approximate price of S&P 500 index\\nIn [128]: px = web.get_data_yahoo('SPY')['Adj Close'] * 10\\nIn [129]: px\\nOut[129]: \\nDate\\n2011-08-01    1261.0\\n2011-08-02    1228.8\\n2011-08-03    1235.5\\n...\\n2012-07-25    1339.6\\n2012-07-26    1361.7\\n2012-07-27    1386.8\\nName: Adj Close, Length: 251\\nNow, a little bit of setup. I put a couple of S&P 500 future contracts and expiry dates\\nin a Series:\\nfrom datetime import datetime\\nexpiry = {'ESU2': datetime(2012, 9, 21),\\n          'ESZ2': datetime(2012, 12, 21)}\\nexpiry = Series(expiry).order()\\nexpiry then looks like:\\nIn [131]: expiry\\nOut[131]: \\nESU2    2012-09-21 00:00:00\\nESZ2    2012-12-21 00:00:00\"),\n",
       " Document(metadata={}, page_content=\"'ESZ2': datetime(2012, 12, 21)}\\nexpiry = Series(expiry).order()\\nexpiry then looks like:\\nIn [131]: expiry\\nOut[131]: \\nESU2    2012-09-21 00:00:00\\nESZ2    2012-12-21 00:00:00\\n348 | Chapter 11: \\u2002Financial and Economic Data Applications\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"Then, I use the Yahoo! Finance prices along with a random walk and some noise to\\nsimulate the two contracts into the future:\\nnp.random.seed(12347)\\nN = 200\\nwalk = (np.random.randint(0, 200, size=N) - 100) * 0.25\\nperturb = (np.random.randint(0, 20, size=N) - 10) * 0.25\\nwalk = walk.cumsum()\\nrng = pd.date_range(px.index[0], periods=len(px) + N, freq='B')\\nnear = np.concatenate([px.values, px.values[-1] + walk])\\nfar = np.concatenate([px.values, px.values[-1] + walk + perturb])\\nprices = DataFrame({'ESU2': near, 'ESZ2': far}, index=rng)\\nprices then has two time series for the contracts that differ from each other by a random\\namount:\\nIn [133]: prices.tail()\\nOut[133]: \\n               ESU2     ESZ2\\n2013-04-16  1416.05  1417.80\\n2013-04-17  1402.30  1404.55\\n2013-04-18  1410.30  1412.05\\n2013-04-19  1426.80  1426.05\\n2013-04-22  1406.80  1404.55\\nOne way to splice time series together into a single continuous series is to construct a\"),\n",
       " Document(metadata={}, page_content=\"2013-04-18  1410.30  1412.05\\n2013-04-19  1426.80  1426.05\\n2013-04-22  1406.80  1404.55\\nOne way to splice time series together into a single continuous series is to construct a\\nweighting matrix. Active contracts would have a weight of 1 until the expiry date ap-\\nproaches. At that point you have to decide on a roll convention. Here is a function that\\ncomputes a weighting matrix with linear decay over a number of periods leading up to\\nexpiry:\\ndef get_roll_weights(start, expiry, items, roll_periods=5):\\n    # start : first date to compute weighting DataFrame\\n    # expiry : Series of ticker -> expiration dates\\n    # items : sequence of contract names\\n    dates = pd.date_range(start, expiry[-1], freq='B')\\n    weights = DataFrame(np.zeros((len(dates), len(items))),\\n                        index=dates, columns=items)\\n    prev_date = weights.index[0]\\n    for i, (item, ex_date) in enumerate(expiry.iteritems()):\\n        if i < len(expiry) - 1:\"),\n",
       " Document(metadata={}, page_content=\"index=dates, columns=items)\\n    prev_date = weights.index[0]\\n    for i, (item, ex_date) in enumerate(expiry.iteritems()):\\n        if i < len(expiry) - 1:\\n            weights.ix[prev_date:ex_date - pd.offsets.BDay(), item] = 1\\n            roll_rng = pd.date_range(end=ex_date - pd.offsets.BDay(),\\n                                     periods=roll_periods + 1, freq='B')\\n            decay_weights = np.linspace(0, 1, roll_periods + 1)\\n            weights.ix[roll_rng, item] = 1 - decay_weights\\n            weights.ix[roll_rng, expiry.index[i + 1]] = decay_weights\\n        else:\\n            weights.ix[prev_date:, item] = 1\\n        prev_date = ex_date\\nMore Example Applications | 349\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"return weights\\nThe weights look like this around the ESU2 expiry:\\nIn [135]: weights = get_roll_weights('6/1/2012', expiry, prices.columns)\\nIn [136]: weights.ix['2012-09-12':'2012-09-21']\\nOut[136]: \\n            ESU2  ESZ2\\n2012-09-12   1.0   0.0\\n2012-09-13   1.0   0.0\\n2012-09-14   0.8   0.2\\n2012-09-17   0.6   0.4\\n2012-09-18   0.4   0.6\\n2012-09-19   0.2   0.8\\n2012-09-20   0.0   1.0\\n2012-09-21   0.0   1.0\\nFinally, the rolled future returns are just a weighted sum of the contract returns:\\nIn [137]: rolled_returns = (prices.pct_change() * weights).sum(1)\\nRolling Correlation and Linear Regression\\nDynamic models play an important role in financial modeling as they can be used to\\nsimulate trading decisions over a historical period. Moving window and exponentially-\\nweighted time series functions are an example of tools that are used for dynamic models.\\nCorrelation is one way to look at the co-movement between the changes in two asset\"),\n",
       " Document(metadata={}, page_content=\"weighted time series functions are an example of tools that are used for dynamic models.\\nCorrelation is one way to look at the co-movement between the changes in two asset\\ntime series. pandas’s rolling_corr function can be called with two return series to\\ncompute the moving window correlation. First, I load some price series from Yahoo!\\nFinance and compute daily returns:\\naapl = web.get_data_yahoo('AAPL', '2000-01-01')['Adj Close']\\nmsft = web.get_data_yahoo('MSFT', '2000-01-01')['Adj Close']\\naapl_rets = aapl.pct_change()\\nmsft_rets = msft.pct_change()\\nThen, I compute and plot the one-year moving correlation (see Figure 11-4):\\nIn [140]: pd.rolling_corr(aapl_rets, msft_rets, 250).plot()\\nOne issue with correlation between two assets is that it does not capture differences in\\nvolatility. Least-squares regression provides another means for modeling the dynamic\\nrelationship between a variable and one or more other predictor variables.\"),\n",
       " Document(metadata={}, page_content=\"volatility. Least-squares regression provides another means for modeling the dynamic\\nrelationship between a variable and one or more other predictor variables.\\nIn [142]: model = pd.ols(y=aapl_rets, x={'MSFT': msft_rets}, window=250)\\nIn [143]: model.beta\\nOut[143]: \\n<class 'pandas.core.frame.DataFrame'>\\nDatetimeIndex: 2913 entries, 2000-12-28 00:00:00 to 2012-07-27 00:00:00\\nData columns:\\n350 | Chapter 11: \\u2002Financial and Economic Data Applications\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"MSFT         2913  non-null values\\nintercept    2913  non-null values\\ndtypes: float64(2)\\nIn [144]: model.beta['MSFT'].plot()\\nFigure 11-4. One-year correlation of Apple with Microsoft\\nFigure 11-5. One-year beta (OLS regression coefficient) of Apple to Microsoft\\npandas’s ols function implements static and dynamic (expanding or rolling window)\\nleast squares regressions. For more sophisticated statistical and econometrics models,\\nsee the statsmodels project (http://statsmodels.sourceforge.net).\\nMore Example Applications | 351\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content='www.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='CHAPTER 12\\nAdvanced NumPy\\nndarray Object Internals\\nThe NumPy ndarray provides a means to interpret a block of homogeneous data (either\\ncontiguous or strided, more on this later) as a multidimensional array object. As you’ve\\nseen, the data type, or dtype, determines how the data is interpreted as being floating\\npoint, integer, boolean, or any of the other types we’ve been looking at.\\nPart of what makes ndarray powerful is that every array object is a strided view on a\\nblock of data. You might wonder, for example, how the array view arr[::2, ::-1] does\\nnot copy any data. Simply put, the ndarray is more than just a chunk of memory and\\na dtype; it also has striding information which enables the array to move through\\nmemory with varying step sizes. More precisely, the ndarray internally consists of the\\nfollowing:\\n• A pointer to data, that is a block of system memory\\n• The data type or dtype\\n• A tuple indicating the array’s shape; For example, a 10 by 5 array would have shape\\n(10, 5)'),\n",
       " Document(metadata={}, page_content='following:\\n• A pointer to data, that is a block of system memory\\n• The data type or dtype\\n• A tuple indicating the array’s shape; For example, a 10 by 5 array would have shape\\n(10, 5)\\nIn [8]: np.ones((10, 5)).shape\\nOut[8]: (10, 5)\\n• A tuple of strides, integers indicating the number of bytes to “step” in order to\\nadvance one element along a dimension; For example, a typical (C order, more on\\nthis later) 3 x 4 x 5 array of float64 (8-byte) values has strides (160, 40, 8)\\nIn [9]: np.ones((3, 4, 5), dtype=np.float64).strides\\nOut[9]: (160, 40, 8)\\nWhile it is rare that a typical NumPy user would be interested in the array strides,\\nthey are the critical ingredient in constructing copyless array views. Strides can\\neven be negative which enables an array to move backward through memory, which\\nwould be the case in a slice like obj[::-1] or obj[:, ::-1].\\n353\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='See Figure 12-1 for a simple mockup the ndarray innards.\\nFigure 12-1. The NumPy ndarray object\\nNumPy dtype Hierarchy\\nYou may occasionally have code which needs to check whether an array contains in-\\ntegers, floating point numbers, strings, or Python objects. Because there are many types\\nof floating point numbers (float16 through float128), checking that the dtype is among\\na list of types would be very verbose. Fortunately, the dtypes have superclasses such as\\nnp.integer and np.floating which can be used in conjunction with the np.issubd\\ntype function:\\nIn [10]: ints = np.ones(10, dtype=np.uint16)\\nIn [11]: floats = np.ones(10, dtype=np.float32)\\nIn [12]: np.issubdtype(ints.dtype, np.integer)\\nOut[12]: True\\nIn [13]: np.issubdtype(floats.dtype, np.floating)\\nOut[13]: True\\nYou can see all of the parent classes of a specific dtype by calling the type’s mro method:\\nIn [14]: np.float64.mro()\\nOut[14]: \\n[numpy.float64,\\n numpy.floating,\\n numpy.inexact,\\n numpy.number,\\n numpy.generic,\\n float,\\n object]'),\n",
       " Document(metadata={}, page_content='In [14]: np.float64.mro()\\nOut[14]: \\n[numpy.float64,\\n numpy.floating,\\n numpy.inexact,\\n numpy.number,\\n numpy.generic,\\n float,\\n object]\\nMost NumPy users will never have to know about this, but it occasionally comes in\\nhandy. See Figure 12-2  for a graph of the dtype hierarchy and parent-subclass\\nrelationships 1.\\n1. Some of the dtypes have trailing underscores in their names. These are there to avoid variable name\\nconflicts between the NumPy-specific types and the Python built-in ones.\\n354 | Chapter 12: \\u2002Advanced NumPy\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Advanced Array Manipulation\\nThere are many ways to work with arrays beyond fancy indexing, slicing, and boolean\\nsubsetting. While much of the heavy lifting for data analysis applications is handled by\\nhigher level functions in pandas, you may at some point need to write a data algorithm\\nthat is not found in one of the existing libraries.\\nReshaping Arrays\\nGiven what we know about NumPy arrays, it should come as little surprise that you\\ncan convert an array from one shape to another without copying any data. To do this,\\npass a tuple indicating the new shape to the reshape array instance method. For exam-\\nple, suppose we had a one-dimensional array of values that we wished to rearrange into\\na matrix:\\nIn [15]: arr = np.arange(8)\\nIn [16]: arr\\nOut[16]: array([0, 1, 2, 3, 4, 5, 6, 7])\\nIn [17]: arr.reshape((4, 2))\\nOut[17]: \\narray([[0, 1],\\n       [2, 3],\\n       [4, 5],\\n       [6, 7]])\\nA multidimensional array can also be reshaped:\\nIn [18]: arr.reshape((4, 2)).reshape((2, 4))\\nOut[18]:'),\n",
       " Document(metadata={}, page_content='Out[17]: \\narray([[0, 1],\\n       [2, 3],\\n       [4, 5],\\n       [6, 7]])\\nA multidimensional array can also be reshaped:\\nIn [18]: arr.reshape((4, 2)).reshape((2, 4))\\nOut[18]: \\nFigure 12-2. The NumPy dtype class hierarchy\\nAdvanced Array Manipulation | 355\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='array([[0, 1, 2, 3],\\n       [4, 5, 6, 7]])\\nOne of the passed shape dimensions can be -1, in which case the value used for that\\ndimension will be inferred from the data:\\nIn [19]: arr = np.arange(15)      In [20]: arr.reshape((5, -1))\\n                                  Out[20]:                     \\n                                  array([[ 0,  1,  2],         \\n                                         [ 3,  4,  5],         \\n                                         [ 6,  7,  8],         \\n                                         [ 9, 10, 11],         \\n                                         [12, 13, 14]])\\nSince an array’s shape attribute is a tuple, it can be passed to reshape, too:\\nIn [21]: other_arr = np.ones((3, 5))\\nIn [22]: other_arr.shape\\nOut[22]: (3, 5)\\nIn [23]: arr.reshape(other_arr.shape)\\nOut[23]: \\narray([[ 0,  1,  2,  3,  4],\\n       [ 5,  6,  7,  8,  9],\\n       [10, 11, 12, 13, 14]])\\nThe opposite operation of reshape from one-dimensional to a higher dimension is typ-'),\n",
       " Document(metadata={}, page_content='Out[23]: \\narray([[ 0,  1,  2,  3,  4],\\n       [ 5,  6,  7,  8,  9],\\n       [10, 11, 12, 13, 14]])\\nThe opposite operation of reshape from one-dimensional to a higher dimension is typ-\\nically known as flattening or raveling:\\nIn [24]: arr = np.arange(15).reshape((5, 3))      In [25]: arr         \\n                                                  Out[25]:             \\n                                                  array([[ 0,  1,  2], \\n                                                         [ 3,  4,  5], \\n                                                         [ 6,  7,  8], \\n                                                         [ 9, 10, 11], \\n                                                         [12, 13, 14]])\\n                                                                       \\nIn [26]: arr.ravel()\\nOut[26]: array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])\\nravel does not produce a copy of the underlying data if it does not have to (more on'),\n",
       " Document(metadata={}, page_content='In [26]: arr.ravel()\\nOut[26]: array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])\\nravel does not produce a copy of the underlying data if it does not have to (more on\\nthis below). The flatten method behaves like ravel except it always returns a copy of\\nthe data:\\nIn [27]: arr.flatten()\\nOut[27]: array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14])\\nThe data can be reshaped or raveled in different orders. This is a slightly nuanced topic\\nfor new NumPy users and is therefore the next subtopic.\\nC versus Fortran Order\\nContrary to some other scientific computing environments like R and MATLAB,\\nNumPy gives you much more control and flexibility over the layout of your data in\\n356 | Chapter 12: \\u2002Advanced NumPy\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"memory. By default, NumPy arrays are created in row major order. Spatially this means\\nthat if you have a two-dimensional array of data, the items in each row of the array are\\nstored in adjacent memory locations. The alternative to row major ordering is column\\nmajor order, which means that (you guessed it) values within each column of data are\\nstored in adjacent memory locations.\\nFor historical reasons, row and column major order are also know as C and Fortran\\norder, respectively. In FORTRAN 77, the language of our forebears, matrices were all\\ncolumn major.\\nFunctions like reshape and ravel, accept an order argument indicating the order to use\\nthe data in the array. This can be 'C' or 'F' in most cases (there are also less commonly-\\nused options 'A' and 'K'; see the NumPy documentation). These are illustrated in\\nFigure 12-3.\\nIn [28]: arr = np.arange(12).reshape((3, 4))\\nIn [29]: arr\\nOut[29]: \\narray([[ 0,  1,  2,  3],\\n       [ 4,  5,  6,  7],\\n       [ 8,  9, 10, 11]])\\nIn [30]: arr.ravel()\"),\n",
       " Document(metadata={}, page_content=\"Figure 12-3.\\nIn [28]: arr = np.arange(12).reshape((3, 4))\\nIn [29]: arr\\nOut[29]: \\narray([[ 0,  1,  2,  3],\\n       [ 4,  5,  6,  7],\\n       [ 8,  9, 10, 11]])\\nIn [30]: arr.ravel()\\nOut[30]: array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\\nIn [31]: arr.ravel('F')\\nOut[31]: array([ 0,  4,  8,  1,  5,  9,  2,  6, 10,  3,  7, 11])\\nReshaping arrays with more than two dimensions can be a bit mind-bending. The key\\ndifference between C and Fortran order is the order in which the dimensions are\\nwalked:\\n• C / row major order: traverse higher dimensions first (e.g. axis 1 before advancing\\non axis 0).\\n• Fortran / column major order:  traverse higher dimensions last (e.g. axis 0 before\\nadvancing on axis 1).\\nConcatenating and Splitting Arrays\\nnumpy.concatenate takes a sequence (tuple, list, etc.) of arrays and joins them together\\nin order along the input axis.\\nIn [32]: arr1 = np.array([[1, 2, 3], [4, 5, 6]])\\nIn [33]: arr2 = np.array([[7, 8, 9], [10, 11, 12]])\"),\n",
       " Document(metadata={}, page_content='in order along the input axis.\\nIn [32]: arr1 = np.array([[1, 2, 3], [4, 5, 6]])\\nIn [33]: arr2 = np.array([[7, 8, 9], [10, 11, 12]])\\nIn [34]: np.concatenate([arr1, arr2], axis=0)\\nOut[34]: \\narray([[ 1,  2,  3],\\n       [ 4,  5,  6],\\nAdvanced Array Manipulation | 357\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='[ 7,  8,  9],\\n       [10, 11, 12]])\\nIn [35]: np.concatenate([arr1, arr2], axis=1)\\nOut[35]: \\narray([[ 1,  2,  3,  7,  8,  9],\\n       [ 4,  5,  6, 10, 11, 12]])\\nFigure 12-3. Reshaping in C (row major) or Fortran (column major) order\\nThere are some convenience functions, like vstack and hstack, for common kinds of\\nconcatenation. The above operations could have been expressed as:\\nIn [36]: np.vstack((arr1, arr2))      In [37]: np.hstack((arr1, arr2)) \\nOut[36]:                              Out[37]:                         \\narray([[ 1,  2,  3],                  array([[ 1,  2,  3,  7,  8,  9], \\n       [ 4,  5,  6],                         [ 4,  5,  6, 10, 11, 12]])\\n       [ 7,  8,  9],                                                   \\n       [10, 11, 12]])\\nsplit, on the other hand, slices apart an array into multiple arrays along an axis:\\nIn [38]: from numpy.random import randn\\nIn [39]: arr = randn(5, 2)      In [40]: arr'),\n",
       " Document(metadata={}, page_content='split, on the other hand, slices apart an array into multiple arrays along an axis:\\nIn [38]: from numpy.random import randn\\nIn [39]: arr = randn(5, 2)      In [40]: arr               \\n                                Out[40]:                   \\n                                array([[ 0.1689,  0.3287], \\n                                       [ 0.4703,  0.8989], \\n                                       [ 0.1535,  0.0243], \\n                                       [-0.2832,  1.1536], \\n                                       [ 0.2707,  0.8075]])\\n                                                           \\nIn [41]: first, second, third = np.split(arr, [1, 3])\\nIn [42]: first\\n358 | Chapter 12: \\u2002Advanced NumPy\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Out[42]: array([[ 0.1689,  0.3287]])\\nIn [43]: second                  In [44]: third             \\nOut[43]:                         Out[44]:                   \\narray([[ 0.4703,  0.8989],       array([[-0.2832,  1.1536], \\n       [ 0.1535,  0.0243]])             [ 0.2707,  0.8075]])\\nSee Table 12-1 for a list of all relevant concatenation and splitting functions, some of\\nwhich are provided only as a convenience of the very general purpose concatenate.\\nTable 12-1. Array concatenation functions\\nFunction Description\\nconcatenate Most general function, concatenates collection of arrays along one axis\\nvstack, row_stack Stack arrays row-wise (along axis 0)\\nhstack Stack arrays column-wise (along axis 1)\\ncolumn_stack Like hstack, but converts 1D arrays to 2D column vectors first\\ndstack Stack arrays “depth\"-wise (along axis 2)\\nsplit Split array at passed locations along a particular axis\\nhsplit / vsplit / dsplit Convenience functions for splitting on axis 0, 1, and 2, respectively.'),\n",
       " Document(metadata={}, page_content='split Split array at passed locations along a particular axis\\nhsplit / vsplit / dsplit Convenience functions for splitting on axis 0, 1, and 2, respectively.\\nStacking helpers: r_ and c_\\nThere are two special objects in the NumPy namespace, r_ and c_, that make stacking\\narrays more concise:\\nIn [45]: arr = np.arange(6)\\nIn [46]: arr1 = arr.reshape((3, 2))\\nIn [47]: arr2 = randn(3, 2)\\nIn [48]: np.r_[arr1, arr2]       In [49]: np.c_[np.r_[arr1, arr2], arr]\\nOut[48]:                         Out[49]:                              \\narray([[ 0.    ,  1.    ],       array([[ 0.    ,  1.    ,  0.    ],   \\n       [ 2.    ,  3.    ],              [ 2.    ,  3.    ,  1.    ],   \\n       [ 4.    ,  5.    ],              [ 4.    ,  5.    ,  2.    ],   \\n       [ 0.7258, -1.5325],              [ 0.7258, -1.5325,  3.    ],   \\n       [-0.4696, -0.2127],              [-0.4696, -0.2127,  4.    ],   \\n       [-0.1072,  1.2871]])             [-0.1072,  1.2871,  5.    ]])'),\n",
       " Document(metadata={}, page_content='[-0.4696, -0.2127],              [-0.4696, -0.2127,  4.    ],   \\n       [-0.1072,  1.2871]])             [-0.1072,  1.2871,  5.    ]])\\nThese additionally can translate slices to arrays:\\nIn [50]: np.c_[1:6, -10:-5]\\nOut[50]: \\narray([[  1, -10],\\n       [  2,  -9],\\n       [  3,  -8],\\n       [  4,  -7],\\n       [  5,  -6]])\\nSee the docstring for more on what you can do with c_ and r_.\\nAdvanced Array Manipulation | 359\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Repeating Elements: Tile and Repeat\\nThe need to replicate or repeat arrays is less common with NumPy than\\nit is with other popular array programming languages like MATLAB.\\nThe main reason for this is that broadcasting fulfills this need better,\\nwhich is the subject of the next section.\\nThe two main tools for repeating or replicating arrays to produce larger arrays are the \\nrepeat and tile functions. repeat replicates each element in an array some number of\\ntimes, producing a larger array:\\nIn [51]: arr = np.arange(3)\\nIn [52]: arr.repeat(3)\\nOut[52]: array([0, 0, 0, 1, 1, 1, 2, 2, 2])\\nBy default, if you pass an integer, each element will be repeated that number of times.\\nIf you pass an array of integers, each element can be repeated a different number of\\ntimes:\\nIn [53]: arr.repeat([2, 3, 4])\\nOut[53]: array([0, 0, 1, 1, 1, 2, 2, 2, 2])\\nMultidimensional arrays can have their elements repeated along a particular axis.\\nIn [54]: arr = randn(2, 2)'),\n",
       " Document(metadata={}, page_content='times:\\nIn [53]: arr.repeat([2, 3, 4])\\nOut[53]: array([0, 0, 1, 1, 1, 2, 2, 2, 2])\\nMultidimensional arrays can have their elements repeated along a particular axis.\\nIn [54]: arr = randn(2, 2)\\nIn [55]: arr                       In [56]: arr.repeat(2, axis=0)\\nOut[55]:                           Out[56]:                      \\narray([[ 0.7157, -0.6387],         array([[ 0.7157, -0.6387],    \\n       [ 0.3626,  0.849 ]])               [ 0.7157, -0.6387],    \\n                                          [ 0.3626,  0.849 ],    \\n                                          [ 0.3626,  0.849 ]])\\nNote that if no axis is passed, the array will be flattened first, which is likely not what\\nyou want. Similarly you can pass an array of integers when repeating a multidimen-\\nsional array to repeat a given slice a different number of times:\\nIn [57]: arr.repeat([2, 3], axis=0)\\nOut[57]: \\narray([[ 0.7157, -0.6387],\\n       [ 0.7157, -0.6387],\\n       [ 0.3626,  0.849 ],\\n       [ 0.3626,  0.849 ],'),\n",
       " Document(metadata={}, page_content='In [57]: arr.repeat([2, 3], axis=0)\\nOut[57]: \\narray([[ 0.7157, -0.6387],\\n       [ 0.7157, -0.6387],\\n       [ 0.3626,  0.849 ],\\n       [ 0.3626,  0.849 ],\\n       [ 0.3626,  0.849 ]])\\nIn [58]: arr.repeat([2, 3], axis=1)\\nOut[58]: \\narray([[ 0.7157,  0.7157, -0.6387, -0.6387, -0.6387],\\n       [ 0.3626,  0.3626,  0.849 ,  0.849 ,  0.849 ]])\\n360 | Chapter 12: \\u2002Advanced NumPy\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='tile, on the other hand, is a shortcut for stacking copies of an array along an axis. You\\ncan visually think about it as like “laying down tiles”:\\nIn [59]: arr\\nOut[59]: \\narray([[ 0.7157, -0.6387],\\n       [ 0.3626,  0.849 ]])\\nIn [60]: np.tile(arr, 2)\\nOut[60]: \\narray([[ 0.7157, -0.6387,  0.7157, -0.6387],\\n       [ 0.3626,  0.849 ,  0.3626,  0.849 ]])\\nThe second argument is the number of tiles; with a scalar, the tiling is made row-by-\\nrow, rather than column by column: The second argument to tile can be a tuple in-\\ndicating the layout of the “tiling”:\\nIn [61]: arr\\nOut[61]: \\narray([[ 0.7157, -0.6387],\\n       [ 0.3626,  0.849 ]])\\nIn [62]: np.tile(arr, (2, 1))      In [63]: np.tile(arr, (3, 2))                \\nOut[62]:                           Out[63]:                                     \\narray([[ 0.7157, -0.6387],         array([[ 0.7157, -0.6387,  0.7157, -0.6387], \\n       [ 0.3626,  0.849 ],                [ 0.3626,  0.849 ,  0.3626,  0.849 ],'),\n",
       " Document(metadata={}, page_content='array([[ 0.7157, -0.6387],         array([[ 0.7157, -0.6387,  0.7157, -0.6387], \\n       [ 0.3626,  0.849 ],                [ 0.3626,  0.849 ,  0.3626,  0.849 ], \\n       [ 0.7157, -0.6387],                [ 0.7157, -0.6387,  0.7157, -0.6387], \\n       [ 0.3626,  0.849 ]])               [ 0.3626,  0.849 ,  0.3626,  0.849 ], \\n                                          [ 0.7157, -0.6387,  0.7157, -0.6387], \\n                                          [ 0.3626,  0.849 ,  0.3626,  0.849 ]])\\nFancy Indexing Equivalents: Take and Put\\nAs you may recall from Chapter 4, one way to get and set subsets of arrays is by \\nfancy indexing using integer arrays:\\nIn [64]: arr = np.arange(10) * 100\\nIn [65]: inds = [7, 1, 2, 6]        In [66]: arr[inds]                  \\n                                    Out[66]: array([700, 100, 200, 600])\\nThere are alternate ndarray methods that are useful in the special case of only making\\na selection on a single axis:\\nIn [67]: arr.take(inds)'),\n",
       " Document(metadata={}, page_content='There are alternate ndarray methods that are useful in the special case of only making\\na selection on a single axis:\\nIn [67]: arr.take(inds)\\nOut[67]: array([700, 100, 200, 600])\\nIn [68]: arr.put(inds, 42)\\nIn [69]: arr\\nOut[69]: array([  0,  42,  42, 300, 400, 500,  42,  42, 800, 900])\\nIn [70]: arr.put(inds, [40, 41, 42, 43])\\nAdvanced Array Manipulation | 361\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='In [71]: arr\\nOut[71]: array([  0,  41,  42, 300, 400, 500,  43,  40, 800, 900])\\nTo use take along other axes, you can pass the axis keyword:\\nIn [72]: inds = [2, 0, 2, 1]\\nIn [73]: arr = randn(2, 4)\\nIn [74]: arr\\nOut[74]: \\narray([[-0.8237,  2.6047, -0.4578, -1.    ],\\n       [ 2.3198, -1.0792,  0.518 ,  0.2527]])\\nIn [75]: arr.take(inds, axis=1)\\nOut[75]: \\narray([[-0.4578, -0.8237, -0.4578,  2.6047],\\n       [ 0.518 ,  2.3198,  0.518 , -1.0792]])\\nput does not accept an axis argument but rather indexes into the flattened (one-di-\\nmensional, C order) version of the array (this could be changed in principle). Thus,\\nwhen you need to set elements using an index array on other axes, you will want to use\\nfancy indexing.\\nAs of this writing, the take and put functions in general have better\\nperformance than their fancy indexing equivalents by a significant mar-\\ngin. I regard this as a “bug” and something to be fixed in NumPy, but'),\n",
       " Document(metadata={}, page_content='performance than their fancy indexing equivalents by a significant mar-\\ngin. I regard this as a “bug” and something to be fixed in NumPy, but\\nit’s something worth keeping in mind if you’re selecting subsets of large\\narrays using integer arrays:\\nIn [76]: arr = randn(1000, 50)\\n# Random sample of 500 rows\\nIn [77]: inds = np.random.permutation(1000)[:500]\\nIn [78]: %timeit arr[inds]\\n1000 loops, best of 3: 356 us per loop\\nIn [79]: %timeit arr.take(inds, axis=0)\\n10000 loops, best of 3: 34 us per loop\\nBroadcasting\\nBroadcasting describes how arithmetic works between arrays of different shapes. It is\\na very powerful feature, but one that can be easily misunderstood, even by experienced\\nusers. The simplest example of broadcasting occurs when combining a scalar value\\nwith an array:\\nIn [80]: arr = np.arange(5)\\nIn [81]: arr                           In [82]: arr * 4                    \\nOut[81]: array([0, 1, 2, 3, 4])        Out[82]: array([ 0,  4,  8, 12, 16])\\n362 | Chapter 12: \\u2002Advanced NumPy'),\n",
       " Document(metadata={}, page_content='In [81]: arr                           In [82]: arr * 4                    \\nOut[81]: array([0, 1, 2, 3, 4])        Out[82]: array([ 0,  4,  8, 12, 16])\\n362 | Chapter 12: \\u2002Advanced NumPy\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Here we say that the scalar value 4 has been broadcast to all of the other elements in\\nthe multiplication operation.\\nFor example, we can demean each column of an array by subtracting the column means.\\nIn this case, it is very simple:\\nIn [83]: arr = randn(4, 3)\\nIn [84]: arr.mean(0)\\nOut[84]: array([ 0.1321,  0.552 ,  0.8571])\\nIn [85]: demeaned = arr - arr.mean(0)\\nIn [86]: demeaned                           In [87]: demeaned.mean(0)      \\nOut[86]:                                    Out[87]: array([ 0., -0., -0.])\\narray([[ 0.1718, -0.1972, -1.3669],                                        \\n       [-0.1292,  1.6529, -0.3429],                                        \\n       [-0.2891, -0.0435,  1.2322],                                        \\n       [ 0.2465, -1.4122,  0.4776]])\\nSee Figure 12-4 for an illustration of this operation. Demeaning the rows as a broadcast\\noperation requires a bit more care. Fortunately, broadcasting potentially lower dimen-'),\n",
       " Document(metadata={}, page_content='See Figure 12-4 for an illustration of this operation. Demeaning the rows as a broadcast\\noperation requires a bit more care. Fortunately, broadcasting potentially lower dimen-\\nsional values across any dimension of an array (like subtracting the row means from\\neach column of a two-dimensional array) is possible as long as you follow the rules.\\nThis brings us to:\\nFigure 12-4. Broadcasting over axis 0 with a 1D array\\nThe Broadcasting Ru\\nTwo arrays are compatible for broadcasting if for each trailing dimension (that is, start-\\ning from the end), the axis lengths match or if either of the lengths is 1. Broadcasting\\nis then performed over the missing and / or length 1 dimensions.\\nEven as an experienced NumPy user, I often must stop to draw pictures and think about\\nthe broadcasting rule. Consider the last example and suppose we wished instead to\\nsubtract the mean value from each row. Since arr.mean(0) has length 3, it is compatible\\nBroadcasting | 363\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='for broadcasting across axis 0 because the trailing dimension in arr is 3 and therefore\\nmatches. According to the rules, to subtract over axis 1 (that is, subtract the row mean\\nfrom each row), the smaller array must have shape (4, 1):\\nIn [88]: arr\\nOut[88]: \\narray([[ 0.3039,  0.3548, -0.5097],\\n       [ 0.0029,  2.2049,  0.5142],\\n       [-0.1571,  0.5085,  2.0893],\\n       [ 0.3786, -0.8602,  1.3347]])\\nIn [89]: row_means = arr.mean(1)        In [90]: row_means.reshape((4, 1))\\n                                        Out[90]:                          \\n                                        array([[ 0.0496],                 \\n                                               [ 0.9073],                 \\n                                               [ 0.8136],                 \\n                                               [ 0.2844]])                \\n                                                                          \\nIn [91]: demeaned = arr - row_means.reshape((4, 1))'),\n",
       " Document(metadata={}, page_content='In [91]: demeaned = arr - row_means.reshape((4, 1))\\nIn [92]: demeaned.mean(1)\\nOut[92]: array([ 0.,  0.,  0.,  0.])\\nHas your head exploded yet? See Figure 12-5 for an illustration of this operation.\\nFigure 12-5. Broadcasting over axis 1 of a 2D array\\nSee Figure 12-6 for another illustration, this time subtracting a two-dimensional array\\nfrom a three-dimensional one across axis 0.\\nBroadcasting Over Other Axes\\nBroadcasting with higher dimensional arrays can seem even more mind-bending, but\\nit is really a matter of following the rules. If you don’t, you’ll get an error like this:\\nIn [93]: arr - arr.mean(1)\\n---------------------------------------------------------------------------\\nValueError                                Traceback (most recent call last)\\n<ipython-input-93-7b87b85a20b2> in <module>()\\n364 | Chapter 12: \\u2002Advanced NumPy\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='----> 1 arr - arr.mean(1)\\nValueError: operands could not be broadcast together with shapes (4,3) (4)\\nFigure 12-6. Broadcasting over axis 0 of a 3D array\\nIt’s quite common to want to perform an arithmetic operation with a lower dimensional\\narray across axes other than axis 0. According to the broadcasting rule, the “broadcast\\ndimensions” must be 1 in the smaller array. In the example of row demeaning above\\nthis meant reshaping the row means to be shape (4, 1) instead of (4,):\\nIn [94]: arr - arr.mean(1).reshape((4, 1))\\nOut[94]: \\narray([[ 0.2542,  0.3051, -0.5594],\\n       [-0.9044,  1.2976, -0.3931],\\n       [-0.9707, -0.3051,  1.2757],\\n       [ 0.0942, -1.1446,  1.0503]])\\nIn the three-dimensional case, broadcasting over any of the three dimensions is only a\\nmatter of reshaping the data to be shape-compatible. See Figure 12-7 for a nice visual-\\nization of the shapes required to broadcast over each axis of a three-dimensional array.'),\n",
       " Document(metadata={}, page_content='matter of reshaping the data to be shape-compatible. See Figure 12-7 for a nice visual-\\nization of the shapes required to broadcast over each axis of a three-dimensional array.\\nA very common problem, therefore, is needing to add a new axis with length 1 specif-\\nically for broadcasting purposes, especially in generic algorithms. Using reshape is one\\noption, but inserting an axis requires constructing a tuple indicating the new shape.\\nThis can often be a tedious exercise. Thus, NumPy arrays offer a special syntax for\\ninserting new axes by indexing. We use the special np.newaxis attribute along with\\n“full” slices to insert the new axis:\\nIn [95]: arr = np.zeros((4, 4))\\nIn [96]: arr_3d = arr[:, np.newaxis, :]      In [97]: arr_3d.shape\\n                                             Out[97]: (4, 1, 4)\\nIn [98]: arr_1d = np.random.normal(size=3)\\nIn [99]: arr_1d[:, np.newaxis]      In [100]: arr_1d[np.newaxis, :]'),\n",
       " Document(metadata={}, page_content='Out[97]: (4, 1, 4)\\nIn [98]: arr_1d = np.random.normal(size=3)\\nIn [99]: arr_1d[:, np.newaxis]      In [100]: arr_1d[np.newaxis, :]               \\nOut[99]:                            Out[100]: array([[-0.3899,  0.396 , -0.1852]])\\nBroadcasting | 365\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='array([[-0.3899],                                                                 \\n       [ 0.396 ],                                                                 \\n       [-0.1852]])\\nFigure 12-7. Compatible 2D array shapes for broadcasting over a 3D array\\nThus, if we had a three-dimensional array and wanted to demean axis 2, say, we would\\nonly need to write:\\nIn [101]: arr = randn(3, 4, 5)\\nIn [102]: depth_means = arr.mean(2)\\nIn [103]: depth_means\\nOut[103]: \\narray([[ 0.1097,  0.3118, -0.5473,  0.2663],\\n       [ 0.1747,  0.1379,  0.1146, -0.4224],\\n       [ 0.0217,  0.3686, -0.0468,  1.3026]])\\nIn [104]: demeaned = arr - depth_means[:, :, np.newaxis]\\nIn [105]: demeaned.mean(2)\\nOut[105]: \\narray([[ 0.,  0., -0.,  0.],\\n       [ 0., -0., -0.,  0.],\\n       [-0., -0.,  0.,  0.]])\\nIf you’re completely confused by this, don’t worry. With practice you will get the hang\\nof it!\\n366 | Chapter 12: \\u2002Advanced NumPy\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Some readers might wonder if there’s a way to generalize demeaning over an axis\\nwithout sacrificing performance. There is, in fact, but it requires some indexing\\ngymnastics:\\ndef demean_axis(arr, axis=0):\\n    means = arr.mean(axis)\\n    # This generalized things like [:, :, np.newaxis] to N dimensions\\n    indexer = [slice(None)] * arr.ndim\\n    indexer[axis] = np.newaxis\\n    return arr - means[indexer]\\nSetting Array Values by Broadcasting\\nThe same broadcasting rule governing arithmetic operations also applies to setting\\nvalues via array indexing. In the simplest case, we can do things like:\\nIn [106]: arr = np.zeros((4, 3))\\nIn [107]: arr[:] = 5        In [108]: arr           \\n                            Out[108]:               \\n                            array([[ 5.,  5.,  5.], \\n                                   [ 5.,  5.,  5.], \\n                                   [ 5.,  5.,  5.], \\n                                   [ 5.,  5.,  5.]])'),\n",
       " Document(metadata={}, page_content='[ 5.,  5.,  5.], \\n                                   [ 5.,  5.,  5.], \\n                                   [ 5.,  5.,  5.]])\\nHowever, if we had a one-dimensional array of values we wanted to set into the columns\\nof the array, we can do that as long as the shape is compatible:\\nIn [109]: col = np.array([1.28, -0.42, 0.44, 1.6])\\nIn [110]: arr[:] = col[:, np.newaxis]       In [111]: arr                 \\n                                            Out[111]:                     \\n                                            array([[ 1.28,  1.28,  1.28], \\n                                                   [-0.42, -0.42, -0.42], \\n                                                   [ 0.44,  0.44,  0.44], \\n                                                   [ 1.6 ,  1.6 ,  1.6 ]])\\n                                                                          \\nIn [112]: arr[:2] = [[-1.37], [0.509]]      In [113]: arr'),\n",
       " Document(metadata={}, page_content='In [112]: arr[:2] = [[-1.37], [0.509]]      In [113]: arr                    \\n                                            Out[113]:                        \\n                                            array([[-1.37 , -1.37 , -1.37 ], \\n                                                   [ 0.509,  0.509,  0.509], \\n                                                   [ 0.44 ,  0.44 ,  0.44 ], \\n                                                   [ 1.6  ,  1.6  ,  1.6  ]])\\nAdvanced ufunc Usage\\nWhile many NumPy users will only make use of the fast element-wise operations pro-\\nvided by the universal functions, there are a number of additional features that occa-\\nsionally can help you write more concise code without loops.\\nAdvanced ufunc Usage | 367\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='ufunc Instance Methods\\nEach of NumPy’s binary ufuncs has special methods for performing certain kinds of\\nspecial vectorized operations. These are summarized in Table 12-2, but I’ll give a few\\nconcrete examples to illustrate how they work.\\nreduce takes a single array and aggregates its values, optionally along an axis, by per-\\nforming a sequence of binary operations. For example, an alternate way to sum ele-\\nments in an array is to use np.add.reduce:\\nIn [114]: arr = np.arange(10)\\nIn [115]: np.add.reduce(arr)\\nOut[115]: 45\\nIn [116]: arr.sum()\\nOut[116]: 45\\nThe starting value (0 for add) depends on the ufunc. If an axis is passed, the reduction\\nis performed along that axis. This allows you to answer certain kinds of questions in a\\nconcise way. As a less trivial example, we can use np.logical_and to check whether the\\nvalues in each row of an array are sorted:\\nIn [118]: arr = randn(5, 5)\\nIn [119]: arr[::2].sort(1) # sort a few rows\\nIn [120]: arr[:, :-1] < arr[:, 1:]\\nOut[120]:'),\n",
       " Document(metadata={}, page_content='values in each row of an array are sorted:\\nIn [118]: arr = randn(5, 5)\\nIn [119]: arr[::2].sort(1) # sort a few rows\\nIn [120]: arr[:, :-1] < arr[:, 1:]\\nOut[120]: \\narray([[ True,  True,  True,  True],\\n       [False,  True, False, False],\\n       [ True,  True,  True,  True],\\n       [ True, False,  True,  True],\\n       [ True,  True,  True,  True]], dtype=bool)\\nIn [121]: np.logical_and.reduce(arr[:, :-1] < arr[:, 1:], axis=1)\\nOut[121]: array([ True, False,  True, False,  True], dtype=bool)\\nOf course, logical_and.reduce is equivalent to the all method.\\naccumulate is related to reduce like cumsum is related to sum. It produces an array of the\\nsame size with the intermediate “accumulated” values:\\nIn [122]: arr = np.arange(15).reshape((3, 5))\\nIn [123]: np.add.accumulate(arr, axis=1)\\nOut[123]: \\narray([[ 0,  1,  3,  6, 10],\\n       [ 5, 11, 18, 26, 35],\\n       [10, 21, 33, 46, 60]])\\nouter performs a pairwise cross-product between two arrays:\\nIn [124]: arr = np.arange(3).repeat([1, 2, 2])'),\n",
       " Document(metadata={}, page_content='array([[ 0,  1,  3,  6, 10],\\n       [ 5, 11, 18, 26, 35],\\n       [10, 21, 33, 46, 60]])\\nouter performs a pairwise cross-product between two arrays:\\nIn [124]: arr = np.arange(3).repeat([1, 2, 2])\\n368 | Chapter 12: \\u2002Advanced NumPy\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='In [125]: arr\\nOut[125]: array([0, 1, 1, 2, 2])\\nIn [126]: np.multiply.outer(arr, np.arange(5))\\nOut[126]: \\narray([[0, 0, 0, 0, 0],\\n       [0, 1, 2, 3, 4],\\n       [0, 1, 2, 3, 4],\\n       [0, 2, 4, 6, 8],\\n       [0, 2, 4, 6, 8]])\\nThe output of outer will have a dimension that is the sum of the dimensions of the\\ninputs:\\nIn [127]: result = np.subtract.outer(randn(3, 4), randn(5))\\nIn [128]: result.shape\\nOut[128]: (3, 4, 5)\\nThe last method, reduceat, performs a “local reduce”, in essence an array groupby op-\\neration in which slices of the array are aggregated together. While it’s less flexible than\\nthe GroupBy capabilities in pandas, it can be very fast and powerful in the right cir-\\ncumstances. It accepts a sequence of “bin edges” which indicate how to split and ag-\\ngregate the values:\\nIn [129]: arr = np.arange(10)\\nIn [130]: np.add.reduceat(arr, [0, 5, 8])\\nOut[130]: array([10, 18, 17])\\nThe results are the reductions (here, sums) performed over arr[0:5], arr[5:8], and'),\n",
       " Document(metadata={}, page_content='In [129]: arr = np.arange(10)\\nIn [130]: np.add.reduceat(arr, [0, 5, 8])\\nOut[130]: array([10, 18, 17])\\nThe results are the reductions (here, sums) performed over arr[0:5], arr[5:8], and\\narr[8:]. Like the other methods, you can pass an axis argument:\\nIn [131]: arr = np.multiply.outer(np.arange(4), np.arange(5))\\nIn [132]: arr                      In [133]: np.add.reduceat(arr, [0, 2, 4], axis=1)\\nOut[132]:                          Out[133]:                                        \\narray([[ 0,  0,  0,  0,  0],       array([[ 0,  0,  0],                             \\n       [ 0,  1,  2,  3,  4],              [ 1,  5,  4],                             \\n       [ 0,  2,  4,  6,  8],              [ 2, 10,  8],                             \\n       [ 0,  3,  6,  9, 12]])             [ 3, 15, 12]])\\nTable 12-2. ufunc methods\\nMethod Description\\nreduce(x) Aggregate values by successive applications of the operation\\naccumulate(x) Aggregate values, preserving all partial aggregates'),\n",
       " Document(metadata={}, page_content='Table 12-2. ufunc methods\\nMethod Description\\nreduce(x) Aggregate values by successive applications of the operation\\naccumulate(x) Aggregate values, preserving all partial aggregates\\nreduceat(x, bins) “Local” reduce or “group by”. Reduce contiguous slices of data to produce aggregated\\narray.\\nouter(x, y) Apply operation to all pairs of elements in x and y. Result array has shape x.shape +\\ny.shape\\nAdvanced ufunc Usage | 369\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Custom ufuncs\\nThere are a couple facilities for creating your own functions with ufunc-like semantics. \\nnumpy.frompyfunc accepts a Python function along with a specification for the number\\nof inputs and outputs. For example, a simple function that adds element-wise would\\nbe specified as:\\nIn [134]: def add_elements(x, y):\\n   .....:     return x + y\\nIn [135]: add_them = np.frompyfunc(add_elements, 2, 1)\\nIn [136]: add_them(np.arange(8), np.arange(8))\\nOut[136]: array([0, 2, 4, 6, 8, 10, 12, 14], dtype=object)\\nFunctions created using frompyfunc always return arrays of Python objects which isn’t\\nvery convenient. Fortunately, there is an alternate, but slightly less featureful function \\nnumpy.vectorize that is a bit more intelligent about type inference:\\nIn [137]: add_them = np.vectorize(add_elements, otypes=[np.float64])\\nIn [138]: add_them(np.arange(8), np.arange(8))\\nOut[138]: array([  0.,   2.,   4.,   6.,   8.,  10.,  12.,  14.])'),\n",
       " Document(metadata={}, page_content='In [137]: add_them = np.vectorize(add_elements, otypes=[np.float64])\\nIn [138]: add_them(np.arange(8), np.arange(8))\\nOut[138]: array([  0.,   2.,   4.,   6.,   8.,  10.,  12.,  14.])\\nThese functions provide a way to create ufunc-like functions, but they are very slow\\nbecause they require a Python function call to compute each element, which is a lot\\nslower than NumPy’s C-based ufunc loops:\\nIn [139]: arr = randn(10000)\\nIn [140]: %timeit add_them(arr, arr)\\n100 loops, best of 3: 2.12 ms per loop\\nIn [141]: %timeit np.add(arr, arr)\\n100000 loops, best of 3: 11.6 us per loop\\nThere are a number of projects under way in the scientific Python community to make\\nit easier to define new ufuncs whose performance is closer to that of the built-in ones.\\nStructured and Record Arrays\\nYou may have noticed up until now that ndarray is a homogeneous data container; that\\nis, it represents a block of memory in which each element takes up the same number'),\n",
       " Document(metadata={}, page_content=\"Structured and Record Arrays\\nYou may have noticed up until now that ndarray is a homogeneous data container; that\\nis, it represents a block of memory in which each element takes up the same number\\nof bytes, determined by the dtype. On the surface, this would appear to not allow you\\nto represent heterogeneous or tabular-like data. A structured array is an ndarray in\\nwhich each element can be thought of as representing a struct in C (hence the “struc-\\ntured” name) or a row in a SQL table with multiple named fields:\\nIn [142]: dtype = [('x', np.float64), ('y', np.int32)]\\nIn [143]: sarr = np.array([(1.5, 6), (np.pi, -2)], dtype=dtype)\\n370 | Chapter 12: \\u2002Advanced NumPy\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"In [144]: sarr\\nOut[144]: \\narray([(1.5, 6), (3.141592653589793, -2)], \\n      dtype=[('x', '<f8'), ('y', '<i4')])\\nThere are several ways to specify a structured dtype (see the online NumPy documen-\\ntation). One typical way is as a list of tuples with (field_name, field_data_type). Now,\\nthe elements of the array are tuple-like objects whose elements can be accessed like a\\ndictionary:\\nIn [145]: sarr[0]\\nOut[145]: (1.5, 6)\\nIn [146]: sarr[0]['y']\\nOut[146]: 6\\nThe field names are stored in the dtype.names attribute. On accessing a field on the\\nstructured array, a strided view on the data is returned thus copying nothing:\\nIn [147]: sarr['x']\\nOut[147]: array([ 1.5   ,  3.1416])\\nNested dtypes and Multidimensional Fields\\nWhen specifying a structured dtype, you can additionally pass a shape (as an int or\\ntuple):\\nIn [148]: dtype = [('x', np.int64, 3), ('y', np.int32)]\\nIn [149]: arr = np.zeros(4, dtype=dtype)\\nIn [150]: arr\\nOut[150]:\"),\n",
       " Document(metadata={}, page_content=\"tuple):\\nIn [148]: dtype = [('x', np.int64, 3), ('y', np.int32)]\\nIn [149]: arr = np.zeros(4, dtype=dtype)\\nIn [150]: arr\\nOut[150]: \\narray([([0, 0, 0], 0), ([0, 0, 0], 0), ([0, 0, 0], 0), ([0, 0, 0], 0)], \\n      dtype=[('x', '<i8', (3,)), ('y', '<i4')])\\nIn this case, the x field now refers to an array of length three for each record:\\nIn [151]: arr[0]['x']\\nOut[151]: array([0, 0, 0])\\nConveniently, accessing arr['x'] then returns a two-dimensional array instead of a\\none-dimensional array as in prior examples:\\nIn [152]: arr['x']\\nOut[152]: \\narray([[0, 0, 0],\\n       [0, 0, 0],\\n       [0, 0, 0],\\n       [0, 0, 0]])\\nThis enables you to express more complicated, nested structures as a single block of\\nmemory in an array. Though, since dtypes can be arbitrarily complex, why not nested\\ndtypes? Here is a simple example:\\nStructured and Record Arrays | 371\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"In [153]: dtype = [('x', [('a', 'f8'), ('b', 'f4')]), ('y', np.int32)]\\nIn [154]: data = np.array([((1, 2), 5), ((3, 4), 6)], dtype=dtype)\\nIn [155]: data['x']\\nOut[155]: \\narray([(1.0, 2.0), (3.0, 4.0)], \\n      dtype=[('a', '<f8'), ('b', '<f4')])\\nIn [156]: data['y']\\nOut[156]: array([5, 6], dtype=int32)\\nIn [157]: data['x']['a']\\nOut[157]: array([ 1.,  3.])\\nAs you can see, variable-shape fields and nested records is a very rich feature that can\\nbe the right tool in certain circumstances. A DataFrame from pandas, by contrast, does\\nnot support this feature directly, though it is similar to hierarchical indexing.\\nWhy Use Structured Arrays?\\nCompared with, say, a DataFrame from pandas, NumPy structured arrays are a com-\\nparatively low-level tool. They provide a means to interpreting a block of memory as a\\ntabular structure with arbitrarily complex nested columns. Since each element in the\\narray is represented in memory as a fixed number of bytes, structured arrays provide a\"),\n",
       " Document(metadata={}, page_content='tabular structure with arbitrarily complex nested columns. Since each element in the\\narray is represented in memory as a fixed number of bytes, structured arrays provide a\\nvery fast and efficient way of writing data to and from disk (including memory maps,\\nmore on this later), transporting it over the network, and other such use.\\nAs another common use for structured arrays, writing data files as fixed length record\\nbyte streams is a common way to serialize data in C and C++ code, which is commonly\\nfound in legacy systems in industry. As long as the format of the file is known (the size\\nof each record and the order, byte size, and data type of each element), the data can be\\nread into memory using np.fromfile. Specialized uses like this are beyond the scope of\\nthis book, but it’s worth knowing that such things are possible.\\nStructured Array Manipulations: numpy.lib.recfunctions\\nWhile there is not as much functionality available for structured arrays as for Data-'),\n",
       " Document(metadata={}, page_content='Structured Array Manipulations: numpy.lib.recfunctions\\nWhile there is not as much functionality available for structured arrays as for Data-\\nFrames, the NumPy module numpy.lib.recfunctions has some helpful tools for adding\\nand dropping fields or doing basic join-like operations. The thing to remember with\\nthese tools is that it is typically necessary to create a new array to make any modifica-\\ntions to the dtype (like adding or dropping a column). These functions are left to the\\ninterested reader to explore as I do not use them anywhere in this book.\\n372 | Chapter 12: \\u2002Advanced NumPy\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='More About Sorting\\nLike Python’s built-in list, the ndarray sort instance method is an in-place sort, meaning\\nthat the array contents are rearranged without producing a new array:\\nIn [158]: arr = randn(6)\\nIn [159]: arr.sort()\\nIn [160]: arr\\nOut[160]: array([-1.082 ,  0.3759,  0.8014,  1.1397,  1.2888,  1.8413])\\nWhen sorting arrays in-place, remember that if the array is a view on a different ndarray,\\nthe original array will be modified:\\nIn [161]: arr = randn(3, 5)\\nIn [162]: arr\\nOut[162]: \\narray([[-0.3318, -1.4711,  0.8705, -0.0847, -1.1329],\\n       [-1.0111, -0.3436,  2.1714,  0.1234, -0.0189],\\n       [ 0.1773,  0.7424,  0.8548,  1.038 , -0.329 ]])\\nIn [163]: arr[:, 0].sort()  # Sort first column values in-place\\nIn [164]: arr\\nOut[164]: \\narray([[-1.0111, -1.4711,  0.8705, -0.0847, -1.1329],\\n       [-0.3318, -0.3436,  2.1714,  0.1234, -0.0189],\\n       [ 0.1773,  0.7424,  0.8548,  1.038 , -0.329 ]])\\nOn the other hand, numpy.sort creates a new, sorted copy of an array. Otherwise it'),\n",
       " Document(metadata={}, page_content='[-0.3318, -0.3436,  2.1714,  0.1234, -0.0189],\\n       [ 0.1773,  0.7424,  0.8548,  1.038 , -0.329 ]])\\nOn the other hand, numpy.sort creates a new, sorted copy of an array. Otherwise it\\naccepts the same arguments (such as kind, more on this below) as ndarray.sort:\\nIn [165]: arr = randn(5)\\nIn [166]: arr\\nOut[166]: array([-1.1181, -0.2415, -2.0051,  0.7379, -1.0614])\\nIn [167]: np.sort(arr)\\nOut[167]: array([-2.0051, -1.1181, -1.0614, -0.2415,  0.7379])\\nIn [168]: arr\\nOut[168]: array([-1.1181, -0.2415, -2.0051,  0.7379, -1.0614])\\nAll of these sort methods take an axis argument for sorting the sections of data along\\nthe passed axis independently:\\nIn [169]: arr = randn(3, 5)\\nIn [170]: arr\\nOut[170]: \\narray([[ 0.5955, -0.2682,  1.3389, -0.1872,  0.9111],\\n       [-0.3215,  1.0054, -0.5168,  1.1925, -0.1989],\\n       [ 0.3969, -1.7638,  0.6071, -0.2222, -0.2171]])\\nMore About Sorting | 373\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='In [171]: arr.sort(axis=1)\\nIn [172]: arr\\nOut[172]: \\narray([[-0.2682, -0.1872,  0.5955,  0.9111,  1.3389],\\n       [-0.5168, -0.3215, -0.1989,  1.0054,  1.1925],\\n       [-1.7638, -0.2222, -0.2171,  0.3969,  0.6071]])\\nYou may notice that none of the sort methods have an option to sort in descending\\norder. This is not actually a big deal because array slicing produces views, thus not\\nproducing a copy or requiring any computational work. Many Python users are familiar\\nwith the “trick” that for a list values, values[::-1] returns a list in reverse order. The\\nsame is true for ndarrays:\\nIn [173]: arr[:, ::-1]\\nOut[173]: \\narray([[ 1.3389,  0.9111,  0.5955, -0.1872, -0.2682],\\n       [ 1.1925,  1.0054, -0.1989, -0.3215, -0.5168],\\n       [ 0.6071,  0.3969, -0.2171, -0.2222, -1.7638]])\\nIndirect Sorts: argsort and lexsort\\nIn data analysis it’s very common to need to reorder data sets by one or more keys. For\\nexample, a table of data about some students might need to be sorted by last name then'),\n",
       " Document(metadata={}, page_content='In data analysis it’s very common to need to reorder data sets by one or more keys. For\\nexample, a table of data about some students might need to be sorted by last name then\\nby first name. This is an example of an indirect sort, and if you’ve read the pandas-\\nrelated chapters you have already seen many higher-level examples. Given a key or keys\\n(an array or values or multiple arrays of values), you wish to obtain an array of integer\\nindices (I refer to them colloquially as indexers) that tells you how to reorder the data\\nto be in sorted order. The two main methods for this are argsort and numpy.lexsort.\\nAs a trivial example:\\nIn [174]: values = np.array([5, 0, 1, 3, 2])\\nIn [175]: indexer = values.argsort()\\nIn [176]: indexer\\nOut[176]: array([1, 2, 4, 3, 0])\\nIn [177]: values[indexer]\\nOut[177]: array([0, 1, 2, 3, 5])\\nAs a less trivial example, this code reorders a 2D array by its first row:\\nIn [178]: arr = randn(3, 5)\\nIn [179]: arr[0] = values\\nIn [180]: arr\\nOut[180]:'),\n",
       " Document(metadata={}, page_content='Out[177]: array([0, 1, 2, 3, 5])\\nAs a less trivial example, this code reorders a 2D array by its first row:\\nIn [178]: arr = randn(3, 5)\\nIn [179]: arr[0] = values\\nIn [180]: arr\\nOut[180]: \\narray([[ 5.    ,  0.    ,  1.    ,  3.    ,  2.    ],\\n       [-0.3636, -0.1378,  2.1777, -0.4728,  0.8356],\\n       [-0.2089,  0.2316,  0.728 , -1.3918,  1.9956]])\\n374 | Chapter 12: \\u2002Advanced NumPy\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"In [181]: arr[:, arr[0].argsort()]\\nOut[181]: \\narray([[ 0.    ,  1.    ,  2.    ,  3.    ,  5.    ],\\n       [-0.1378,  2.1777,  0.8356, -0.4728, -0.3636],\\n       [ 0.2316,  0.728 ,  1.9956, -1.3918, -0.2089]])\\nlexsort is similar to argsort, but it performs an indirect lexicographical sort on multiple\\nkey arrays. Suppose we wanted to sort some data identified by first and last names:\\nIn [182]: first_name = np.array(['Bob', 'Jane', 'Steve', 'Bill', 'Barbara'])\\nIn [183]: last_name = np.array(['Jones', 'Arnold', 'Arnold', 'Jones', 'Walters'])\\nIn [184]: sorter = np.lexsort((first_name, last_name))\\nIn [185]: zip(last_name[sorter], first_name[sorter])\\nOut[185]: \\n[('Arnold', 'Jane'),\\n ('Arnold', 'Steve'),\\n ('Jones', 'Bill'),\\n ('Jones', 'Bob'),\\n ('Walters', 'Barbara')]\\nlexsort can be a bit confusing the first time you use it because the order in which the\\nkeys are used to order the data starts with the last array passed. As you can see,\\nlast_name was used before first_name.\"),\n",
       " Document(metadata={}, page_content=\"keys are used to order the data starts with the last array passed. As you can see,\\nlast_name was used before first_name.\\npandas methods like Series’s and DataFrame’s sort_index methods and\\nthe Series order method are implemented with variants of these func-\\ntions (which also must take into account missing values)\\nAlternate Sort Algorithms\\nA stable sorting algorithm preserves the relative position of equal elements. This can\\nbe especially important in indirect sorts where the relative ordering is meaningful:\\nIn [186]: values = np.array(['2:first', '2:second', '1:first', '1:second', '1:third'])\\nIn [187]: key = np.array([2, 2, 1, 1, 1])\\nIn [188]: indexer = key.argsort(kind='mergesort')\\nIn [189]: indexer\\nOut[189]: array([2, 3, 4, 0, 1])\\nIn [190]: values.take(indexer)\\nOut[190]: \\narray(['1:first', '1:second', '1:third', '2:first', '2:second'], \\n      dtype='|S8')\\nThe only stable sort available is mergesort which has guaranteed O(n log n) performance\"),\n",
       " Document(metadata={}, page_content=\"Out[190]: \\narray(['1:first', '1:second', '1:third', '2:first', '2:second'], \\n      dtype='|S8')\\nThe only stable sort available is mergesort which has guaranteed O(n log n) performance\\n(for complexity buffs), but its performance is on average worse than the default\\nMore About Sorting | 375\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"quicksort method. See Table 12-3 for a summary of available methods and their relative\\nperformance (and performance guarantees). This is not something that most users will\\never have to think about but useful to know that it’s there.\\nTable 12-3. Array sorting methods\\nKind Speed Stable Work space Worst-case\\n'quicksort' 1 No 0 O(n2)\\n'mergesort' 2 Yes n / 2 O(n log n)\\n'heapsort' 3 No 0 O(n log n)\\nAt the time of this writing, sort algorithms other than quicksort are not\\navailable on arrays of Python objects (dtype=object). This means occa-\\nsionally that algorithms requiring stable sorting will require work-\\narounds when dealing with Python objects.\\nnumpy.searchsorted: Finding elements in a Sorted Array\\nsearchsorted is an array method that performs a binary search on a sorted array, re-\\nturning the location in the array where the value would need to be inserted to maintain\\nsortedness:\\nIn [191]: arr = np.array([0, 1, 7, 12, 15])\\nIn [192]: arr.searchsorted(9)\\nOut[192]: 3\"),\n",
       " Document(metadata={}, page_content=\"turning the location in the array where the value would need to be inserted to maintain\\nsortedness:\\nIn [191]: arr = np.array([0, 1, 7, 12, 15])\\nIn [192]: arr.searchsorted(9)\\nOut[192]: 3\\nAs you might expect, you can also pass an array of values to get an array of indices back:\\nIn [193]: arr.searchsorted([0, 8, 11, 16])\\nOut[193]: array([0, 3, 3, 5])\\nYou might have noticed that searchsorted returned 0 for the 0 element. This is because\\nthe default behavior is to return the index at the left side of a group of equal values:\\nIn [194]: arr = np.array([0, 0, 0, 1, 1, 1, 1])\\nIn [195]: arr.searchsorted([0, 1])\\nOut[195]: array([0, 3])\\nIn [196]: arr.searchsorted([0, 1], side='right')\\nOut[196]: array([3, 7])\\nAs another application of searchsorted, suppose we had an array of values between 0\\nand 10,000) and a separate array of “bucket edges” that we wanted to use to bin the\\ndata:\\nIn [197]: data = np.floor(np.random.uniform(0, 10000, size=50))\\nIn [198]: bins = np.array([0, 100, 1000, 5000, 10000])\"),\n",
       " Document(metadata={}, page_content='data:\\nIn [197]: data = np.floor(np.random.uniform(0, 10000, size=50))\\nIn [198]: bins = np.array([0, 100, 1000, 5000, 10000])\\nIn [199]: data\\n376 | Chapter 12: \\u2002Advanced NumPy\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Out[199]: \\narray([ 8304.,  4181.,  9352.,  4907.,  3250.,  8546.,  2673.,  6152.,\\n        2774.,  5130.,  9553.,  4997.,  1794.,  9688.,   426.,  1612.,\\n         651.,  8653.,  1695.,  4764.,  1052.,  4836.,  8020.,  3479.,\\n        1513.,  5872.,  8992.,  7656.,  4764.,  5383.,  2319.,  4280.,\\n        4150.,  8601.,  3946.,  9904.,  7286.,  9969.,  6032.,  4574.,\\n        8480.,  4298.,  2708.,  7358.,  6439.,  7916.,  3899.,  9182.,\\n         871.,  7973.])\\nTo then get a labeling of which interval each data point belongs to (where 1 would\\nmean the bucket [0, 100)), we can simply use searchsorted:\\nIn [200]: labels = bins.searchsorted(data)\\nIn [201]: labels\\nOut[201]: \\narray([4, 3, 4, 3, 3, 4, 3, 4, 3, 4, 4, 3, 3, 4, 2, 3, 2, 4, 3, 3, 3, 3, 4,\\n       3, 3, 4, 4, 4, 3, 4, 3, 3, 3, 4, 3, 4, 4, 4, 4, 3, 4, 3, 3, 4, 4, 4,\\n       3, 4, 2, 4])\\nThis, combined with pandas’s groupby, can be used to easily bin data:\\nIn [202]: Series(data).groupby(labels).mean()\\nOut[202]: \\n2     649.333333'),\n",
       " Document(metadata={}, page_content='3, 4, 2, 4])\\nThis, combined with pandas’s groupby, can be used to easily bin data:\\nIn [202]: Series(data).groupby(labels).mean()\\nOut[202]: \\n2     649.333333\\n3    3411.521739\\n4    7935.041667\\nNote that NumPy actually has a function digitize that computes this bin labeling:\\nIn [203]: np.digitize(data, bins)\\nOut[203]: \\narray([4, 3, 4, 3, 3, 4, 3, 4, 3, 4, 4, 3, 3, 4, 2, 3, 2, 4, 3, 3, 3, 3, 4,\\n       3, 3, 4, 4, 4, 3, 4, 3, 3, 3, 4, 3, 4, 4, 4, 4, 3, 4, 3, 3, 4, 4, 4,\\n       3, 4, 2, 4])\\nNumPy Matrix Class\\nCompared with other languages for matrix operations and linear algebra, like MAT-\\nLAB, Julia, and GAUSS, NumPy’s linear algebra syntax can often be quite verbose. One\\nreason is that matrix multiplication requires using numpy.dot. Also NumPy’s indexing\\nsemantics are different, which makes porting code to Python less straightforward at\\ntimes. Selecting a single row (e.g. X[1, :]) or column (e.g. X[:, 1]) from a 2D array'),\n",
       " Document(metadata={}, page_content='semantics are different, which makes porting code to Python less straightforward at\\ntimes. Selecting a single row (e.g. X[1, :]) or column (e.g. X[:, 1]) from a 2D array\\nyields a 1D array compared with a 2D array as in, say, MATLAB.\\nIn [204]: X =  np.array([[ 8.82768214,  3.82222409, -1.14276475,  2.04411587],\\n   .....:                [ 3.82222409,  6.75272284,  0.83909108,  2.08293758],\\n   .....:                [-1.14276475,  0.83909108,  5.01690521,  0.79573241],\\n   .....:                [ 2.04411587,  2.08293758,  0.79573241,  6.24095859]])\\nIn [205]: X[:, 0]  # one-dimensional\\nOut[205]: array([ 8.8277,  3.8222, -1.1428,  2.0441])\\nIn [206]: y = X[:, :1]  # two-dimensional by slicing\\nNumPy Matrix Class | 377\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='In [207]: X\\nOut[207]: \\narray([[ 8.8277,  3.8222, -1.1428,  2.0441],\\n       [ 3.8222,  6.7527,  0.8391,  2.0829],\\n       [-1.1428,  0.8391,  5.0169,  0.7957],\\n       [ 2.0441,  2.0829,  0.7957,  6.241 ]])\\nIn [208]: y\\nOut[208]: \\narray([[ 8.8277],\\n       [ 3.8222],\\n       [-1.1428],\\n       [ 2.0441]])\\nIn this case, the product yT X y would be expressed like so:\\nIn [209]: np.dot(y.T, np.dot(X, y))\\nOut[209]: array([[ 1195.468]])\\nTo aid in writing code with a lot of matrix operations, NumPy has a matrix class which\\nhas modified indexing behavior to make it more MATLAB-like: single rows and col-\\numns come back two-dimensional and multiplication with * is matrix multiplication.\\nThe above operation with numpy.matrix would look like:\\nIn [210]: Xm = np.matrix(X)\\nIn [211]: ym = Xm[:, 0]\\nIn [212]: Xm\\nOut[212]: \\nmatrix([[ 8.8277,  3.8222, -1.1428,  2.0441],\\n        [ 3.8222,  6.7527,  0.8391,  2.0829],\\n        [-1.1428,  0.8391,  5.0169,  0.7957],\\n        [ 2.0441,  2.0829,  0.7957,  6.241 ]])'),\n",
       " Document(metadata={}, page_content='Out[212]: \\nmatrix([[ 8.8277,  3.8222, -1.1428,  2.0441],\\n        [ 3.8222,  6.7527,  0.8391,  2.0829],\\n        [-1.1428,  0.8391,  5.0169,  0.7957],\\n        [ 2.0441,  2.0829,  0.7957,  6.241 ]])\\nIn [213]: ym\\nOut[213]: \\nmatrix([[ 8.8277],\\n        [ 3.8222],\\n        [-1.1428],\\n        [ 2.0441]])\\nIn [214]: ym.T * Xm * ym\\nOut[214]: matrix([[ 1195.468]])\\nmatrix also has a special attribute I which returns the matrix inverse:\\nIn [215]: Xm.I * X\\nOut[215]: \\nmatrix([[ 1., -0., -0., -0.],\\n        [ 0.,  1.,  0.,  0.],\\n        [ 0.,  0.,  1.,  0.],\\n        [ 0.,  0.,  0.,  1.]])\\n378 | Chapter 12: \\u2002Advanced NumPy\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='I do not recommend using numpy.matrix as a replacement for regular ndarrays because\\nthey are generally more seldom used. In individual functions with lots of linear algebra,\\nit may be helpful to convert the function argument to matrix type, then cast back to\\nregular arrays with np.asarray (which does not copy any data) before returning them.\\nAdvanced Array Input and Output\\nIn Chapter 4, I introduced you to np.save and np.load for storing arrays in binary format\\non disk. There are a number of additional options to consider for more sophisticated\\nuse. In particular, memory maps have the additional benefit of enabling you to work\\nwith data sets that do not fit into RAM.\\nMemory-mapped Files\\nA memory-mapped file is a method for treating potentially very large binary data on\\ndisk as an in-memory array. NumPy implements a memmap object that is ndarray-like,\\nenabling small segments of a large file to be read and written without reading the whole'),\n",
       " Document(metadata={}, page_content=\"disk as an in-memory array. NumPy implements a memmap object that is ndarray-like,\\nenabling small segments of a large file to be read and written without reading the whole\\narray into memory. Additionally, a memmap has the same methods as an in-memory array\\nand thus can be substituted into many algorithms where an ndarray would be expected.\\nTo create a new memmap, use the function np.memmap and pass a file path, dtype, shape,\\nand file mode:\\nIn [216]: mmap = np.memmap('mymmap', dtype='float64', mode='w+', shape=(10000, 10000))\\nIn [217]: mmap\\nOut[217]: \\nmemmap([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\\n       ..., \\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])\\nSlicing a memmap returns views on the data on disk:\\nIn [218]: section = mmap[:5]\"),\n",
       " Document(metadata={}, page_content='[ 0.,  0.,  0., ...,  0.,  0.,  0.],\\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])\\nSlicing a memmap returns views on the data on disk:\\nIn [218]: section = mmap[:5]\\nIf you assign data to these, it will be buffered in memory (like a Python file object), but\\ncan be written to disk by calling flush:\\nIn [219]: section[:] = np.random.randn(5, 10000)\\nIn [220]: mmap.flush()\\nIn [221]: mmap\\nOut[221]: \\nmemmap([[-0.1614, -0.1768,  0.422 , ..., -0.2195, -0.1256, -0.4012],\\n       [ 0.4898, -2.2219, -0.7684, ..., -2.3517, -1.0782,  1.3208],\\n       [-0.6875,  1.6901, -0.7444, ..., -1.4218, -0.0509,  1.2224],\\nAdvanced Array Input and Output | 379\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"..., \\n       [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\\n       [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\\n       [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ]])\\nIn [222]: del mmap\\nWhenever a memory map falls out of scope and is garbage-collected, any changes will\\nbe flushed to disk also. When opening an existing memory map, you still have to specify\\nthe dtype and shape as the file is just a block of binary data with no metadata on disk:\\nIn [223]: mmap = np.memmap('mymmap', dtype='float64', shape=(10000, 10000))\\nIn [224]: mmap\\nOut[224]: \\nmemmap([[-0.1614, -0.1768,  0.422 , ..., -0.2195, -0.1256, -0.4012],\\n       [ 0.4898, -2.2219, -0.7684, ..., -2.3517, -1.0782,  1.3208],\\n       [-0.6875,  1.6901, -0.7444, ..., -1.4218, -0.0509,  1.2224],\\n       ..., \\n       [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\\n       [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\"),\n",
       " Document(metadata={}, page_content='..., \\n       [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\\n       [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\\n       [ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ]])\\nSince a memory map is just an on-disk ndarray, there are no issues using a structured\\ndtype as described above.\\nHDF5 and Other Array Storage Options\\nPyTables and h5py are two Python projects providing NumPy-friendly interfaces for\\nstoring array data in the efficient and compressible HDF5 format (HDF stands for\\nhierarchical data format). You can safely store hundreds of gigabytes or even terabytes\\nof data in HDF5 format. The use of these libraries is unfortunately outside the scope\\nof the book.\\nPyTables provides a rich facility for working with structured arrays with advanced\\nquerying features and the ability to add column indexes to accelerate queries. This is\\nvery similar to the table indexing capabilities provided by relational databases.\\nPerformance Tips'),\n",
       " Document(metadata={}, page_content='querying features and the ability to add column indexes to accelerate queries. This is\\nvery similar to the table indexing capabilities provided by relational databases.\\nPerformance Tips\\nGetting good performance out of code utilizing NumPy is often straightforward, as\\narray operations typically replace otherwise comparatively extremely slow pure Python\\nloops. Here is a brief list of some of the things to keep in mind:\\n• Convert Python loops and conditional logic to array operations and boolean array\\noperations\\n• Use broadcasting whenever possible\\n• Avoid copying data using array views (slicing)\\n• Utilize ufuncs and ufunc methods\\n380 | Chapter 12: \\u2002Advanced NumPy\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='If you can’t get the performance you require after exhausting the capabilities provided\\nby NumPy alone, writing code in C, Fortran, or especially Cython (see a bit more on\\nthis below) may be in order. I personally use Cython (http://cython.org) heavily in my\\nown work as an easy way to get C-like performance with minimal development.\\nThe Importance of Contiguous Memory\\nWhile the full extent of this topic is a bit outside the scope of this book, in some ap-\\nplications the memory layout of an array can significantly affect the speed of compu-\\ntations. This is based partly on performance differences having to do with the cache\\nhierarchy of the CPU; operations accessing contiguous blocks of memory (for example,\\nsumming the rows of a C order array) will generally be the fastest because the memory\\nsubsystem will buffer the appropriate blocks of memory into the ultrafast L1 or L2 CPU\\ncache. Also, certain code paths inside NumPy’s C codebase have been optimized for'),\n",
       " Document(metadata={}, page_content=\"subsystem will buffer the appropriate blocks of memory into the ultrafast L1 or L2 CPU\\ncache. Also, certain code paths inside NumPy’s C codebase have been optimized for\\nthe contiguous case in which generic strided memory access can be avoided.\\nTo say that an array’s memory layout is contiguous means that the elements are stored\\nin memory in the order that they appear in the array with respect to Fortran (column\\nmajor) or C (row major) ordering. By default, NumPy arrays are created as C-contigu-\\nous or just simply contiguous. A column major array, such as the transpose of a C-\\ncontiguous array, is thus said to be Fortran-contiguous. These properties can be ex-\\nplicitly checked via the flags attribute on the ndarray:\\nIn [227]: arr_c = np.ones((1000, 1000), order='C')\\nIn [228]: arr_f = np.ones((1000, 1000), order='F')\\nIn [229]: arr_c.flags         In [230]: arr_f.flags \\nOut[229]:                     Out[230]:             \\n  C_CONTIGUOUS : True           C_CONTIGUOUS : False\"),\n",
       " Document(metadata={}, page_content='In [229]: arr_c.flags         In [230]: arr_f.flags \\nOut[229]:                     Out[230]:             \\n  C_CONTIGUOUS : True           C_CONTIGUOUS : False\\n  F_CONTIGUOUS : False          F_CONTIGUOUS : True \\n  OWNDATA : True                OWNDATA : True      \\n  WRITEABLE : True              WRITEABLE : True    \\n  ALIGNED : True                ALIGNED : True      \\n  UPDATEIFCOPY : False          UPDATEIFCOPY : False\\n                                                    \\nIn [231]: arr_f.flags.f_contiguous\\nOut[231]: True\\nIn this example, summing the rows of these arrays should, in theory, be faster for\\narr_c than arr_f since the rows are contiguous in memory. Here I check for sure using\\n%timeit in IPython:\\nIn [232]: %timeit arr_c.sum(1)\\n1000 loops, best of 3: 1.33 ms per loop\\nIn [233]: %timeit arr_f.sum(1)\\n100 loops, best of 3: 8.75 ms per loop\\nPerformance Tips | 381\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content=\"When looking to squeeze more performance out of NumPy, this is often a place to\\ninvest some effort. If you have an array that does not have the desired memory order,\\nyou can use copy and pass either 'C' or 'F':\\nIn [234]: arr_f.copy('C').flags\\nOut[234]: \\n  C_CONTIGUOUS : True\\n  F_CONTIGUOUS : False\\n  OWNDATA : True\\n  WRITEABLE : True\\n  ALIGNED : True\\n  UPDATEIFCOPY : False\\nWhen constructing a view on an array, keep in mind that the result is not guaranteed\\nto be contiguous:\\nIn [235]: arr_c[:50].flags.contiguous      In [236]: arr_c[:, :50].flags\\nOut[235]: True                             Out[236]:                    \\n                                             C_CONTIGUOUS : False       \\n                                             F_CONTIGUOUS : False       \\n                                             OWNDATA : False            \\n                                             WRITEABLE : True           \\n                                             ALIGNED : True\"),\n",
       " Document(metadata={}, page_content='WRITEABLE : True           \\n                                             ALIGNED : True             \\n                                             UPDATEIFCOPY : False\\nOther Speed Options: Cython, f2py, C\\nIn recent years, the Cython project (( http://cython.org) has become the tool of choice\\nfor many scientific Python programmers for implementing fast code that may need to\\ninteract with C or C++ libraries, but without having to write pure C code. You can\\nthink of Cython as Python with static types and the ability to interleave functions im-\\nplemented in C into Python-like code. For example, a simple Cython function to sum\\nthe elements of a one-dimensional array might look like:\\nfrom numpy cimport ndarray, float64_t\\ndef sum_elements(ndarray[float64_t] arr):\\n    cdef Py_ssize_t i, n = len(arr)\\n    cdef float64_t result = 0\\n    for i in range(n):\\n        result += arr[i]\\n    return result'),\n",
       " Document(metadata={}, page_content='def sum_elements(ndarray[float64_t] arr):\\n    cdef Py_ssize_t i, n = len(arr)\\n    cdef float64_t result = 0\\n    for i in range(n):\\n        result += arr[i]\\n    return result\\nCython takes this code, translates it to C, then compiles the generated C code to create\\na Python extension. Cython is an attractive option for performance computing because\\nthe code is only slightly more time-consuming to write than pure Python code and it\\nintegrates closely with NumPy. A common workflow is to get an algorithm working in\\nPython, then translate it to Cython by adding type declarations and a handful of other\\ntweaks. For more, see the project documentation.\\n382 | Chapter 12: \\u2002Advanced NumPy\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Some other options for writing high performance code with NumPy include f2py, a\\nwrapper generator for Fortran 77 and 90 code, and writing pure C extensions.\\nPerformance Tips | 383\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='www.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='APPENDIX\\nPython Language Essentials\\nKnowledge is a treasure, but practice is the key to it.\\n—Thomas Fuller\\nPeople often ask me about good resources for learning Python for data-centric appli-\\ncations. While there are many excellent Python language books, I am usually hesitant\\nto recommend some of them as they are intended for a general audience rather than\\ntailored for someone who wants to load in some data sets, do some computations, and\\nplot some of the results. There are actually a couple of books on “scientific program-\\nming in Python”, but they are geared toward numerical computing and engineering\\napplications: solving differential equations, computing integrals, doing Monte Carlo\\nsimulations, and various topics that are more mathematically-oriented rather than be-\\ning about data analysis and statistics. As this is a book about becoming proficient at\\nworking with data in Python, I think it is valuable to spend some time highlighting the'),\n",
       " Document(metadata={}, page_content='ing about data analysis and statistics. As this is a book about becoming proficient at\\nworking with data in Python, I think it is valuable to spend some time highlighting the\\nmost important features of Python’s built-in data structures and libraries from the per-\\nspective of processing and manipulating structured and unstructured data. As such, I\\nwill only present roughly enough information to enable you to follow along with the\\nrest of the book.\\nThis chapter is not intended to be an exhaustive introduction to the Python language\\nbut rather a biased, no-frills overview of features which are used repeatedly throughout\\nthis book. For new Python programmers, I recommend that you supplement this chap-\\nter with the official Python tutorial ( http://docs.python.org) and potentially one of the\\nmany excellent (and much longer) books on general purpose Python programming. In\\nmy opinion, it is not necessary to become proficient at building good software in Python'),\n",
       " Document(metadata={}, page_content='many excellent (and much longer) books on general purpose Python programming. In\\nmy opinion, it is not necessary to become proficient at building good software in Python\\nto be able to productively do data analysis. I encourage you to use IPython to experi-\\nment with the code examples and to explore the documentation for the various types,\\nfunctions, and methods. Note that some of the code used in the examples may not\\nnecessarily be fully-introduced at this point.\\nMuch of this book focuses on high performance array-based computing tools for work-\\ning with large data sets. In order to use those tools you must often first do some munging\\nto corral messy data into a more nicely structured form. Fortunately, Python is one of\\n385\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='the easiest-to-use languages for rapidly whipping your data into shape. The greater your\\nfacility with Python, the language, the easier it will be for you to prepare new data sets\\nfor analysis.\\nThe Python Interpreter\\nPython is an interpreted language. The Python interpreter runs a program by executing\\none statement at a time. The standard interactive Python interpreter can be invoked on\\nthe command line with the python command:\\n$ python\\nPython 2.7.2 (default, Oct  4 2011, 20:06:09)\\n[GCC 4.6.1] on linux2\\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\\n>>> a = 5\\n>>> print a\\n5\\nThe >>> you see is the prompt where you’ll type expressions. To exit the Python inter-\\npreter and return to the command prompt, you can either type exit() or press Ctrl-D.\\nRunning Python programs is as simple as calling python with a .py file as its first argu-\\nment. Suppose we had created hello_world.py with these contents:\\nprint \\'Hello world\\'\\nThis can be run from the terminal simply as:'),\n",
       " Document(metadata={}, page_content='ment. Suppose we had created hello_world.py with these contents:\\nprint \\'Hello world\\'\\nThis can be run from the terminal simply as:\\n$ python hello_world.py\\nHello world\\nWhile many Python programmers execute all of their Python code in this way, many\\nscientific Python programmers make use of IPython, an enhanced interactive Python\\ninterpreter. Chapter 3 is dedicated to the IPython system. By using the %run command,\\nIPython executes the code in the specified file in the same process, enabling you to\\nexplore the results interactively when it’s done.\\n$ ipython\\nPython 2.7.2 |EPD 7.1-2 (64-bit)| (default, Jul  3 2011, 15:17:51)\\nType \"copyright\", \"credits\" or \"license\" for more information.\\nIPython 0.12 -- An enhanced Interactive Python.\\n?         -> Introduction and overview of IPython\\'s features.\\n%quickref -> Quick reference.\\nhelp      -> Python\\'s own help system.\\nobject?   -> Details about \\'object\\', use \\'object??\\' for extra details.\\nIn [1]: %run hello_world.py\\nHello world\\nIn [2]:'),\n",
       " Document(metadata={}, page_content=\"%quickref -> Quick reference.\\nhelp      -> Python's own help system.\\nobject?   -> Details about 'object', use 'object??' for extra details.\\nIn [1]: %run hello_world.py\\nHello world\\nIn [2]:\\n386 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content='The default IPython prompt adopts the numbered In [2]: style compared with the\\nstandard >>> prompt.\\nThe Basics\\nLanguage Semantics\\nThe Python language design is distinguished by its emphasis on readability, simplicity,\\nand explicitness. Some people go so far as to liken it to “executable pseudocode”.\\nIndentation, not braces\\nPython uses whitespace (tabs or spaces) to structure code instead of using braces as in\\nmany other languages like R, C++, Java, and Perl. Take the for loop in the above\\nquicksort algorithm:\\nfor x in array:\\n    if x < pivot:\\n        less.append(x)\\n    else:\\n        greater.append(x)\\nA colon denotes the start of an indented code block after which all of the code must be\\nindented by the same amount until the end of the block. In another language, you might\\ninstead have something like:\\nfor x in array {\\n        if x < pivot {\\n            less.append(x)\\n        } else {\\n            greater.append(x)\\n        }\\n    }'),\n",
       " Document(metadata={}, page_content='instead have something like:\\nfor x in array {\\n        if x < pivot {\\n            less.append(x)\\n        } else {\\n            greater.append(x)\\n        }\\n    }\\nOne major reason that whitespace matters is that it results in most Python code looking\\ncosmetically similar, which means less cognitive dissonance when you read a piece of\\ncode that you didn’t write yourself (or wrote in a hurry a year ago!). In a language\\nwithout significant whitespace, you might stumble on some differently formatted code\\nlike:\\nfor x in array\\n    {\\n      if x < pivot\\n      {\\n        less.append(x)\\n      }\\n      else\\n      {\\n        greater.append(x)\\nThe Basics | 387\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='}\\n    }\\nLove it or hate it, significant whitespace is a fact of life for Python programmers, and\\nin my experience it helps make Python code a lot more readable than other languages\\nI’ve used. While it may seem foreign at first, I suspect that it will grow on you after a\\nwhile.\\nI strongly recommend that you use 4 spaces to as your default indenta-\\ntion and that your editor replace tabs with 4 spaces. Many text editors\\nhave a setting that will replace tab stops with spaces automatically (do\\nthis!). Some people use tabs or a different number of spaces, with 2\\nspaces not being terribly uncommon. 4 spaces is by and large the stan-\\ndard adopted by the vast majority of Python programmers, so I recom-\\nmend doing that in the absence of a compelling reason otherwise.\\nAs you can see by now, Python statements also do not need to be terminated by sem-\\nicolons. Semicolons can be used, however, to separate multiple statements on a single\\nline:\\na = 5; b = 6; c = 7'),\n",
       " Document(metadata={}, page_content='As you can see by now, Python statements also do not need to be terminated by sem-\\nicolons. Semicolons can be used, however, to separate multiple statements on a single\\nline:\\na = 5; b = 6; c = 7\\nPutting multiple statements on one line is generally discouraged in Python as it often\\nmakes code less readable.\\nEverything is an object\\nAn important characteristic of the Python language is the consistency of its object\\nmodel. Every number, string, data structure, function, class, module, and so on exists\\nin the Python interpreter in its own “box” which is referred to as a Python object. Each\\nobject has an associated type (for example, string or function) and internal data. In\\npractice this makes the language very flexible, as even functions can be treated just like\\nany other object.\\nComments\\nAny text preceded by the hash mark (pound sign) # is ignored by the Python interpreter.\\nThis is often used to add comments to code. At times you may also want to exclude'),\n",
       " Document(metadata={}, page_content=\"any other object.\\nComments\\nAny text preceded by the hash mark (pound sign) # is ignored by the Python interpreter.\\nThis is often used to add comments to code. At times you may also want to exclude\\ncertain blocks of code without deleting them. An easy solution is to comment out the\\ncode:\\nresults = []\\nfor line in file_handle:\\n    # keep the empty lines for now\\n    # if len(line) == 0:\\n    #   continue\\n    results.append(line.replace('foo', 'bar'))\\n388 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"Function and object method calls\\nFunctions are called using parentheses and passing zero or more arguments, optionally\\nassigning the returned value to a variable:\\nresult = f(x, y, z)\\ng()\\nAlmost every object in Python has attached functions, known as methods, that have\\naccess to the object’s internal contents. They can be called using the syntax:\\nobj.some_method(x, y, z)\\nFunctions can take both positional and keyword arguments:\\nresult = f(a, b, c, d=5, e='foo')\\nMore on this later.\\nVariables and pass-by-reference\\nWhen assigning a variable (or name) in Python, you are creating a reference to the object\\non the right hand side of the equals sign. In practical terms, consider a list of integers:\\nIn [241]: a = [1, 2, 3]\\nSuppose we assign a to a new variable b:\\nIn [242]: b = a\\nIn some languages, this assignment would cause the data [1, 2, 3] to be copied. In\\nPython, a and b actually now refer to the same object, the original list [1, 2, 3] (see\"),\n",
       " Document(metadata={}, page_content='In [242]: b = a\\nIn some languages, this assignment would cause the data [1, 2, 3] to be copied. In\\nPython, a and b actually now refer to the same object, the original list [1, 2, 3] (see\\nFigure A-1 for a mockup). You can prove this to yourself by appending an element to\\na and then examining b:\\nIn [243]: a.append(4)\\nIn [244]: b\\nOut[244]: [1, 2, 3, 4]\\nFigure A-1. Two references for the same object\\nUnderstanding the semantics of references in Python and when, how, and why data is\\ncopied is especially critical when working with larger data sets in Python.\\nThe Basics | 389\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Assignment is also referred to as binding, as we are binding a name to\\nan object. Variables names that have been assigned may occasionally be\\nreferred to as bound variables.\\nWhen you pass objects as arguments to a function, you are only passing references; no\\ncopying occurs. Thus, Python is said to pass by reference, whereas some other languages\\nsupport both pass by value (creating copies) and pass by reference. This means that a\\nfunction can mutate the internals of its arguments. Suppose we had the following func-\\ntion:\\ndef append_element(some_list, element):\\n    some_list.append(element)\\nThen given what’s been said, this should not come as a surprise:\\nIn [2]: data = [1, 2, 3]\\nIn [3]: append_element(data, 4)\\nIn [4]: data\\nOut[4]: [1, 2, 3, 4]\\nDynamic references, strong types\\nIn contrast with many compiled languages, such as Java and C++, object references in\\nPython have no type associated with them. There is no problem with the following:\\nIn [245]: a = 5        In [246]: type(a)'),\n",
       " Document(metadata={}, page_content=\"Python have no type associated with them. There is no problem with the following:\\nIn [245]: a = 5        In [246]: type(a)\\n                       Out[246]: int    \\n                                        \\nIn [247]: a = 'foo'    In [248]: type(a)\\n                       Out[248]: str\\nVariables are names for objects within a particular namespace; the type information is\\nstored in the object itself. Some observers might hastily conclude that Python is not a\\n“typed language”. This is not true; consider this example:\\nIn [249]: '5' + 5\\n---------------------------------------------------------------------------\\nTypeError                                 Traceback (most recent call last)\\n<ipython-input-249-f9dbf5f0b234> in <module>()\\n----> 1 '5' + 5\\nTypeError: cannot concatenate 'str' and 'int' objects\\nIn some languages, such as Visual Basic, the string '5' might get implicitly converted\\n(or casted) to an integer, thus yielding 10. Yet in other languages, such as JavaScript,\"),\n",
       " Document(metadata={}, page_content=\"In some languages, such as Visual Basic, the string '5' might get implicitly converted\\n(or casted) to an integer, thus yielding 10. Yet in other languages, such as JavaScript,\\nthe integer 5 might be casted to a string, yielding the concatenated string '55'. In this\\nregard Python is considered a strongly-typed language, which means that every object\\nhas a specific type (or class), and implicit conversions will occur only in certain obvious\\ncircumstances, such as the following:\\n390 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"In [250]: a = 4.5\\nIn [251]: b = 2\\n# String formatting, to be visited later\\nIn [252]: print 'a is %s, b is %s' % (type(a), type(b))\\na is <type 'float'>, b is <type 'int'>\\nIn [253]: a / b\\nOut[253]: 2.25\\nKnowing the type of an object is important, and it’s useful to be able to write functions\\nthat can handle many different kinds of input. You can check that an object is an\\ninstance of a particular type using the isinstance function:\\nIn [254]: a = 5        In [255]: isinstance(a, int)\\n                       Out[255]: True\\nisinstance can accept a tuple of types if you want to check that an object’s type is\\namong those present in the tuple:\\nIn [256]: a = 5; b = 4.5\\nIn [257]: isinstance(a, (int, float))      In [258]: isinstance(b, (int, float))\\nOut[257]: True                             Out[258]: True\\nAttributes and methods\\nObjects in Python typically have both attributes, other Python objects stored “inside”\"),\n",
       " Document(metadata={}, page_content=\"Out[257]: True                             Out[258]: True\\nAttributes and methods\\nObjects in Python typically have both attributes, other Python objects stored “inside”\\nthe object, and methods, functions associated with an object which can have access to\\nthe object’s internal data. Both of them are accessed via the syntax obj.attribute_name:\\nIn [1]: a = 'foo'\\nIn [2]: a.<Tab>\\na.capitalize  a.format      a.isupper     a.rindex      a.strip\\na.center      a.index       a.join        a.rjust       a.swapcase\\na.count       a.isalnum     a.ljust       a.rpartition  a.title\\na.decode      a.isalpha     a.lower       a.rsplit      a.translate\\na.encode      a.isdigit     a.lstrip      a.rstrip      a.upper\\na.endswith    a.islower     a.partition   a.split       a.zfill\\na.expandtabs  a.isspace     a.replace     a.splitlines\\na.find        a.istitle     a.rfind       a.startswith\\nAttributes and methods can also be accessed by name using the getattr function:\\n>>> getattr(a, 'split')\\n<function split>\"),\n",
       " Document(metadata={}, page_content=\"a.find        a.istitle     a.rfind       a.startswith\\nAttributes and methods can also be accessed by name using the getattr function:\\n>>> getattr(a, 'split')\\n<function split>\\nWhile we will not extensively use the functions getattr and related functions hasattr\\nand setattr in this book, they can be used very effectively to write generic, reusable\\ncode.\\nThe Basics | 391\\nwww.it-ebooks.info\"),\n",
       " Document(metadata={}, page_content=\"“Duck” typing\\nOften you may not care about the type of an object but rather only whether it has certain\\nmethods or behavior. For example, you can verify that an object is iterable if it imple-\\nmented the iterator protocol. For many objects, this means it has a __iter__ “magic\\nmethod”, though an alternative and better way to check is to try using the iter function:\\ndef isiterable(obj):\\n    try:\\n        iter(obj)\\n        return True\\n    except TypeError: # not iterable\\n        return False\\nThis function would return True for strings as well as most Python collection types:\\nIn [260]: isiterable('a string')        In [261]: isiterable([1, 2, 3])\\nOut[260]: True                          Out[261]: True                 \\n                                                                       \\nIn [262]: isiterable(5)\\nOut[262]: False\\nA place where I use this functionality all the time is to write functions that can accept\"),\n",
       " Document(metadata={}, page_content='In [262]: isiterable(5)\\nOut[262]: False\\nA place where I use this functionality all the time is to write functions that can accept\\nmultiple kinds of input. A common case is writing a function that can accept any kind\\nof sequence (list, tuple, ndarray) or even an iterator. You can first check if the object is\\na list (or a NumPy array) and, if it is not, convert it to be one:\\nif not isinstance(x, list) and isiterable(x):\\n    x = list(x)\\nImports\\nIn Python a module is simply a .py file containing function and variable definitions\\nalong with such things imported from other .py files. Suppose that we had the following\\nmodule:\\n# some_module.py\\nPI = 3.14159\\ndef f(x):\\n    return x + 2\\ndef g(a, b):\\n    return a + b\\nIf we wanted to access the variables and functions defined in some_module.py, from\\nanother file in the same directory we could do:\\nimport some_module\\nresult = some_module.f(5)\\npi = some_module.PI\\nOr equivalently:\\nfrom some_module import f, g, PI\\nresult = g(5, PI)'),\n",
       " Document(metadata={}, page_content='another file in the same directory we could do:\\nimport some_module\\nresult = some_module.f(5)\\npi = some_module.PI\\nOr equivalently:\\nfrom some_module import f, g, PI\\nresult = g(5, PI)\\n392 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='By using the as keyword you can give imports different variable names:\\nimport some_module as sm\\nfrom some_module import PI as pi, g as gf\\nr1 = sm.f(pi)\\nr2 = gf(6, pi)\\nBinary operators and comparisons\\nMost of the binary math operations and comparisons are as you might expect:\\nIn [263]: 5 - 7        In [264]: 12 + 21.5\\nOut[263]: -2           Out[264]: 33.5     \\n                                          \\nIn [265]: 5 <= 2\\nOut[265]: False\\nSee Table A-1 for all of the available binary operators.\\nTo check if two references refer to the same object, use the is keyword. is not is also\\nperfectly valid if you want to check that two objects are not the same:\\nIn [266]: a = [1, 2, 3]\\nIn [267]: b = a\\n# Note, the list function always creates a new list\\nIn [268]: c = list(a)\\nIn [269]: a is b        In [270]: a is not c\\nOut[269]: True          Out[270]: True\\nNote this is not the same thing is comparing with ==, because in this case we have:\\nIn [271]: a == c\\nOut[271]: True'),\n",
       " Document(metadata={}, page_content='In [269]: a is b        In [270]: a is not c\\nOut[269]: True          Out[270]: True\\nNote this is not the same thing is comparing with ==, because in this case we have:\\nIn [271]: a == c\\nOut[271]: True\\nA very common use of is and is not is to check if a variable is None, since there is only\\none instance of None:\\nIn [272]: a = None\\nIn [273]: a is None\\nOut[273]: True\\nTable A-1. Binary operators\\nOperation Description\\na + b Add a and b\\na - b Subtract b from a\\na * b Multiply a by b\\na / b Divide a by b\\na // b Floor-divide a by b, dropping any fractional remainder\\nThe Basics | 393\\nwww.it-ebooks.info'),\n",
       " Document(metadata={}, page_content='Operation Description\\na ** b Raise a to the b power\\na & b True if both a and b are True. For integers, take the bitwise AND.\\na | b True if either a or b is True. For integers, take the bitwise OR.\\na ^ b For booleans, True if a or b is True, but not both. For integers, take the bitwise EXCLUSIVE-OR.\\na == b True if a equals b\\na != b True if a is not equal to b\\na <= b, a < b True if a is less than (less than or equal) to b\\na > b, a >= b True if a is greater than (greater than or equal) to b\\na is b True if a and b reference same Python object\\na is not b True if a and b reference different Python objects\\nStrictness versus laziness\\nWhen using any programming language, it’s important to understand when expressions\\nare evaluated. Consider the simple expression:\\na = b = c = 5\\nd = a + b * c\\nIn Python, once these statements are evaluated, the calculation is immediately (or\\nstrictly) carried out, setting the value of d to 30. In another programming paradigm,'),\n",
       " Document(metadata={}, page_content='a = b = c = 5\\nd = a + b * c\\nIn Python, once these statements are evaluated, the calculation is immediately (or\\nstrictly) carried out, setting the value of d to 30. In another programming paradigm,\\nsuch as in a pure functional programming language like Haskell, the value of d might\\nnot be evaluated until it is actually used elsewhere. The idea of deferring computations\\nin this way is commonly known as lazy evaluation. Python, on the other hand, is a very \\nstrict (or eager) language. Nearly all of the time, computations and expressions are\\nevaluated immediately. Even in the above simple expression, the result of b * c is\\ncomputed as a separate step before adding it to a.\\nThere are Python techniques, especially using iterators and generators, which can be\\nused to achieve laziness. When performing very expensive computations which are only\\nnecessary some of the time, this can be an important technique in data-intensive ap-\\nplications.\\nMutable and immutable objects'),\n",
       " Document(metadata={}, page_content=\"necessary some of the time, this can be an important technique in data-intensive ap-\\nplications.\\nMutable and immutable objects\\nMost objects in Python are mutable, such as lists, dicts, NumPy arrays, or most user-\\ndefined types (classes). This means that the object or values that they contain can be\\nmodified.\\nIn [274]: a_list = ['foo', 2, [4, 5]]\\nIn [275]: a_list[2] = (3, 4)\\nIn [276]: a_list\\nOut[276]: ['foo', 2, (3, 4)]\\n394 | Appendix: \\u2002Python Language Essentials\\nwww.it-ebooks.info\"),\n",
       " ...]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_text=create_chunks(trimmed_extracted_doc)\n",
    "chunks_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60be2bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1152"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#it has created 1152 chunks of data from the pdf file\n",
    "len(chunks_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816c8760",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lk/l_9wb9tn2274jcs087m45ctc0000gn/T/ipykernel_10504/3830145207.py:7: LangChainDeprecationWarning: The class `HuggingFaceBgeEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings=HuggingFaceBgeEmbeddings(model_name=model_name)\n",
      "/Users/akashsharma/miniconda3/envs/chatbot/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#now embedding the chunks of data converting into embeddings\n",
    "\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "def embed_chunks(chunks_text):\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    embeddings=HuggingFaceBgeEmbeddings(model_name=model_name)\n",
    "    return embeddings\n",
    "embeddings=embed_chunks(chunks_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eda8e402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceBgeEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       "), model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, query_instruction='Represent this question for searching relevant passages: ', embed_instruction='', show_progress=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4019f35d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.033530112355947495,\n",
       " 0.09727564454078674,\n",
       " 0.07112990319728851,\n",
       " -0.01218468975275755,\n",
       " -0.04355277866125107,\n",
       " 0.06468386948108673,\n",
       " 0.06274686753749847,\n",
       " 0.021055547520518303,\n",
       " 0.06129947304725647,\n",
       " -0.021669398993253708,\n",
       " 0.046337854117155075,\n",
       " -0.03871845081448555,\n",
       " -0.02270014025270939,\n",
       " -0.034857023507356644,\n",
       " -0.02378496341407299,\n",
       " -0.01237558014690876,\n",
       " -0.10450831800699234,\n",
       " 0.03307962417602539,\n",
       " -0.016889186576008797,\n",
       " -0.07807300984859467,\n",
       " 0.039430830627679825,\n",
       " 0.04632019251585007,\n",
       " 0.007087348494678736,\n",
       " 0.020857060328125954,\n",
       " 0.033982884138822556,\n",
       " -0.04539338871836662,\n",
       " 0.0180234182626009,\n",
       " -0.054730795323848724,\n",
       " 0.08952298015356064,\n",
       " 0.03776724264025688,\n",
       " -0.020334267988801003,\n",
       " 0.06835142523050308,\n",
       " -0.00026336326845921576,\n",
       " 0.037590570747852325,\n",
       " -0.07571468502283096,\n",
       " 0.1255035549402237,\n",
       " -0.009903074242174625,\n",
       " -0.01002566795796156,\n",
       " 0.06373600661754608,\n",
       " -0.024974854663014412,\n",
       " -0.05270494148135185,\n",
       " -0.0021340064704418182,\n",
       " 0.015552852302789688,\n",
       " -0.05634294077754021,\n",
       " 0.019470088183879852,\n",
       " -0.0033759253565222025,\n",
       " -0.049464888870716095,\n",
       " 0.08569976687431335,\n",
       " -0.08234414458274841,\n",
       " 0.0057389759458601475,\n",
       " -0.07101414352655411,\n",
       " -0.040284670889377594,\n",
       " -0.04243466258049011,\n",
       " 0.03644891828298569,\n",
       " -0.010096763260662556,\n",
       " -0.03066174127161503,\n",
       " -0.04656806215643883,\n",
       " 0.07280601561069489,\n",
       " 0.03216901794075966,\n",
       " -0.01876344159245491,\n",
       " 0.040332626551389694,\n",
       " 0.04283328354358673,\n",
       " -0.05899624526500702,\n",
       " 0.0859362930059433,\n",
       " 0.016520483419299126,\n",
       " -0.02527044340968132,\n",
       " 0.05604790896177292,\n",
       " 0.03713736683130264,\n",
       " -0.06544939428567886,\n",
       " 0.029150474816560745,\n",
       " 0.005907345563173294,\n",
       " -0.032062090933322906,\n",
       " 0.04831144958734512,\n",
       " -0.009604190476238728,\n",
       " -0.03991248086094856,\n",
       " -0.07159027457237244,\n",
       " -0.038337066769599915,\n",
       " -0.04071744158864021,\n",
       " -0.04089650884270668,\n",
       " -0.07218117266893387,\n",
       " 0.06640039384365082,\n",
       " 0.13739459216594696,\n",
       " 0.029528187587857246,\n",
       " 0.002142508514225483,\n",
       " -0.06565552949905396,\n",
       " -0.007443868089467287,\n",
       " 0.019375111907720566,\n",
       " -0.008947244845330715,\n",
       " 0.04364841803908348,\n",
       " -0.0017103509744629264,\n",
       " -0.006686408072710037,\n",
       " -0.09284718334674835,\n",
       " 0.0034268044400960207,\n",
       " -0.039385728538036346,\n",
       " -0.022506700828671455,\n",
       " 0.06748493760824203,\n",
       " -0.02828463539481163,\n",
       " 0.06771405041217804,\n",
       " 0.05449417233467102,\n",
       " 0.0468798391520977,\n",
       " 0.052239228039979935,\n",
       " 0.03133917227387428,\n",
       " 0.0829937681555748,\n",
       " -0.014350764453411102,\n",
       " -0.1261560320854187,\n",
       " -0.03969389200210571,\n",
       " -0.07224690169095993,\n",
       " -0.02200884185731411,\n",
       " 0.06590766459703445,\n",
       " -0.059264831244945526,\n",
       " -0.047133609652519226,\n",
       " -0.006058146711438894,\n",
       " -0.09538490325212479,\n",
       " -0.06243792176246643,\n",
       " -0.11689049005508423,\n",
       " 0.03387578949332237,\n",
       " 0.010923012159764767,\n",
       " -0.04952386021614075,\n",
       " -0.0006535365246236324,\n",
       " 0.10537918657064438,\n",
       " -0.009769456461071968,\n",
       " 0.005550795700401068,\n",
       " -0.0060292799025774,\n",
       " -0.008728249929845333,\n",
       " -0.03243288770318031,\n",
       " -0.06260061264038086,\n",
       " 0.0006854816456325352,\n",
       " -4.900325177259118e-33,\n",
       " 0.06532958149909973,\n",
       " -0.06870312988758087,\n",
       " 0.028823090717196465,\n",
       " -0.0018492245581001043,\n",
       " 0.03559119254350662,\n",
       " -0.07436774671077728,\n",
       " 0.023064669221639633,\n",
       " -0.05927871912717819,\n",
       " -0.03935099393129349,\n",
       " -0.10326388478279114,\n",
       " 0.01831565797328949,\n",
       " -0.03417332470417023,\n",
       " -0.058448079973459244,\n",
       " 0.03569934889674187,\n",
       " -0.0017738979076966643,\n",
       " 0.006492895539849997,\n",
       " -0.05324570834636688,\n",
       " -0.0294027216732502,\n",
       " 0.043971218168735504,\n",
       " 0.04577747732400894,\n",
       " 0.015180324204266071,\n",
       " 0.04239680990576744,\n",
       " 0.036318909376859665,\n",
       " -0.04937141761183739,\n",
       " -0.004449470899999142,\n",
       " -0.019223079085350037,\n",
       " 0.06820521503686905,\n",
       " -0.042614974081516266,\n",
       " 0.008438370190560818,\n",
       " 0.019875995814800262,\n",
       " -0.040930554270744324,\n",
       " -0.026477200910449028,\n",
       " -0.06779644638299942,\n",
       " 0.025446413084864616,\n",
       " 0.009604070335626602,\n",
       " 0.04827423021197319,\n",
       " 0.032191887497901917,\n",
       " -0.07578643411397934,\n",
       " 0.019650781527161598,\n",
       " -0.02129235491156578,\n",
       " 0.019724030047655106,\n",
       " -0.0009035428520292044,\n",
       " 0.06678520143032074,\n",
       " 0.013118116185069084,\n",
       " 0.010558489710092545,\n",
       " 0.020460357889533043,\n",
       " 0.0010835997527465224,\n",
       " 0.005850203800946474,\n",
       " -0.02241227962076664,\n",
       " 0.07460709661245346,\n",
       " 0.042889222502708435,\n",
       " 0.014674259349703789,\n",
       " -0.034979820251464844,\n",
       " 0.014994349330663681,\n",
       " 0.07684965431690216,\n",
       " 0.08607766777276993,\n",
       " -0.04533553868532181,\n",
       " -0.002852632896974683,\n",
       " -0.07265891879796982,\n",
       " 0.07305781543254852,\n",
       " 0.06509941816329956,\n",
       " -0.03315271809697151,\n",
       " 0.053721487522125244,\n",
       " 0.01579476147890091,\n",
       " -0.016883553937077522,\n",
       " 0.00106682232581079,\n",
       " -0.0803888738155365,\n",
       " -0.0552944578230381,\n",
       " 0.11135518550872803,\n",
       " -0.03860393911600113,\n",
       " -0.04334442317485809,\n",
       " -0.06683526188135147,\n",
       " -0.013268906623125076,\n",
       " 0.11740010231733322,\n",
       " 0.029748504981398582,\n",
       " 0.04523516446352005,\n",
       " -0.028638511896133423,\n",
       " -0.010397390462458134,\n",
       " -0.006178095005452633,\n",
       " -0.017048608511686325,\n",
       " -0.10660829395055771,\n",
       " -0.0568573996424675,\n",
       " -0.024586403742432594,\n",
       " 0.08092427253723145,\n",
       " -0.03279058635234833,\n",
       " 0.002205106895416975,\n",
       " 0.027683354914188385,\n",
       " -0.07394968718290329,\n",
       " 0.04148215055465698,\n",
       " 0.003075673710554838,\n",
       " -0.013760747388005257,\n",
       " 0.030591897666454315,\n",
       " -0.036184005439281464,\n",
       " -0.03810237720608711,\n",
       " -0.010596714913845062,\n",
       " 9.664977000598235e-34,\n",
       " 0.0007478873594664037,\n",
       " -0.04393966123461723,\n",
       " -0.038647182285785675,\n",
       " 0.021887123584747314,\n",
       " -0.08434101194143295,\n",
       " -0.01987391896545887,\n",
       " -0.0020890042651444674,\n",
       " 0.0863000750541687,\n",
       " 0.03716074302792549,\n",
       " 0.050122492015361786,\n",
       " -0.05037715286016464,\n",
       " -0.027689464390277863,\n",
       " 0.06757180392742157,\n",
       " 0.05266892910003662,\n",
       " -0.03085450269281864,\n",
       " 0.018464576452970505,\n",
       " 0.06724884361028671,\n",
       " 0.016249360516667366,\n",
       " 0.05023619160056114,\n",
       " 0.0684269368648529,\n",
       " -0.005156863480806351,\n",
       " 0.06641711294651031,\n",
       " -0.059104882180690765,\n",
       " -0.001053486717864871,\n",
       " -0.03557438403367996,\n",
       " -0.0016003047348931432,\n",
       " 0.02286979742348194,\n",
       " 0.00420439662411809,\n",
       " -0.016021117568016052,\n",
       " 0.060176629573106766,\n",
       " 0.023003965616226196,\n",
       " -0.11676370352506638,\n",
       " -0.048279859125614166,\n",
       " 0.035848453640937805,\n",
       " -0.031591951847076416,\n",
       " 0.004475083202123642,\n",
       " 0.064881831407547,\n",
       " -0.01576336845755577,\n",
       " -0.034192293882369995,\n",
       " -0.022595372051000595,\n",
       " 0.043073590844869614,\n",
       " -0.009935888461768627,\n",
       " 0.00484683271497488,\n",
       " 0.013320877216756344,\n",
       " -0.04886319488286972,\n",
       " -0.010557759553194046,\n",
       " -0.06274232268333435,\n",
       " 0.06190381199121475,\n",
       " 0.010914075188338757,\n",
       " -0.09142017364501953,\n",
       " 0.07046778500080109,\n",
       " 0.024784257635474205,\n",
       " 0.04614733159542084,\n",
       " -0.06257247179746628,\n",
       " -0.0685281977057457,\n",
       " 0.046612050384283066,\n",
       " -0.015030371956527233,\n",
       " -0.00877207238227129,\n",
       " 0.04013996571302414,\n",
       " -0.07118360698223114,\n",
       " 0.020643137395381927,\n",
       " 0.06142008677124977,\n",
       " -0.034024231135845184,\n",
       " -0.0061789024621248245,\n",
       " 0.0664159432053566,\n",
       " -0.043523237109184265,\n",
       " -0.05295608192682266,\n",
       " -0.016726437956094742,\n",
       " 0.030985381454229355,\n",
       " -0.08355006575584412,\n",
       " 0.07757899910211563,\n",
       " -0.06575624644756317,\n",
       " -0.03955882787704468,\n",
       " 0.04700392112135887,\n",
       " -0.03820006921887398,\n",
       " -0.012134157121181488,\n",
       " -0.01038853544741869,\n",
       " -0.11349066346883774,\n",
       " 0.018023531883955002,\n",
       " -0.027381721884012222,\n",
       " 0.04010564461350441,\n",
       " -0.03807685524225235,\n",
       " -0.018324021250009537,\n",
       " -0.013472279533743858,\n",
       " 0.04060956835746765,\n",
       " -0.0008283417555503547,\n",
       " -0.03524135798215866,\n",
       " 0.07054977864027023,\n",
       " 0.0546344555914402,\n",
       " 0.01902259886264801,\n",
       " 0.06306666135787964,\n",
       " 0.020368322730064392,\n",
       " -0.12402874231338501,\n",
       " 0.0258040614426136,\n",
       " 0.009532890282571316,\n",
       " -2.16593960544742e-08,\n",
       " -0.030406154692173004,\n",
       " -0.07251817733049393,\n",
       " -0.08051029592752457,\n",
       " -0.03746835142374039,\n",
       " -0.06525581330060959,\n",
       " -0.05627312511205673,\n",
       " 0.15553665161132812,\n",
       " -0.02276749350130558,\n",
       " -0.05931940674781799,\n",
       " -0.001270027132704854,\n",
       " 0.006221940275281668,\n",
       " 0.08801497519016266,\n",
       " -0.05652237311005592,\n",
       " -0.018480196595191956,\n",
       " 0.0031177490018308163,\n",
       " 0.027315959334373474,\n",
       " 0.054514989256858826,\n",
       " 0.03771870210766792,\n",
       " -0.07141532748937607,\n",
       " 0.013644251972436905,\n",
       " 0.04018537327647209,\n",
       " 0.04661007598042488,\n",
       " -0.03897184506058693,\n",
       " 0.001234617200680077,\n",
       " -0.09628506004810333,\n",
       " 0.0788951888680458,\n",
       " 0.05975736305117607,\n",
       " 0.1228494718670845,\n",
       " -0.03025166317820549,\n",
       " -0.05642377957701683,\n",
       " 0.03610836714506149,\n",
       " 0.08470684289932251,\n",
       " -0.0749368891119957,\n",
       " -0.0320795513689518,\n",
       " -0.060395848006010056,\n",
       " 0.07735315710306168,\n",
       " -0.09466917812824249,\n",
       " -0.06099705398082733,\n",
       " 0.06484590470790863,\n",
       " 0.004247247241437435,\n",
       " 0.02759348601102829,\n",
       " 0.045259419828653336,\n",
       " 0.03386376053094864,\n",
       " 0.015085432678461075,\n",
       " -0.0006304104463197291,\n",
       " -0.07752376049757004,\n",
       " -0.02259095199406147,\n",
       " 0.08397005498409271,\n",
       " 0.03814452141523361,\n",
       " -0.009966516867280006,\n",
       " -0.04519286006689072,\n",
       " 0.011813276447355747,\n",
       " -0.029871469363570213,\n",
       " -0.0007472219876945019,\n",
       " -0.037874262779951096,\n",
       " -0.07212135195732117,\n",
       " -0.02111324481666088,\n",
       " 0.02174183540046215,\n",
       " -0.0354996882379055,\n",
       " -0.04235728830099106,\n",
       " 0.12408988922834396,\n",
       " 0.07975238561630249,\n",
       " 0.12214735895395279,\n",
       " -0.01991569809615612]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector=embeddings.embed_query(\"What is Langchain ?\")\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b0501a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n"
     ]
    }
   ],
   "source": [
    "#lenght of the dimensions of the vector\n",
    "print(len(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6ab84720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "02e0f1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PINECONE_API_KEY=os.getenv(\"PINECONE_API_KEY\")\n",
    "LLM_API_KEY=os.getenv(\"LLM_API_KEY\")\n",
    "\n",
    "os.environ[\"PINECONE_API_KEY\"]=PINECONE_API_KEY\n",
    "os.environ[\"LLM_API_KEY\"]=LLM_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "07eef9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "pinecone_api_key=PINECONE_API_KEY\n",
    "pc= Pinecone(api_key=pinecone_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c17abe03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pinecone.pinecone.Pinecone at 0x132d5ba00>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c68c06",
   "metadata": {},
   "outputs": [
    {
     "ename": "PineconeApiException",
     "evalue": "(400)\nReason: Bad Request\nHTTP response headers: HTTPHeaderDict({'content-type': 'text/plain; charset=utf-8', 'access-control-allow-origin': '*', 'vary': 'origin,access-control-request-method,access-control-request-headers', 'access-control-expose-headers': '*', 'x-pinecone-api-version': '2025-04', 'x-cloud-trace-context': '2921199a4e0f20351582548e6220f64b', 'date': 'Thu, 04 Sep 2025 04:20:28 GMT', 'server': 'Google Frontend', 'Content-Length': '125', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\nHTTP response body: {\"error\":{\"code\":\"INVALID_ARGUMENT\",\"message\":\"Name must consist of lower case alphanumeric characters or '-'\"},\"status\":400}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPineconeApiException\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m index_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_scientist_chatbot\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pc\u001b[38;5;241m.\u001b[39mhas_index(index_name):\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mpc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_index\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdimension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m384\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcosine\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mServerlessSpec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcloud\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maws\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mus-east-1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m   \n\u001b[1;32m     11\u001b[0m index\u001b[38;5;241m=\u001b[39mpc\u001b[38;5;241m.\u001b[39mindex(index_name)\n\u001b[1;32m     12\u001b[0m index\n",
      "File \u001b[0;32m~/miniconda3/envs/chatbot/lib/python3.10/site-packages/pinecone/pinecone.py:334\u001b[0m, in \u001b[0;36mPinecone.create_index\u001b[0;34m(self, name, spec, dimension, metric, timeout, deletion_protection, vector_type, tags)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate_index\u001b[39m(\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    325\u001b[0m     name: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    332\u001b[0m     tags: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    333\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndexModel\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mspec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdimension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdimension\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeletion_protection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeletion_protection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvector_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvector_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/chatbot/lib/python3.10/site-packages/pinecone/utils/require_kwargs.py:14\u001b[0m, in \u001b[0;36mrequire_kwargs.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m     param_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(inspect\u001b[38;5;241m.\u001b[39msignature(func)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mkeys())[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# Skip self\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() requires keyword arguments. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=value\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mname\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mparam_names)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m     )\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/chatbot/lib/python3.10/site-packages/pinecone/db_control/resources/sync/index.py:81\u001b[0m, in \u001b[0;36mIndexResource.create\u001b[0;34m(self, name, spec, dimension, metric, timeout, deletion_protection, vector_type, tags)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;129m@require_kwargs\u001b[39m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m     tags: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     71\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m IndexModel:\n\u001b[1;32m     72\u001b[0m     req \u001b[38;5;241m=\u001b[39m PineconeDBControlRequestFactory\u001b[38;5;241m.\u001b[39mcreate_index_request(\n\u001b[1;32m     73\u001b[0m         name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m     74\u001b[0m         spec\u001b[38;5;241m=\u001b[39mspec,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m         tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[1;32m     80\u001b[0m     )\n\u001b[0;32m---> 81\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_index_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcreate_index_request\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m IndexModel(resp)\n",
      "File \u001b[0;32m~/miniconda3/envs/chatbot/lib/python3.10/site-packages/pinecone/openapi_support/endpoint.py:102\u001b[0m, in \u001b[0;36mEndpoint.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     92\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This method is invoked when endpoints are called\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m \n\u001b[1;32m    101\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/chatbot/lib/python3.10/site-packages/pinecone/core/openapi/db_control/api/manage_indexes_api.py:322\u001b[0m, in \u001b[0;36mManageIndexesApi.__init__.<locals>.__create_index\u001b[0;34m(self, create_index_request, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_openapi_kwargs(kwargs)\n\u001b[1;32m    321\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate_index_request\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m create_index_request\n\u001b[0;32m--> 322\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_with_http_info\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/chatbot/lib/python3.10/site-packages/pinecone/openapi_support/endpoint.py:134\u001b[0m, in \u001b[0;36mEndpoint.call_with_http_info\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m params \u001b[38;5;241m=\u001b[39m EndpointUtils\u001b[38;5;241m.\u001b[39mgather_params(\n\u001b[1;32m    125\u001b[0m     attribute_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattribute_map,\n\u001b[1;32m    126\u001b[0m     location_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocation_map,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    129\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m    130\u001b[0m )\n\u001b[1;32m    132\u001b[0m HeaderUtil\u001b[38;5;241m.\u001b[39mprepare_headers(headers_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders_map, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mendpoint_path\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttp_method\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheader_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbody\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mform\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfile\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msettings\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43masync_req\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43masync_req\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43masync_threadpool_executor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43masync_threadpool_executor\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_check_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_check_return_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_return_http_data_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_return_http_data_only\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_preload_content\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_request_timeout\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_formats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcollection_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/chatbot/lib/python3.10/site-packages/pinecone/openapi_support/api_client.py:306\u001b[0m, in \u001b[0;36mApiClient.call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, async_req, async_threadpool_executor, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthreadpool_executor\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__call_api,\n\u001b[1;32m    287\u001b[0m         resource_path,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    302\u001b[0m         _check_type,\n\u001b[1;32m    303\u001b[0m     )\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m async_req:\n\u001b[0;32m--> 306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__call_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresource_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheader_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauth_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_return_http_data_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollection_formats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_check_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool\u001b[38;5;241m.\u001b[39mapply_async(\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__call_api,\n\u001b[1;32m    327\u001b[0m     (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    344\u001b[0m     ),\n\u001b[1;32m    345\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/chatbot/lib/python3.10/site-packages/pinecone/openapi_support/api_client.py:182\u001b[0m, in \u001b[0;36mApiClient.__call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m PineconeApiException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m     e\u001b[38;5;241m.\u001b[39mbody \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mbody\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 182\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_response \u001b[38;5;241m=\u001b[39m response_data\n\u001b[1;32m    186\u001b[0m return_data \u001b[38;5;241m=\u001b[39m response_data\n",
      "File \u001b[0;32m~/miniconda3/envs/chatbot/lib/python3.10/site-packages/pinecone/openapi_support/api_client.py:170\u001b[0m, in \u001b[0;36mApiClient.__call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[1;32m    161\u001b[0m url \u001b[38;5;241m=\u001b[39m build_request_url(\n\u001b[1;32m    162\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m    163\u001b[0m     processed_path_params\u001b[38;5;241m=\u001b[39mpath_params_tuple,\n\u001b[1;32m    164\u001b[0m     resource_path\u001b[38;5;241m=\u001b[39mresource_path,\n\u001b[1;32m    165\u001b[0m     _host\u001b[38;5;241m=\u001b[39m_host,\n\u001b[1;32m    166\u001b[0m )\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# perform request and return response\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m     response_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocessed_query_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocessed_post_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m PineconeApiException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m     e\u001b[38;5;241m.\u001b[39mbody \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mbody\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/chatbot/lib/python3.10/site-packages/pinecone/openapi_support/api_client.py:386\u001b[0m, in \u001b[0;36mApiClient.request\u001b[0;34m(self, method, url, query_params, headers, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrest_client\u001b[38;5;241m.\u001b[39mOPTIONS(\n\u001b[1;32m    377\u001b[0m         url,\n\u001b[1;32m    378\u001b[0m         query_params\u001b[38;5;241m=\u001b[39mquery_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    383\u001b[0m         body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    384\u001b[0m     )\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPOST\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrest_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPOST\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPUT\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrest_client\u001b[38;5;241m.\u001b[39mPUT(\n\u001b[1;32m    397\u001b[0m         url,\n\u001b[1;32m    398\u001b[0m         query_params\u001b[38;5;241m=\u001b[39mquery_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    403\u001b[0m         body\u001b[38;5;241m=\u001b[39mbody,\n\u001b[1;32m    404\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/chatbot/lib/python3.10/site-packages/pinecone/openapi_support/rest_utils.py:146\u001b[0m, in \u001b[0;36mRestClientInterface.POST\u001b[0;34m(self, url, headers, query_params, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mPOST\u001b[39m(\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    138\u001b[0m     url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    144\u001b[0m     _request_timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    145\u001b[0m ):\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpost_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_preload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_preload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/chatbot/lib/python3.10/site-packages/pinecone/openapi_support/rest_urllib3.py:267\u001b[0m, in \u001b[0;36mUrllib3RestClient.request\u001b[0;34m(self, method, url, query_params, headers, body, post_params, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;66;03m# log response body\u001b[39;00m\n\u001b[1;32m    265\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse body: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, r\u001b[38;5;241m.\u001b[39mdata)\n\u001b[0;32m--> 267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mraise_exceptions_or_return\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/chatbot/lib/python3.10/site-packages/pinecone/openapi_support/rest_utils.py:49\u001b[0m, in \u001b[0;36mraise_exceptions_or_return\u001b[0;34m(r)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m500\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m599\u001b[39m:\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ServiceException(http_resp\u001b[38;5;241m=\u001b[39mr)\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PineconeApiException(http_resp\u001b[38;5;241m=\u001b[39mr)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "\u001b[0;31mPineconeApiException\u001b[0m: (400)\nReason: Bad Request\nHTTP response headers: HTTPHeaderDict({'content-type': 'text/plain; charset=utf-8', 'access-control-allow-origin': '*', 'vary': 'origin,access-control-request-method,access-control-request-headers', 'access-control-expose-headers': '*', 'x-pinecone-api-version': '2025-04', 'x-cloud-trace-context': '2921199a4e0f20351582548e6220f64b', 'date': 'Thu, 04 Sep 2025 04:20:28 GMT', 'server': 'Google Frontend', 'Content-Length': '125', 'Via': '1.1 google', 'Alt-Svc': 'h3=\":443\"; ma=2592000,h3-29=\":443\"; ma=2592000'})\nHTTP response body: {\"error\":{\"code\":\"INVALID_ARGUMENT\",\"message\":\"Name must consist of lower case alphanumeric characters or '-'\"},\"status\":400}\n"
     ]
    }
   ],
   "source": [
    "#pinecode index creation for pinecode and in deployment in render freed service\n",
    "from pinecone import ServerlessSpec\n",
    "index_name=\"data_scientist_chatbot\"\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )   \n",
    "index=pc.index(index_name)\n",
    "index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec19bb93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
